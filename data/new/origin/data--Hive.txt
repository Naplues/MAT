//  FIXME: probably this should also be integrated with isSame() logics 
//  Wait for t1 to block, just be sure. Not ideal... 
// HIVE-17328: not sure this is correct... I don't think is gets wrapped in UDFToInteger.... 
//  We expect this to never happen in practice. Can pool paths even have angled braces? 
//  TODO: This does not work correctly. None of the partitions is created, but the folder   for the first two is created. It is because in HiveMetaStore.add_partitions_core when   going through the partitions, the first two are already put and started in the thread   pool when the exception occurs in the third one. When the exception occurs, we go to   the finally part, but the map can be empty (it depends on the progress of the other   threads) so the folders won't be deleted.      Assert.assertFalse(metaStore.isPathExists(new Path(tableLocation + "/year=2016"))); 
//  TODO: remove this after stashing only rqd pieces from opconverter 
//  TODO: should we try to make a giant array for one cache call to avoid overhead? 
//  Forward the row to reducer as is.   Discard the row.   Vectorized - may forward the row, not sure yet. 
//  7. Handle any move session requests. The way move session works right now is   a) sessions get moved to destination pool if there is capacity in destination pool   b) if there is no capacity in destination pool, the session gets killed (since we cannot pause a query)   TODO: in future this the process of killing can be delayed until the point where a session is actually required.   We could consider delaying the move (when destination capacity is full) until there is claim in src pool.   May be change command to support ... DELAYED MOVE TO etl ... which will run under src cluster fraction as long 
//  Not thread-safe. 
// it would be better if AlreadyExistsException had an errorCode field.... 
//  TODO: this can probably be replaced with much less code via dynamic dispatch and/or templates. 
//  TODO Handle this properly 
//  TODO: Do we need maxLength checking? 
//  TODO: prewarm and update can probably be merged. 
//  @deprecated in favour of {@link HCatTable.#getTblProps()}. To be removed in Hive 0.16. 
//  TODO: disabling this test as tez publishes counters only after task completion which will cause write side counters   to be not validated correctly (DAG will be completed before validation)    @Test(timeout = 60000) 
//  TODO: might want to increase the default batch size. 1024 is viable; MS gets OOM if too high. 
//  TODO: remove when averageTypeValueSize method RelMdSize 
//  We store all caches in variables to change the main one based on config.   This is not thread safe between different split generations (and wasn't anyway). 
//  $x/$user/appcache/$appId/${dagId}/output/$mapId   TODO: Once Shuffle is out of NM, this can use MR APIs to convert   between App and Job 
// todo: does this need the finalDestination? 
//  todo: add LIMIT 1 instead of count - should be more efficient 
/*      * TODO: client.executeStatement do not support listing resources command     * (beeline> list jar)      */
//  Note: scheduler will call this based on lack of sources at schedule time and set this         to true... there's no easy way to work around this. Need better classes 
//  TODO: will this also fix windowing? try 
//  but let's make it a little bit more explicit. 
//  TODO: This should inherit from VolcanoCost and should just override isLE   method. 
//  TODO: 1) check for duplicates 2) We assume in clause values to be   present in NDV which may not be correct (Range check can find it) 3) We   assume values in NDV set is uniformly distributed over col values   (account for skewness - histogram). 
//  TODO: if we didn't care about the column order, we could switch join sides here 
//  TODO: we could remember if it's unsupported and stop sending calls; although, it might         be a bad idea for HS2+standalone metastore that could be updated with support.         Maybe we should just remember this for some time. 
//  @deprecated in favour of {@link HCatTable.#location(String)}. To be removed in Hive 0.16. 
//  TODO: why is this like that? 
/*  TODO Escape handling may be changed by a follow on.     * The largest issue is ; which are treated as statement     * terminators for the cli. Once the cli is fixed this     * code should be re-investigated      */
//  Delete data?   Fail if table doesn't exist?   Need results back? 
//  TODO: This does not work correctly. None of the partitions is created, but the folder   for the first two is created. It is because in HiveMetaStore.add_partitions_core when   going through the partitions, the first two are already put and started in the thread   pool when the exception occurs in the third one.   When the exception occurs, we go to the finally part, but the map can be empty   (it depends on the progress of the other threads) so the folders won't be deleted.   Assert.assertTrue(metaStore.isPathExists(new Path(partition1.getSd().getLocation())));   Assert.assertTrue(metaStore.isPathExists(new Path(partition2.getSd().getLocation())));   Assert.assertTrue(metaStore.isPathExists(new Path(partition3.getSd().getLocation()))); 
//  Note: we assume that this isn't an already malformed query;         we don't check for that here - it will fail later anyway. 
//  TODO - not clear if we should cache these or not.  For now, don't bother 
//  TODO: this needs to be removed; see TestReplicationScenarios* comments. 
// BUG: This will not work in remote mode - HIVE-5153 
//  TODO Change this method to make the output easier to parse (parse programmatically) 
//  TODO: remove? 
//  Wouldn't it make more sense to return the first element of the list returned by the   previous call? 
//  a better logic would be to find the alias 
//  we need to enforce the size here even the types are the same 
//  This method may not be safe as it can throw an NPE if a key or value is null. 
//  TODO: this is an ugly hack because Tez plugin isolation does not make sense for LLAP plugins.         We are going to register a thread-local here for now, so that the scheduler, initializing         in the same thread after the communicator, will pick up. Or the other way around. 
//  TODO MS-SPLIT uncomment once we move EventMessage over 
//  Follow hive's rules for type inference as oppose to Calcite's   for return type.  TODO: Perhaps we should do this for all functions, not just +,- 
//  TODO: [HIVE-6289] while getting stats from metastore, we currently only get one col at         a time; this could be improved - get all necessary columns in advance, then use local.   TODO: [HIVE-6292] aggregations could be done directly in metastore. Hive over MySQL! 
//  TODO: see planIndexReading; this is not needed here. 
//  TODO: why is it stored in both table and dpCtx? 
//  TODO: Use threadpool for more concurrency?   TODO: check/set all files, or only directories  
//  TODO HIVE-15865 Handle additional reasons like OS launch failed 
//  TODO: 1) handle Agg Func Name translation 2) is it correct to add func 
//  TODO: Both of these are TException, why do we need these separate clauses? 
//  TODO: if two HS2s start at exactly the same time, which could happen during a coordinated         restart, they could start generating the same IDs. Should we store the startTime 
//  TODO: given the specific data and lookups, perhaps the nested thing should not be a map         In fact, CSLM has slow single-threaded operation, and one file is probably often read         by just one (or few) threads, so a much more simple DS with locking might be better.         Let's use CSLM for now, since it's available. 
//  TODO Ideally this should be done independent of whether mr is setup or not. 
//  This is not modeled as a @Before, because it needs to be parameterized per-test.   If there is a better way to do this, we should do it. 
//  todo HIVE-5269 
//  Make sure null-valued ConfVar properties do not override the Hadoop Configuration   NOTE: Comment out the following test case for now until a better way to test is found,   as this test case cannot be reliably tested. The reason for this is that Hive does   overwrite fs.default.name in HiveConf if the property is set in system properties.   checkHadoopConf(ConfVars.HADOOPFS.varname, "core-site.xml");   checkConfVar(ConfVars.HADOOPFS, null);   checkHiveConf(ConfVars.HADOOPFS.varname, "core-site.xml"); 
//  @deprecated in favour of {@link HCatPartition.#getDatabaseName()}. To be removed in Hive 0.16. 
// TODO: due to value 101 this probably should throw an exception 
//  @deprecated in favour of {@link HCatTable.#getSortCols()}. To be removed in Hive 0.16. 
//  Hmm.. not good,   the only type expected here is STRUCT, which maps to HCatRecord   - anything else is an error. Return null as the inspector. 
//  TODO: should these rather be arrays? 
//  TODO Change all this to be based on a regular interface instead of relying on the Proto service - Exception signatures cannot be controlled without this for the moment. 
//  PolicyChangeListener will be implemented later 
//  TODO: do we actually need this reader? the caller just extracts child readers. 
//  TODO HIVE-14042. What is mergeWork, and why is it not part of the regular operator chain.   The mergeMapOp.initialize call further down can block, and will not receive information   about an abort request. 
//  TODO HIVE-15163. Handle cases where nodes go down and come back on the same port. Historic information 
//  TODO: most protocol exceptions are probably unrecoverable... throw? 
//  TODO: do we also need to remove the MapJoin from the list of RS's children? 
//  TODO: why is there a TezSession in MR ExecDriver? 
//  Replace INSERT OVERWRITE by MERGE equivalent rewriting.   Here we need to do this complex AST rewriting that generates the same plan   that a MERGE clause would generate because CBO does not support MERGE yet.   TODO: Support MERGE as first class member in CBO to simplify this logic. 
//  TODO: Current implementation of replication will result in DROP_PARTITION under replication   scope being called per-partition instead of multiple partitions. However, to be robust, we   must still handle the case of multiple partitions in case this assumption changes in the   future. However, if this assumption changes, we will not be very performant if we fetch   each partition one-by-one, and then decide on inspection whether or not this is a candidate   for dropping. Thus, we need a way to push this filter (replicationSpec.allowEventReplacementInto)   to the  metastore to allow it to do drop a partition or not, depending on a Predicate on the   parameter key values. 
//  TODO: should we pass curr instead of null? 
//  Really not sure if this should go here.  Will have   to see how the storage mechanism evolves. 
//  TODO Force fs to file://, setup staging dir?        conf.set("fs.defaultFS", "file:///");        conf.set(TezConfiguration.TEZ_AM_STAGING_DIR, "/tmp"); 
// todo: why not just use getRootDir()? 
//  TODO: lossy conversion! 
//  TODO: remove the copy after ORC-158 and ORC-197 
//  TODO: If the DB name doesn't match with the metadata from dump, then need to rewrite the original and expanded   texts using new DB name. Currently it refers to the source database name. 
//  TODO: remove this constructor 
//  TODO: add the ability to extractFileTail to read from multiple buffers? 
//  TODO : instead of simply restricting by message format, we should eventually   move to a jdbc-driver-stype registering of message format, and picking message   factory per event to decode. For now, however, since all messages have the   same factory, restricting by message format is effectively a guard against   older leftover data that would cause us problems. 
//  TODO: When hive moves to java8, make updateTimezone() as default method in 
//  Not strictly necessary, noone will look at it. 
//  2**exponent part is scaling down while 10**scale is scaling up.   Now it's tricky.   unscaledValue = significand * 10**scale / 2**twoScaleDown 
//  TODO CAT - I am fairly certain that most calls to this are in error.  This should only be   used when the database location is unset, which should never happen except when a   new database is being created.  Once I have confirmation of this change calls of this to   getDatabasePath(), since it does the right thing.  Also, merge this with   determineDatabasePath() as it duplicates much of the logic. 
//  TODO: Should this be the concern of the mutator? 
//  TODO: Provide support for reporting errors   This should never happen as server always returns a valid status on success 
/*  TODO: ideally, when the splits UDF is made a proper API, coordinator should not   *        be managed as a global. HS2 should create it and then pass it around.  */
//  TODO: convert sqlState, etc. 
//  TODO Include EXTERNAL_PREEMPTION in this list? 
//  To workaround AvroUTF8 
//  TODO needs to go in InitializeInput? as part of InputJobInfo 
//  Note that this logic may drop some of the tables of the database   even if the drop database fail for any reason   TODO: Fix this 
//  Some walkers extending DefaultGraphWalker e.g. ForwardWalker   do not use opQueue and rely uniquely in the toWalk structure,   thus we store the results produced by the dispatcher here   TODO: rewriting the logic of those walkers to use opQueue 
//  This is a corner case where we have an extract of time unit like day/month pushed as Extraction Fn  @TODO The best way to fix this is to add explicit output Druid types to Calcite Extraction Functions impls 
//  Workaround for HADOOP-12659 - remove when Hadoop 2.7.X is no longer supported. 
//  We could pass in the number of nodes that we expect instead of -1.   Also, a single concurrent request per node is currently hardcoded. 
//  TODO: Strangely the default parametrization is to ignore missing tables 
//  FIXME : should clean up TEST_PATH, but not doing it now, for debugging's sake 
//  This will throw error if we close pout early. 
//  It is not a very clean way, and should be modified later - due to   compatibility reasons,   user sees the results as json for custom scripts and has no way for   specifying that.   Right now, it is hard-coded in the code 
//  TODO This test passes fine locally but fails on Linux, not sure why 
//  TODO: SeekableInputStream.readFully eventually calls a Hadoop method that used to be         buggy in 2.7 and also anyway just does a copy for a direct buffer. Do a copy here.   ((SeekableInputStream)stream).readFully(bb); 
//  TODO: 1) Expand to other functions as needed 2) What about types other than primitive. 
//  Set footer cache for current split generation. See field comment - not thread safe. 
//  TODO: the below seem like they should just be combined into partitionDesc 
//  CallableWithNdc inherits from NDC only when call() is invoked. CallableWithNdc has to   extended to provide access to its ndcStack that is cloned during creation. Until, then   we will use reflection to access the private field.   FIXME: HIVE-14243 follow to remove this reflection 
//  TODO: this is brittle. Who said everyone has to upgrade using upgrade process? 
/*    * Check whether a task can run to completion or may end up blocking on it's sources.   * This currently happens via looking up source state.   * TODO: Eventually, this should lookup the Hive Processor to figure out whether   * it's reached a state where it can finish - especially in cases of failures   * after data has been fetched.   *   * @return true if the task can finish, false otherwise    */
//  UNDONE: How to look for all NULLs in a multi-key?????  Let nulls through for now. 
//  Note: this can still conflict with parallel transactions. We do not currently handle         parallel changes from two admins (by design :(). 
//  TODO: time good enough for now - we'll likely improve this.   We may also work in something the equivalent of pid, thrid and move to nanos to ensure   uniqueness. 
//  FIXME: somehow place pointers that re-execution compilation have failed; the query have been successfully compiled before? 
//  UNDONE: Haven't finished isRepeated 
//  hack, instead figure out a way to get the db paths 
// TODO: support complex types   for complex type we simply return 0 
//  TODO: the control flow for this needs to be defined. Hive is supposed to be thread-local. 
// todo: should be at the top of the file... 
//  Get rid of TOK_SELEXPR 
//  Clear the work map after build. TODO: remove caching instead? 
//  DO_NOT_UPDATE_STATS is supposed to be a transient parameter that is only passed via RPC   We want to avoid this property from being persistent.     NOTE: If this property *is* set as table property we will remove it which is incorrect but   we can't distinguish between these two cases     This problem was introduced by HIVE-10228. A better approach would be to pass the property 
//  This is using the payload from the RootVertexInitializer corresponding   to InputName. Ideally it should be using it's own configuration class -   but that 
//  like HiveHBaseTableInputFormat cannot be used with this (todo) 
//  @deprecated in favour of {@link HCatTable.#collectionItemsTerminatedBy()}. To be removed in Hive 0.16. 
//  TODO: Once HBASE-11163 is completed, use that API, or switch to   using mapreduce version of the APIs. rather than mapred   Copied from HBase's TableMapreduceUtil since it is not public API 
//  TODO: Fetch partitions in batches?   TODO: Threadpool to process partitions? 
//  TODO : simple wrap & rethrow for now, clean up with error codes 
// todo: add method to only get current i.e. skip history - more efficient 
/*  * Simple wrapper of object with ObjectInspector. *  * TODO: we need to redefine the hashCode and equals methods, so that it can be * put into a HashMap as a key. *  * This class also serves as a facility for a function that returns both an * object and an ObjectInspector.  */
//  The ordering of types here is used to determine which numeric types   are common/convertible to one another. Probably better to rely on the   ordering explicitly defined here than to assume that the enum values   that were arbitrarily assigned in PrimitiveCategory work for our purposes. 
//  Something else is wrong 
//  TODO: Comments in RexShuttle.visitCall() mention other   types in this category. Need to resolve those together   and preferably in the base class RexShuttle. 
//  TODO: Verify if this is needed (Why can't it be always null/empty 
//  is the outer join that we saw most recently is a right outer join? 
//  TODO: this relies on HDFS not changing the format; we assume if we could get inode ID, this         is still going to work. Otherwise, file IDs can be turned off. Later, we should use         as public utility method in HDFS to obtain the inode-based path. 
//  TODO: should it rather do a prefix? 
/*    * TODO This method is temporary. Ideally Hive should only need to pass to Tez the amount of memory   *      it requires to do the map join, and Tez should take care of figuring out how much to allocate   * Adjust the percentage of memory to be reserved for the processor from Tez   * based on the actual requested memory by the Map Join, i.e. HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD   * @return the adjusted percentage    */
//  A hack to verify that authorization check passed. Exception can be thrown be cause   the functions are not being called with valid params.   verify that exception has come from ObjectStore code, which means that the 
//  TODO: should this use getUserFromAuthenticator? 
// TODO: Move this to calcite 
//  TODO: add this to metadatareader in ORC - SI => metadata buffer, not just metadata. 
//  TODO: This should come from type system; Currently there is no definition 
//  TODO At the moment there's no way of knowing whether a query is running or not.   A race is possible between dagComplete and registerFragment - where the registerFragment   is processed after a dagCompletes.   May need to keep track of completed dags for a certain time duration to avoid this.   Alternately - send in an explicit dag start message before any other message is processed.   Multiple threads communicating from a single AM gets in the way of this. 
/*     Should we allow writing to non-transactional tables in an explicit transaction?  The user may    issue ROLLBACK but these tables won't rollback.    Can do this by checking ReadEntity/WriteEntity to determine whether it's reading/writing    any non acid and raise an appropriate error    * Driver.acidSinks and Driver.transactionalInQuery can be used if any acid is in the query */
//  TODO: implement? 
// todo: what is this checking???? 
//  I think this is wrong, the drop table statement should come on the table topic not the   DB topic - Alan. 
//  TODO: rather, Tez sessions should not depend on SessionState. 
//  todo: support tez/vectorization 
//  TODO: read this somewhere useful, like the task scheduler 
//  TODO Add support for serialization of values here 
//  TODO: Convert genIncludedColumns and setSearchArgument to use TypeDescription. 
//  TODO: when this code is a little less hot, change most logs to debug.   We will determine what to do under lock and then do stuff outside of the lock.   The approach is state-based. We consider the task to have a duck when we have decided to   give it one; the sends below merely fix the discrepancy with the actual state. We may add the   ability to wait for LLAPs to positively ack the revokes in future.   The "procedural" approach requires that we track the ducks traveling on network,   concurrent terminations, etc. So, while more precise it's much more complex. 
//  TODO: why doesn't this use context? 
//  Note: this is redundant with types 
//  The following 2 lines are exactly what MySQL does TODO: why do we do this? 
//  temporary variable for testing. This is added just to turn off this feature in case of a bug in   deployment. It has not been documented in hive-default.xml intentionally, this should be removed 
//  TODO need session handle 
//  This is kind of hacky - the read entity contains the old table, whereas   the write entity   contains the new table. This is needed for rename - both the old and the   new table names are   passed 
//  UNDONE: For now, don't add more small keys... 
//  TODO: can this ever happen? 
//  TODO: there's a potential problem here if some table uses external schema like Avro,         with a very large type name. It seems like the view does not derive the SerDe from         the table, so it won't be able to just get the type from the deserializer like the         table does; we won't be able to properly store the type in the RDBMS metastore. 
//  TODO: this should depends on input format and be in a map, or something. 
//  TODO: we can actually consider storing ALL the delta encoded row offsets - not a lot of         overhead compared to the data itself, and with row offsets, we could use columnar         blocks for inconsistent splits. We are not optimizing for inconsistent splits for now. 
//  TODO: should this just use physical IDs? 
//  TODO: wtf?!! why is this in this method? This has nothing to do with anything. 
//  This class isn't used and I suspect does totally the wrong thing.  It's only here so that I   can provide some output format to the tables and partitions I create.  I actually write to 
//  TODO: does arg need type cast? 
// what??!! 
//  TODO not sure this is the right exception 
//  we need to convert the Hive type to the SQL type name   TODO: this would be better handled in an enum 
//  TODO: NPE should not be thrown 
//  CONSIDER: Allocate a larger initial size. 
//  No good way to find out (may even have no app). 
//  Hardcode SASL here. ZKDTSM only supports none or sasl and we never want none. 
//  We support List<Object>, Set<Object> and Object[]   so we have to do differently. 
//  If wh is still null after just having initialized it, bail out - something's very wrong. 
//  TODO Convert this to an Assert.fail once HIVE-14682 is fixed 
//  TODO: we should be able to enable caches separately 
//  For dynamic partitioned hash join, the big table will also be coming from a ReduceSinkOperator   Check for this condition.   TODO: use indexOf(), or parentRS.getTag()? 
//  TODO:   Following HiveSubQueryFinder has been copied from RexUtil::SubQueryFinder   since there is BUG in there (CALCITE-1726).   Once CALCITE-1726 is fixed we should get rid of the following code 
//  TODO: Extend rule so it can be applied for these cases. 
//  TODO: Support case DATE: 
//  TODO figure out a better way to set repeat for Binary type 
//  TODO: If we are ok with breaking compatibility of existing 3rd party StorageHandlers,   this method could be moved to the HiveStorageHandler interface. 
//  TODO: This global lock may not be necessary as all concurrent methods in ICacheableMetaStoreClient   are synchronized. 
// todo: FileSystem#setPermission() - should this make sure to set 777 on jobs/ ? 
/*  * Builder for relational expressions. * TODO: *  Note that this is copied from Calcite's RelBulder *  because CALCITE-1493 hasn't been fixed yet *  This should be deleted and replaced with RelBuilder in SubqueryRemoveRule *  once CALCITE-1493 is fixed. *  EDIT: Although CALCITE-1493 has been fixed and released but HIVE now has special handling *    in join (it gets a flag to see if semi join is to be created or not). So we still can not *    replace this with Calcite's RelBuilder * * <p>{@code RelBuilder} does not make possible anything that you could not * also accomplish by calling the factory methods of the particular relational * expression. But it makes common tasks more straightforward and concise. * * <p>{@code RelBuilder} uses factories to create relational expressions. * By default, it uses the default factories, which create logical relational * expressions ({@link org.apache.calcite.rel.logical.LogicalFilter}, * {@link org.apache.calcite.rel.logical.LogicalProject} and so forth). * But you could override those factories so that, say, {@code filter} creates * instead a {@code HiveFilter}. * * <p>It is not thread-safe.  */
//  TODO: Setting autocommit should not generate an exception as long as it is set to false   beeLine.autocommitStatus(getConnection()); 
//  TODO: precision and scale would be practically invalid for string conversion (38,38) 
//  TODO: Should this really default to FETCH_NEXT? 
//  TODO: Not sure about the use of this. Should we instead use workerIdentity as sessionId? 
//  @deprecated in favour of {@link HCatTable.#tableType(HCatTable.Type)}. To be removed in Hive 0.16. 
//  TODO: trace ranges here? Between data cache and incomplete cb cache 
//  FIXME: retain old error; or create a new one? 
//  we don't remove the work from the sparkWork here. The removal is done later. 
//  TODO: need to move from Python to Java for the rest of the script. 
//  check if input pruning is possible   TODO: this code is buggy - it relies on having one file per bucket; no MM support (by design). 
//  todo: strictly speaking you can commit an empty txn, thus 2nd conjunct is wrong but   only   possible for for multi-stmt txns 
//  View DDL   "alter view add partition" does not work because of the nature of implementation   of the DDL in hive. Hive will internally invoke another Driver on the select statement,   and HCat does not let "select" statement through. I cannot find a way to get around it   without modifying hive code. So just leave it unsupported. 
// todo: to test these need to link against 3.x libs - maven profiles?  runStatementOnDriver("create table TFlat (a int, b int) stored as orc tblproperties('transactional'='false')");  runStatementOnDriver("create table TFlatText (a int, b int) stored as textfile tblproperties('transactional'='false')"); 
//  NOTE : This is hacky, and this section of code is fragile depending on DN code varnames   so it's likely to stop working at some time in the future, especially if we upgrade DN   versions, so we actively need to find a better way to make sure the leak doesn't happen   instead of just clearing out the cache after every call. 
//  TODO: should never happen? 
//  UNDONE: Don't know why HIVE-12894 causes this to return 0?   assertEquals(0.33, reader.getProgress(), 0.01); 
//  @deprecated in favour of {@link HCatTable.#getLocation()}. To be removed in Hive 0.16. 
//  we do not support Windows, we will revisit this if we really need it for windows. 
//  Pass an empty list as no columns will be written to the file.   TODO I should be able to make this work for update 
//  TODO: throw an exception? 
//  TODO: move to DynamicSerDe when it's ready 
//  TODO: Replace with direct call to ProgressHelper, when reliably available. 
//  TODO: this is the only place that uses keepTmpDir. Why? 
//  TODO: use serdeConstants.COLLECTION_DELIM when the typo is fixed 
//  Indicates whether a node had a recent communication failure.   This is primarily for tracking and logging purposes for the moment.   TODO At some point, treat task rejection and communication failures differently. 
//  TODO: move other protocols to use this too. 
//  TODO Move the following 2 properties out of Configuration to a constant. 
/*  * Creates size estimators for java objects. The estimators attempt to do most of the reflection * work at initialization time, and also take some shortcuts, to minimize the amount of work done * during the actual estimation. * TODO: clean up  */
//  TODO: move to a base class? 
//  Since the MapJoin has had all of its other parents removed at this point,   it would be bad here if processReduceSinkToHashJoin() tries to do anything 
// todo: should this be done for MM?  is it ok to use CombineHiveInputFormat with MM 
//  todo this should be changed to be evaluated lazily, especially for single segment case 
//  TODO Disable blacklisting in Tez when using LLAP, until this is properly supported.   Blacklisting can cause containers to move to a terminating state, which can cause attempt to be marked as failed.   This becomes problematic when we set #allowedFailures to 0   TODO HIVE-13484 What happens when we try scheduling a task on a node that Tez at this point thinks is blacklisted. 
// todo should this check be in conformToAcid()? 
//  TODO: Split count is not same as no of buckets 
// TODO: Cols that come through PTF should it retain (VirtualColumness)? 
//  TODO : verify if any quoting is needed for keys 
// todo: this should check that the job actually completed and likely use completion time 
//  TODO: move this to logicalEquals 
/*  * TODO:<br> * 1. Could we use combined RR instead of list of RR ?<br> * 2. Use Column Processing from TypeCheckProcFactory<br> * 3. Why not use GB expr ?  */
//  TODO: Temporary for debugging. Doesn't interfere with MTT failures (unlike LOG.debug). 
//  TODO Deprecation reason does not seem to reflect in the config ?   The ordering is important in case of keys which are also deprecated.   Unset will unset the deprecated keys and all its variants. 
//  GrpSet Col already part of input RS   TODO: Can't we just copy the ExprNodeDEsc from input (Do we need to   explicitly set table alias to null & VC to false 
// todo: DataOperationType is set conservatively here, we'd really want to distinguish update/delete  and insert/select and if resource (that is written to) is ACID or not 
//  TODO [MM gap]: CTAS may currently be broken. It used to work. See the old code, and why isCtas isn't used? 
// todo: add partitioned table that needs conversion to MM/Acid 
//  TODO: Do we really need all this nonsense? 
//  This is bad, but we have to sort the keys of the maps in order   to be commutative. 
// @TODO this seems to be the same as org.apache.hadoop.hive.ql.parse.CalcitePlanner.TableType.DRUID do we really need both 
//  TODO: NPE should not be thrown. 
/*  * Tests for the worker thread and its MR jobs. * todo: most delta files in this test suite use txn id range, i.e. [N,N+M] * That means that they all look like they were created by compaction or by streaming api. * Delta files created by SQL should have [N,N] range (and a suffix in v1.3 and later) * Need to change some of these to have better test coverage.  */
//  not good if we reach here, this was initialized at setMetaStoreHandler() time.   this means handler.getWh() is returning null. Error out. 
/*  * The processor context for partition pruner. This contains the table alias * that is being currently processed. * TODO: this class may be not useful.  */
//  @deprecated in favour of {@link HCatTable.#nullDefinedAs()}. To be removed in Hive 0.16. 
//  TODO: enable this for production debug, switching between two small buffers?  new CasLog(); 
/*  * A helper class to isolate newer HBase features from users running against older versions of * HBase that don't provide those features. * * TODO: remove this class when it's okay to drop support for earlier version of HBase.  */
//  NOTE: For Uniform Hash or no buckets/partitions, when the key is empty, we will use the VectorReduceSinkEmptyKeyOperator instead. 
//  TODO: verify if this is needed 
//  TODO: this class is completely unnecessary... 1-on-1 mapping with parent. 
/*    * If moving across different FileSystems or differnent encryption zone, need to do a File copy instead of rename.   * TODO- consider if need to do this for different file authority.   * @throws HiveException    */
//  TODO: support propagation for partitioning/ordering in windowing 
//  named columns join   TODO: we can also do the same for semi join but it seems that other   DBMS does not support it yet. 
/*  * Helper class that generates SQL queries with syntax specific to target DB * todo: why throw MetaException?  */
//  We can continue   TODO: Need to check that this is the same MV that we are rebuilding 
//  Guess if CommonJoinResolver will work. If CommonJoinResolver may   convert a join operation, correlation optimizer will not merge that join.   TODO: If hive.auto.convert.join.noconditionaltask=true, for a JoinOperator   that has both intermediate tables and query input tables as input tables,   we should be able to guess if this JoinOperator will be converted to a MapJoin   based on hive.auto.convert.join.noconditionaltask.size. 
//  TODO: could we log in from ticket cache instead? no good method on UGI right now. 
//  TODO: support for binary spec? presumably we'd parse it somewhere earlier 
//  redundant TODO: callers of this often get part_vals out of name for no reason... 
//  TODO MS-SPLIT for now, keep a copy of HiveConf around as we need to call other methods with   it. This should be changed to Configuration once everything that this calls that requires   HiveConf is moved to the standalone metastore. 
/*    * Closes the client releasing any {@link IMetaStoreClient meta store} connections held. Does not notify any open   * transactions (TODO: perhaps it should?)    */
//  TODO: will this work? 
//  We don't have the entire part; copy both whatever we intended to cache, and the rest,   to an allocated buffer. We could try to optimize a bit if we have contiguous buffers   with gaps, but it's probably not needed. 
/*      * TODO: Hack fix until HIVE-5848 is addressed. non-exact type shouldn't be promoted     * to exact type, as FunctionRegistry.getCommonClass() might do. This corrects     * that.      */
//  FIXME: old implementation returned null; exception maybe? 
//  Optimize the scenario when there are no grouping keys - only 1 reducer is needed 
//  TODO: is Decimal an exact numeric or approximate numeric? 
//  TODO: why doesn't this use one of the existing options implementations?! 
//  Ensure there's no threadlocal. We don't expect one.   We don't ever want to create key paths with world visibility. Why is that even an option?!! 
/*    * todo: when job is complete, should print the msgCount table to log     */
//  TODO explain should use a FetchTask for reading 
//  This detail not desired. 
//  TODO: use the other HdfsUtils here 
//  First, check if the registry has been updated since the error, and skip the error if   we have received new, valid registry info (TODO: externally, add a grace period for this?). 
//  TODO: calculate from cached values. 
//  Array size not big enough? 
//  UNDONE: Needed to longTest? 
//  TODO Reduce the number of lookups that happen here. This shouldn't go to HDFS for each call. 
//  No data to read for this stripe. Check if we have some included index-only columns.   TODO: there may be a bug here. Could there be partial RG filtering on index-only column? 
//  FIXME. Do the right thing Luke. 
//  TODO make it so I can randomize the column order 
//  TODO: right now we treat each slice as a stripe with a single RG and never bother         with indexes. In phase 4, we need to add indexing and filtering. 
//  TODO: a pattern from Curator. Better error handling? 
//  IO thread pool. Listening is used for unhandled errors for now (TODO: remove?) 
//  Now, we need to look for any values that the user set that MetastoreConf doesn't know about.   TODO Commenting this out for now, as it breaks because the conf values aren't getting properly   interpolated in case of variables.  See HIVE-17788. 
//  FIXME: including this in the signature will almost certenly differ even if the operator is doing the same   there might be conflicting usages of logicalCompare? 
//  MultiMRInput may not. Fix once TEZ-3302 is resolved. 
//  Preempt only if there are no pending preemptions on the same host   When the premption registers, the request at the highest priority will be given the slot,   even if the initial preemption was caused by some other task.   TODO Maybe register which task the preemption was for, to avoid a bad non-local allocation. 
//  TODO: this is wrong; this test sets up dummy txn manager and so it cannot create ACID tables.         This used to work by accident, now this works due a test flag. The test needs to be fixed.         Also applies for a couple more tests. 
//  TODO: why is this needed? we could just save the host and port? 
//  TODO There needs to be a mechanism to figure out different attempts for the same task. Delays   could potentially be changed based on this. 
// it's not wrong to take all delete events for bucketed tables but it's more efficient  to only take those that belong to the 'bucket' assuming we trust the file name  un-bucketed table - get all files 
/*        * Determine an *initial* input vector expression.       *       * Note: we may have to convert it later from DECIMAL_64 to regular decimal.        */
//  Note - this is a little bit confusing; the special treatment of stripe-level buffers   is because we run the ColumnStreamData refcount one ahead (as specified above). It   may look like this would release the buffers too many times (one release from the   consumer, one from releaseInitialRefcounts below, and one here); however, this is   merely handling a special case where all the batches that are sharing the stripe-   level stream have been processed before we got here; they have all decRef-ed the CSD,   but have not released the buffers because of that extra refCount. So, this is   essentially the "consumer" refcount being released here. 
//  this doesn't always work, since some JDBC drivers (e.g.,   Oracle's) return a blank string from getTableName. 
//  TODO: Can this be moved out of the main callback path 
// TODO: add more expected test result here 
//  TODO MS-SPLIT - for now we have construct this by reflection because IMetaStoreClient   can't be   moved until after HiveMetaStore is moved, which can't be moved until this is moved. 
// todo: need a test where we actually have more than 1 file 
//  Here comes the ugly part... 
//  To be removed in Hive 0.16. 
//  OPTIMIZATION for later. 
//  Is there a way to provide char length here?   It might actually be ok as long as there is an object inspector (with char length)   receiving this value. 
//  This command exists solely to output this message. TODO: can we do it w/o an error? 
//  TODO: Add support for AND clauses under OR clauses   first-cut takes a known minimal tree and no others.   $expr = (a=1)           (a=1 or a=2)           (a in (1,2))           ($expr and *) 
//  TODO: Set this up as a tree, instead of a flat list. 
/*  * Operator factory for predicate pushdown processing of operator graph Each * operator determines the pushdown predicates by walking the expression tree. * Each operator merges its own pushdown predicates with those of its children * Finally the TableScan operator gathers all the predicates and inserts a * filter operator after itself. TODO: Further optimizations 1) Multi-insert * case 2) Create a filter operator for those predicates that couldn't be pushed * to the previous operators in the data flow 3) Merge multiple sequential * filter predicates into so that plans are more readable 4) Remove predicates * from filter operators that have been pushed. Currently these pushed * predicates are evaluated twice.  */
//  HIVE-12244 call currently ineffective 
//  Note: no location check; the buffer is always locked for move here. 
//  TODO MS-SPLIT For now if we cannot load the default PartitionExpressionForMetastore   class (since it's from ql) load the DefaultPartitionExpressionProxy, which just throws   UnsupportedOperationExceptions.  This allows existing Hive instances to work but also   allows us to instantiate the metastore stand alone for testing.  Not sure if this is   the best long term solution. 
//  TODO: this is very brittle given that Hive supports nested directories in the tables.         The caller should pass a flag explicitly telling us if the directories in the         input are data, or parent of data. For now, retain this for backward compat. 
//  TODO: these appear to always be called under write lock. Do they need sync? 
//  TODO: we could perhaps reuse the same directory for HiveResources? 
/*    * If there's a mismatch between static and object name, or a mismatch between   * vector and non-vector operator name, the optimizer doens't work correctly.    */
//  Not really sure how to refer to this (or if we can).   TODO: We could find a different from branch for the union, that might have an alias?         Or we could add an alias here to refer to, but that might break other branches. 
//  TODO : instantiating FS objects are generally costly. Refactor 
//  TODO: when PB is upgraded to 2.6, newInstance(ByteBuffer) method should be used here. 
/*  All the code paths below propagate nulls even if neither arg2 nor arg3     * have nulls. This is to reduce the number of code paths and shorten the     * code, at the expense of maybe doing unnecessary work if neither input     * has nulls. This could be improved in the future by expanding the number     * of code paths.      */
//  @deprecated in favour of {@link HCatPartition.#getPartitionKeyValMap()}. To be removed in Hive 0.16. 
// TODO: Remove this once Calcite FilterProjectTransposeRule can take rule operand 
//  TODO: we could fall back to trying one by one and only ignore the failed ones. 
//  @deprecated in favour of {@link HCatTable.#getStorageHandler()}. To be removed in Hive 0.16. 
//  First try temp table   TODO CAT - I think the right thing here is to always put temp tables in the current   catalog.  But we don't yet have a notion of current catalog, so we'll have to hold on   until we do. 
// TODO: set other Table properties as needed 
//  Don't pass in the pool set - not thread safe; if the user is trying to force us to   use a non-existent pool, we want to fail anyway. We will fail later during get. 
// TODO: Since OperationLog is moved to package o.a.h.h.ql.session,   we may add a Enum there and map FetchOrientation to it. 
//  TODO: ideally this only needs to be called if the result   type will also change. However, since that requires   support from type inference rules to tell whether a rule   decides return type based on input types, for now all   operators will be recreated with new type if any operand   changed, unless the operator has "built-in" type. 
//  TODO: ideally we should store shortened representation of only the necessary fields         in HBase; it will probably require custom SARG application code. 
//  This is a massive hack.  The compactor threads have to access packages in ql (such as   AcidInputFormat).  ql depends on metastore so we can't directly access those.  To deal   with this the compactor thread classes have been put in ql and they are instantiated here   dyanmically.  This is not ideal but it avoids a massive refactoring of Hive packages.     Wrap the start of the threads in a catch Throwable loop so that any failures   don't doom the rest of the metastore. 
//  FIXME: there were 2 afterclass methods...i guess this is the right order...maybe not 
//  TODO HIVE-13483 Get all of these properties from the registry. This will need to take care of different instances   publishing potentially different values when we support changing configurations dynamically. 
//  TODO: replace with withTimeout after we get the relevant guava upgrade. 
/*    * TODO: need to turn on rules that's commented out and add more if necessary.    */
/* todo: handle renaming files somewhere */
//  @deprecated in favour of {@link HCatTable.#getBucketCols()}. To be removed in Hive 0.16. 
//  TODO: this doesn't include superclass. 
//  FIXME: move this to ColStat related part 
//  TODO: Not sure that this is the correct behavior. It doesn't make sense to create the   partition without column info. This should be investigated later. 
//  Note : preservePartitionSpecs=true implies inheritTableSpecs=false but   but preservePartitionSpecs=false(default) here is not sufficient enough   info to set inheritTableSpecs=true 
//  FIXME: this should be changeto valueOf ...   that will also kill that fallback 'none' which is I think more like a problem than a   feature ;) 
//  TODO Setup a set of threads to process incoming requests.   Make sure requests for a single dag/query are handled by the same thread 
//  TODO: add alter database support in HCat 
//  TODO add tests for partitions in other catalogs 
//  TODO I suspect we could skip much of the stuff above this in the function in the case   of update and delete.  But I don't understand all of the side effects of the above   code and don't want to skip over it yet. 
//  TODO: expireAfterAccess locks cache segments on put and expired get. It doesn't look too bad,         but if we find some perf issues it might be a good idea to remove this - we are probably         not caching that many constructors.   Note that weakKeys causes "==" to be used for key compare; this will only work   for classes in the same classloader. Should be ok in this case. 
//  TODO Create and init session sets up queue, isDefault - but does not initialize the configuration 
//  @deprecated in favour of {@link HCatTable.#sortCols(Map<String, String>)}.   To be removed in Hive 0.16. 
//  TODO: Handle Query Hints; currently we ignore them 
//  TODO: Currently we only support EQUAL operator on two references.   We might extend the logic to support other (order-preserving)   UDFs here. 
//  is the right was at the left side of a right outer join? 
//  TODO: why does this only kill non-default sessions?   Nothing for workload management since that only deals with default ones. 
//  TODO: should this use getPartitionDescFromPathRecursively? That's what other code uses. 
//  TODO: Fix this 
//  ### FIXME: doing the multi-line handling down here means   higher-level logic never sees the extra lines. So,   for example, if a script is being saved, it won't include   the continuation lines! This is logged as sf.net   bug 879518. 
//  REVIEW jhyde 29-Oct-2007: This rule is non-static, depends on the state   of members in RelDecorrelator, and has side-effects in the decorrelator.   This breaks the contract of a planner rule, and the rule will not be   reusable in other planners. 
//  We can loop thru all the tables to check if they are ACID first and then perform cleanup,   but it's more efficient to unconditionally perform cleanup for the database, especially   when there are a lot of tables 
//  TODO: Implement this 
//  TODO: allow using unsafe optionally.   bounds check first, to trigger bugs whether the first byte matches or not 
//  TODO: this doesn't appear to be used anywhere. 
//  for auto convert map-joins, it not safe to dedup in here (todo) 
//  We're scanning a tree from roots to leaf (this is not technically   correct, demux and mux operators might form a diamond shape, but   we will only scan one path and ignore the others, because the   diamond shape is always contained in a single vertex). The scan   is depth first and because we remove parents when we pack a pipeline   into a vertex we will never visit any node twice. But because of that   we might have a situation where we need to connect 'work' that comes after   the 'work' we're currently looking at.     Also note: the concept of leaf and root is reversed in hive for historical 
//  The following parameters are not supported yet. TODO Add support 
//  TODO: this should be   unique 
// todo: Concurrent insert/update of same partition - should pass 
//  @deprecated in favour of {@link HCatTable.#fieldsTerminatedBy()}. To be removed in Hive 0.16. 
//  TODO: this is invalid for ACID tables, and we cannot access AcidUtils here. 
/*    * After each major compaction, stats need to be updated on each column of the   * table/partition which previously had stats.   * 1. create a bucketed ORC backed table (Orc is currently required by ACID)   * 2. populate 2 partitions with data   * 3. compute stats   * 4. insert some data into the table using StreamingAPI   * 5. Trigger major compaction (which should update stats)   * 6. check that stats have been updated   *   * @throws Exception todo:   *                   2. add non-partitioned test   *                   4. add a test with sorted table?    */
/*    * in Hive 1.3.0 delta file names changed to delta_xxxx_yyyy_zzzz; prior to that   * the name was delta_xxxx_yyyy.  We want to run compaction tests such that both formats   * are used since new (1.3) code has to be able to read old files.    */
//  TODO: should call HiveHFileOutputFormat#setOutputPath 
//  TODO: we could try to get superclass or generic interfaces. 
//  TODO: this actually calls the metrics system and getMetrics - that may be expensive.         For now it looks like it should be ok to do on WM thread. 
//  TODO: get rid of the builders - they serve no purpose... just call ctors directly. 
//  TODO: Verify GB having is not a separate filter (if so we shouldn't   introduce derived table) 
/* Q: why don't we lock the snapshot here???  Instead of having client make an explicit call    whenever it chooses    A: If we want to rely on locks for transaction scheduling we must get the snapshot after lock    acquisition.  Relying on locks is a pessimistic strategy which works better under high    contention. */
//  TODO: temporary, need to expose from ORC utils (note the difference in null checks) 
//  TODO: maybe use stack of est+obj pairs instead of recursion. 
//  TODO: shortcut for last col below length? 
//  TEMPORARY: In order to avoid a new version of storage-api, do the conversion here... 
// TODO: these constraints should be supported for partition columns 
//  We have a nested setcolref. Process that and start from scratch TODO: use stack? 
//  TODO: is this correct? based on the same logic as HIVE-14200 
//  TODO refactor the following into the pipeline 
//  TODO: change to FileInputFormat.... field after MAPREDUCE-7086. 
//  TODO: if we ever use this endpoint for anything else, refactor cycling into a separate class. 
//  something is seriously wrong if this is happening 
//  TODO: Ordering seems to affect the distinctness, needs checking, disabling. 
//  TODO: avoid put() by working directly in OutStream? 
//  Can't fetch prefix on colqual, must pull the entire qualifier   TODO use an iterator to do the filter, server-side. 
//  These tests inherently cause exceptions to be written to the test output   logs. This is undesirable, since you it might appear to someone looking   at the test output logs as if something is failing when it isn't. Not   sure 
//     return HiveConf.getPositionFromInternalName(fieldName);   The above line should have been all the implementation that   we need, but due to a bug in that impl which recognizes   only single-digit columns, we need another impl here. 
//  @deprecated in favour of {@link HCatTable.#getPartCols()}. To be removed in Hive 0.16. 
//  TODO: Verify if we need to use ConstantObjectInspector to unwrap data 
//  2) Generate HiveTableFunctionScan RelNode for lateral view   TODO: Support different functions (not only INLINE) with LATERAL VIEW JOIN 
// todo: this is actually not adding anything since LockComponent uses a Trie to "promote" a lock  except by accident - when we have a partitioned target table we have a ReadEntity and WriteEntity  for the table, so we mark ReadEntity and then delete WriteEntity (replace with Partition entries)  so DbTxnManager skips Read lock on the ReadEntity....  input.noLockNeeded()? 
// TODO- cleanup once parquet support Timestamp type annotation. 
//  TODO: buffers are accounted for at allocation time, but ideally we should report the memory         overhead from the java objects to memory manager and remove it when discarding file. 
//  TODO: cleanup this 
//  TODO: HIVE-13624 Do we need maxLength checking? 
//  In addBatchToWriter, we have passed the batch to both ORC and operator pipeline   (neither ever changes the vectors). We'd need a set of vectors batch to write to.   TODO: for now, create this from scratch. Ideally we should return the vectors from ops.         We could also have the ORC thread create it for us in its spare time... 
//  TODO: enforce max length 
//  TODO: would it make sense to return buffers asynchronously? 
//  TODO: The best solution is to support NaN in expression reduction. 
//  TODO: ideally, this should be moved outside to HiveMetaStore to be shared between         all the RawStore-s. Right now there's no method to create a pool. 
//  TODO HIVE-14042. Abort handling. 
// todo: update to search by ID once HIVE-13353 is done 
//  Note: type param is not available here. 
//  TODO: When function privileges are implemented, they should be deleted here. 
//  TODO: perhaps move to Orc InStream? 
// TODO: should not throw different exceptions for different HMS deployment types 
//  TODO: Clean up all the other paths that are created. 
//  TODO: ideally when col-stats-accurate stuff is stored in some sane structure, this should         to retrieve partsToUpdate in a single query; no checking partition params in java. 
//  UNDONE: Missing date/time interval data types 
//  @deprecated in favour of {@link HCatTable.#getCols()}. To be removed in Hive 0.16. 
//  TODO: refactor this out 
//  TODO This needs to be looked at. Map of Map to Map... Made concurrent for now since split generation   can happen in parallel. 
//  TODO: Fix the expressions later. 
//  @deprecated in favour of {@link #create(HCatTable)}. To be removed in Hive 0.16. 
//  TODO: handle ExprNodeColumnListDesc 
//  5. Push Down Semi Joins  TODO: Enable this later 
// todo: add TXNS.COMMENT filed and set it to 'aborted by system due to timeout'  easier to read logs 
//  TODO RIVEN switch this back to package level when we can move TestHadoopAuthBridge23 into   riven. 
//  TODO Change this over to just store local dir indices, instead of the entire path. Far more efficient. 
//  TODO: we might revisit this in create-drop-recreate cases, needs some thinking on. 
//  TODO: why is this in text formatter?!! 
/*  * NOTE: this rule is replicated from Calcite's SubqueryRemoveRule * Transform that converts IN, EXISTS and scalar sub-queries into joins. * TODO: *  Reason this is replicated instead of using Calcite's is *    Calcite creates null literal with null type but hive needs it to be properly typed * * <p>Sub-queries are represented by {@link RexSubQuery} expressions. * * <p>A sub-query may or may not be correlated. If a sub-query is correlated, * the wrapped {@link RelNode} will contain a {@link RexCorrelVariable} before * the rewrite, and the product of the rewrite will be a {@link Correlate}. * The Correlate can be removed using {@link RelDecorrelator}.  */
//  truncate (TODO: posix_fallocate?) 
//  NOTE: This code tries to get all key-value pairs out of the map.   It's not very efficient. The more efficient way should be to let MapOI   return an Iterator. This is currently not supported by MapOI yet. 
//  UNDONE: Why do we need to specify BinarySortableSerDe explicitly here??? 
// todo: last param is bogus. why is this hardcoded? 
//  TODO: move this to a common method   Note: this gets IDs by name, so we assume indices don't need to be adjusted for ACID. 
//  DecorrelateRexShuttle ends up decorrelating expressions cor.col1 <> $4   to $4=$4 if value generator is not generated, $4<>$4 is further simplified   to false. This is wrong and messes up the whole tree. To prevent this visitCall   is overridden to rewrite/simply such predicates to is not null.   we also need to take care that we do this only for correlated predicates and   not user specified explicit predicates   TODO:  This code should be removed once CALCITE-1851 is fixed and   there is support of not equal 
//  TODO: need proper clone. Meanwhile, let's at least keep this horror in one place 
//  Note that we pass job config to the record reader, but use global config for LLAP IO.   TODO: add tracing to serde reader 
//  @deprecated in favour of {@link HCatPartition.#getTableName()}. To be removed in Hive 0.16. 
//  FIXME: support template types. It currently has conflict with ExprNodeConstantDesc 
/*  * This rule is a copy of {@link org.apache.calcite.rel.rules.AggregateReduceFunctionsRule} * that regenerates Hive specific aggregate operators. * * TODO: When CALCITE-2216 is completed, we should be able to remove much of this code and * just override the relevant methods. * * Planner rule that reduces aggregate functions in * {@link org.apache.calcite.rel.core.Aggregate}s to simpler forms. * * <p>Rewrites: * <ul> * * <li>AVG(x) &rarr; SUM(x) / COUNT(x) * * <li>STDDEV_POP(x) &rarr; SQRT( *     (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x)) *    / COUNT(x)) * * <li>STDDEV_SAMP(x) &rarr; SQRT( *     (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x)) *     / CASE COUNT(x) WHEN 1 THEN NULL ELSE COUNT(x) - 1 END) * * <li>VAR_POP(x) &rarr; (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x)) *     / COUNT(x) * * <li>VAR_SAMP(x) &rarr; (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x)) *        / CASE COUNT(x) WHEN 1 THEN NULL ELSE COUNT(x) - 1 END * </ul>  */
//  TODO: Handle more than 2 inputs for setop 
//  TODO: separate model is needed for compressedOops, which can be guessed from memory size. 
// TODO: Should we convert MultiJoin to be a child of HiveJoin 
//  TODO should be doing security check here.  Users should not be   able to see each other's locks. 
//  We do no handle anything but OK for now. Again, we need a real client for this API.   TODO: handle 401 and return a new connection? nothing for now 
//  TODO: close basically resets the object to a bunch of nulls.         We should ideally not reuse the object because it's pointless and error-prone. 
// todo: createOptionsForReader() assumes it's !isOriginal.... why? 
//  TODO: Required due to SessionState.getHDFSSessionPath. Why wasn't it required before? 
//  TODO Change this to not serialize the entire Configuration - minor. 
//  TODO: For object inspector fields, assigning 16KB for now. To better estimate the memory size every   object inspectors have to implement MemoryEstimate interface which is a lot of change with little benefit compared 
//  UNDONE: Add support for DATE, TIMESTAMP, INTERVAL_YEAR_MONTH, INTERVAL_DAY_TIME... 
//  TODO: there was code here to create guess-estimate for collection wrt how usage changes   when removing elements. However it's too error-prone for anything involving   pre-allocated capacity, so it was discarded. 
//  Extract the buckedID from pathFilesMap, this is more accurate method,   however. it may not work in certain cases where buckets are named   after files used while loading data. In such case, fallback to old   potential inaccurate method.   The accepted file names are such as 000000_0, 000001_0_copy_1. 
//  This is a bit hackish to fix mismatch between SARG and Hive types   for Timestamp and Date. TODO: Move those types to storage-api. 
//  TODO HIVE-14042. Move to using a loop and a timed wait once TEZ-3302 is fixed. 
//  TODO: add a configurable option to skip the history and just drop it? 
//  TODO: refactor this into an utility, LLAP tests use this pattern a lot 
//  TODO: make aliases unique, otherwise needless rewriting takes place 
/*    * Derive additional attributes to be rendered by EXPLAIN.   * TODO: this method is relied upon by custom input formats to set jobconf properties.   *       This is madness? - This is Hive Storage Handlers!    */
/*    * Connects to the {@link IMetaStoreClient meta store} that will be used to manage {@link Transaction} life-cycles.   * Also checks that the tables destined to receive mutation events are able to do so. The client should only hold one   * open transaction at any given time (TODO: enforce this).    */
//  TODO: this should have an option for directory to inherit from the parent table,         including bucketing and list bucketing, for the use in compaction when the         latter runs inside a transaction. 
//  UNDONE: Parameterize for implementation variation? 
//  TODO: ifExists could be moved to metastore. In fact it already supports that. Check it         for now since we get parts for output anyway, so we can get the error message         earlier... If we get rid of output, we can get rid of this. 
//  FIXME: moved default value to here...for now   i think this features is never really used from the command line 
//  TODO : Should be moved out. 
//  TODO - types need to be checked. 
//  TODO: handle multi joins 
//  TODO: ugly hack because Java doesn't have dtors and Tez input hangs on shutdown. 
//  TODO: Make script output prefixing configurable. Had to disable this since   it results in lots of test diffs. 
//  TODO: also support fileKey in splits, like OrcSplit does 
//  TODO: when txn stats are implemented, use writeIds to determine stats accuracy 
//  Hive is pretty simple (read: stupid) in writing out values via the serializer.   We're just going to go through, matching indices.  Hive formats normally   handle mismatches with null.  We don't have that option, so instead we'll   end up throwing an exception for invalid records. 
//  TODO handle negations 
//  TODO: handle task to container map events in case of hard failures 
// TODO: partition names in getPartitionsByNames are not case insensitive 
//  todo this should be configured in serde 
//  TODO: Have to put in the support for AS clause 
//  Ideally we should use HiveRelNode convention. However, since Volcano planner   throws in that case because DruidQuery does not implement the interface,   we set it as Bindable. Currently, we do not use convention in Hive, hence that   should be fine.   TODO: If we want to make use of convention (e.g., while directly generating operator   tree instead of AST), this should be changed. 
// Call getSplit on the InputFormat, create an HCatSplit for each  underlying split. When the desired number of input splits is missing,  use a default number (denoted by zero).  TODO(malewicz): Currently each partition is split independently into  a desired number. However, we want the union of all partitions to be  split into a desired number while maintaining balanced sizes of input 
//  TODO: do we need to handle the "this is what MySQL does" here? 
//  @deprecated in favour of {@link HCatTable.#bucketCols(List<FieldSchema>) and HCatTable.#numBuckets(int)}.   To be removed in Hive 0.16. 
//  TODO Watches on the output dirs need to be cancelled at some point. For now - via the expiry. 
/*          * TODO: Use the hard link feature of hdfs         * once https://issues.apache.org/jira/browse/HDFS-3370 is done          */
//  TODO: allow >1 port per host? 
//  NOTE: This is for generating the internal path name for partitions. Users   should always use the MetaStore API to get the path name for a partition.   Users should not directly take partition values and turn it into a path   name by themselves, because the logic below may change in the future.     In the future, it's OK to add new chars to the escape list, and old data   won't be corrupt, because the full path name in metastore is stored.   In that case, Hive will continue to read the old data, but when it creates   new partitions, it will use new names.   edit : There are some use cases for which adding new chars does not seem   to be backward compatible - Eg. if partition was created with name having   a special char that you want to start escaping, and then you try dropping   the partition with a hive version that now escapes the special char using   the list below, then the drop partition fails to work. 
//  TODO can we be more precise than string,string? 
//  TODO: this is wrong; this test sets up dummy txn manager and so it cannot create ACID tables.         If I change it to use proper txn manager, the setup for some tests hangs.         This used to work by accident, now this works due a test flag. The test needs to be fixed.   Create table 
//  TODO - currently no way to test alter partition, as HCatClient doesn't support it. 
//  Note - some of these scenarios could be handled, but they are not supported right now.   The reason is that we bind a query to app/user using the signed token information, and   we don't want to bother figuring out which one to use in case of ambiguity w/o a use case.   Ambiguous user.   Ambiguous user.   Ambiguous user.   Ambiguous app. 
//  only support bulkload when a hfile.family.path has been specified.   TODO: support detecting cf's from column mapping   TODO: support loading into multiple CF's at a time 
//  TODO Why is this changed from the default in hive-conf? 
//  TODO: need to set catalog parameter 
//  TODO: Should have a check on the server side. Embedded metastore throws   NullPointerException, remote throws TTransportException 
//  TODO: this will probably send a message to AM. Is that needed here? 
//  There's full hash code stored in front of the key. We could check that first. If keyLength   is <= 4 it obviously doesn't make sense, less bytes to check in a key. Then, if there's a   match, we check it in vain. But what is the proportion of matches? For writes it could be 0   if all keys are unique, for reads we hope it's really high. Then if there's a mismatch what   probability is there that key mismatches in <4 bytes (so just checking the key is faster)? 
/*    * Given a RexCall & TableScan find max no of nulls. Currently it picks the   * col with max no of nulls.   *    * TODO: improve this   *    * @param call   * @param t   * @return    */
//  NOTE: BeeLineOpts uses Reflector in an extensive way to call getters and setters on itself   If you want to add any getters or setters to this class, but not have it interfere with   saved variables in beeline.properties, careful use of this marker is needed.   Also possible to get this by naming these functions obtainBlah instead of getBlah   and so on, but that is not explicit and will likely surprise people looking at the   code in the future. Better to be explicit in intent. 
//  TODO: should this be currentDirs? 
//  TODO What else is required in this environment map. 
//  FIXME: HIVE-18703 should probably move this method somewhere else 
// TODO these bytes should be versioned 
// TODO 1.0 miniCluster is slow this test times out, make it work 
/*    * Send dropped table notifications. Subscribers can receive these notifications for   * dropped tables by listening on topic "HCAT" with message selector string   * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_EVENT} =   * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_DROP_TABLE_EVENT}   * </br>   * TODO: DataNucleus 2.0.3, currently used by the HiveMetaStore for persistence, has been   * found to throw NPE when serializing objects that contain null. For this reason we override   * some fields in the StorageDescriptor of this notification. This should be fixed after   * HIVE-2084 "Upgrade datanucleus from 2.0.3 to 3.0.1" is resolved.    */
/*  * This utility is designed to help with upgrading to Hive 3.0.  On-disk layout for transactional * tables has changed in 3.0 and require pre-processing before upgrade to ensure they are readable * by Hive 3.0.  Some transactional tables (identified by this utility) require Major compaction * to be run on them before upgrading to 3.0.  Once this compaction starts, no more * update/delete/merge statements may be executed on these tables until upgrade is finished. * * Additionally, a new type of transactional tables was added in 3.0 - insert-only tables.  These * tables support ACID semantics and work with any Input/OutputFormat.  Any Managed tables may * be made insert-only transactional table. These tables don't support Update/Delete/Merge commands. * * This utility works in 2 modes: preUpgrade and postUpgrade. * In preUpgrade mode it has to have 2.x Hive jars on the classpath.  It will perform analysis on * existing transactional tables, determine which require compaction and generate a set of SQL * commands to launch all of these compactions. * * Note that depending on the number of tables/partitions and amount of data in them compactions * may take a significant amount of time and resources.  The script output by this utility includes * some heuristics that may help estimate the time required.  If no script is produced, no action * is needed.  For compactions to run an instance of standalone Hive Metastore must be running. * Please make sure hive.compactor.worker.threads is sufficiently high - this specifies the limit * of concurrent compactions that may be run.  Each compaction job is a Map-Reduce job. * hive.compactor.job.queue may be used to set a Yarn queue ame where all compaction jobs will be * submitted. * * In postUpgrade mode, Hive 3.0 jars/hive-site.xml should be on the classpath. This utility will * find all the tables that may be made transactional (with ful CRUD support) and generate * Alter Table commands to do so.  It will also find all tables that may not support full CRUD * but can be made insert-only transactional tables and generate corresponding Alter Table commands. * * TODO: rename files * * "execute" option may be supplied in both modes to have the utility automatically execute the * equivalent of the generated commands * * "location" option may be supplied followed by a path to set the location for the generated * scripts.  */
//  srcs = new FileStatus[0]; Why is this needed? 
//  TODO: should probably throw an exception here. 
//  2. Convert Agg Fn args and type of args to Calcite   TODO: Does HQL allows expressions as aggregate args or can it only be 
//  TODO: note that the token is not renewable right now and will last for 2 weeks by default. 
//  TODO: Should this be also TOP_DOWN? 
/*  * TODO: Most of the code in this class is ripped from ZooKeeper tests. Instead * of redoing it, we should contribute updates to their code which let us more * easily access testing helper objects. * *XXX: copied from the only used class by qtestutil from hbase-tests  */
//  If it's constant = constant or column = column, we can't fetch any ranges   TODO We can try to be smarter and push up the value to some node which 
//  TODO: Implement propConstDistUDAFParams 
//  TODO: implement implicit AsyncRDDActions conversion instead of jc.monitor()?   TODO: how to handle stage failures? 
//  TODO: should we check isAssignableFrom? 
//  For replication add-ptns, we need to follow a insert-if-not-exist, alter-if-exists scenario.   TODO : ideally, we should push this mechanism to the metastore, because, otherwise, we have   no choice but to iterate over the partitions here. 
//  TODO: should we create the batch from vrbctx, and reuse the vectors, like below? Future work. 
//  this is a hacky way of doing the quotes since it will match any 2 of   these, so   "[ hello this is something to split [" would be considered to be quoted. 
//  TODO: Do the type checking of the expressions 
//  Make a tree out of the filter.   TODO: this is all pretty ugly. The only reason we need all these transformations         is to maintain support for simple filters for HCat users that query metastore.         If forcing everyone to use thick client is out of the question, maybe we could         parse the filter into standard hive expressions and not all this separate tree 
//  Not safe to continue for RS-GBY-GBY-LIM kind of pipelines. See HIVE-10607 for more. 
//  Hack for tables with no columns   Treat it as a table with a single column called "col" 
//  TODO: if we expect one dir why don't we enforce it? 
//  Post serialization, separators are automatically inserted between different fields in the   struct. Currently there is not way to disable that. So the work around here is to pad the 
//  We only support limited unselected column following by order by.   TODO: support unselected columns in genericUDTF and windowing functions.   We examine the order by in this query block and adds in column needed   by order by in select list. 
//  TODO: if we want to be explicit about this dump not being a replication dump, we can   uncomment this else section, but currently unneeded. Will require a lot of golden file   regen if we do so. 
//  TODO: refactor this in HIVE-6366 
//  TODO: (a = 1) and NOT (a is NULL) can be potentially folded earlier into a NO-OP 
//  @todo: remove this. 8/28/14 hb   for now adding because RelOptUtil.classifyFilters has an assertion about   column counts that is not true for semiJoins. 
//  TODO: this seems to indicate that priorities change too little...         perhaps we need to adjust the policy. 
//  Note - we need srcFs rather than fs, because it is possible that the _files lists files   which are from a different filesystem than the fs where the _files file itself was loaded   from. Currently, it is possible, for eg., to do REPL LOAD hdfs://<ip>/dir/ and for the _files   in it to contain hdfs://<name>/ entries, and/or vice-versa, and this causes errors.   It might also be possible that there will be a mix of them in a given _files file.   TODO: revisit close to the end of replv2 dev, to see if our assumption now still holds,   and if not so, optimize. 
//  Note: with some trickery, we could add logic for each type in ConfVars; for now the   potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around. 
//  FIXME: hiveServer2SiteUrl is not settable? 
//  The following check is only a guard against failures.   TODO: Knowing which expr is constant in GBY's aggregation function   arguments could be better done using Metadata provider of Calcite.  check the corresponding expression in exprs to see if it is literal 
//  TODO: Remove in Hive 0.16.   This is required only to support the deprecated HCatAddPartitionDesc.Builder interfaces. 
// TODO: if partitions are loaded lazily via the iterator then we will have to avoid conversion of everything here as it defeats the purpose. 
/*      * Figures out the aliases for whom it is safe to push predicates based on     * ANSI SQL semantics. The join conditions are left associative so "a     * RIGHT OUTER JOIN b LEFT OUTER JOIN c INNER JOIN d" is interpreted as     * "((a RIGHT OUTER JOIN b) LEFT OUTER JOIN c) INNER JOIN d".  For inner     * joins, both the left and right join subexpressions are considered for     * pushing down aliases, for the right outer join, the right subexpression     * is considered and the left ignored and for the left outer join, the     * left subexpression is considered and the left ignored. Here, aliases b     * and d are eligible to be pushed up.     *     * TODO: further optimization opportunity for the case a.c1 = b.c1 and b.c2     * = c.c2 a and b are first joined and then the result with c. But the     * second join op currently treats a and b as separate aliases and thus     * disallowing predicate expr containing both tables a and b (such as a.c3     * + a.c4 > 20). Such predicates also can be pushed just above the second     * join and below the first join     *     * @param op     *          Join Operator     * @param rr     *          Row resolver     * @return set of qualified aliases      */
//  TODO: not stopping umbilical explicitly as some taskKill requests may get scheduled during queryComplete   which will be using the umbilical. HIVE-16021 should fix this, until then leave umbilical open and wait for   it to be closed after max idle timeout (10s default) 
//  TODO [MM gap?]: by design; no-one seems to use LB tables. They will work, but not convert.                   It's possible to work around this by re-creating and re-inserting the table. 
//  TODO: Should be moved out. 
//  TODO: I/O threadpool could be here - one thread per stripe; for now, linear. 
//  Optionally, do some filtering of rows...   UNDONE 
//  TODO: Should have a check on the server side. Embedded metastore throws   InvalidObjectException, remote throws TApplicationException 
//  FIXME: Support pruning dynamic partitioning. 
//  FIXME: sideeffect will leave the last query set at the session level 
//  TODO: use faster non-sync inputstream 
//  TODO: should this rather use a threadlocal for NUMA affinity? 
// TODO: this object is created once to call one method and then immediately destroyed.  So it's basically just a roundabout way to pass arguments to a static method. Simplify? 
//  TODO:pc implement max 
//  TODO Make sure this method is eventually used to find the prep / batch scripts. 
/*      * This doesn't throw any exceptions because we don't want the Compaction to appear as failed     * if stats gathering fails since this prevents Cleaner from doing it's job and if there are     * multiple failures, auto initiated compactions will stop which leads to problems that are     * much worse than stale stats.     *     * todo: longer term we should write something COMPACTION_QUEUE.CQ_META_INFO.  This is a binary     * field so need to figure out the msg format and how to surface it in SHOW COMPACTIONS, etc      */
//  TODO: transitive dependencies warning? 
//  We are in HS2, get the token locally.   TODO: coordinator should be passed in; HIVE-13698. Must be initialized for now. 
//  TODO: Ideally, AcidUtils class and various constants should be in common. 
//  HACK: We actually need BlockMissingException, but that is not available 
//  TODO HIVE-14042. Handling of dummyOps, and propagating abort information to them 
//  TODO This - at least for the session pool - will always be the hive user. How does doAs above this affect things ? 
//  TODO: this is currently broken. We need to set memory manager to a bogus implementation         to avoid problems with memory manager actually tracking the usage. 
//  TODO: for one-block case, we could move notification for the last block out of the loop. 
//  TODO: figure out a better data structure for node list(?) 
// why isn't PPD working.... - it is working but storage layer doesn't do row level filtering; only row group level 
//  Note : Currently, this implementation does not "fall back" to regular copy if distcp   is tried and it fails. We depend upon that behaviour in cases like replication,   wherein if distcp fails, there is good reason to not plod along with a trivial   implementation, and fail instead. 
//  TODO: this is never used 
//  TODO: versions could also be picked at build time. 
//  Hack!! - refactor once the metadata APIs with types are ready 
//  TODO: if this cannot evict enough, it will spin infinitely. Terminate at some point? 
//  but it is not OK to convert if the join is on (a,c) 
//  TODO: ideally we should have a test for session itself. 
//  TODO : if needed, verify that recordschema entry for fieldname matches appropriate type. 
//  FIXME: null value is treated differently on the other end..when those filter will be 
/*  * TODO:<br> * 1. Change the output col/ExprNodeColumn names to external names.<br> * 2. Verify if we need to use the "KEY."/"VALUE." in RS cols; switch to * external names if possible.<br> * 3. In ExprNode & in ColumnInfo the tableAlias/VirtualColumn is specified * differently for different GB/RS in pipeline. Remove the different treatments. * 4. VirtualColMap needs to be maintained *  */
//  Replace the entire current DiskRange with new cached range.   In case of an inexact match in either of the below it may throw. We do not currently   support the case where the caller requests a single cache buffer via multiple smaller   sub-ranges; if that happens, this may throw. Noone does it now, though.   TODO: should we actively assert here for cache buffers larger than range? 
//  TODO: Fill in when PARTITION_DONE_EVENT is supported. 
//  TODO For now, this affects non broadcast unsorted cases as well. Make use of the edge   property when it's available. 
//  TODO: most other options are probably unrecoverable... throw? 
//  this is a temporary hack to fix things that are not fixed in the compiler 
//  Should be fixed in Accumulo 1.5.2 and 1.6.1 
//  This is hackery, but having hive-common depend on standalone-metastore is really bad   because it will pull all of the metastore code into every module.  We need to check that   we aren't using the standalone metastore.  If we are, we should treat it the same as a 
// todo: fix this - it has to run in 3.0 since tables may be unbucketed 
//  Not sure why this method doesn't throw any exceptions,   but since the interface doesn't allow it we'll just swallow them and   move on.  This OK-ish since releaseLocks() is only called for RO/AC queries; it  would be really bad to eat exceptions here for write operations 
//  TODO: Implement this when tez is upgraded. TEZ-3550 
//  TODO: there should be a better way to do this, code just needs to be modified 
//  TODO Session re-use completely disabled for doAs=true. Always launches a new session. 
//  TODO In case of a failure to heartbeat, tasks for the specific DAG should ideally be KILLED 
//  TODO Check if all required tables are allowed, if so, get it from cache 
//  TODO: Only the qualified name should be left here 
//  FIXME: this is a secret contract; reusein getAggrKey() creates a more closer relation to the StatsGatherer 
//  Note: it's not quite clear why this is done inside this if. Seems like it should be on the top level. 
//  TODO: why is this synchronized? 
//  TODO: local cache is created once, so the configs for future queries will not be honored. 
//  Support for dynamic partitions can be added later   The following is not optimized:   insert overwrite table T1(ds='1', hr) select key, value, hr from T2 where ds = '1';   where T1 and T2 are bucketed by the same keys and partitioned by ds. hr 
//  FIXME: using real scaling by new/old ration might yield better results? 
// todo: try this with acid default - it seem making table acid in listener is too late 
/*  * Context class for operator tree walker for partition pruner. * TODO: this class may be not useful.  */
//  Thrift cannot write read-only buffers... oh well.   TODO: actually thrift never writes to the buffer, so we could use reflection to         unset the unnecessary read-only flag if allocation/copy perf becomes a problem. 
//  TODO should be replaced by CliServiceClient 
//  @deprecated in favour of {@link #Builder(HCatTable, boolean)}. To be removed in Hive 0.16. 
//  TODO: Change ExprNodeConverter to be independent of Partition Expr 
//  We are not going to verify SD for each partition. Just verify for the table.   ToDo: we need verify the partition column instead 
// todo: ConditionalTask#addDependentTask(Task) doesn't do the right thing: HIVE-18978 
// TODO: Can columns retain virtualness out of union 
//  TODO: why do we invent our own error path op top of the one from Future.get? 
//  TODO is there a more correct way to get the literal value for the Object? 
/*  * todo: This need review re: thread safety.  Various places (see callsers of * {@link SessionState#setCurrentSessionState(SessionState)}) pass SessionState to forked threads. * Currently it looks like those threads only read metadata but this is fragile. * Also, maps (in SessionState) where tempt table metadata is stored are concurrent and so * any put/get crosses a memory barrier and so does using most {@code java.util.concurrent.*} * so the readers of the objects in these maps should have the most recent view of the object. * But again, could be fragile.  */
//  arguments then we can use a more efficient form. 
//  TODO: Something is preventing the process from terminating after main(), adding exit() as hacky solution. 
//  TODO: write error to the channel? there's no mechanism for that now. 
//  TODO: reuse columnvector-s on hasBatch - save the array by column? take apart each list. 
//  todo: hold onto this predicate, so that we don't add it to the Filter Operator. 
//  TODO Ideally, remove elements from this once it's known that no tasks are linked to the instance (all deallocated) 
//  TODO: can we blindly copy sort trait? What if inputs changed and we   are now sorting by different cols 
//  TODO: This works different in remote and embedded mode.   In embedded mode, no exception happens. 
//  UNDONE: Need to copy the object. 
//  did remove those and gave CBO the proper AST. That is kinda hacky. 
//  FIXME: consider other operator info as well..not just conf? 
//  check # of dp   TODO: add an option to skip this if number of partitions checks is done by Triggers via   CREATED_DYNAMIC_PARTITION counter 
/*  vertex is started, but not complete  */
//  TODO: need the description of how these maps are kept consistent. 
//  UNDONE: Need to copy the object? 
//  TODO filter->expr   TODO functionCache   TODO constraintCache   TODO need sd nested copy?   TODO String intern   TODO monitor event queue   TODO initial load slow?   TODO size estimation 
//  TODO not 100% sure about this.  This call doesn't set the compression type in the conf   file the way getHiveRecordWriter does, as ORC appears to read the value for itself.  Not   sure if this is correct or not. 
/*    * TODO : Refactor   *   * There is an upcoming patch that refactors this bit of code. Currently, the idea is the following:   *   * By default, ReplCopyWork will behave similarly to CopyWork, and simply copy   * along data from the source to destination.   * If the flag readSrcAsFilesList is set, changes the source behaviour of this CopyTask, and   * instead of copying explicit files, this will then fall back to a behaviour wherein an _files is   * read from the source, and the files specified by the _files are then copied to the destination.   *   * This allows us a lazy-copy-on-source and a pull-from destination semantic that we want   * to use from replication.    */
/*  A scratch variable is created here. This could be optimized in the future     * by perhaps using thread-local storage to allocate this scratch field.      */
//  Ideally there should be a better way to determine that the followingWork contains   a dynamic partitioned hash join, but in some cases (createReduceWork()) it looks like   the work must be created/connected first, before the GenTezProcContext can be updated   with the mapjoin/work relationship. 
// Future thought: this may be expensive so consider having a thread pool run in parallel 
/*      * Number of rows processed between checks for minReductionHashAggr factor     * TODO: there is overlap between numRowsCompareHashAggr and checkInterval      */
//  TODO: LlapNodeId is just a host+port pair; we could make this class more generic. 
//  Note: we assume here that plan has been validated beforehand, so we don't verify 
//  Implement in future, if needed. 
//  TODO: move this into ctor? EW would need to create CacheWriter then 
//  After SPARK-2321, we only use JobMetricsListener to get job metrics   TODO: remove it when the new API provides equivalent functionality 
//  TODO: could we do this only if the OF is actually used? 
//  TODO: we may add app name, etc. later 
//  TODO no fk across catalogs 
//  TODO: This does not work because materialized views need the creation metadata   to be updated in case tables used were replicated to a different database.  run("CREATE MATERIALIZED VIEW " + dbName + ".mat_view AS SELECT a FROM " + dbName + ".ptned where b=1", driver);  verifySetup("SELECT a from " + dbName + ".mat_view", ptn_data_1, driver); 
//  FIXME: oss seems to contain duplicates 
//  TODO: danger of stack overflow... needs a retry limit? 
//  Cannot drop db1 because mv1 uses one of its tables   TODO: Error message coming from metastore is currently not very concise   (foreign key violation), we should make it easily understandable 
//  TODO: ideally, QueryTracker should have fragment-to-query mapping. 
//  TODO: WTF? The old code seems to just drop the ball here. 
//  Hack to initialize cache with 0 expiry time causing it to return a new hive client every time   Otherwise the cache doesn't play well with the second test method with the client gets closed() in the   tearDown() of the previous test 
//  @deprecated in favour of {@link HCatPartition.#getLocation()}. To be removed in Hive 0.16. 
//  TODO HIVE-12449. Make use of progress notifications once Hive starts sending them out.   progressNotified = task.getAndClearProgressNotification(); 
//  TODO Evil!  Need to figure out a way to remove this sleep. 
//  TODO: all the extrapolation logic should be moved out of this class, 
//  FIXME: this add seems suspicious...10 lines below the value returned by this method used as betterDS 
//  this is a hack for now to handle the group by case 
//  All other distinct keys will just be forwarded. This could be optimized... 
//  This is a workaround for DERBY-6358 and Oracle bug; it is pretty horrible. 
// @TODO This is fetching all the rows at once from broker or multiple historical nodes   Move to use scan query to avoid GC back pressure on the nodes 
// todo: strictly speaking there is a bug here.  heartbeat*() commits but both heartbeat and  checkLock() are in the same retry block, so if checkLock() throws, heartbeat is also retired 
//  @deprecated in favour of {@link HCatTable.#partCols(List<FieldSchema>)}. To be removed in Hive 0.16. 
//  Wow, something's really wrong. 
//  UNDONE: Does this random range need to go as high as 38? 
//  TODO : Java 1.7+ support using String with switches, but IDEs don't all seem to know that.   If casing is fine for now. But we should eventually remove this. Also, I didn't want to   create another enum just for this. 
//  We have found an invalid decimal value while enforcing precision and   scale. Ideally,   we would replace it with null here, which is what Hive does. However,   we need to plumb   this thru up somehow, because otherwise having different expression   type in AST causes   the plan generation to fail after CBO, probably due to some residual   state in SA/QB.   For now, we will not run CBO in the presence of invalid decimal 
//  Code initially inspired by Google ObjectExplorer.   TODO: roll in the direct-only estimators from fields. Various other optimizations possible. 
//  Dirty hack as this will throw away spaces and other things - find a better 
//  TODO: should local cache also be by fileId? Preserve the original logic for now. 
//  TODO: do we need to get to child? 
//  TODO: refactor with cache impl? it has the same merge logic 
/*    * Dirty hack to set the environment variables using reflection code. This method is for testing   * purposes only and should not be used elsewhere    */
//  TODO: Decorelation of subquery should be done before attempting   Partition Pruning; otherwise Expression evaluation may try to execute   corelated sub query. 
/*    * TODO: 1) isSamplingPred 2) sampleDesc 3) isSortedFilter    */
// @TODO it will be nice to refactor it 
// todo: this should not throw  todo: this should take "comment" as parameter to set in CC_META_INFO to provide some context for the failure 
//  TODO: replace this with a Map? 
//  TODO: remove some of these fields as needed? 
//  TODO: this is not valid. Function names for built-in UDFs are specified in   FunctionRegistry, and only happen to match annotations. For user UDFs, the   name is what user specifies at creation time (annotation can be absent, 
//  TODO: verify that this is correct 
//  TODO: we could try to get the declaring object and infer argument... stupid Java. 
//  close&destroy is used in seq coupling most of the time - the difference is either not clear; or not relevant - remove? 
// todo: this doesn;t check if compaction is already running (even though Initiator does but we 
//  This is a little bit weird. We'll do the MS call outside of the lock. Our caller calls us   under lock, so we'd preserve the lock state for them; their finally block will release the 
// todo: we really need some comments to explain exactly why each of these is removed 
//  ### FIXME: this is broken for multi-line SQL 
// Not implemented 
//  Slice boundaries may not match split boundaries due to torn rows in either direction,   so this counter may not be consistent with splits. This is also why we increment   requested bytes here, instead of based on the split - we don't want the metrics to be   inconsistent with each other. No matter what we determine here, at least we'll account   for both in the same manner. 
//  This is kinda hacky - we "know" these are LlaSerDeDataBuffer-s. 
//  TODO: perhaps can be made more efficient by creating a byte[] directly 
// this is not strictly accurate, but 'type' cannot be null. 
//  TODO: for non columnar we don't need to do this... might as well update all stats. 
//  TODO: this should ideally not create AddPartitionDesc per partition 
//  TODO: should we also whitelist input formats here? from mapred.input.format.class 
//  Not public since we must have the deserialize read object. 
//  TODO: checking 2 children is useless, compare already does that. 
//  TODO: why does the original code not just use _dataStream that it passes in as stream? 
//  This is a bogus hack because it copies the contents of the SQL file   intended for creating derby databases, and thus will inexorably get   out of date with it.  I'm open to any suggestions on how to make this   read the file in a build friendly way. 
//  This if/else chain looks ugly in the inner loop, but given that it will be 100% the same   for a given operator branch prediction should work quite nicely on it.   RecordUpdateer expects to get the actual row, not a serialized version of it.  Thus we 
// TODO this has to find a better home, it's also hardcoded as default in hive would be nice 
//  @deprecated in favour of {@link HCatTable.#sortCols(ArrayList<Order>)}. To be removed in Hive 0.16. 
//  TODO? add upstream? 
//  TODO: Should have a check on the server side. Embedded metastore throws   NullPointerException, remote throws MetaException 
//  load the list of DP partitions and return the list of partition specs   TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions   to use Utilities.getFullDPSpecs() to get the list of full partSpecs.   After that check the number of DPs created to not exceed the limit and   iterate over it and call loadPartition() here.   The reason we don't do inside HIVE-1361 is the latter is large and we 
//  There are 3 options for this ConditionalTask:   1) Merge the partitions   2) Move the partitions (i.e. don't merge the partitions)   3) Merge some partitions and move other partitions (i.e. merge some partitions and don't   merge others) in this case the merge is done first followed by the move to prevent   conflicts.   TODO: if we are not dealing with concatenate DDL, we should not create a merge+move path 
/*    * TODO: this would be more flexible doing a SQL select statement rather than using InputFormat directly   * see {@link org.apache.hive.hcatalog.streaming.TestStreaming#checkDataWritten2(Path, long, long, int, String, String...)}   * @param numSplitsExpected   * @return   * @throws Exception    */
//  @deprecated in favour of {@link HCatTable.#getDbName()}. To be removed in Hive 0.16. 
//  TODO HIVE-16134. Differentiate between EXTERNAL_PREEMPTION_WAITQUEU vs EXTERNAL_PREEMPTION_FINISHABLE? 
//  TODO: pointless 
//  This is kind of not pretty, but this is how we detect whether buffer was cached.   We would always set this for lookups at put time. 
//  TODO: move these test parameters to more specific places... there's no need to have them here 
//  TODO: fs checksum only available on hdfs, need to         find a solution for other fs (eg, local fs, s3, etc) 
//  FIXME: possible alternative: move both OpSignature/OpTreeSignature into   under some class as nested ones; and that way this factory level caching can be made "transparent" 
//  Neither "expired" nor "olderThan" criteria selected. This better not be an attempt to delete tokens. 
//  FIXME: Hadoop3 made the incompatible change for dfs.client.datanode-restart.timeout   while spark2 is still using Hadoop2.   Spark requires Hive to support Hadoop3 first then Spark can start   working on Hadoop3 support. Remove this after Spark supports Hadoop3. 
/*    * Variables used by LLAP daemons.   * TODO: Eventually auto-populate this based on prefixes. The conf variables   * will need to be renamed for this.    */
//  TODO: setFileMetadata could just create schema. Called in two places; clean up later. 
//  @deprecated in favour of {@link HCatTable.#comment()}. To be removed in Hive 0.16. 
//  this is workaround for hadoop-17 - libjars are not added to classpath of the 
/* todo: we need some sort of validation phase over original AST to make things user friendly; for example, if     original command refers to a column that doesn't exist, this will be caught when processing the rewritten query but     the errors will point at locations that the user can't map to anything     - VALUES clause must have the same number of values as target table (including partition cols).  Part cols go last in Select clause of Insert as Select     todo: do we care to preserve comments in original SQL?     todo: check if identifiers are propertly escaped/quoted in the generated SQL - it's currently inconsistent      Look at UnparseTranslator.addIdentifierTranslation() - it does unescape + unparse...     todo: consider "WHEN NOT MATCHED BY SOURCE THEN UPDATE SET TargetTable.Col1 = SourceTable.Col1 "; what happens when source is empty?  This should be a runtime error - maybe not      the outer side of ROJ is empty => the join produces 0 rows.  If supporting WHEN NOT MATCHED BY SOURCE, then this should be a runtime error     */
//  TODO: wtf? This doesn't do anything. 
//  TODO: why is this needed? 
/*  * NOTE: this whole logic is replicated from Calcite's RelDecorrelator *  and is exteneded to make it suitable for HIVE *    We should get rid of this and replace it with Calcite's RelDecorrelator *    once that works with Join, Project etc instead of Join, Project. *    At this point this has differed from Calcite's version significantly so cannot *    get rid of this. * * RelDecorrelator replaces all correlated expressions (corExp) in a relational * expression (RelNode) tree with non-correlated expressions that are produced * from joining the RelNode that produces the corExp with the RelNode that * references it. * * <p>TODO:</p> * <ul> *   <li>replace {@code CorelMap} constructor parameter with a RelNode *   <li>make {@link #currentRel} immutable (would require a fresh *      RelDecorrelator for each node being decorrelated)</li> *   <li>make fields of {@code CorelMap} immutable</li> *   <li>make sub-class rules static, and have them create their own *   de-correlator</li> * </ul>  */
//  @deprecated in favour of {@link HCatTable.#serdeParam(Map<String, String>)}.   To be removed in Hive 0.16. 
//  CONCERN: Leaking scratch column? 
//  TODO: this is an ugly hack; see the same in LlapTaskCommunicator for discussion. 
//  Derby and Oracle do not interpret filters ANSI-properly in some cases and need a workaround. 
//  TODO : currently not testing the following scenarios:     a) Multi-db wh-level REPL LOAD - need to add that     b) Insert into tables - quite a few cases need to be enumerated there, including dyn adds. 
/*    * TODO: 1. PPD needs to get pushed in to TS   *   * @param scanRel   * @return    */
//  2. Convert NONACIDORCTBL to ACID table.  //todo: remove trans_prop after HIVE-17089 
//  FIXME: possibly the distinction between table/partition is not need; however it was like this before....will change it later 
//  don't take directories into account for quick stats TODO: wtf? 
/*    * Fixup the children and parents of a new vector child.   *   * 1) Add new vector child to the vector parent's children list.   *   * 2) Copy and fixup the parent list of the original child instead of just assuming a 1:1   *    relationship.   *   *    a) When the child is MapJoinOperator, it will have an extra parent HashTableDummyOperator   *       for the MapJoinOperator's small table.  It needs to be fixed up, too.    */
//  TODO: The copy of data is unnecessary, but there is no work-around 
/*    * Get the list of partitions that need to update statistics.   * TODO: we should reuse the Partitions generated at compile time   * since getting the list of partitions is quite expensive.   *   * @return a list of partitions that need to update statistics.   * @throws HiveException    */
//  TODO: This test should be removed once ACID tables replication is supported. 
//  TODO: there is no easy and reliable way to compute the memory used by the executor threads and on-heap cache. 
//  TODO: normally, the result is not necessary; might make sense to pass false 
//  Remove new-alloc flag on first use. Full unlock after that would imply force-discarding   this buffer is acceptable. This is kind of an ugly compact between the cache and us. 
//  Only attempt to do this, if cmd was successful.   FIXME: it would be probably better to move this to an after-execution 
//  TODO Cleanup pending tasks etc, so that the next dag is not affected. 
//  TODO: does this include partition columns? 
//            work on BytesColumnVector output columns??? 
//  Two ReduceSinkOperators are correlated means that   they have same sorting columns (key columns), same partitioning columns,   same sorting orders, and no conflict on the numbers of reducers.   TODO: we should relax this condition   TODO: we need to handle aggregation functions with distinct keyword. In this case,   distinct columns will be added to the key columns. 
//  FIXME: move TestJsonSerDe from hcat to serde2 
//  TODO Confirm this is safe 
//  TODO: execute errors like this currently don't return good error 
//  THIS IS NOT WORKING workaround is to set it as part of java opts -Duser.timezone="UTC" 
//  simply get the next day and go back half a day. This is not ideal but seems to work. 
//  TODO: maybe we should throw this as-is too. ThriftCLIService currently catches Exception,         so the combination determines what would kill the HS2 executor thread. For now,         let's only allow OOM to propagate. 
//  TODO May be possible to do finer grained locks. 
//  TODO: this check is somewhat bogus as the maxJvmMemory != Xmx parameters (see annotation in LlapServiceDriver) 
//  TODO: Check if maximum size compatible with AbsoluteKeyOffset.maxSize. 
//  @deprecated in favour of {@link HCatTable.#escapeChar()}. 
//  If the sorted columns is a superset of bucketed columns, store this fact.   It can be later used to   optimize some group-by queries. Note that, the order does not matter as   long as it in the first 
//  FIXME : replace with hive copy once that is copied 
//  TODO: revisit the fence 
//  TODO: 1) How to handle collisions? 2) Should we be cloning ColumnInfo or not? 
//  Optimize the scenario when there are no grouping keys and no distinct - 2   map-reduce jobs are not needed 
//  Join key expression is likely some expression involving functions/operators, so there   is no actual table column for this. But the ReduceSink operator should still have an   output column corresponding to this expression, using the columnInternalName.   TODO: does tableAlias matter for this kind of expression? 
//  TODO: why does Tez API use "Object" for this? 
//  TODO: this log statement looks wrong 
//  Relying on a task succeeding to reset the exponent.   There's no notifications on whether a task gets accepted or not. That would be ideal to   reset this. 
//  We need to override these methods due to difference in nullability between Hive and   Calcite for the return types of the aggregation  (in particular, for COUNT and SUM0).   TODO: We should close the semantics gaps between Hive and Calcite for nullability of   aggregation calls return types. This might be useful to trigger some additional   rewriting rules that would remove unnecessary predicates, etc. 
//  Cancel job if the monitor found job submission timeout.   TODO: If the timeout is because of lack of resources in the cluster, we should   ideally also cancel the app request here. But w/o facilities from Spark or YARN,   it's difficult to do it on hive side alone. See HIVE-12650. 
//  TODO: Should be checked on server side. On Embedded metastore it throws   NullPointerException, on Remote metastore it throws TTransportException 
//  TODO: do we ever need the port? we could just do away with nodeId altogether. 
/*    * Metastore related options that the db is initialized against. When a conf   * var in this is list is changed, the metastore instance for the CLI will   * be recreated so that the change will take effect.   * TODO - I suspect the vast majority of these don't need to be here.  But it requires testing   * before just pulling them out.    */
//  Originals split won't work due to MAPREDUCE-7086 issue in FileInputFormat. 
//  TODO: VectorizedParquetRecordReader doesn't support map, array now, the value of   ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR should be updated after support these data   types. 
//  TODO: if there are more fields perhaps there should be an array of class. 
//  Some columns in select are pruned. This may happen if those are constants.   TODO: the best solution is to hook the operator before fs with the select operator.    See smb_mapjoin_20.q for more details.  
//  TODO: getUserFromAuthenticator? 
//  TODO: this seems wrong (following what Hive Regular does) 
//  TODO: we're asking the metastore what its configuration for this var is - we may   want to revisit to pull from client side instead. The reason I have it this way   is because the metastore is more likely to have a reasonable config for this than   an arbitrary client. 
//  TODO: can this result in cross-thread reuse of session state? 
//  This condition-check could have been avoided, but to honour the old   default of not calling if it wasn't set, we retain that behaviour.   TODO:cleanup after verification that the outer if isn't really needed here 
//  TODO support only non nested case 
//  @deprecated in favour of {@link HCatTable.#mapKeysTerminatedBy()}. To be removed in Hive 0.16. 
//  TODO: get rid of this 
// todo: this is not remotely accurate if you have many (relevant) original files 
//  TODO: we could remove extra copy for isUncompressed case by copying directly to cache. 
//  TODO HIVE-11687 It's possible for a bunch of tasks to come in around the same time, without the   actual executor threads picking up any work. This will lead to unnecessary rejection of tasks. 
//  conflict when loaded. Some issue with framework which needs to be relook into later. 
//  @deprecated in favour of {@link HCatTable.#fileFormat()}. To be removed in Hive 0.16. 
//  dangerous; let's explicitly add an incomplete CB. 
//  This is not a complete list, barely make information schema work 
//  TODO: perhaps we should also summarize the triggers pointing to invalid pools. 
//  TODO: could we instead get FS from path here and add normal files for every UGI? 
//  We cannot get to root TableScan operator, likely because there is a join or group-by   between topOp and root TableScan operator. We don't handle that case, and simply return 
//  REVIEW:  are we supposed to be applying the getReadColumnIDs 
/*    * todo: this should accept a file of table names to exclude from non-acid to acid conversion   * todo: change script comments to a preamble instead of a footer   *   * how does rename script work?  "hadoop fs -mv oldname newname"    * and what what about S3?   * How does this actually get executed?   * all other actions are done via embedded JDBC   *   *    */
//  TODO: the global lock might be to coarse here. 
// todo: enums? that have both field name and value list 
//  if the result is not boolean and not all partition agree on the   result, we don't remove the condition. Potentially, it can miss   the case like "where ds % 3 == 1 or ds % 3 == 2"   TODO: handle this case by making result vector to handle all   constant values. 
//  Need to remove this static hack. But this is the way currently to get a session. 
//  TODO : code section copied over from SerDeUtils because of non-standard json production there   should use quotes for all field names. We should fix this there, and then remove this copy.   See http://jackson.codehaus.org/1.7.3/javadoc/org/codehaus/jackson/JsonParser.Feature.html#ALLOW_UNQUOTED_FIELD_NAMES   for details - trying to enable Jackson to ignore that doesn't seem to work(compilation failure   when attempting to use that feature, so having to change the production itself. 
//  Assume we should have the exact same object.   TODO: we could also compare the schema and SerDe, and pass only those to the call         instead; most of the time these would be the same and LLAP IO can handle that. 
// todo: HIVE-15549  SORT_PARTITION_EDGE 
//  TODO - Allow the branch to be specified as a parameter to ptest, rather than requiring a separate property file. 
//  Note that the tableExists flag as used by Auth is kinda a hack and   assumes only 1 table will ever be imported - this assumption is broken by   REPL LOAD.     However, we've not chosen to expand this to a map of tables/etc, since   we have expanded how auth works with REPL DUMP / REPL LOAD to simply   require ADMIN privileges, rather than checking each object, which   quickly becomes untenable, and even more so, costly on memory. 
//  TODO Change after HIVE-9987. For now, there's no rack matching. 
/*    * Only a small set of operations is allowed inside an explicit transactions, e.g. DML on   * Acid tables or ops w/o persistent side effects like USE DATABASE, SHOW TABLES, etc so   * that rollback is meaningful   * todo: mark all operations appropriately    */
// todo: once multistatement txns are supported, add a test to run next 2 statements in a single txn 
//  TODO HiveQueryId extraction by parsing the Processor payload is ugly. This can be improved   once TEZ-2672 is fixed. 
//  UNDONE: Why don't these methods take decimalPlaces? 
/*     * Serializes decimal64 up to the maximum 64-bit precision (18 decimal digits).    *    * NOTE: Major assumption: the fast decimal has already been bounds checked and a least    * has a precision <= DECIMAL64_DECIMAL_DIGITS.  We do not bounds check here for better    * performance.     */
//  TODO: Refactor this and do in a more object oriented manner 
//  Some data is missing from the stream for PPD uncompressed read (because index offset is   relative to the entire stream and we only read part of stream if RGs are filtered; unlike   with compressed data where PPD only filters CBs, so we always get full CB, and index offset   is relative to CB). To take care of the case when UncompressedStream goes seeking around by   its incorrect (relative to partial stream) index offset, we will increase the length by our   offset-relative-to-the-stream, and also account for it in buffers (see createDiskRangeInfo).   So, index offset now works; as long as noone seeks into this data before the RG (why would   they), everything works. This is hacky... Stream shouldn't depend on having all the data. 
//  TODO HIVE-15865 Ideally sort these by completion time, once that is available. 
/*    * Generate a temporary path for dynamic partition pruning in Spark branch   * TODO: no longer need this if we use accumulator!   * @param basePath   * @param id   * @return    */
//  TODO: probably temporary before HIVE-13698; after that we may create one per session. 
//  FIXME manager's EndOfBatch threadlocal can be deleted 
//  TODO: most of the time, there's no in-memory. Use an array? 
//  TODO: could we tell the policy that we don't care about these and have them evicted? or we         could just deallocate them when unlocked, and free memory + handle that in eviction.         For now, just abandon the blocks - eventually, they'll get evicted. 
//  A single concurrent request per node is currently hardcoded. The node includes a port number   so different AMs on the same host count as different nodes; we only have one request type,   and it is not useful to send more than one in parallel. 
//  TODO MS-SPLIT Switch this back once HiveMetaStoreClient is moved.  req.setCapabilities(HiveMetaStoreClient.TEST_VERSION); 
//  @deprecated in favour of {@link HCatTable.#getTableType()}. To be removed in Hive 0.16. 
//  Currently MAP type is not supported. Add it back when Arrow 1.0 is released. 
//  TODO: why CAS if the result is not checked? 
//  TODO: Shouldn't we propgate vc? is it vc col from tab or all vc 
//  TODO: readEncodedColumns is not supposed to throw; errors should be propagated thru   consumer. It is potentially holding locked buffers, and must perform its own cleanup.   Also, currently readEncodedColumns is not stoppable. The consumer will discard the   data it receives for one stripe. We could probably interrupt it, if it checked that. 
//  REVIEW jvs 29-Oct-2007:  Shouldn't it also be incorporating   the flavor attribute into the description? 
//  Requirements: for Bucket, bucketed by their keys on both sides and fitting in memory   Obtain number of buckets  TODO: Incase of non bucketed splits would be computed based on data size/max part size 
//  Note: later we may be able to set multiple things together (except LIKE). 
//  May be null in tests   TODO: see javadoc 
//  XXX: makes no sense for me... possibly not needed anymore 
//        So, the indices should line up... to be fixed in SE v2? 
/*     * The method for altering table props; may set the table to MM, non-MM, or not affect MM.    * todo: All such validation logic should be TransactionValidationListener    * @param tbl object image before alter table command (or null if not retrieved yet).    * @param props prop values set in this alter table command     */
//  TODO : verify if skipping charset here is okay 
//  TODO: support dynamic partition for CTAS 
//  TODO :  Some extra validation can also be added as this is a user provided parameter. 
//  todo: add time of abort, which is not currently tracked. 
//  TODO: dump the end if wrapping around? 
//  TODO: Currently ignores GBY and PTF which may also buffer data in memory. 
//  The queryId could either be picked up from the current request being processed, or   generated. The current request isn't exactly correct since the query is 'done' once we   return the results. Generating a new one has the added benefit of working once this   is moved out of a UDTF into a proper API.   Setting this to the generated AppId which is unique.   Despite the differences in TaskSpec, the vertex spec should be the same. 
//  Workaround for DN bug on Postgres:   http://www.datanucleus.org/servlet/forum/viewthread_thread,7985_offset 
//  If necessary, divide and multiply to get rid of fractional digits. 
// TODO: use common thread pool later? 
//  register all permanent functions. need improvement 
//  TODO: perhaps this could use a better implementation... for now even the Hive query result         set doesn't support this, so assume the user knows what he's doing when calling us. 
//  This is our problem -- it means the configuration was wrong. 
//  to be used by clients of ServiceRegistry TODO: this is unnecessary 
//  TODO: should this also handle ACID operation, etc.? seems to miss a lot of stuff from HIF. 
//  TODO Will these checks work if some other user logs in. Isn't a doAs check required somewhere here as well.   Should a doAs check happen here instead of after the user test.   With HiveServer2 - who is the incoming user in terms of UGI (the hive user itself, or the user who actually submitted the query) 
//  TODO: get rid of deepCopy after making sure callers don't use references 
//  TODO: this is fishy - we init object inspectors based on first tag. We         should either init for each tag, or if rowInspector doesn't really         matter, then we can create this in ctor and get rid of firstRow. 
//  Snapshot was outdated when locks were acquired, hence regenerate context,   txn list and retry   TODO: Lock acquisition should be moved before analyze, this is a bit hackish.   Currently, we acquire a snapshot, we compile the query wrt that snapshot,   and then, we acquire locks. If snapshot is still valid, we continue as usual. 
//  TODO: option to allow converting ORC file to insert-only transactional? 
//  TODO: calculate this instead. Just because we're writing to the location doesn't mean that it'll   always be wanted in the meta store right away. 
//  TODO: propagate this error to TezJobMonitor somehow? Without using killQuery 
//  All users belong to public role implicitly, add that role   TODO MS-SPLIT Change this back to HiveMetaStore.PUBLIC once HiveMetaStore has moved to   stand-alone metastore.  MRole publicRole = new MRole(HiveMetaStore.PUBLIC, 0, HiveMetaStore.PUBLIC); 
/*    * Create a bare TableSnapshotRegionSplit. Needed because Writables require a   * default-constructed instance to hydrate from the DataInput.   *   * TODO: remove once HBASE-11555 is fixed.    */
//  TODO: do better with handling types of Exception here 
//  todo: returns json string. should recreate object from it? 
//  TODO: in these methods, do we really need to deepcopy? 
//  TODO: numTasksToPreempt is currently always 1. 
//  TODO Optimization: Add a check to see if there's any capacity available. No point in 
//  Wait a while for existing tasks to terminate   XXX this will wait forever... :) 
//  todo replace below with joda-time, which supports timezone 
//  TODO: why is this inconsistent with what we get by names? 
// Not implemented. 
// TODO this should be returning a class not just an int 
//  there are 2 or more distincts, or distinct is not on count   TODO: may be the same count(distinct key), count(distinct key)   TODO: deal with duplicate count distinct key 
//  id - TODO: use a non-zero index to check for offset errors. 
//  @deprecated in favour of {@link HCatTable.#comment(String)}. To be removed in Hive 0.16. 
//  TODO:pc need to enhance this with complex fields and getType_all function 
//  Hive adds the same mapping twice... I wish we could fix stuff like that. 
// todo: this should be moved to be an inner class of ReaderWrite as that is the only place it    is used 
//  HarFileSystem has a bug where this method does not work properly   if the underlying FS is HDFS. See MAPREDUCE-1877 for more   information. This method is from FileSystem. 
// TODO: Clean up/refactor assumptions 
//  TODO This executor seems unnecessary. Here and TezChild 
//  TODO: Verify GB having is not a seperate filter (if so we shouldn't   introduce derived table) 
/*  This is the TestPerformance Cli Driver for integrating performance regression tests as part of the Hive Unit tests. Currently this includes support for : 1. Running explain plans for TPCDS workload (non-partitioned dataset)  on 30TB scaleset. TODO : 1. Support for partitioned data set 2. Use HBase Metastore instead of DerbyThis suite differs from TestCliDriver w.r.t the fact that we modify the underlying metastoredatabase to reflect the dataset before running the queries. */
//  Hack note - different split strategies return differently typed lists, yay Java.   This works purely by magic, because we know which strategy produces which type. 
//  TODO: deriveExplainAttributes should be called here, code is too fragile to move it around. 
/*  * Stores binary key/value in sorted manner to get top-n key/value * TODO: rename to TopNHeap?  */
//  FIXME: isNull is not updated; which might cause problems 
//  TODO: Should we use grpbyExprNDesc.getTypeInfo()? what if expr is   UDF 
//  TODO: we should really probably throw. Keep the existing logic for now. 
/*  * It's column level Parquet reader which is used to read a batch of records for a list column. * TODO Currently List type only support non nested case.  */
//  TODO: not clear why we don't do the rest of the cleanup if dagClient is not created.         E.g. jobClose will be called if we fail after dagClient creation but no before... 
// TODO HIVE-16862: Implement a similar feature like "hive.tez.dynamic.semijoin.reduction" in hive on spark 
//  TODO: Currently we do not expose any runtime info for non-streaming tables.   In future extend this add more information regarding table status.   e.g. Total size of segments in druid, loadstatus of table on historical nodes etc. 
//  This is rather obscure. The end of last row cached is precisely at the split end offset.   If the split is in the middle of the file, LRR would read one more row after that,   therefore as unfortunate as it is, we have to do a one-row read. However, for that to   have happened, someone should have supplied a split that ends inside the last row, i.e.   a few bytes earlier than the current split, which is pretty unlikely. What is more likely   is that the split, and the last row, both end at the end of file. Check for this. 
//  TODO: this should also happen on any error. Right now this task will just fail. 
//  TODO null can also mean that this operation was interrupted. Should we really try to re-create the session in that case ? 
//  TODO: Should we wait for the entry to actually be deleted from HDFS? Would have to   poll the reader count, waiting for it to reach 0, at which point cleanup should occur. 
//  FIXME for ctas this is still needed because location is not set sometimes 
//  TODO Consolidate this code with TezChild. 
//  TODO: is this check even needed given what the caller checks? 
//  FIXME: file paths in strings should be changed to either File or Path ... anything but String 
//  TODO: why is "&& !isAcidIUDoperation" needed here? 
//  Get all simple fields for partitions and related objects, which we can map one-on-one.   We will do this in 2 queries to use different existing indices for each one.   We do not get table and DB name, assuming they are the same as we are using to filter.   TODO: We might want to tune the indexes instead. With current ones MySQL performs   poorly, esp. with 'order by' w/o index on large tables, even if the number of actual   results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by   just adding a \"PART_ID\" IN (...) filter that doesn't alter the results to it, probably 
//  Tez session relies on a threadlocal for open... If we are on some non-session thread,   just use the same SessionState we used for the initial sessions.   Technically, given that all pool sessions are initially based on this state, shoudln't   we also set this at all times and not rely on an external session stuff? We should   probably just get rid of the thread local usage in TezSessionState. 
//  hive input format doesn't handle the special condition of no paths + 1   split correctly. 
//     not external itself. Is that the case? Why? 
//  TODO: this is a stopgap fix. We really need to change all mappings by unique node ID,         or at least (in this case) track the latest unique ID for LlapNode and retry all 
//  FIXME: druid storage handler relies on query.id to maintain some staging directories   expose queryid to session level 
//  TODO - we need to speed this up for the normal path where all partitions are under   the table and we don't have to stat every partition 
//  if this function is frequently used, we need to optimize this. 
//  This is ugly in two ways...   1) We assume that LlapWrappableInputFormatInterface has NullWritable as first parameter.      Since we are using Java and not, say, a programming language, there's no way to check.   2) We ignore the fact that 2nd arg is completely incompatible (VRB -> Writable), because      vectorization currently works by magic, getting VRB from IF with non-VRB value param.   So we just cast blindly and hope for the best (which is obviously what happens). 
//  TODO: two possible improvements         1) Right now we kill all the queries here; we could just kill -qpDelta.         2) After the queries are killed queued queries would take their place.            If we could somehow restart queries we could instead put them at the front 
//  This method is inefficient. It's only used when something crosses buffer boundaries. 
//  TODO: why doesn't this check class name rather than toString? 
//  TODO: Modify Thrift IDL to generate export stage if needed 
//  Workaround for testing since tests can't set the env vars. 
//  TODO: the memory release could be optimized - we could release original buffers after we         are fully done with each original buffer from disk. For now release all at the end;         it doesn't increase the total amount of memory we hold, just the duration a bit.         This is much simpler - we can just remember original ranges after reading them, and         release them at the end. In a few cases where it's easy to determine that a buffer         can be freed in advance, we remove it from the map. 
//  TODO: perhaps add counters for separate things and multiple buffer cases. 
//  TODO: this might only be applicable to TezSessionPoolManager; try moving it there? 
//  TODO CAT - a number of these need to be updated.  Don't bother with deprecated methods as   this is just an internal class.  Wait until we're ready to move all the catalog stuff up   into ql. 
//  in case of outer joins, we need to pull in records from the sides we still   need to produce output for apart from the big table. for e.g. full outer join   TODO: this reproduces the logic of the loop that was here before, assuming 
//  TODO: not clear why this check and skipSeek are needed. 
//  TODO: is this a safe assumption (name collision, external names...) 
//  TODO: Cast Function in Calcite have a bug where it infer type on cast throws 
// @TODO FIX this, we actually do not need this anymore, 
//  @deprecated in favour of {@link HCatTable.#getTableName()}. To be removed in Hive 0.16. 
//  Based on UserGroupInformation::createProxyUser.   TODO: use a proper method after we can depend on HADOOP-13081. 
//  TODO : isn't there a prior impl of an isDirectory utility PathFilter so users don't have to write their own? 
//  get rid of trivial case first, so that we can safely assume non-null 
//  get the tmp URI path; it will be a hdfs path if not local mode   TODO [MM gap?]: this doesn't work, however this is MR only.        The path for writer and reader mismatch:        Dump the side-table for tag ... -local-10004/HashTable-Stage-1/MapJoin-a-00-(ds%3D2008-04-08)mm_2.hashtable        Load back 1 hashtable file      -local-10004/HashTable-Stage-1/MapJoin-a-00-srcsortbucket3outof4.txt.hashtable 
//  TODO: this duplicates a method in ORC, but the method should actually be here. 
//  TODO: will this work correctly with ACID? 
//  filter columns may have -1 as index which could be partition column in SARG.   TODO: should this then be >=? 
//  TODO: this interface is ugly. The two implementations are so far apart feature-wise 
//  @deprecated in favour of {@link HCatTable.#linesTerminatedBy()}. To be removed in Hive 0.16. 
//  validate is false by default if we enable the constraint   TODO: A constraint like NOT NULL could be enabled using ALTER but VALIDATE remains    false in such cases. Ideally VALIDATE should be set to true to validate existing data 
//  TODO: this needs to be enhanced once change management based filesystem is implemented 
// todo: it would be nice to check the contents of the files... could use orc.FileDump - it has   methods to print to a supplied stream but those are package private 
/*      * Why no null and class checks?     * With the new design a WindowingSpec must contain a WindowFunctionSpec.     * todo: cleanup datastructs.      */
/*      * RW lock ensures we have a consistent view of the file data, which is important given that     * we generate "stripe" boundaries arbitrarily. Reading buffer data itself doesn't require     * that this lock is held; however, everything else in stripes list does.     * TODO: make more granular? We only care that each one reader sees consistent boundaries.     *       So, we could shallow-copy the stripes list, then have individual locks inside each.      */
//  TODO: wtf? 
//  TODO Avoid reading this from the environment 
//  TODO: we wish we could cache the Hive object, but it's not thread safe, and each         threadlocal we "cache" would need to be reinitialized for every query. This is         a huge PITA. Hive object will be cached internally, but the compat check will be         done every time inside get(). 
//  TODO: can we merge neighboring splits? So we don't init so many readers. 
//  Don't break! We might find a better match later. 
//  No stats exist for this key; add a new object to the cache   TODO: get rid of deepCopy after making sure callers don't use references 
//  TODO: in the current impl, triggers are added to RP. For tez, no pool triggers (mapping between trigger name and   pool name) will exist which means all triggers applies to tez. For LLAP, pool triggers has to exist for attaching   triggers to specific pools.   For usability,   Provide a way for triggers sharing/inheritance possibly with following modes   ONLY - only to pool   INHERIT - child pools inherit from parent 
//  TODO: Using 0 might be wrong; might need to walk down to find the 
//  TODO: this should come through RelBuilder to the constructor as opposed to 
//  TODO : the contains message check is fragile, we should refactor SemanticException to be   queriable for error code, and not simply have a message   NOTE : IF_EXISTS might also want to invoke this, but there's a good possibility   that IF_EXISTS is stricter about table existence, and applies only to the ptn.   Therefore, ignoring IF_EXISTS here. 
//  TODO : we were discussing an iter interface, and also a LazyTuple   change this when plans for that solidifies. 
//  TODO: add method to UDFBridge to say if it is a cast func 
//  TODO Unregister the task for state updates, which could in turn unregister the node. 
//  Note: we assume 0-length split is correct given now LRR interprets offsets (reading an   extra row). Should we instead assume 1+ chars and add 1 for isUnfortunate? 
//  TODO Maybe add the YARN URL for the app. 
//  Note: this particular bit will not work for MM tables, as there can be multiple         directories for different MM IDs. We could put the path here that would account         for the current MM ID being written, but it will not guarantee that other MM IDs         have the correct buckets. The existing code discards the inferred data when the 
//  TODO: not having aliases for path usually means some bug. Should it give up? 
//  TODO Get rid of MapOutputInfo if possible 
//  TODO: this object is created once to call one method and then immediately destroyed.         So it's basically just a roundabout way to pass arguments to a static method. Simplify? 
//  TODO This should be passed in the TaskAttemptContext instead 
//  FIXME: use a different exception type? 
/*      * This is little complicated.  First we look for our own config values on this.  If those     * aren't set we use the Hive ones.  But Hive also has multiple ways to do this, so we need to     * look in both of theirs as well.  We can't use theirs directly because they wrap the     * codahale reporters in their own and we do not.      */
// TODO should replace with BytesWritable.copyData() once Hive  removes support for the Hadoop 0.20 series. 
//  If the import statement specified that we're importing to an external   table, we seem to be doing the following:      a) We don't allow replacement in an unpartitioned pre-existing table      b) We don't allow replacement in a partitioned pre-existing table where that table is external   TODO : Does this simply mean we don't allow replacement in external tables if they already exist?      If so(i.e. the check is superfluous and wrong), this can be a simpler check. If not, then      what we seem to be saying is that the only case we allow is to allow an IMPORT into an EXTERNAL      table in the statement, if a destination partitioned table exists, so long as it is actually 
// todo: handle Insert Overwrite as well: HIVE-18154 
//  TODO HIVE-15865: Include information about pending requests, and last   allocation time once YARN Service provides this information. 
//  TODO: At this point we don't know the slot number of the requested host, so can't rollover to next available 
//  will this be true here?   Don't create a new object if we are already out of memory 
//  TODO Replace this with a ExceptionHandler / ShutdownHook 
//  NOTE: this can be called outside of HS2, without calling setupPool. Basically it should be         able to handle not being initialized. Perhaps we should get rid of the instance and 
/*    * RowResolver of outer query. This is used to resolve co-rrelated columns in Filter   * TODO:   *  this currently will only be able to resolve reference to parent query's column   *  this will not work for references to grand-parent column    */
//  TODO use stripe statistics to jump over stripes 
//  @deprecated in favour of {@link HCatTable.#fileFormat(String)}. To be removed in Hive 0.16. 
//  TODO: Why is this needed (doesn't represent any cols) 
//  TODO: this is invalid for SMB. Keep this for now for legacy reasons. See the other overload. 
//  TODO: fix this 
//  This probably should not happen; but it does... at least also stop the consumer. 
//  This seems like a very wrong implementation. 
// todo: should make abortTxns() write something into TXNS.TXN_META_INFO about this 
//  registry again just in case. TODO: maybe we should enforce that. 
//  TODO: lossy conversion, distance is considered seconds similar to timestamp 
//  TODO: This is fraught with peril. 
//  TODO Not the best way to share the address 
/*  * This maps a split (path + offset) to an index based on the number of locations provided. * * If locations do not change across jobs, the intention is to map the same split to the same node. * * A big problem is when nodes change (added, removed, temporarily removed and re-added) etc. That changes * the number of locations / position of locations - and will cause the cache to be almost completely invalidated. * * TODO: Support for consistent hashing when combining the split location generator and the ServiceRegistry. *  */
//  The list is empty.   Too many concurrent operations; spurious failure.   List is drained and recreated concurrently.   Same for the OTHER list; spurious.   TODO: the fact that concurrent re-creation of other list necessitates full stop is not         ideal... the reason is that the list NOT being re-created still uses the list         being re-created for boundary check; it needs the old value of the other marker.         However, NO_DELTA means the other marker was already set to a new value. For now,         assume concurrent re-creation is rare and the gap before commit is tiny. 
//  The record count from these counters may not be correct if the input vertex has   edges to more than one vertex, since this value counts the records going to all   destination vertices. 
//  TODO: why is this copy-pasted from HiveInputFormat? 
//  negative length should take precedence over positive value? 
//  TODO: use DiskRangeList instead 
// todo: shouldn't ignoreEmptyFiles be set based on ExecutionEngine? 
//  All ErrorAndSolutions that ErrorHeuristic has generated. For the same error, they   should be the same though it's possible that different file paths etc 
//  Different paths if running locally vs a remote fileSystem. Ideally this difference should not exist. 
// todo: should this not be passed in the c'tor? 
//  Currently, deserialization of complex types is not supported 
//  TODO: what about partitions not in the default location? 
//  TODO: we currently put task info everywhere before we submit it and know the "real" node id.         Therefore, we are going to store this separately. Ideally, we should roll uniqueness 
//  It would be nice if OI could return typeInfo... 
// TODO replace IgnoreKeyTextOutputFormat with a  HiveOutputFormatWrapper in StorageHandler 
//  @deprecated in favour of {@link HCatTable.#getNumBuckets()}. 
// TODO This line can be removed once precommit jenkins jobs move to Java 8 
//  Recovery is not implemented yet for PPD path. 
//  ShimLoader.getHadoopShims().isSecurityEnabled() will only check that   hadoopAuth is not simple, it does not guarantee it is kerberos 
//  Find the class that has this method.   Note that Method.getDeclaringClass() may not work here because the method 
//  @deprecated in favour of {@link HCatAddPartitionDesc.#create(HCatPartition)}. To be removed in Hive 0.16. 
//  fs.permission.AccessControlException removed by HADOOP-11356, but Hive users on older   Hadoop versions may still see this exception .. have to reference by name. 
// this is dumb. HiveOperation is not always set. see HIVE-16447/HIVE-16443 
//  TODO: Once HIVE-18948 is in, should be able to retrieve writeIdList from the conf.  cachedWriteIdList = AcidUtils.getValidTxnWriteIdList(conf);   
//  This is a non-pool session, get rid of it. 
//  TODO: remove this 
//  Not implemented 
//  TODO: we only ever use one row of these at a time. Why do we need to cache multiple? 
/*    * getDeserializer   *   * Get the Deserializer for a table.   *   * @param conf   *          - hadoop config   * @param table   *          the table   * @return   *   Returns instantiated deserializer by looking up class name of deserializer stored in   *   storage descriptor of passed in table. Also, initializes the deserializer with schema   *   of table.   * @exception MetaException   *              if any problems instantiating the Deserializer   *   *              todo - this should move somewhere into serde.jar   *    */
//  Not pretty, but we need a way to get the size 
//  See ctor comment.   TODO: we should get rid of this 
//  in addition to that Druid allow numeric dimensions now so this check is not accurate 
//  converted = true; // [TODO]: should we check & convert type to String and set it to true? 
//  TODO: ideally we should make a special form of insert overwrite so that we:         1) Could use fast merge path for ORC and RC.         2) Didn't have to create a table. 
//  TODO: policy on deserialization errors 
//  TODO: Do we need to keep track of RR, ColNameToPosMap for every op or 
// Creating new QueryState unfortunately causes all .q.out to change - do this in a separate ticket  Sharing QueryState between generating the plan and executing the query seems bad 
//  TODO: we currently pass null counters because this doesn't use LlapRecordReader.         Create counters for non-elevator-using fragments also? 
//  TODO: Not sure that this is the correct behavior. It doesn't make sense to create the   partition with column with invalid type. This should be investigated later. 
//  todo: use SemanticAnalyzer::genExprNodeDesc   currently SA not available to PTFTranslator. 
//  TODO: we might as well kill the AM at this point. How do we do that from here? 
//  bitsets can't be correctly serialized by Kryo's default serializer 
//  TODO Ideally move some of the other cleanup code from resetCurrentDag over here 
/*  * TODO:pc remove application logic to a separate interface.  */
// todo: should this be OK for MM table? 
//  TODO: this is a limitation of the AST rewriting approach that we will   not be able to overcome till proper integration of full multi-insert   queries with Calcite is implemented.   The current rewriting gather references from insert clauses and then   updates them with the new subquery references. However, if insert   clauses use * or tab.*, we cannot resolve the columns that we are   referring to. Thus, we just bail out and those queries will not be   currently optimized by Calcite.   An example of such query is:   FROM T_A a LEFT JOIN T_B b ON a.id = b.id   INSERT OVERWRITE TABLE join_result_1   SELECT a.*, b.*   INSERT OVERWRITE TABLE join_result_3   SELECT a.*, b.*; 
//  TODO: delete tables/databases? 
// TODO: Improve this 
//  TODO: this only applies to current thread, so it's not useful at all. 
//  Tracks tasks which could not be allocated immediately.   Tasks are tracked in the order requests come in, at different priority levels.   TODO HIVE-13538 For tasks at the same priority level, it may be worth attempting to schedule tasks with 
//  FIXME: current objective is to keep the previous outputs...but this is possibly bad.. 
//  Assume the IO is enabled on the daemon by default. We cannot reasonably check it here. 
// todo: since HIVE-16669 is not done, Minor compact compacts insert delta as well - it should not  Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected2[i][1])); 
/*    * Send dropped partition notifications. Subscribers can receive these notifications for a   * particular table by listening on a topic named "dbName.tableName" with message selector   * string {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_EVENT} =   * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_DROP_PARTITION_EVENT}.   * </br>   * TODO: DataNucleus 2.0.3, currently used by the HiveMetaStore for persistence, has been   * found to throw NPE when serializing objects that contain null. For this reason we override   * some fields in the StorageDescriptor of this notification. This should be fixed after   * HIVE-2084 "Upgrade datanucleus from 2.0.3 to 3.0.1" is resolved.    */
/*  A nice error message should be given to user.  */
//  Is smbJoin possible? We need correct order 
//  A lot of these methods could be done more efficiently by operating on the Text value   directly, rather than converting to HiveChar. 
//  it's only for GBY which should forward all values associated with the key in the range   of limit. new value should be attatched with the key but in current implementation,   only one values is allowed. with map-aggreagtion which is true by default,   this is not common case, so just forward new key/value and forget that (todo) 
//  ### REVIEW: oops, somebody left the last command   unterminated; should we fix it for them or complain?   For now be nice and fix it. 
/*    * Checks whether a given url is in a valid format.   *   * The current uri format is: jdbc:hive://[host[:port]]   *   * jdbc:hive:// - run in embedded mode jdbc:hive://localhost - connect to   * localhost default port (10000) jdbc:hive://localhost:5050 - connect to   * localhost port 5050   *   * TODO: - write a better regex. - decide on uri format    */
//  Don't fail -- would be better to actually compute a range of [90,+inf) 
// todo: is this pre or post upgrade?  todo: can different tables be in different FileSystems? 
//  Bloating partinfo with inputJobInfo is not good 
//  We do not need to generate the QB again, but rather we use it directly 
//  TODO: ideally we'd register TezCounters here, but it seems impossible before registerTask. 
//  TODO: We have a cache for Table objects in SemanticAnalyzer::getTableObjectByName()   We need to move that cache elsewhere and use it from places like this. 
//  TODO Change the indexCache to be a guava loading cache, rather than a custom implementation. 
//  todo: fix comment as of HIVE-14988 
//  TODO: some SessionState internals are not thread safe. The compile-time internals are synced         via session-scope or global compile lock. The run-time internals work by magic!         They probably work because races are relatively unlikely and few tools run parallel         queries from the same session.         1) OperationState should be refactored out of SessionState, and made thread-local. 
//  FIXME extract the right info type 
//  TODO: should this be configurable via annotation or extending @RunWith annotation? 
//  Needs more explanation here   Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction   from this, to get actual usable memory before it goes into GC 
//  TODO: in union20.q the tab alias is not properly propagated down the   operator tree. This happens when UNION ALL is used as sub query. Hence, even   if column statistics are available, the tab alias will be null which will fail   to get proper column statistics. For now assume, worst case in which   denominator is 2. 
//  Optional feature not implemented 
//  TODO, we don't support this, but we should, since users may create an empty partition and   then load data into it. 
//  TODO: backward compat for Hive <= 0.12. Can be removed later. 
//  @deprecated in favour of {@link HCatTable.#storageHandler(String)}. To be removed in Hive 0.16. 
//  FIXME : implement consolidateEvent(..) similar to dumpEvent(ev,evRoot) 
//  TODO: Support checking multiple child operators to merge further. 
//  According to calcite, it is going to be removed before Calcite-2.0   TODO: to handle CorrelationId 
//  TODO: if there's versioning/etc., it will come in here. For now we rely on external         locking or ordering of calls. This should potentially return a Future for that. 
// todo: should this be done for MM?  is it ok to use CombineHiveInputFormat with MM? 
//  Settings borrowed from TestJdbcWithMiniHS2 
//  Used for sending notifications about a vertex completed. For canFinish 
//  which should be interpreted in Instant semantics 
//  Unwrap the bag. 
//  Initialize dfs 
//  Piggybacking in Import logic for now 
//  This version of Hadoop does not support FileSystem.access(). 
//  Go over the associated fields and look up the dependencies 
/*          * Single-Column Long specific repeated lookup.          */
//  If all BigTable input columns to key expressions are isRepeating, then   calculate key once; lookup once. 
// JDK 1.7 
//  Don't fail the query just because of any lineage issue. 
//  This shouldn't really happen on a byte array. 
//  add new filter op 
// execute in sync mode 
//  add plugin module jars on demand 
//    <Cleanup> <VectorExprArgType> 
//  Update the lastInputFile with the currentInputFile. 
//  attach the predicate and group by to the return clause 
//  SORT_COLS 
// SqlStdOperatorTable.COUNT, 
//  This should just generate one strategy with splits for base and insert_deltas. 
//  The following 2 methods are for java serialization use only. 
//  normal case, convert all parameters 
//  Thread to generate the separate rows beside the normal thread. 
//  -1 day 10 hrs 11 mins 172800 secs = -1 day 10 hrs 11 mins + 2 days = 1 day 10 hrs 11 mins 
//  only second stripes will satisfy condition and hence single split 
//  How much we can write to current write buffer, out of what we need. 
// -----------------------------------------------------------------------------------------------   Comparison methods.  ----------------------------------------------------------------------------------------------- 
//  given a subquery it checks to see what is the aggegate function 
/*    * number of rows received.    */
// llap IO summary 
//  metadata 
//  If the predicate is on a numeric column, and it specifies an   open range e.g. key < 20 , we do not support conversion, as negative   values are lexicographically stored after positive values and thus they   would be returned. 
//  submit few queries 
// however, allow the partition comments to change 
//  Add spark.hadoop prefix for yarn properties as SparkConf only accept properties   started with spark prefix, Spark would remove spark.hadoop prefix lately and add   it to its hadoop configuration. 
//  Only fetch the table if we actually have a listener 
//  worst case 
//  Technically, for ifNotExists case, we could insert one and discard the other   because the first one now "exists", but it seems better to report the problem   upstream as such a command doesn't make sense. 
//  key * 265 
//  Buddy block is free and in the same free list we have locked. Take it out for merge. 
//  Now that we have made sure that the argument is of primitive type, we can get the primitive   category 
//  Interrupted. 
//  return size 
//    Rewrite the above plan:     CorrelateRel(left correlation, condition = true)     LeftInputRel     Project-A (a RexNode)       Aggregate (groupby(0), agg0(), agg1()...)         Project-B (references coVar)           RightInputRel (no correlated reference)   
//  number n of elements   average of x elements   average of y elements   n times the covariance 
// it happens when using -f and the line of cmds does not end with ; 
//  Make sure zero trimming doesn't extend into the integer digits. 
//  traverse all the joins and convert them if necessary 
//  rc will be 1 at this point indicating failure. 
// 'state' should not be null - future proofing 
// link lockId to queryId 
//  replace each of the position alias in ORDERBY with the actual column 
// do nothing by default 
//  Note: this sets LoadFileType incorrectly for ACID; is that relevant for import?         See setLoadFileType and setIsAcidIow calls elsewhere for an example. 
//  Describe how to serialize data out to user script 
//  two dots?? 
//  make sure MAP task environment points to HADOOP_CREDSTORE_PASSWORD 
//  stop in reserve order of start 
/*      * 1. setup resolve, make connections      */
//  Update job state with the childJob id 
//  Exists aleady. 
//  As this partition (partdate=2008-01-01/partcity=london) is the only   partition under (partdate=2008-01-01) 
//  Verify. 
//  can the mapjoin present be converted to a bucketed mapjoin 
//  skip Druid properties which are used in DruidSerde, since they are also updated   after SerDeInfo properties are copied. 
//  Note that permanent functions can only be properly checked from the session registry.   If permanent functions are read from the metastore during Hive initialization,   the JARs are not loaded for the UDFs during that time and so Hive is unable to instantiate   the UDf classes to add to the persistent functions set.   Once a permanent UDF has been referenced in a session its FunctionInfo should be registered   in the session registry (and persistent set updated), so it can be looked up there. 
//  middle item 
// if there is WriteEntity with WriteType=UPDATE/DELETE for target table, replace it with  WriteEntity for each partition 
//  The underlying SSLSocket object is bound to host:port with the given SO_TIMEOUT 
//  Since we want to display all the met and not met conditions in EXPLAIN, we determine all   information first.... 
// partition root 
//  If a new MoveWork was created, then we should link all dependent tasks from the MoveWork to link. 
//  Note that the code updating the state of the task does it when it's out of the queue.   So, the priorities in the queue should be correct; if the top task is not killable then   no task in the queue would be killable. 
/*    * Create a new grouping key for grouping id.   * A dummy grouping id. is added. At runtime, the group by operator   * creates 'n' rows per input row, where 'n' is the number of grouping sets.    */
// because 'local' inpath doesn't delete source files 
//  deal with dynamic partition columns: convert ExprNodeDesc type to String?? 
//  Try 2-arg version 
//  replacing any previous mapping from old input). 
//  sales.txt will need to be added as a resource. 
/*  This is an illustration, not a functioning example.  */
//  There should be 1 new directory: base_0000001. 
//  remainder is always positive 
//  when converting from char, the value should be stripped of any trailing spaces. 
// check if May 23, 1968 is Julian Day 2440000 
//  This should not start the actual Tez AM. 
//  for each dir, get the InputFormat, and do getSplits. 
// safety valve for extreme cases 
//  Note that we have 3 regex'es below 
//  In case of unions or map-joins, it is possible that the file has   already been seen. 
//  In theory this should not happen - if the original copy of the UDF had this   data, we should be able to set the UDF copy with this same settableData. 
//  Parse validTxnList 
//  inputs.add(new ReadEntity(partn)); // is this needed at all? 
// test table without db portion 
//  Record the final counters. 
//  Stop HiveServer2 
/*  first_name = 'alan'   */
/*          * if skipNulls is true and there are no rows in valueChain => all rows         * in partition are null so far; so add null in o/p          */
//  Expect query to be successfully completed now 
//  if old view has partitions, it could not be replaced 
// Oracle specific parser 
//  Base case. 
// Asynch write completed  Up the semaphore 
/*    * Override this method if you want to customize how the node dumps out its   * children.    */
// e.g. analyze table page_view partition(dt='10/15/2014',country=US) 
//  Does same thing with LazyFactory#createLazyObjectInspector except that this replaces   original keyOI with OI which is create by HBaseKeyFactory provided by serde property for hbase 
//  We dont set perms or groups for default dir. 
//  Setup timer task to check for hearbeat timeouts 
// start concurrent txn 
//  Also, throws IOException when Binary is detected. 
//  Some nasty examples that show how S3 log format is broken ... and to   test the regex   These are all sourced from genuine S3 logs   Text sample = new   Text("04ff331638adc13885d6c42059584deabbdeabcd55bf0bee491172a79a87b196 img.zemanta.com [09/Apr/2009:22:00:07 +0000] 190.225.84.114 65a011a29cdf8ec533ec3d1ccaae921c F4FC3FEAD8C00024 REST.GET.OBJECT pixy.gif \"GET /pixy.gif?x-id=23d25db1-160b-48bb-a932-e7dc1e88c321 HTTP/1.1\" 304 - - 828 3 - \"http://www.viamujer.com/2009/03/horoscopo-acuario-abril-mayo-y-junio-2009/\" \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\"");   Text sample = new   Text("04ff331638adc13885d6c42059584deabbdeabcd55bf0bee491172a79a87b196 img.zemanta.com [09/Apr/2009:22:19:49 +0000] 60.28.204.7 65a011a29cdf8ec533ec3d1ccaae921c 7D87B6835125671E REST.GET.OBJECT pixy.gif \"GET /pixy.gif?x-id=b50a4544-938b-4a63-992c-721d1a644b28 HTTP/1.1\" 200 - 828 828 4 3 \"\" \"ZhuaXia.com\"");   Text sample = new   Text("04ff331638adc13885d6c42059584deabbdeabcd55bf0bee491172a79a87b196 static.zemanta.com [09/Apr/2009:23:12:39 +0000] 65.94.12.181 65a011a29cdf8ec533ec3d1ccaae921c EEE6FFE9B9F9EA29 REST.HEAD.OBJECT readside/loader.js%22+defer%3D%22defer \"HEAD /readside/loader.js\"+defer=\"defer HTTP/1.0\" 403 AccessDenied 231 - 7 - \"-\" \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)\""); 
//  Call the function 
//     LOG.info("har location : " + harLocn); 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#executeUpdate(java.lang.String, java.lang.String[])    */
/*      * A WindowFrame may contain just the Start Boundary or in the     * between style of expressing a WindowFrame both boundaries     * are specified.      */
//  The partitioning columns of the child RS are more specific than   those of the parent RS. 
//  Sample the first batch processed for variable sizes. 
//  Phase 1 - create objects. 
//  update for "{\"writeid\":0,\"bucketid\":536936448,\"rowid\":1}\t60\t80" 
//  count (1) 
// case 2: 23:59:59.999999999 
//  Fail - "transactional" is set to true, and the table is bucketed, but doesn't use ORC 
//  Use the expressions from Reduce Sink. 
//  want the values which are not skewed 
//  Only specified nodes of these types will be walked.   Empty set means all the nodes will be walked. 
//  Valid txn list might be generated for a query compiled using this   command, thus we need to reset it 
//  run in the vectorized mode. 
//  At this point. one p=2 task and task3(p=1) running. Ask for another p1 task. 
//  Write files inside the sub-directory. 
//  If it is a sub-directory, then recursively list the files. 
// for thread safe 
//  Otherwise, we add to the join specs 
//  If the target hash table is on disk, spill this row to disk as well to be processed later 
// this is a contrived example, in practice this query would of course fail after drop table 
//  It is a view 
//  Not propagated. 
//  Create the final Reduce Sink Operator 
//  From https://msdn.microsoft.com/en-us/library/ms190476.aspx   e1 / e2   Precision: p1 - s1 + s2 + max(6, s1 + p2 + 1)   Scale: max(6, s1 + p2 + 1) 
//  Let the Processor control start for Broadcast inputs. 
//  since we are adding the user name to the scratch dir, we do not   need to give more permissions here 
//  If the schema version is already checked, then go ahead and use this metastore 
//  6.1 child can be EXPR AS ALIAS, or EXPR. 
//  leftInputRel contains unique keys   i.e. each row is distinct and can group by on all the left 
//  Store the inputs in a HashMap since we can't get a ReadEntity from inputs since it is   implemented as a set.ReadEntity is used as the key so that the HashMap has the same behavior   of equals and hashCode 
//  If this operator has been visited already by the rule,   we do not need to apply the optimization 
//  the oldSplit may be null during the split phase 
//  First create the archive in a tmp dir so that if the job fails, the 
//  Close the session before we have to wait again. 
// deltes can't be raw format 
//  OPERATION_ID 
//  short-circuited on client-side, verifying that it's an empty object, not null 
//  Read the newly added db via CachedStore 
/*        * Multi-Key specific declarations.        */
//  The number of files for the table should be same as number of buckets. 
//  get the join keys from parent ReduceSink operators 
//  whether all files are not sufficient to reach sizeLeft 
//  Not in VRB mode - the new cache data is ready, we should use it. 
//  Directory removal will be handled by cleanup at the SessionState level. 
//  Use only 1 reducer if no reduce keys 
//  Only set when setting up the secure config for ZK. 
//  REPL LOAD 
//  Make room for remainder. 
//  Resolve storage handler (if any) 
//  Restore default cost model 
//  Test that input name does not change IOContext returned, and that each thread gets its own. 
//  Ignore exceptions from destroy 
//  this is always non-null when conversion is completed 
//  True if both are null 
// Utility methods used to store pairs of ints as long. 
//  is the nullIndicator 
//  for group-by, values with same key on top-K should be forwarded  flag used to control how TopN handled for PTF/Windowing partitions. 
// get default value to be be stored in metastore 
//  Add a new key or add a value to an existing key? 
//  This one can be pushed 
//  there is already a predicate on this src. 
//  1. OB Expr sanity test   in strict mode, in the presence of order by, limit must be 
//  adding DP ColumnInfo to the RowSchema signature 
//  There are two areas of exploration for optimization here.   1.  We're serializing the schema with every object.  If we assume the schema       provided by the table is always correct, we don't need to do this and       and can just send the serialized bytes.   2.  We serialize/deserialize to/from bytes immediately.  We may save some       time but doing this lazily, but until there's evidence this is useful, 
//  Get input data 
//  Master node will serialize readercontext and will make it available  at slaves. 
/*  256 files x 1000 size for 1 splits  */
// arrowAllocatorLimit is ignored if an allocator was previously created 
//  Is the grouping sets data consumed in the current in MR job, or 
// completion of txnid:txnIdSelect 
//  Write a value to the column vector, and return back the byte buffer used. 
//  When reading the file for first time we get the orc tail from the orc reader and cache it   in the footer cache. Subsequent requests will get the orc tail from the cache (if file   length and modification time is not changed) and populate the split info. If the split info   object contains the orc tail from the cache then we can skip creating orc reader avoiding   filesystem calls. 
//  Obviously different expressions. 
//  Assert the actual stack traces are exactly equal to the written ones,    and are contained in "stackTraces" list in the submission order: 
//  map from newInput 
//  retain only the largest value for a register index 
//  Probe space should be at least equal to the size of our designated wbSize 
//  Skewed columns stuff. 
//  key * 21 
//  1. deal with non-partition columns 
//  dummy instantiation to make sure any static/ctor code blocks of that   driver are loaded and ready to go. 
//  the name of the dag is what is displayed in the AM/Job UI 
//  Note that this is not real double hashing since we have consistent hash on top. 
//  Can't vectorize 
//  Read the value and key length for the first record. 
//  List of conf entries not to turn into env vars 
//  if limit is greater than available rows then do not update   statistics 
/*  this is the bad part - the vectorized UDF returns the right result  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setAsciiStream(java.lang.String,   * java.io.InputStream)    */
//  An actual list, deser its values 
//  create new agg function calls and rest of project list together 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setFetchSize(int)    */
//  partition walker working on 
//  instantiate new factory instance only if current one is not valid. 
//  extra columns is difference between referenced columns vs needed   columns. The difference could be partition columns. 
//  check if this user has grant privileges for this privileges on this   object 
/*        * for each predicate:       * - does it refer to one or many aliases       * - if one: add it to the filterForPushing list of that alias       * - if many: add as a filter from merging trees.        */
//  Cannot entries while we currently hold read lock, so keep track of them to delete later. 
//  Definitely a short; most shorts fall here 
//  Use the serialization scale and format the string with trailing zeroes (or   round the decimal) if necessary. 
//  Report the row if its the first row 
//  Serialize using the SerDe, then below deserialize using DeserializeRead. 
//  Original hashcode, with 0-d low bits. 
//  Return an evaluator depending on the return type 
//  We assume each partition has an unique SD. 
//  FIELD_SCHEMAS 
//  Check the output of FixAcidKeyIndex - it should indicate the index was fixed. 
// nothing to do 
//  Export is trivially undoable - in that nothing needs doing to undo it. 
//  this returns the row as is because this formatter is only called when   the ThriftJDBCBinarySerDe was used to serialize the rows to thrift-able objects. 
//  Serialization ctor. 
//  Query that should fetch one column 
//  Set the default to 0 -- if the created date is null, there was 
//  referenced by the current expression node. 
//  Descriptor is not defined because it takes variable number of arguments with different   data types. 
// note that in ToolRunner this is expected to be a local FS path  see GenericOptionsParser.getLibJars() 
//  generate a selection operator for group-by keys only 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setDate(java.lang.String, java.sql.Date)    */
//  Change the type back to int so that the same alter can be attempted from connection 2. 
//  2/ test LazyBinaryMap 
//  This will turn on setugi on both client and server processes of the test. 
//  leave it for GC to clean up 
//  the last block that add() should append to   the current block where the cursor is in 
//  Regrettable that we have to wrap the IOException into a RuntimeException,   but throwing the exception is the appropriate result here, and hasNext()   signature will only allow RuntimeExceptions. Iterator.hasNext() really   should have allowed IOExceptions 
//  Verify that addNotificationEvent() updates the NotificationEvent with the new event ID 
//  spacing 
// give up 
//  extract partition spec to file name part from path 
//  Retrieve all partitions generated from partition pruner and partition 
//  2. Test with doAs=true 
//  validate database 
//  If we're merging to the same location, we can avoid some metastore calls 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareStatement(java.lang.String)    */
// LOG.debug("Running Hive Query: "+ sql); 
//  The location is input and table is output for alter-table add partitions 
//  Sort the n-gram list by frequencies in descending order 
//  In this case we should expect the test to have failed at the very last read() check. 
//  PROGRESSED_PERCENTAGE 
//  Only offer these when the input file format is not the fast vectorized formats. 
//  In that case, it will be a Select, but the rowOI need not be amended 
//  Null-safe isSame 
//  this path is intermediate data 
//  Start the server 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTimestamp(int, java.util.Calendar)    */
//  If we are caching the MV, we include it in the cache 
//  We use longs because we don't have unsigned ints 
//  The input column can either be a string or a list of integer values. 
//  Integer is too large -- cannot recover by trimming fractional digits. 
//  if ast has two children   the 2nd child could be partition spec or columnName 
//  typically alter table .. concatenate is run on only one partition/one table, 
//  first call FileUtils.mkdir to make sure that destf directory exists, if not, it creates 
//  add the new op as the old 
//  this is the case when we have a map-side SMB join   one input of the join is treated as a dummy vertex 
//  IS_SET_SCHEDULING_POLICY 
//  we need to count the current one as a map table then. 
//  -33440539101030154945490585226577271520 is expected result 
//  IS_SET_QUERY_PARALLELISM 
//  use the short user name for the request 
//  Changes the owner to a role and verify the change 
//  partial test init 
//  optional tblName was specified. 
//  update stats, but don't update NDV as it will not change 
//  for every sampled alias, we figure out splits to be sampled and add   them to return list 
//  Cleanup the synthetic predicate in the tablescan operator and filter by   replacing it with "true" 
//  Test that 2 exclusive partition locks coalesce to one 
//  HiveServer2 global init file location 
/* Example code to test specific scenarios:    LowLevelCacheImpl cache = new LowLevelCacheImpl(        LlapDaemonCacheMetrics.create("test", "1"), new DummyCachePolicy(),        new DummyAllocator(), true, -1); // no cleanup thread    final int FILE = 1;    cache.putFileData(FILE, gaps(3756206, 4261729, 7294767, 7547564), fbs(3), 0, Priority.NORMAL, null, null);    cache.putFileData(FILE, gaps(7790545, 11051556), fbs(1), 0, Priority.NORMAL, null, null);    cache.putFileData(FILE, gaps(11864971, 11912961, 13350968, 13393630), fbs(3), 0, Priority.NORMAL, null, null);    DiskRangeList dr = dr(3756206, 7313562);    MutateHelper mh = new MutateHelper(dr);    dr = dr.insertAfter(dr(7790545, 11051556));    dr = dr.insertAfter(dr(11864971, 13393630));    BooleanRef g = new BooleanRef();    dr = cache.getFileData(FILE, mh.next, 0, testFactory, null, g); */
//  Go through the root tasks, and verify the input format of the map reduce task(s) is   HiveSortedInputFormat 
//  bucket is the second field in the record id 
//  Update the info of SEL operator based on the pruned reordered columns 
//  In case cols are null 
//  Get the UDTF Path 
//  in alter-table-add-partition, the table is output, and location is input 
//  try non-null path 
//  Get the value length. 
//  makes spilling prediction (isMemoryFull) to be too defensive which results in unnecessary spilling 
//  since no division has occurred, don't format with a decimal point 
//  Build the bitset with not null columns 
//  Don't set inputs and outputs - the locks have already been taken so it's pointless. 
//  requests before it is started. 
//  create the local work for this plan 
//  Needed until there is no junit release with @BeforeParam, @AfterParam (junit 4.13)   https://github.com/junit-team/junit4/commit/1bf8438b65858565dbb64736bfe13aae9cfc1b5a   Then we should remove our own copy 
//  Empty values except first column 
/*    * add array<struct> to the list of columns    */
//  This also resets SessionState.get. 
//  get the create table statement for the table and populate the output 
//  replace each of the position alias in ORDERBY with the actual column name, 
//  Get columnNames from the first parent 
//  Create the walker, the rules dispatcher and the context. 
//  check the ObjectInspector 
//  HiveParser.identifier | HiveParse.KW_IF | HiveParse.KW_LEFT |   HiveParse.KW_RIGHT 
//  Shutdown existing session manager 
//  no hits on the index so return a no match range 
//  This will be used by hadoop only in unsecure(/non kerberos) mode 
//  null values 
//  Create the context for the walker 
//  Build the type property string if not supplied 
//  whether it contains common join op; if contains, return this common join op 
/*    * Struct    */
// should never happen - just in case 
//  Add task to insert / delete materialized view from registry if needed 
//  If using async, we could also reserve one by one. 
// if here it must be delta_x_y - insert events only, so we must be compacting 
//  Connect parent to fileSinkOp 
//  run 5 sql inserts concurrently 
//  Test in binary mode 
/* getAcidState() is smart not to return any deltas in current if there is a base that covers    * them, i.e. if they were compacted but not yet cleaned.  This means re-checking if    * compaction is needed should cheap(er) */
//  It's possible that old metadata still refers to "decimal" as a column type w/o   precision/scale. In this case, the default (10,0) is assumed. Thus, do nothing here. 
//  In the case where agg is count(*) or count($corVar), it is   changed to count(nullIndicator).   Note:  any non-nullable field from the RHS can be used as   the indicator however a "true" field is added to the   projection list from the RHS for simplicity to avoid   searching for non-null fields.     Project-A' (all gby keys + rewritten nullable ProjExpr)     Aggregate (groupby(all left input refs),                   count(nullIndicator), other aggs...)       Project-B' (all left input refs plus                      the rewritten original projected exprs)         Join(replace corvar to input ref from LeftInputRel)           LeftInputRel           Project (everything from RightInputRel plus                       the nullIndicator "true")             RightInputRel   
//  If the new port is invalid, find one - starting with the default client port.   If the default client port is not specified, starting with a random port.   The random port is selected from the range between 49152 to 65535. These ports cannot be   registered with IANA and are intended for dynamic allocation (see http://bit.ly/dynports). 
// AND, NOT etc 
//  This position is an aggregation.   As we store in oneRow only the aggregate results, we need to adjust to the correct position   if there are keys in the GBy operator. 
//  Subtract from self. 
//  Set isNull before calls in case tney change their mind. 
// if maxRetries is 0 code retries until batch decays to zero 
//  We should fail if we try to get info out of the params 
//  execution to fall back to row mode. 
/*            * Optionally, read current value's big length.  {Big Value Len} {Big Value Bytes}           * Since this is the first record, the valueRefWord directs us.            */
//  Avro requires nullable types to be defined as unions of some type T   and NULL. This is annoying and we're going to hide it from the user. 
//  Verify the raw object that's been created 
//  we will create subquery expression of boolean type 
/*    * todo: fix https://issues.apache.org/jira/browse/HIVE-9995   * @throws Exception    */
//  This is a valid YARN Service name. Parse it out. 
//  Default value is empty string in which case no properties will be inherited. 
//  reset to the hive-site.xml values for following param 
//  Create the un-grouped splits 
//  Not using the gauge to avoid races. 
//  All data intitializations 
//  If any table is empty, an inner join involving the tables should yield 0 rows. 
//  keep the arguments for reference - we want all the non-numeric   arguments to be the same 
//  Trying using the cardinality from the value range. 
//  Go through all joins - it should only contain selects and filters between 
//  currently getProcedureColumns always returns an empty resultset for Hive 
//  can be null in-case of junit tests. skip reset.   reset thread name at release time. 
//  We skipped 'like', other ops should all work as long as the types are right. 
//  Arithmetic with a type date (LongColumnVector storing epoch days) and type interval_day_time (IntervalDayTimeColumnVector storing 
//  Convert the Accumulo token in a Hadoop token 
/*    * is this expr a UDAF invocation; does it imply windowing   * @return   * 0 if implies neither   * 1 if implies aggregation   * 2 if implies count   * 3 if implies windowing    */
// if here, txn was not found (in expected state) 
//  Change different fields and verify they were altered 
//  Create session domain if not present 
// put the new mapping 
//  create a fake directory to throw exception 
//  Assume db and table names are the same for all partition, as provided in arguments. 
/*  @bgen(jjtree) Extends  */
//  We found the Union 
//  2) Build conditions for join and filter and start adding 
/*  * An bytes key hash set optimized for vector map join. * * This is the abstract base for the multi-key and string bytes key hash set implementations.  */
//  An optional byte array of FastHiveDecimal.FAST_SCRATCH_BUFFER_LEN_BIG_INTEGER_BYTES. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setObject(int, java.lang.Object)    */
//  Table itself doesn't exist in metastore, nothing to validate. 
//  MSCK called to add missing paritions into metastore and there are   missing partitions. 
//  Cache pathExpr 
//  Ignore shutdown errors since there are negative tests 
//  Otherwise, if enabled, deserialize rows using regular Serde and add the object   inspect-able Object[] row to a VectorizedRowBatch in the VectorMapOperator. 
//  count number of digits in the value 
//  Note, here we create a fake directory along with fake files as original directories/files 
//  return zz for "xx + yy AS zz" 
//  Skip 
// private static final String testTableName = "auto_purge_test_table"; 
//  No deserialization of key(s) here -- just get reference to bytes. 
//  validate the metadata for the getColumns result set 
//  Bigger addition. 
//  first set back the backup task with its children task. 
//  authenticate using delegation tokens via the "DIGEST" mechanism 
//  write stats objs persistently 
//  TODO: plumb progress info thru the reader if we can get metadata from loader first. 
/*    * HiveMetaStore keys reserved for updating ListenerEvent parameters.   *   * HIVE_METASTORE_TRANSACTION_ACTIVE This key is used to check if a listener event is run inside a current   *                                   transaction. A boolean value is used for active (true) or no active (false).    */
//  4M 
//  Hadoop vars 
//  Boolean to store information about whether valid txn list was generated 
// need to determine if a different type is needed for dummy partitions 
//  ignore zero-divide cases 
//  replication scope allows replacement, and does not require empty directories 
//  @@protoc_insertion_point(class_scope:GroupInputSpecProto) 
//  This is the last time we'll see the Table objects for views, so add it to the inputs   now. isInsideView will tell if this view is embedded in another view. 
//  For bootstrap load, the create function should be always performed. 
//  Determine which rows are non matches by determining the delta between inputSelected and   (current) batch selected. 
//  Create the 'hive' catalog 
//  basePersistDirectory - use druid default, no need to be configured by user 
//  These represent the bucketed columns 
//  Reuse an existing connection 
/* note that will throw if Anonymous mode is not allowed & user.name is not in query string of the request;      * this ensures that in the context of WebHCat, PseudoAuthenticationHandler allows Anonymous even though      * WebHCat itself will throw if it can't figure out user.name */
//  No alphabet needed. 
/*      * Write a bunch of random rows that will be used for read benchmark.      */
//  (End user) Transaction timeout, in milliseconds. 
//  First entry.   Existence. 
//  Remove old temp table entry, and add new entry to list of temp tables. 
//  frequently occurring error 
//  Normalize   i.e., significand is even 
//  version with SEQ   version with RCF 
//  append the path substring since previous match 
//  Don't bother with aggregation in this case, it will probably be invalid. 
//  Failure / help 
//  optional bool is_guaranteed = 2; 
//  Second time. 
//  @@protoc_insertion_point(class_scope:QueryCompleteRequestProto) 
//  null :: for non - partitioned table. 
//  After all the rows are processed, continue to generate results for the rows that results haven't generated.   For the case: X following and Y following, process first Y-X results and then insert X nulls.   For the case X preceding and Y following, process Y results. 
//  Setting an empty collection of ranges will, unexpectedly, scan all data 
//  skip the big table pos 
//  first search from the posToVertex 
//  ppd for multi-insert query is not yet implemented   we assume that nothing can is pushed beyond this operator 
//  We haven't fixed the TMP path for this mapper yet 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#close()    */
//  Test that two different tables don't collide on their locks 
//  Note: this assumes both paths are qualified; which they are, currently. 
//  VarChar 
// lazy-caching of the version. 
//  Note that we use the same vectors in both batches. Clever, very clever. 
//  AST specific data 
//  2. Build Rel for where Clause 
//  Delete the temp file if the JVM terminate normally through Hadoop job   kill command.   Caveat: it won't be deleted if JVM is killed by 'kill -9'. 
//  Important, signum is given by the firstByte, not by byte[keep]! 
//  entry 2 will be null due to zero-divide 
// these have been localized already 
//  If lists, recursively compare the list element types 
//  clear out the union set. we don't need it anymore. 
//  Note: this is called by someone who has ensured the buffer is not going to be moved. 
//  Get the field objectInspector, fieldName and the field object. 
//  1. Get agg fn ret type in Calcite 
// --------------------------- PTF handling ----------------------------------- 
//  Update has failed. We won't try a low pri task. 
//  using vint instead of 4 bytes 
//  Create case sensitive columns list 
// needed by kyro 
//  Only set A = B command needs updating the configuration stored in client side. 
//  If the field that is passed in is NOT a primitive, and either the   field is not declared (no schema was given at initialization), or   the field is declared as a primitive in initialization, serialize   the data to JSON string. Otherwise serialize the data in the 
// assert !frame.corVarOutputPos.isEmpty(); 
//  Bucket count 
//           The first query happens to have 2 full batches. 
//  The primitives. 
//  Change the resource plan - resize B and C down, D up, and remove A remapping users to B.   Everything will be killed in A and B, C won't change, D will start one more query from   the queue, and the query queued in A will be re-queued in B and started.   The fractions will also all change. 
// test getLogicalLength() w/o side file 
//  It's ok to skip a failed message if the target has changed back to the old value. 
//  Check that the values in the older list are also in newer. Lists should already be sorted. 
/*  * SemanticException.java * * Created on April 1, 2008, 1:20 PM * * To change this template, choose Tools | Template Manager * and open the template in the editor.  */
//  Store token in the cache 
//  versions dont match, return false. 
//  The list of families that have been added to the scan 
//  empty maps 
//  Initialize the constants for the grouping sets, so that they can be re-used for 
//  Merge bytes 
/*  1 files x 100 size for 99 splits  */
//  to keep backward compatibility 
//  no files in the partition 
/*        * Repeating THEN expression?        */
//  generate map reduce plans 
//  SW.SR: Lock we are examining is shared read 
//  Trim the bins down to the correct number of bins. 
/*    * Lookup a byte array key in the hash map.   *   * @param keyBytes   *         A byte array containing the key within a range.   * @param keyStart   *         The offset the beginning of the key.   * @param keyLength   *         The length of the key.   * @param hashMapResult   *         The object to receive small table value(s) information on a MATCH.   *         Or, for SPILL, it has information on where to spill the big table row.   *   * @return   *         Whether the lookup was a match, no match, or spill (the partition with the key   *         is currently spilled).    */
//  Found an exact match 
//  We are missing a section at the end of the part... copy the start to non-cached. 
//  needs at least --instances 
//  4. Validate the effective Window Frames with the rules in {@link validateWindowFrame} 
//  partition level column statistics test 
//  This lock is used to make sure removalListener won't close a client that is being contemplated for returning by get() 
//  thread local filesystem stats is private and cannot be cloned. So make a copy to new class 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setHoldability(int)    */
//  Fully copy over the contents of the new SD into the old SD, 
// view just being created   already created view being loaded 
//  Each range checks for overflow first, then moves digits. 
/*      * @since 1.3.0     * This can be set to -1 to make the system generate old style (delta_xxxx_yyyy) file names.     * This is primarily needed for testing to make sure 1.3 code can still read files created     * by older code.  Also used by Comactor.      */
//  Now release another session; the thread that gave up on reuse can proceed. 
//  see autoColumnStats_2.q under TestMiniLlapLocalCliDriver 
//  Merge cost 
//  This currently only happens in tests. See getFileData comment on the interface. 
//        AM reg info changes; add notifications, ignore errors, and update alloc. 
// say table T has partition p, we are generating  select cardinality_violation(ROW_ID, p) WHERE ... GROUP BY ROW__ID, p 
//  there to avoid clashes. 
//  Extract the bits of num into bytes[] from right to left 
//  When we are doing row deserialization, these are the regular deserializer,   partition object inspector, and vector row assigner. 
//  optional int32 num_self_and_upstream_completed_tasks = 2; 
/*  several hive classes depend on the metastore APIs, which is not included             * in hive-exec.jar. These are the relatively safe ones - operators & io classes.              */
//  Revert the projected columns back, because vrg will be re-used. 
//  sure that NULL is produced. 
//  an object of Map 
//  Make a copy of the source-table, as would be done across class-loaders. 
//  If a child of this expression uses a materialized view,   then we decrease its cost by a certain factor. This is   useful for e.g. partial rewritings, where a part of plan   does not use the materialization, but we still want to   decrease its cost so it is chosen instead of the original   plan 
//  The BigDecimal class recommends not converting directly from double to BigDecimal,   so we convert through a string... 
//  Whatever remains.   Take toTake blocks by splitting the block at offset. 
//  NAME 
//  select *, constant and casts can be allowed without a threshold check 
// There were no more batches AND  this is either the first batch or we've used up the current batch buffer.  goto return false 
//  virtual columns needed 
//  If unable to find stats for this StatsType, return null so we can build stats 
//  acc is short for accumulator. It's used to build the row before forwarding 
//  rowId <= 'd' 
// change the location of position alias process here 
//  Keys are always primitive 
//  We have to evaluate the input format to see if vectorization is enabled, so   we do not set it right here. 
//  Update the cache to add this new aggregate node 
//  Find most significant bit with starting index as 0 
//  Just run our value expressions over input batch. 
//  test getBoolean rules on non-boolean columns 
//  block. 
//  Gather input works operators 
//  key/value of the index is removed. retrieve memory usage 
//  TODO: Convert to Semantic Exception 
//  First, try to use the blocks of half size in every arena. 
//  give the cloned work a different name 
// "originals" (written before table was converted to acid) is considered written by   writeid:0 which is always committed so there is no need to check wrt invalid write Ids  But originals written by Load Data for example can be in base_x or delta_x_x so we must  check if 'x' is committed or not evn if ROW_ID is not needed in the Operator pipeline. 
//  "." : FIELD Expression 
//  Unless this is overridden, it does nothing 
//  > 100 
//  Return something. 
//  given an ast node this method recursively goes over checkExpr ast. If it finds a node of type TOK_SUBQUERY_EXPR   it throws an error. 
//  under exceptional load, hadoop may not be able to look up status   of finished jobs (because it has purged them from memory). From   hive's perspective - it's equivalent to the job having failed.   So raise a meaningful exception 
//  4. Perform a major compaction 
//  -------------------------------- Second Pass ---------------------------------- //   Process operator tree in two steps: first we process the extra op trees generated   in the first pass. Then we process the main op tree, and the result task will depend 
// create scratch dir 
//  case when mv2.deptno IS NULL AND mv2.deptname IS NULL then s else source.s + mv2.s end 
/*              * Single-Column String outer get key.              */
//  Since left has a longer digit tail and it doesn't move; we will shift the right digits   as we do our addition into the result. 
//  we are in secure mode. 
//  to return to a previous block. 
//  The primitive object inspector of the source data type for any column being   converted.  Otherwise, null. 
//  print out all the stages that have no childStages. 
//  Assume. 
//  Make sure the token made it into the UGI 
//  HashStruct 
//  1) test with txn.Commit() 
//  Now finish task1, which will make capacity for task3 to run. Nothing is coming out of the delayed queue yet. 
//  reset the result for the evicted index 
// GenericUDFBridge udfBridge = (GenericUDFBridge) expr.getGenericUDF(); 
//  drop any functions before dropping db 
//  Apply the group and permissions to the leaf partition and files.   Need not bother in case of HDFS as permission is taken care of by setting UMask 
//  For cases where the table is temporary 
//  not an hbase row key. This should either be a prefix or an individual qualifier 
//  Tests that different threads get the same object per attempt per input, and different   between attempts/inputs; that attempt is inherited between threads; and that clearing   the attempt produces a different result. 
//  get privileges for this user and its role on this object 
//  Make certain the target directory exists. 
/*  the following test uses a query that returns a group and a user entry.       the ldap atn should use the groupMembershipKey to identify the users for the returned group       and the authentication should succeed for the users of that group as well as the lone user4 in this case     */
//  Pass the request on to the responder 
//  This should never be thrown. 
//  COLUMN_NAME 
//  14. Give our final state to UI/API requests if any. 
/*  We'll assume that there *may* be nulls in the input if !noNulls is true       * for an input vector. This is to be more forgiving of errors in loading       * the vectors. A properly-written vectorized iterator will make sure that       * isNull[0] is set if !noNulls and isRepeating are true for the vector.        */
//  wait for failover to close sessions 
/*        * For this particular file, how many columns will we actually read?        */
//  FOREIGN_KEY_COLS 
//  Second value. 
//  Verify SerializationUtils first. 
//  we pushed the whole thing down 
//  To avoid timing issues with notifications (and given that HDFS check is anyway the   authoritative one), don't wait infinitely for the notifier, just wait a little bit 
/*    * (non-Javadoc)   *   * @see   * org.apache.hadoop.hive.ql.exec.UDFMethodResolver#getEvalMethod(java.util   * .List)    */
//  time to actually run the dag (actual dag runtime) 
//  FKTABLE_NAME 
//  Find functions which name contains _to_find_ or _hidden_ in the default database 
//  Create argument capturer   a class variable cast to this generic of generic class 
//  only user belonging to admin role can drop existing role 
//  optional int32 key_id = 4; 
//  Expected to fail due to canceled token 
//  TODO: Make use of this config to configure fetch size 
// this should use HiveChar.getPaddedValue() but it's protected; currently (v0.13) 
//  need to create the correct table descriptor for key/value 
// exclude trailing comma 
//  2. Add Dist UDAF args to reduce keys 
//  Evaluate the key expressions of just this first batch to get the correct key. 
//  need to iterate through all children even if one is found to be not a   candidate   in case if the other children could be individually pushed up 
//  try to be more tolerant   if the input is invalid instead of incomplete, we'll hit exception here again 
/*  Cookies Manager is used to cache cookie returned by service.       The goal us to avoid doing KDC requests for every request. */
//  find all targets recursively 
//  Leverage the nice batching behaviour of async Loggers/Appenders:   we can signal the file manager that it needs to flush the buffer   to disk at the end of a batch.   From a user's point of view, this means that all log events are   _always_ available in the log file, without incurring the overhead   of immediateFlush=true. 
//  Exists primarily to allow for easier unit tests. 
//  register two tasks of same query but different am 
//  For completed instances 
//  print out the cbo info 
/*  It's valid case if a partition:  */
//  Finally we remove the expression from the tree 
//  these 5 delims passed as serde params 
//  Tracks all instances, including ones which have been disabled in the past. 
//  Login from the keytab 
//  Serialize the struct into a mutation 
//  init input 
//  INCLUDE_BITSET 
//  Prefer date type arguments over other method signatures 
/*    * It is a idempotent function to add various intermediate files as the source   * for the union. The plan has already been created.    */
//  IS_SET_DEFAULT_POOL_PATH 
// https://msdn.microsoft.com/en-us/library/ms189499.aspx  https://msdn.microsoft.com/en-us/library/ms187373.aspx 
//  Blindly add this as a union type containing int and double!   Should be sufficient for the test case. 
//  If the second operator has more than one child, we stop gathering 
//  If the filter is already on top of a TableScan, 
//  Find the first ancestor of this MoveTask which is some form of map reduce task   (Either standard, local, or a merge) 
//  1. Construct ExpressionNodeDesc representing Join Condition 
//  Array for the values to pass to evaluator. 
//  End of master thread state 
/*    * An implementation of KvSource that can handle key and value as BytesWritable objects.    */
//  Return a single ArrayList where the first element is the number of histogram bins,   and subsequent elements represent histogram (x,y) pairs. 
//  used for dynamic partitioning 
//  With the integer type range checking, we need to know the Hive data type. 
// abort all remaining txns 
/*  100 files x 1000 size for 11 splits  */
//  Close file streams to avoid resource leaking 
//  For now, this can simply be fetched from a single registry instance. 
//  If there is no "AS" clause, the output schema will be "key,value" 
//  10^16 
//  Note: other standard ones include e.g. ClientUser and ClientHostname,         but we don't need them for now. 
// get KeyValuesReaders from the LogicalInput and add them to priority queue 
//  Notify listeners of the changed value 
/*    * (non-Javadoc)   * we should ideally not modify the tree we traverse.   * However, since we need to walk the tree at any time when we modify the operator,   * we might as well do it here.    */
//  There should be 2 delta dirs, plus 2 base dirs in the location 
//  Now deprecated. 
//  Adjust the memory - we have to account for what we have just evicted. 
//  OWNER_TYPE 
//  hash table loading happens in server side, LlapDecider could kick out some fragments to run outside of LLAP.   Flip the flag at runtime in case if we are running outside of LLAP 
//  How many ways each block splits into target size.   How many target-sized blocks remain from last split.   The header index for the beginning of the remainder. 
//  A very simple counter to keep track of join entries for a key 
//  This is an operator - so check whether it is unary or binary operator 
// Divide operations are not CHECKED because the output is always of the type double 
//  only the pattern of "VALUE._col([0-9]+)" should be handled. 
//  4. Construct JoinLeafPredicateInfo 
//  For some strange reason BigDecimal 0 can have a scale.  We do not support that. 
// a convenience method that makes the intended owner for the delegation  token request the current user 
//  Also determine if any nulls are present since for a join that means no match. 
//  remove the context words from the end of the list 
//  stripeRgs should have been initialized by this time with an empty array. 
//  create the reloading folder to place jar files if not exist 
//  Look for Pass-Thru case where InputFileFormat has VectorizedInputFormatInterface   and reads VectorizedRowBatch as a "row". 
//  Fix up the case where parent expression's output data type physical variations is DECIMAL whereas   at least one of its children is DECIMAL_64. Some expressions like x % y for example only accepts DECIMAL   for x and y (at this time there is only DecimalColModuloDecimalColumn so both x and y has to be DECIMAL). 
// must get statementId from file name since Acid 1.0 doesn't write it into bucketProperty 
//  nulls on right, no nulls on left 
//  if ndvProduct is 0 then column stats state must be partial and we are missing 
//  validate that the set of partition columns found in custom path must match 
//  TODO: unpause fetching 
//  if copy of jar to change management fails we fail the metastore transaction, since the   user might delete the jars on HDFS externally after dropping the function, hence having 
//  fails, the finally clause will remove the lock 
//  we'll encode the absolute value (sign is separate) 
//  Java TimeZone has no mention of thread safety. Use thread local instance to be safe. 
//  [A: 1, B: 0] 
//  reset the array to null values 
//  This removes order-by only expressions from the projections. 
//  no partitions, bail early. 
//  Make the columns list for the temp table (input data file). 
//  InputStream open, if the given sequence file is broken) to RCFile 
//  In current implementation it will never happen, but we leave it   here to make the logic complete. 
//  Try to get the session quickly. 
//  10^15 
//  Here we do some query rewrite. We first get the new fetchRN, which is   a sum of offset and fetch.   We then push it through by creating a new branchSort with the new   fetchRN but no offset. 
//  optional string container_id_string = 5; 
//  Check the restricted configs that the users cannot set. 
//  Insert some data -> this will again generate only insert deltas and no delete deltas: delta_4_4 
//  Expected exception - Embedded MetaStore 
//  If the user has specified a location - external or not, check if the user 
//  We need to copy the data byte by byte only in case the   "outputLength < length" (which means there is at least one escaped 
//  If the view is Inside another view, it should have at least one parent 
//  Read database via CachedStore 
//  TEZ..) 
//  Generate the partition columns from the parent input 
//  Test incorrect totals. We don't normalize; just make sure we don't under- or overshoot. 
//  The table containing the partition is not yet loaded in cache 
//  collect the hiveConfList and HiveVarList separately so that they can be 
//  Job properties are only relevant for non-native tables, so   for native tables, leave it null to avoid cluttering up 
/*  Output of final result of the aggregation      */
//  USER_NAME 
//  create some "cover" to the result? 
//  Metastore always support concurrency, but certain ACID tests depend on this being set.  We 
//  Verify dropPartition recycle part files 
//  isRepeating, and no nulls 
/*  Move common logic to PrunerUtils.walkExprTree(...) so that it can be reused.  */
//  The expression to identify the partition to be dropped 
//  This method gets called only in the scope that a destination table already exists, so   we're validating if the table is an appropriate destination to import into 
//  With trim=false, parsing cannot handle spaces 
//  Read all credentials into the credentials instance stored in JobConf. 
/*    * (non-Javadoc)   *   * @see   * org.apache.hadoop.hive.ql.optimizer.Transform#transform(org.apache.hadoop   * .hive.ql.parse.ParseContext)    */
/*      * this is the case when the big table is a sub-query and is probably already bucketed by the     * join column in say a group by operation      */
//  The fractional digits are gone; when rounding, clear remaining round digits and add 1. 
// HIVE-15458: we need to add a Project on top of Join since SemiJoin with Join as it's right input 
//  varchar columns should have correct display size/precision 
// https://github.com/brettwooldridge/HikariCP 
//  Logger jobconf 
//  plan needs to be complete before we execute and not modify it while execution in the driver. 
//  Well, don't recurse but make sure all children are initialized. 
/*    * Overall information on this vectorized Map operation.    */
//  fetch by namespace 
//  TopN query results 
//  Read the first batch.   Oh! the first batch itself was null. Close the reader. 
//  Set up recursive reads for sub-directories. 
/*  Returns an immutable map with the identity [0: 0, .., count-1: count-1].  */
//  used for statistics 
//  Add the expression to partition specification 
//  10**38=   v[0]=0(0),v[1]=160047680(98a2240),v[2]=1518781562(5a86c47a),v[3]=1262177448(4b3b4ca8) 
/*            * Optionally, the next value's small length could be a 2nd integer in the value's           * information.            */
//  The set of TableScanOperators for pruning OP trees 
//  Native vectorization NOT supported. 
//  establish mapping from the output column to the input column 
//  If it is a LEFT / FULL OUTER JOIN and the left record did not produce   results, we need to take that record, replace the right side with NULL   values, and produce the records 
//  TODO: logging currently goes to hive.log 
//  optional string vertex_name = 6; 
/*  * Read from Arrow stream batch-by-batch  */
//  TableDesc#getDeserializer() passes a null Configuration into the SerDe.   We shouldn't fail immediately in this case 
//  If the bottom operator is not synthetic and it does not contain a limit, 
//  skip query hints 
//  1024 is the default value   Clean up 
//  show column level privileges 
//  Pattern to remove the timestamp and other infrastructural info from the out file 
//  We have a decimal.  After we enforce precision and scale, will it become a NULL? 
//  max can be 1, even when ndv is larger in IN clause than in column stats 
// Authenticate using keytab 
//  continue to read it or move to the secondary. 
/*      * 3. build Reduce-side Op Graph      */
//  can't be null   can't be null   can't be null 
//  intentional fall through 
//  Normal deduplication 
// Copy table level hcat.* keys to the partition 
// Again we done want to exit because of logging issues. 
//  a timeout occurs 
//  Replace this with valueOf. 
//  we first merge all the adjacent bitvectors that we could merge and   derive new partition names and index. 
//  If this is a q-test, let's order the params map (lexicographically) by   key. This is to get consistent param ordering between Java7 and Java8. 
//  Clean TXN_TO_WRITE_ID table for entries under min_uncommitted_txn referred by any open txns. 
//  worst case when there are no column statistics 
//  Set isNull before call in case it changes it mind. 
//  Setup for actual notifications, if not already done for a previous task. 
//  obtain a second lock.  This shouldn't block cleaner as it was acquired after the initial 
//  No expression, therefore scan the whole table 
//  Init and run are both potentially long, and blocking operations. Synchronization   with the 'abort' operation will not work since if they end up blocking on a monitor   which does not belong to the lock, the abort will end up getting blocked.   Both of these method invocations need to handle the abort call on their own. 
// re-throw the exception as an IOException 
//  We build a hash map from colName to object for old ColumnStats. 
//  First we traverse the batch to evaluate and prepare the KeyWrappers 
//  project. 
//  LlapIoImpl.LOG.debug("Writing batch " + batch); 
/*    * Serializes decimal64 up to the maximum 64-bit precision (18 decimal digits).    */
//  for the first grandkid replace the original parent 
//  Get one of the default separators to avoid having to set a custom separator 
//  optional bool is_guaranteed = 12 [default = false]; 
//  Otherwise, heuristics. 
//  We're folding multiple masked lines into one. 
//  if left-semi-join found a match and we do not have any additional predicates,   skipping the rest of the rows in the rhs table of the semijoin 
//  zero reducers 
//  We only do the minimum cast for decimals. Other types are assumed safe; fix if needed.   We also don't do anything for non-primitive children (maybe we should assert). 
//  (since Timestamps are averaged with double, we don't need a PARTIAL2 class)   (and, since Timestamps are output as double for AVG, we don't need a FINAL class, either)   {"VectorUDAFAvgMerge", "VectorUDAFAvgTimestampPartial2", "PARTIAL2"},   {"VectorUDAFAvgMerge", "VectorUDAFAvgTimestampFinal", "FINAL"}, 
//  STARTED_TIME 
//  to the branch represented by the list. 
// case HiveParser.TOK_ALTERVIEW_ADDPARTS: 
//  This time, it completes by adding just foreign key constraints for table t2. 
//  Perform REPL-DUMP/LOAD 
//  as seen by the users) 
//  open client session 
//  checking for null in the for-loop condition prevents null-ptr exception   and allows us to fail more gracefully with a parsing error. 
//  also the location field in partition 
//  Check that we do find all expected columns 
//  used by FS based stats collector 
//  Use full partition path for error case. 
// HiveException is expected 
//  create a paritioned table 
//  By default, this will be same as that of super class BaseSemanticAnalyzer. But need to obtain again 
//  DEFAULT_CONSTRAINT_COLS 
//  Finally try to reuse with something in the queue. Due to fairness this won't work. 
//  The following repeatedX values will be set, if any of the columns are repeating. 
/*  * TestLazyHBaseObject is a test for the LazyHBaseXXX classes.  */
//  Check if the partitions exist in the destTable 
//  an extra value so that we can return it while reading ahead 
// This string constant will be persisted in metastore to indicate whether corresponding 
//  TBL_PATTERNS 
//  mark the start of the sync   write sync   update lastSyncPos 
//  2. Perform a major compaction. There should be an extra base dir now. 
// Defining partition names in unsorted order 
//  find any referenced resources 
/*    * Abstract method to be overridden for task execution.    */
//  contain the vc. 
//  Add keys of this grouping set. 
//  This is map of which vectorized row batch columns are the value columns. 
// simulate Insert into 2 partitions 
//  for last batch in row group, adjust the batch size 
//  In a YARN/Tez job, don't have the Kerberos credentials anymore, use the delegation token 
//  We need to loop here to handle the case where consumer goes away. 
// so that test doesn't block 
//  We have deleteRecordId < currRecordIdInBatch, we must now move on to find   next the larger deleteRecordId that can possibly match anything in the batch. 
/*  id > 12 or  */
//  total # of blocks   total # of elements in the RowContainer   temporary file holding the spilled blocks 
//  value in the configuration object. 
//  The below group of fields (pools, etc.) can only be modified by the master thread. 
//  SERDE_TYPE 
//  Methods should really be protected, but some places have to use this as a field. 
//  the column map can not be generated 
//  find the value of matched column 
//  This is used to communicate over the LlapUmbilicalProtocol. Not related to tokens used to   talk to LLAP daemons itself via the securit work. 
// note that recent metastore stores decimal in string. 
//  write out a header for the payload 
//  Timeseries query 
//  value columns 
/*    * A PTF input that represents a source in the overall Query. This could be a Table or a SubQuery.   * If a PTF chain requires execution by multiple PTF Operators;   * then the original Invocation object is decomposed into a set of Component Invocations.   * Every component Invocation but the first one ends in a PTFQueryInputSpec instance.   * During the construction of the Operator plan a PTFQueryInputSpec object in the chain implies connect the PTF Operator to the   * 'input' i.e. has been generated so far.    */
//  Start hive server2 
//  create N MapWorks and add them to the SparkWork 
//  optional bool is_guaranteed = 3; 
//  check if it is noscan command 
//  It is not available do nothing 
//  And, use set to remember which virtual columns were actually referenced. 
//  schema evolution will insert the acid columns to row schema for ACID read 
//  Verify that when we have no kerberos credentials, we pull the serialized Token 
//  set output isRepeating to true to make sure it gets over-written   similarly with noNulls 
//  FOREIGN_DB_NAME 
//  Same primitive category 
// then need to create metastore client that proxies as that user. 
//  User takes precendence over groups unless ordered explicitly. 
//  optional   optional   optional 
//  1. If equalsCheck is true and the inputOI is the same as the outputOI OR 
//  space usually 
//  Since the UDTF operator feeds into a LVJ operator that will rename   all the internal names, we can just use field name from the UDTF's OI   as the internal name 
//  VALIDATE_CSTR 
//  another quick path 
//  if this columnFamily/columnQualifier pair is defined in the index build a new mutation   so key=value, cf=columnFamily_columnQualifer, cq=rowKey, cv=columnVisibility value=[] 
//  10 digit int is all in lowest 16 decimal digit longword. 
//  standard case 
//  Pull the table schema out of the Split info 
//  Despite STRING being a primitive, it can't be serialized as binary 
//  1/ reserve spaces for the byte size of the list   which is a integer and takes four bytes 
//  Testing negative substring index.   Start index -6 should yield the last 6 characters of the string 
//  backward/forward compatible 
//  Creating dummy table to control the event ID of TRUNCATE not to be 10 or 100 or 1000... 
//  Ignore the pre-upgrade script errors 
//  over here we should have some checks of the deserialized object against   the orginal object 
//  remove from src pool 
// should be an error since p=3 exists 
//  for partitionless table, initialize partValue to null 
//  1. Insert some rows into MM table 
//  as long as they are still in the same stream and are not already released. 
//  Invoke the OutputFormat entrypoint 
//  File name 
//  add added jars 
/*  Ideally we want to specify the different arguments to updateLocation as separate argNames.     * However if we did that, HelpFormatter swallows all but the last argument. Note that this is     * a know issue with the HelpFormatter class that has not been fixed. We specify all arguments     * with a single argName to workaround this HelpFormatter bug.      */
//  the context along 
//  Case 1: Test with just originals => Single split strategy with two splits. 
//  we have found a match. insert this distinct clause to head. 
//  Wait if no exception happens, otherwise, retry immediately 
// for conditional task, next task list should return the children tasks of each task, which  is contained in the conditional task. 
//  Note the "& 0xff" is just a way to convert unsigned bytes to signed integer. 
//  read from hive to test 
//  Don't sync. 
//  If this is a duplicate invocation of a function; don't add to WindowingSpec. 
//  of objs) 
/*      * Used only for Debugging or testing purposes      */
//  fields belong to one of the next entries 
//  Repeating null 
//  The map should now be empty. 
// do nothing 
//  for each big table's bucket, call the start forward 
//  Normal case - an active session was removed from the pool.   Session was restarted out of bounds, any user-side handling should be ignored. 
//  TXNS table should have atleast one entry because we just inserted the newly opened txns. 
//  need to set this only for replication tasks 
//  Read count 
//  This will only get called once, since CompactRecordReader only returns one record,   the input split.   Based on the split we're passed we go instantiate the real reader and then iterate on it   until it finishes.  since there is no way to parametrize instance of Class 
//  merge join work. 
// set authorization mode to V2 
//  fill forwardCache with skipvector 
//  begin conversion.. 
//  we first take a look if any fieldSchemas contain COMMA 
//  100 < key 
//  Make sure we get a correct number of sessions in each queue and that we don't crash. 
//  This path is only potentially encountered during setup   Otherwise, a specific part_xxxx file name is generated and passed in. 
//  HiveAuthorizer.filterListCmdObjects should not filter any object 
// now tell LaunchMapper which files it should add to HADOOP_CLASSPATH 
//  based on the ErrorMsg set in HiveException. 
//  Serialize/deserialize 
//  Data columns.   Partition columns. 
//  Acquire 1st Txn Batch 
// ALL privilege is expanded to these, so it is not needed here 
/*    * If hasNulls is true, then this array contains true if the value   * is null, otherwise false. The array is always allocated, so a batch can be re-used   * later and nulls added.    */
//  Cache has found an old buffer for the key and put it into array instead of our new one. 
//  extract the record type 
//  Initialize all children first 
// This is either the first batch or we've used up the current batch buffer 
/*  Register a running task into the runningTasks structure  */
//  normal close. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#closeSession(org.apache.hive.service.cli.SessionHandle)    */
//  We decided to treat this collection as regular object. 
//  Clear the other ones. 
//  -c <named url in the beeline-hs2-connection.xml> 
//  We cannot abandon the attempt here; the concurrent operations might have released   all the buffer comprising our buddy block, necessitating a merge into a higher   list. That may deadlock with another thread locking its own victims (one can only   take list locks separately, or moving DOWN). The alternative would be to release   the free list lock before reserving, however iterating the list that way is   difficult (we'd have to keep track of things on the main path to avoid re-trying   the same headers repeatedly - we'd rather keep track of extra things on failure). 
//  metastore schema version is different than Hive distribution needs 
// copied from ErrorMsg.java 
//  MAPPINGS 
//  Check if stats are same, no need to update 
//  Task data structures have been initialized 
//  Read via object store 
// List<?> c16Value = (List<?>) rowValues[15];  assertEquals(0, c16Value.size()); 
//  Invalid expression => throw some exception, but not incompatible metastore. 
//  if different sign, just add up the absolute values 
// this is set by Utilities.copyTablePropertiesToConf() 
//  All good, combine the base/original only ETL strategies. 
//  If partitions do not match, we currently do not merge 
//  Split -9,223,372,036,854,775,808 into 16 digit middle and lowest longwords by hand. 
//  optional string app_id = 1; 
//  4. Perform a MINOR compaction again. This time it will remove the subdir for aborted transaction. 
//  need this for Jackson to work 
//  used for readFields 
//  Create the mapping for this column, with configured encoding 
//  Optimize local fetch does not work with LLAP due to different local directories   used by containers and LLAP 
//  drop table 
/*    * Initiate cancel request to cancel the thread execution and interrupt the thread.   * If thread interruption is not handled by jobExecuteCallable then thread may continue   * running to completion. The cancel call may fail for some scenarios. In that case,   * retry the cancel call until it returns true or max retry count is reached.   *   * @param future   *          Future object which has handle to cancel the thread.   *    */
//  @@protoc_insertion_point(builder_scope:TerminateFragmentRequestProto) 
//  Test that listPartitionsByFilter() returns an empty-set, if the filter selects no partitions. 
//  Move clock backwards (so that t1 allocation is after t2 allocation) 
//  The partitions are "unknown" if the call says so due to the expression   evaluator returning null for a partition, or if we sent a partial expression to 
//  check if any operator had a fatal error or early exit during   execution 
//  do not cache this if its child RDD is intend to be cached. 
//  Get the aggregate function matching the name in the query. 
//  Register that we have visited this operator in this rule 
//  Only the hive catalog should be cached 
//  store the config in system properties 
//  to call getVarcharMaxLength() on every deserialize call. 
//  create the default white list from list of safe config params   and regex list 
//  Stats values for col3 
//  nested complex types trigger Kryo issue #216 in plan deserialization 
//  The offset of the first input does not need to change. 
// partial spec 
//  number of aliases 
//  this avoids extra serialization & deserialization of these objects 
//  POOL_PATH 
//  The key portion of the entry will be the internal column name for the join key expression. 
//  kill server 
//  If it's a leaf, add the move task as a child 
//  UNDONE 
//  invoked for test methods 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getTypeInfo(org.apache.hive.service.cli.SessionHandle)    */
//  schema:   principal_name,principal_type,grant_option,grantor,grantor_type,grant_time 
//  Encountered partitioning column, this will be better handled by MetadataOnly optimizer. 
//  Table may not be found when materialization of CTE is on. 
//  then let's check the one we know about 
//  If column type is not specified, use a string 
/*  Returns the node currently on the top of the stack.  */
/*    * Job request executor to list job status requests.    */
//  Stats values for col2 
//  Test in mixed case 
//  set hashtable memory usage 
//  Remove the locks we didn't see so we don't look for them again next time 
//  Now get all the one-to-many things. Start with partitions. 
// we are good since subquery is top level expression 
//  Get the following out of the way when you start the session these take a 
//  Try to read the dropped "tbl1" via CachedStore (should throw exception) 
//  If it's not some operator, pass it back 
//  If 1) RS has been removed, or 2) it does not have a child (for instance, it is a   semijoin RS), we can quickly skip this one 
//  We won't do metastore-side PPD for the things we have locally. 
//  Sanity check that, for a map, we got 2 encodings 
//  null means we cannot wrap; the cause is logged inside. 
//  See method comment. 
//  invalid merge -- smaller register merge to bigger 
//  alpha order, PLEASE! 
//  Everything comes from cache. 
//  This mapper class is used for serializaiton/deserializaiton of merge   file work. 
//  HiveStatement#getUpdateCount blocks until the async query is complete 
//  See also: the usage of VectorDeserializeType, for binary. For now, we only want text. 
//  Disabling rewriting, removing from cache 
//  the column must be an aggregate column inserted by GBY. We   don't have to account for this column when computing product   of NDVs 
//  Choose cumulative 
// export works at file level so if you have copy_N in the table dir, you'll have those in output 
//  Change the filter condition into a join condition 
//  Can only happen if there's no evictor, or if thread is interrupted. 
//  Set null information in the small table results area. 
//  Multiply by 2 to make room for 0 sign bit. 
//  There maybe more than 1 splits in the group, however, they all have 1 unique path.   Assert that. 
//  Check individual elements of subrecord 
//  Process the batch 
//  left repeats 
//  check the contents of the file 
//  Now HiveDecimal 
//  start offset of each field 
//  String object. 
//  Inject a behavior where REPL LOAD failed when try to load table "t2" and partition "uk". 
//  grammar prohibits more than 1 column so we are guaranteed to have only 1   element in this lists. 
//  The node cannot accept a task at the moment. 
// (1*60*60 + 1*60 + 1) * 10e9 + 1 
//  Most likely this means it's a temp table 
/*  Set double data vector array entries for NULL elements to the correct value.     * Unlike other col-scalar operations, this one doesn't benefit from carrying     * over NaN values from the input array.      */
//  FIELD0 
//  If there's no lock manager, it essentially means we didn't acquire locks in the first place,   thus no need to release locks 
//  For now, do not limit this - one RG per split 
//  detect if there are multiple attributes in join key 
//  Do not call mq.getRowCount(join), will trigger CyclicMetadataException 
//  Used by kyro 
/*      *  If there was a pre-existing work generated for the big-table mapjoin side,     *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)     *  with the pre-existing work.     *     *  Otherwise, we need to associate that the mapjoin op     *  to be linked to the RS work (associated with the RS-MJ pattern).     *      */
//  Recursively remove any of its parent who only have this op as child. 
//  first remove all the grants 
//  Let's make sure we only read the relevant part of the writable in case of reuse 
//  load them into a SetLong 
//  TODO: cache the information from the metastore 
//  2nd pass at removing invalid candidates   If misses so far exceed max tolerable misses 
//  NOTE: The null array is indexed by keyIndex, which is not available internally.  The mapping         from a long, double, etc index to key index is kept once in the separate         VectorColumnSetInfo object. 
//  MRInput is not of interest since it'll always be ready. 
//        out of the request provided that it's signed. 
//  Rule is searching for dynamic pruning expr. There's at least an IN   expression wrapping it. 
// Expecting not to change the size of internal structures 
//  adds the taskId to the fspKey. 
//  check if bucketing in both was done in the same way 
//  Bare cf 
//  the right input to Correlator should produce correlated variables 
//  letter "A" (1 byte)   Latin capital A with grave (2 bytes) 
// throw new IllegalArgumentException("hcatFiledSchema is null; fSchema=" + fSchema + " " +        "(pigSchema, tableSchema)=(" + pigSchema + "," + tableSchema + ")"); 
//  total characters = 2; byte length = 4 
//  Ensure counters are set when data has actually been read. 
//  Do this here; normally communicator does this. 
//  Follow the Reducesink operator upstream which is on small table side. 
//  Try the extremes of precision and scale. 
//  Parse and initialize the HBase columns mapping 
//  filters for pushing 
//  needed columns 
//  the name of the function/table 
//  set register value and compute inverse pow of 2 for register value 
/*  @bgen(jjtree) Senum  */
//  Note: Setting these separately is a very hairy issue in certain combinations, since we         cannot decide what type of table this becomes without taking both into account, and         in many cases the conversion might be illegal.         The only thing we allow is tx = true w/o tx-props, for backward compat. 
//  sum of small tables size in this join exceeds configured limit   hence cannot convert. 
//  PART_NAMES 
//  if the object does not exist, we want to add it. 
//  total characters = 2; byte length = 3 
//  lookup the specified type and set this nodes type to it. Precludes   forward and self references for now. 
//  retain the original join desc in the map join. 
//  Currently, we do not support PTF operator. 
//  get list of selected column IDs 
//  Not supported. 
/*  This method assumes that the IN list has no NULL entries. That is enforced elsewhere,     * in the Vectorizer class. If NULL is passed in as a list entry, behavior is not defined.     * If in the future, NULL values are allowed in the IN list, be sure to handle 3-valued     * logic correctly. E.g. NOT (col IN (null)) should be considered UNKNOWN, so that would     * become FALSE in the WHERE clause, and cause the row in question to be filtered out.     * See the discussion in Jira HIVE-5583.      */
//  @@protoc_insertion_point(outer_class_scope) 
//  Round to even 0. 
//  Parse the first byte of a vint/vlong to determine the number of bytes. 
//  SQL92 comment prefix is "--"   beeline also supports shell-style "#" prefix 
//  Write datum out to a stream 
//  This is to optimize queries of the form:   select count(distinct key) from T   where T is sorted and bucketized by key   Partial aggregation is performed on the mapper, and the   reducer gets 1 row (partial result) per mapper. 
//  https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/GroupsMapping.html 
//  what position in the mapjoin the different parent work items will have. 
//  Hopefully this will be helpful in case of NPEs. 
/*  partialSum  */
//  Start concurrent testing 
/*  @bgen(jjtree) DefinitionType  */
//  Merge noMatchs and (match) selected. 
//  it or not. 
//  Expecting only a single instance of a task to be running. 
//  If both categories are primitive return the comparison of type names. 
//  Null out some row column entries.   UNDONE 
//  Should this group by be converted to a map-side group by, because the grouping keys for 
//  the ObjectInspector for array<?> and map<?, ?> expects an extra layer 
//  canColumnStatsMerge guarantees that it is accurate before we do merge 
//  To make comparisons work properly, the "factor" gets the decimal's sign, too. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#close()    */
//  Short.MIN_VALUE 
// now we have base_0001 file 
//  key = "database.table/SP/DP/"LB/   Hive store lowercase table name in metastore, and Counters is character case sensitive, so we 
//  Make a copy of currentMetaVars, there is a race condition that   currentMetaVars might be changed during the execution of the method 
//  write a second non-null element 
//  stats from reader 
/* catalog  */
//  PARENT_DB_NAME 
//  If SSL is enabled, override the given value of "hadoop.rpc.protection" and set it to "authentication"   This disables any encryption provided by SASL, since SSL already provides it 
//  If we can immediately reuse a session, there's nothing to wait for - just return. 
//  get column statistics for all output columns 
//  Test "DROP VIEW" 
//  Grouping sets: we need to transform them into ImmutableBitSet   objects for Calcite 
//  Try temporarily adding the RS as a parent 
// will call RecordUpdater.close(boolean abort) 
//  Create the directory 
//  FIELD2 
//  If parent keys are null or empty, we bail out 
//  GRANTOR_TYPE 
/*        * add row to chain. except in case of UNB preceding: - only 1 firstVal       * needs to be tracked.        */
// forward as arg   forward as arg   work-dir   llap-daemon-site   llap-daemon-site   forward via config.json   forward as arg   used to localize jars   used to localize jars   used to localize jars   forward via config.json   llap-daemon-site if relevant parameter   forward as arg   forward as arg   forward via config.json   llap-daemon-site 
//  type name should already be set by subclass 
//  MySQL can use INT(n)  
//  n-way: first small table 
//  FIELD3 
//  only returns a subset per call 
//  for computing the autogenerated field ids in thrift 
//  TODO: later, we may have a map 
//  Containers are not being tracked for re-use.   This is safe to ignore since a deallocate task will come in. 
//  FIELD1 
//  HIVE-14443 move this fall-back logic to CliConfigs 
//  enable cache and use default strategy 
//  Calcite literal is in millis, convert to seconds 
//  The order of the fields in the LazyBinary small table value must be used, so 
//  Data structures coming from QBJoinTree 
//  get appropriate object from the string representation of the value in partInfo.getPartitionValues() 
//  if we did not see a skew key in this table, continue to next table   we are trying to avoid an extra call of FileSystem.exists() 
/*  * An single long value hash map based on the BytesBytesMultiHashSet. * * We serialize the long key into BinarySortable format into an output buffer accepted by * BytesBytesMultiHashSet.  */
//  5. Insert ReduceSide GB2 
//  dimension 
//  The SchemaEvolution class has added the ACID metadata columns.  Let's update our   readerTypes so PPD code will work correctly. 
/*      * For WindowingTableFunction if:     * a. there is a Rank/DenseRank function: if there are unpushedPred of the form     *    rnkValue < Constant; then use the smallest Constant val as the 'rankLimit'     *    on the WindowingTablFn.     * b. If there are no Wdw Fns with an End Boundary past the current row, the     *    condition can be pushed down as a limit pushdown(mapGroupBy=true)     *     * (non-Javadoc)     * @see org.apache.hadoop.hive.ql.ppd.OpProcFactory.ScriptPPD#process(org.apache.hadoop.hive.ql.lib.Node, java.util.Stack, org.apache.hadoop.hive.ql.lib.NodeProcessorCtx, java.lang.Object[])      */
//  optional int64 current_attempt_start_time = 6; 
//  prepends partition spec of input path to candidate file name 
//  2. Setup TableScan 
//  Add the current constant struct to the right hand side of the IN clause. 
//  Define the expected schema. 
/*  Count of number of true values seen so far  */
//  Construct a CASE expression to handle the null indicator.     This also covers the case where a left correlated subquery   projects fields from outer relation. Since LOJ cannot produce   nulls on the LHS, the projection now need to make a nullable LHS   reference using a nullability indicator. If this this indicator   is null, it means the subquery does not produce any value. As a   result, any RHS ref by this usbquery needs to produce null value. 
//  There is no need to add colname again, otherwise we will get duplicate colNames. 
//  This has to be called before initializing the instance of HMSHandler   Using the hook on startup ensures that the hook always has priority   over settings in *.xml.  The thread local conf needs to be used because at this point 
//  required by input format. 
//  corresponding 8 struct fields at the same time 
//  End UDFRowSequence.java 
//  map priv being granted to required privileges 
//  If this Filter has correlated reference, create value generator 
// Nothing to do 
//  add whether the row is filtered or not. 
//  We bail if there are any changes. Note that we don't care about ABA here - all the   stuff on the left has been taken out already so noone can touch it, and all the stuff   on the right is yet to be seen so we don't care if they changed with this - if it's   in the same free list, the processing sequence will remain the same going right. 
//  If there are no more active client sessions, stop the server 
//  destination in memory   If this is the only partition in memory, proceed without check   Destination partition being empty indicates a write buffer   will be allocated, thus need to check if memory is full   check periodically 
// if the url does not have a database name add the trailing '/' 
//  A test-specific delay just before the check happens. 
//  TABLE_TYPES 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#execute(java.lang.String, java.lang.String[])    */
//  fetch all the hints in qb 
//  set alias to fetch work 
//  Dynamic partition: replace input path (root to dp paths) with dynamic partition 
//  Don't bother validating. 
//  Create a list of top nodes 
//  check if the context matches 
//  User2 privileges:   testdb1: S   testtable1.*: S   testtable2.*: S   testtable3.*: S   testtable4.*: S   testdb2: 
//  make sure miniHS2_2 closes all its connections 
//  If already copied successfully, ignore it. 
//  String[] allAliases = joinTree.getAllAliases(); 
//  All of stats variables are visible for testing. 
//  Return the errors that occur the most frequently 
//  Target compression block is in the middle of the range; slice the range in two. 
//  limit length to 20 chars 
// no need to wait in the last iteration 
//  a map to keep track of which child generated with work 
//  anythingElse <= 'foo' 
//  show tables should be faster than that 
//  Update with new values 
//  if this task was added to pre-emption list, remove it 
//  VARCHAR NOT BETWEEN 
//  Return a single ArrayList where the first element is the number of bins bins,   and subsequent elements represent bins (x,y) pairs. 
//  Set the results 
//  We are not allowed to lose digits in multiply to be compatible with OldHiveDecimal   behavior, so overflow.   CONSIDER: Does it make sense to be so restrictive.  If we just did repeated addition,             it would succeed... 
//  We forwarded the batch in this method. 
// test serialization 
//  Handle both file:// and jar:<url>!{entry} in the case of shaded hive libs 
//  and the operator is a DOT, then it's a table column reference. 
//  Max is disabled, we can safely return true 
//  append new config params to whitelist 
//  RuntimeErrorException happens when an unexpected failure occurs in getAttribute   for example https://issues.apache.org/jira/browse/DAEMON-120 
// c16Value = (List<?>) rowValues[15];  assertEquals(2, c16Value.size());  listVal = (List<?>) c16Value.get(0);  assertEquals(2, listVal.size());  mapVal = (Map<?,?>) listVal.get(0);  assertEquals(0, mapVal.size());  assertEquals(Integer.valueOf(1), listVal.get(1));  listVal = (List<?>) c16Value.get(1);  mapVal = (Map<?,?>) listVal.get(0);  assertEquals(2, mapVal.size());  assertEquals("b", mapVal.get("a"));  assertEquals("d", mapVal.get("c"));  assertEquals(Integer.valueOf(2), listVal.get(1)); 
//  Does not need to be actual time, just non-zero distinct value to test against. 
// both "blockedby" are either there or not 
//  dynamic part vals specified 
//  Get the App report from YARN 
//  In the case where agg is count($corVar), it is changed to   count(nullIndicator).   Note:  any non-nullable field from the RHS can be used as   the indicator however a "true" field is added to the   projection list from the RHS for simplicity to avoid   searching for non-null fields.     Project-A' (all gby keys + rewritten nullable ProjExpr)     Aggregate (groupby(all left input refs),                   count(nullIndicator), other aggs...)       Project-B' (all left input refs plus                      the rewritten original projected exprs)         Join(replace corvar to input ref from LeftInputRel)           LeftInputRel           Project (everything from RightInputRel plus                       the nullIndicator "true")             RightInputRel 
//  LAST_HEARTBEAT_TIME 
//  Must produce the same result as MurmurHash.hash with seed = 0. 
//  the first group. 
// insert (12,12) creates 000000_0_copy_1 
//  instantiate the metastore server handler directly instead of connecting   through the network 
//  Recursive. 
//  We have finished tree walking (correlation detection).   We will first see if we need to abort (the operator tree has not been changed). 
//  The caller probably created the new session with the old config, but update the 
// authorize against the table operation so that location permissions can be checked if any 
/* createReader(FileSystem fs, Path path) throws IOException {      */
//  This is needed to prevent the HikariDataSource from trying to connect to the DB 
//  Set data location and input format, it must be text 
//  pending change to boolean 
/*    * Convert the work containing to sort-merge join into a work, as if it had a regular join.   * Note that the operator tree is not changed - is still contains the SMB join, but the   * plan is changed (aliasToWork etc.) to contain all the paths as if it was a regular join.    */
//  bucket 
//  No need to discard the buffer we cannot lock - eviction takes care of that. 
//  increment the min txn id so that heartbeat thread will heartbeat only from the next open transaction.   the current transaction is going to committed or fail, so don't need heartbeat for current transaction. 
//  no data left in current page, load data from new page 
//  object equality - isSame means that the objects are semantically equal. 
//  currentInputFile will be updated only by inputFileChanged(). If inputFileChanged()   is not called throughout the operator tree, currentInputPath won't be used anyways 
//  destination partition if any   the intermediate destination directory   the final destination directory 
//  The left child is not a join or multijoin operator 
//  We must read through fields we do not want. 
//  Ensure PartInfo's TableInfo is initialized. 
//  3. create subquery 
// returns true if statement represented by line is  not complete and needs additional reading from  console. Used in handleMultiLineCmd method 
//  r2 can only start once 1 fragment has completed. the map should be clear at this point. 
//  TODO: we use MetadataTypedColumnsetSerDe for now, till DynamicSerDe is ready 
/*    * Same responsibility as initializeOI, but for the RawInput.    */
//  hive depends on FileSplits 
//  IMPORTANT NOTE: For Multi-AND, the VectorizationContext class will catch cases with 3 or                   more parameters... 
//  Initialization isn't finished until all parents of all operators   are initialized. For broadcast joins that means initializing the   dummy parent operators as well. 
//  mapjoin later 
//                   newState                    -----------------------------------------   columnStatsState | COMPLETE          PARTIAL      NONE    |                    |________________________________________|           COMPLETE | COMPLETE          PARTIAL      PARTIAL |            PARTIAL | PARTIAL           PARTIAL      PARTIAL |               NONE | COMPLETE          PARTIAL      NONE    | 
//  Case 2: If there's delay for the heartbeat, but the delay is within the reaper's tolerance,           then txt should be able to commit 
//  The number of data columns that the current reader will return.   Only applicable for vector/row deserialization. 
//  Initialize with data type conversion parameters. 
// no validation required.. 
//  Trigger scheduling since a new node became available. 
//  there is nothing to change 
//  Invalid values 
//  DEVENAGARI SIGN VIRAMA U+094D (3 bytes) 
//  Compare the Mapper get at offset method to the list of mappings 
//  Store the previous value for the path specification 
//  All good, no such partition exists, move on. 
//  posMap is an unfortunate consequence of batching/iterating thru MS results. 
//  In case of viewfs we need to lookup where the actual file is to know the filesystem in use.   resolvePath is a sure shot way of knowing which file system the file is. 
//  6. Construct SetOp Rel 
//  localTmpPath is the root of all the stats.   Under it, there will be SEL_1/statsfiles, SEL_2/statsfiles etc where SEL_1 and SEL_2 are the op ids. 
//  apply schema evolution by adding some columns 
//  no more data 
//  ROLE_PRIVILEGES 
//  Format a clustered by statement 
//  Drop one function, see what remains 
//  To compute seconds, we first subtract the milliseconds stored in the nanos field of the   Timestamp from the result of getTime(). 
//  Avoid allocating temporary variables for special cases: signum or scale is zero 
//  We don't expect missing buckets from mere (actually there should be no buckets),   so just pass null as bucketing context. Union suffix should also be accounted for. 
//  optional .QueryIdentifierProto query_identifier = 3; 
//  TABLES_USED 
// There should be one argument that is a array of struct 
//  The join filters out the nulls.  So, it's ok if there are 
//  convert seconds to milliseconds 
//  Native vectorization not supported. 
//  Loop through all the inputs to determine the appropriate return type/length.   Return type:    All CHAR inputs: return CHAR    All VARCHAR inputs: return VARCHAR    All CHAR/VARCHAR inputs: return VARCHAR    All BINARY inputs: return BINARY    Otherwise return STRING 
//  do error checking later and detect just a dot. 
// explicitly close ZKDatabase since ZookeeperServer does not close them 
//  Required for insertion into a TreeMap 
// no op 
//  There can never be more concurrent takers than uncommitted ones. 
//  Also add ZK settings to clusterSpecificConf to make sure these get picked up by whoever started this. 
// Any preexisting datanucleus property should be passed along 
//  Struct value is simply a list of values.   The schema can be used to map the field name to the position in the list. 
//  an IntWritable so we can just sum in the reduce 
//  Walk over the input row resolver and copy in the output 
//  ^(TOK_RESOURCE_URI $resType $resPath) 
//  ==== Hive command operations ends here ==== // 
//  We can be pretty sure that an entire line can be processed as a single command since   we always add a line separator at the end while calling dbCommandParser.buildCommand. 
//  Prevent construction outside the get() method. 
//  note the sync marker "seen" in the header 
//  If data was moved from original location to cache directory, we need to move it back! 
//  now add the tables and columns from the current connection 
// not block each other since they are part of the same txn 
//  If there is a current unread chunk, read from that, or else get the next chunk. 
//  deserialize split 
//  multiple means of lookup 
//  make sure if there is subquery it is top level expression 
//  use existing location 
//  this is where we set the sort columns that we will be using for KeyValueInputMerge 
//  If we cache helper data for deserialization we could avoid having 
//  whatever we have 
//  final long roundMultiplyFactor = powerOfTenTable[LONGWORD_DECIMAL_DIGITS - absRoundPower]; 
//  reset data container to prevent it being added again. 
//  Not a lot you can do here. 
//  if we are dealing with a bag or tuple column - need to worry about subschema 
//  Are we forcing the usage of VectorUDFAdaptor for test purposes? 
//  Update field collations 
//  regex of the form: ${column name}. Following characters are not allowed in column name: 
//  For non-vectorized operator case, wrap the reader if possible. 
//  String storage type overrides table level default of binary storage 
//  Alas, we crossed some DST boundary. If the time of day doesn't matter to the caller, we'll 
//  NULLS 
//  clear JoinTree and OP Parse Context 
//  We might generate a Select operator on top of the join operator for   semijoin 
// serialize path, offset, length using FileSplit 
//  Big case: write the length as a VInt and then the value bytes. 
//  let's validate that the serde exists 
//  Find the immediate parent possible.   For eg: for a query like 'select * from V3', where V3 -> V2, V2 -> V1, V1 -> T   -> implies depends on.   T's parent would be V1   do not check last alias in the array for parent can not be itself. 
//  NOTE: tableAlias must be a valid non-ambiguous table alias,   because we've checked that in TOK_TABLE_OR_COL's process method. 
//  Compute the keys 
//  Can't use toArray here because Java is dumb when it comes to   generics + arrays. 
/*    * Initialize using data type names.   * No projection -- the column range 0 .. types.size()-1    */
// User specified a row limit, set it on the Query 
//  whether this vertex is dummy (which does not really exists but is created), 
//  The set of join operators which can be converted to a bucketed map join 
// simulate Update of 1 partitions; depending on causeConflict, choose one of the partitions 
// replace column references in checkExprAST with corresponding columns in input 
//  UTC has no such adjustment 
//  The first input of a Correlator is always the rel defining 
//  tests not setting maxRows (return all)   tests setting maxRows to 0 (return all) 
//  Nothing to do, bail out 
//  Nothing to do if there is no operator tree associated with   sourceAlias in source or there is not operator tree associated   with targetAlias in target. 
//  events for the source. #colums X #tasks 
//  One of the params is null, then expected is null. 
//  A TS can have multiple branches due to DPP Or Semijoin Opt. 
//  Throw an exception if the user is trying to truncate a column which doesn't exist 
//  Use the object we already have. 
//  This is a new key, keep writing the first record. 
// since this is inside a delta dir created by Hive 2.x or earlier it can only contain  bucket_x or bucket_x__flush_length 
// add the archive file to distributed cache 
// new table 
//  Intermediate key - anything between /key= and the following / 
//  User specified fraction always takes precedence 
//  clone the column stats and return 
//  Replacement is allowed as the existing table is older than event 
//  Now add to the projUniqueKeySet the child keys that are fully   projected. 
//  If the Gby key is a constant 
//  A limit on the number of threads that can be launched 
//  If all BigTable input columns to key expressions are isRepeating, then 
//  Substitution option -d, --define 
//  Make sure metastore doesn't mess with our bogus stats updates. 
//  enforce a minimum precision factor 
//  Just examine the lower word. 
//  Position of the *single* native vector map join small table. 
//  Test that database and table don't coalesce. 
//  Not a HS2 generated cookie, continue. 
//  Serialize the value 
//  URLDecoder is a misnamed class, since it actually decodes   x-www-form-urlencoded MIME type rather than actual   URL encoding (which the file path has). Therefore it would   decode +s to ' 's which is incorrect (spaces are actually   either unencoded or encoded as "%20"). Replace +s first, so   that they are kept sacred during the decoding process. 
//  Granularity (partition) column 
//  Bucket 0 should be small and bucket 1 should be large, make sure that's the case 
//  a list of leaves that weren't under AND expressions 
//  Note: due to TEZ-3846, the session may actually be invalid in case of some errors.         Currently, reopen on an attempted reuse will take care of that; we cannot tell         if the session is usable until we try.   We return this to the pool even if it's unusable; reopen is supposed to handle this. 
//  A udf which sleeps for 100ms to simulate a long running query 
//  Pass. 
//  just return 
//  Update the state to removed-from-list, so that parallel notifyUnlock doesn't modify us. 
//  We need a input object inspector that is for the row we will extract out of the   vectorized row batch, not for example, an original inspector for an ORC table, etc. 
//  Since subtraction is not commutative, we can must subtract in the order passed in. 
//  Set tez execution summary to false. 
// length, file count, directory count 
//  The column is partition column, skip the optimization. 
//  right's signum wins (notice the negation, because we are   subtracting right) 
//  --failover <workerIdentity> 
//  create tables 
//  Best effort 
//  Skip the counting if the values are the same for windowing COUNT(DISTINCT) case 
//  There is only one blank in UTF-8. 
//  get the tables for the desired pattern - populate the output stream 
//  This may change after every setMapJoinKey call 
//  return static variable with results, if it is set to some set of   values 
//  optional int32 physical_edge_count = 3; 
//  Verify mergeAndMoveTask is NOT optimized 
//  This was added during plan generation. 
//  No GROUP BY / DISTRIBUTE BY / SORT BY / CLUSTER BY 
//  race protection 
//  "ready for cleaning" state in this case. 
//  Simple trims. 
//  do not split 
//  The last key column is the dummy grouping set id.     Figure out which (scratch) column was used so we can overwrite the dummy id. 
//  Non-encrypted path (or equals strength) 
//  -f <script file> 
/*      * Ignore any predicates on partition columns because we have already     * accounted for these in the Table row count.      */
//  Thread name for reporter thread 
//  do not overwrite if there are duplicate keys 
//  Partition keys can not be set, but getTableWithAllParametersSet is added one, so remove for 
//  Note: this may just block to wait for a session based on parallelism. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setMaxFieldSize(int)    */
//  Because we append the cq prefix when serializing the column 
//  Stats values for col1 
//  set up the client 
//  stats of the big input 
//  no-op for default output file 
//  generate a ReduceSink operator for the join 
// enables ORC PPD  create delta_0001_0001_0000 (should push predicate here) 
//  string is 2 chars long (a 3 byte and a 4 byte char) 
//  Merge stats from cache with metastore cache 
//  Unique keys for this test. 
//  protected boolean useMinMax; 
//  @@protoc_insertion_point(class_scope:QueryCompleteResponseProto) 
//  remove the candidate filter ops 
//  dictates which operator is allowed 
//  if buffer is already allocated, keep using it, don't re-allocate 
//  Finally, put it in the ranges list for future use (if shared between RGs). 
//  isatty system call will return 1 if the file descriptor is terminal else 0 
//  add shutdown hook to flush the history to history file and it also close all open connections 
//  Get the actual converted schema. 
// TEST FAILED 
//  fastIsShort returns false. 
//  Dynamic value which will be determined during query runtime 
/*  (non-Javadoc)   * @see org.apache.hadoop.mapreduce.RecordReader#getProgress()    */
// this should not vectorize at all 
//  Nothing to localize. 
/*  Restore the hashmap from disk by deserializing it.     * Currently Kryo is used for this purpose.      */
//  has nulls, is repeating 
//  Ignore the keys which are local to source warehouse 
//  2.2 Check if GRpSet require additional MR Job 
/*    * CHAR.    */
//  3 rows 
//  If the configured owner does not own the file, throw 
//  optional int32 vertex_index = 7; 
/*    * Called to set the appropriate input format for tasks    */
//  Grouping sets are not allowed   This restriction can be lifted in future. 
//  index of FetchOperator which is providing smallest one 
//  100 is for 2 longs, BB and java overheads (semi-arbitrary). 
//  - If the child is an AND operator, extract its children 
/*  * An multi-key value hash map optimized for vector map join. * * The key is uninterpreted bytes.  */
//  Modify sourceTable. 
//  Partition droppped after "repl dump" 
//  no conversion needed, and not variable-length argument:   just return what is passed in. 
//  c16:array<struct<m:map<string,string>,n:int>>   c17:timestamp   c18:decimal(16,7)   c19:binary   c20:date   c21:varchar(20)   c22:char(15)   c23:binary 
//  Setup web UI 
// ************************************************************************************************   Decimal String Formatting. 
//  Case 1 - Max in list members: 10; Max query string length: 1KB 
//  15. Notify tests and global async ops. 
//  Only allow integer index for now 
//  test that values that we know are missing are shown to be absent 
//  If reversedMemoryMB is set, make memory allocation fraction adjustment as needed 
//  Change query FetchTask to use new location specified in results cache. 
//  ast expression is not a valid column name for table 
//  Only need to run the logic for tables we missed 
//  initialize configuration 
//  Nothing to check 
//  Register the cache-aware path so that Parquet reader would go thru it. 
//  The following tests use serialized ASTs that I generated using Hive from   branch-0.14. 
//  Test that 2 exclusive db locks coalesce to one 
//  reset the bean: 
//  It ends in a character, this means they appended a time indicator (e.g. 600s) 
// Running a normal async query with no exceptions,then no need to close opHandle 
//  rootTasks is the entry point for all generated tasks 
//  called by map operator. propagated recursively to single parented descendants 
//  Someone already allocated this arena; just do the usual thing. 
//  VALUE 
//  - Otherwise, take the child itself 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setRowId(java.lang.String, java.sql.RowId)    */
//  A vectorized expression that we don't expect will be called due to short-circuit evaluation. 
//  another query referring that property with the conf overlay should fail 
//  fall-through 
//  This check is necessary because for Spark branch, the result array from   getInputPaths() above could be empty, and therefore numThreads could be 0. 
//  Delete the parent node if all the children have been deleted 
//  no need for grouping and the target #of tasks.   This code path should never be triggered at the moment. If grouping is disabled,   DAGUtils uses MRInputAMSplitGenerator. 
//  the list has invalid port, update with valid port 
// the handling results 
//  Get splits 
// int 
//  Bad value type. 
//  Since hashCode is not used, just put an arbitrary number 
//  UDFOPPositive is a no-op.   However, we still create it, and then remove it here, to make sure we   only allow 
//  Check that constraints have catalog name properly set first 
//  Evaluate ELSE expression (only) and copy all its results.   Second input parameter but 3rd column. 
//  ConfVar overridden in in hive-site.xml 
//  oldInput has the original group by keys in the front. 
//  Don't increment the reader count for explain queries. 
//  position of the biggest small table 
//  Do not invert between result   add column expression here 
//  Compute required mapping. 
//  In the above example, TS-1 -> RS-1 and TS-2 -> RS-2 are simple trees 
// DB 
//  Unlock database operation is to release the lock explicitly, the   operation itself don't need to be locked. Set the WriteEntity as   WriteType: DDL_NO_LOCK here, otherwise it will conflict with   Hive's transaction. 
//  We cannot restart in place because the user might receive a failure and return the   session to the master thread without the "irrelevant" flag set. In fact, the query might   have succeeded in the gap and the session might already be returned. Queue restart thru 
/*  Note: In the following section, Metadata-only import handling logic is       interleaved with regular repl-import logic. The rule of thumb being       followed here is that MD-only imports are essentially ALTERs. They do       not load data, and should not be "creating" any metadata - they should       be replacing instead. The only place it makes sense for a MD-only import       to create is in the case of a table that's been dropped and recreated,       or in the case of an unpartitioned table. In all other cases, it should       behave like a noop or a pure MD alter.     */
//  BlockingQueue methods 
//  char 
//  filterMode is 1 if condition is always true, -1 if always false 
/*  Return a random number with length digits, as a string. Results may be   * negative or positive.    */
//  *NON-NATIVE* vector map differences for LEFT OUTER JOIN and Filtered... 
//  Ok, no vectorized class available.  No problem -- try to use the VectorUDFAdaptor   when configured.     NOTE: We assume if hiveVectorAdaptorUsageMode has not been set it because we are   executing a test that didn't create a HiveConf, etc.  No usage of VectorUDFAdaptor in   that case. 
// Create a Server that doesn't interpret any Kerberos stuff 
//  a maybe will kill the or condition 
// S lock on T7 
//  find available privileges 
//  rootOperators are all the table scan operators in sequence   of traversal 
//  if the move hasn't been made already 
//  Debug/test related methods. 
// create more staging data with copy_N files and do LD+Overwrite 
//  Don't wait for the cluster if not started; this is best-effort. 
//  End MultiJoin.java 
//  Test empty database 
//  In case it was done and noone looked at it. 
/*      * Order columns are used as key columns for constructing     * the ReduceSinkOperator     * Since we do not explicitly add these to outputColumnNames,     * we need to set includeKeyCols = false while creating the     * ReduceSinkDesc      */
// This cover the case where hive table may have map<key, value> but the data file is   of type array<struct<value1, value2>>  Using index in place of type name. 
//  We must be on some unix variant.. 
//  set the offset and length for the two elements 
//  When deleteRecordId == currRecordIdInBatch, this record in the batch has been deleted. 
/*  Test parent references from Statement  */
//  Remember all threads that were running at the time we started line processing.   Hook up the custom Ctrl+C handler while processing this line 
//  this will be populated by MergeFileWork.resolveDynamicPartitionStoredAsSubDirsMerge   in case of dynamic partitioning and list bucketing 
//  Dummy mapping used for all db and table name mappings 
//  create a syntax tree for a function call "testudf(col0, col1, col2)" 
/*    * Determine recursively if the PTF LEAD or LAG function is being used in an expression.    */
//  if we have come this far - either the previous commands   are all successful or this is command line. in either case   this counts as a successful run 
//  untyped nulls 
//  Add partition cols if necessary (see VectorizedOrcInputFormat for details). 
//  Combine 
//  check round-ups before settings values to result. 
//  HIVE-14444: pending refactor to push File forward 
//  Get the RootLogger which, if you don't have log4j2-test.properties defined, will only log ERRORs 
/*    * Initialize one column's target related arrays.    */
//  Create the queryId appender for the queryId route 
//  Map? 
//  since same thread creates metastore client for streaming connection thread and heartbeat thread we explicitly   disable metastore client cache 
//  Should fail because of the -1 
//  Set during the init phase of HiveServer2 if auth mode is kerberos 
//  Since this is a terminal operator, update counters explicitly -   forward is not called 
//  max fraction of errors allowed   throw error only after this many errors 
//  Tez processor needs to configure object registry first. 
//  capture arguments in static 
//  return value as constant in case arg is constant 
//  In beeline mode we need to hook to use, connect, go, in case   the ShowDbInPrompt is set, so the database name is needed 
//  serialize using another serde, and read out that object repr. 
// There must be at least one column vector 
//  Trim to the size needed 
//  Create all children 
//  order and null order 
//  MAX_CREATE_TIME 
//  Verify result is rounded to 4 digits 
//  puts int in little endian order 
//  Add new rel & its RR to the maps 
//  1st query acquires the lock and takes 20 secs to compile 
//  The operator tree till the sink operator needs to be processed while   fetching the next row to fetch from the priority queue (possibly containing   multiple files in the small table given a file in the big table). The remaining   tree will be processed while processing the join. 
//  For now, limit the data types we support for Vectorized Struct IN(). 
//  That is implementation-defined. 
//  Only create the movework for non-MM table. No action needed for a MM table. 
//  file:///tmp/hcat_junit_warehouse/employee/_DYN0.7770480401313761/emp_country=IN/emp_state=KA  ->   file:///tmp/hcat_junit_warehouse/employee/emp_country=IN/emp_state=KA 
//  Probably a view. 
//  make sure create table fails. 
//  initializes current key 
//  We'll pass ThreadLocals in the background thread from the foreground (handler) thread.   1) ThreadLocal Hive object needs to be set in background thread   2) The metastore client in Hive is associated with right user.   3) Current UGI will get used by metastore when metastore is in embedded mode 
// test_param_1 != "yellow" 
//  the bucket to task map should have been setup by the big table. 
/*          * add value to chain if it is not null or if skipNulls is false.          */
//  Known to not have any nulls. 
//  assertEquals(o, struct); Cannot do this because types of null lists are   wrong. 
//  Add all unique positions referenced. 
//  Inspect the output type of each key expression. 
//  1. Insert two rows to an MM table 
//  Should go here. 
/*    * todo: handle exclusion list   * Figures out which tables to make Acid, MM and (optionally, performs the operation)    */
//  longest run of trailing zeroes 
//  Update is included with the submit request; callback is via notifyStarted. 
//  FIELDS 
//  Only run for N milliseconds 
//  first argument is charCount, which is consumed in this method below 
//  Invalidate all cache entries using this table. 
//  Verify record was written correctly to Parquet 
//  check metric value: 
// expect to have happened by now since HIVE_TXN_TIMEOUT=1sec 
//  Before anyone else accesses it, it would have been allocated and decompressed locally. 
//  TODO: This depends on Tez creating separate threads, as it does now. If that changes, some         other way to propagate/find out attempt ID would be needed (e.g. see TEZ-2587). 
//  we can't use the cached table because it has spilled. 
//  add tables to outputs 
//  If this is an operator then we need to call the plan generation on the 
//  runtime.getMax() gives a very different number from the actual Xmx sizing.   you can iterate through the   http://docs.oracle.com/javase/7/docs/api/java/lang/management/MemoryPoolMXBean.html   from java.lang.management to figure this out, but the hard-coded params in the llap run.sh   result in 89% usable heap (-XX:NewRatio=8) + a survivor region which is technically not   in the usable space. 
//  if custom pattern is set in case of dynamic partitioning, configure custom path 
//    <ResultCast>, <Cleanup> <VectorExprArgType> 
//  Wait for 1 minute and check again. 
//  If METASTORE_HOME is set, use it, else use HIVE_HOME for backwards compatibility. 
//  RELATIVE_PATH 
/*    * This method generates the map of bucket to file splits.    */
//  Read the newly added partition via CachedStore 
//  Loop to get all task completion events because getTaskCompletionEvents 
/*   From the hive logs(hive.log) we can also check for the info statement  fgrep "Total Tasks" [location of hive.log]  each line indicates one run of loadTask.    */
//  e.g., ds=2008-04-08/hr=11 
//  The connection was closed, so create a new one. 
// Handle table properties 
//  This method takes something like String[], so it only accepts   something like String 
//  the separators array   whether we need to escape the data when writing out   which char to use as the escape char, e.g. '\\'   which chars need to be escaped.  
//  Keep track of view alias to view name and read entity   For eg: for a query like 'select * from V3', where V3 -> V2, V2 -> V1, V1 -> T   keeps track of full view name and read entity corresponding to alias V3, V3:V2, V3:V2:V1. 
//  We are not in HS2; always create a new client for now. 
//  this is for artificially added tokens 
//  production: Field()* 
//  Create output directory if not already created. 
//  Might be under the hive name 
//  Hadoop FS ACLs do not work with LocalFileSystem, so set up MiniDFS. 
//  It isn't an error if the following returns no rows, as the local workers could have died   with  nothing assigned to them. 
//  "+" for numeric types. 
//  Get value element information 
//  Stop HiveServer2 to increase header size 
/*      * Notice the default value for LLAP_IO_ENABLED is overridden to be whether we are     * executing under LLAP.      */
//  create fetchwork for non partitioned table 
/*    * Reduce sink operator is the de-facto operator   * for determining keyCols (emit keys of a map phase)    */
//  Spot check decimal column-column multiply 
//  all columns in cluster and sort are valid columns 
//  If issuing a query for all partitions, verify that we need update the same columns. 
//  remember the mapping in case we scan another branch of the 
// Start with size 100 and double when needed. 
//  Test down-casting when greater than 256. 
//  0 is the function name 
//  write bytes to bos ... 
//  operand 
//  Format: "always madvise [never]" 
//  Set to true only when deregistration happens   Web UI 
// may be the db is getting created in this load 
//  threads while the constructor is running. 
//  parenthesis at the end. 
//  Drop table "tbl1" via ObjectStore 
//  blank " " (1 byte) 
//  The grouping set has not yet been processed. Create a new grouping key   Consider the query: select a,b, count(1) from T group by a,b with cube;   where it is being executed in 2 map-reduce jobs   The plan for 1st MR is TableScan -> GroupBy1 -> ReduceSink -> GroupBy2 -> FileSink   GroupBy1/ReduceSink worked as if grouping sets were not present   This function is called for GroupBy2 to create new rows for grouping sets   For each input row (a,b), 4 rows are created for the example above:   (a,b), (a,null), (null, b), (null, null) 
//  Partnames: [tab1part1...tab1part9] 
//  extract configs for processing by the python fragments in YARN Service 
//  work should be the smallest unit for memory allocation 
// so that we know which version wrote the file 
//  We add Hive keywords, including lower-cased versions 
//  we have found the colName. No need to search more exprNodes. 
//  It is assumed isLazy flag is set only for REPL LOAD flow.   IMPORT always do deep copy. So, distCpDoAsUser will be null by default in ReplCopyWork. 
//  Set the appropriate key in the map and test that we are able to read it back correctly. 
//  1. Recompose filter possibly by pulling out common elements from DNF   expressions 
//  inline merge join operator in a self-join 
// http://https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2/src/packages/templates/conf/hdfs-site.xml 
//  skewed column names 
//  At this point if we haven't found it, screw it, we don't know where it is 
//  alias to key mapping 
//  Unsigned 64 max. 
/*    * Job state of job request. Changes to the state are synchronized using   * setStateAndResult. This is required due to two different threads,   * main thread and job execute thread, tries to change state and organize   * clean up tasks.    */
//  Decimal point. 
// perform some Update/Delete 
//  6. Finally, put uncompressed data to cache. 
//  Object inspector hasn't been cached for this type/params yet, create now 
//  print dependent vertexs 
//  insiderView will tell this TableScan is inside a view or not. 
/*    *  Pre-allocated members for storing information equal key series for small-table matches.   *   *  ~HashMapResultIndices   *                Index into the hashMapResults array for the match.   *  ~AllMatchIndices   *                (Logical) indices into allMatchs to the first row of a match of a   *                possible series of duplicate keys.   *  ~IsSingleValue   *                Whether there is 1 or multiple small table values.   *  ~DuplicateCounts   *                The duplicate count for each matched key.   *    */
//  If checksum does not match, likely the file is changed/removed, retry from CM path 
//  Safety check for postconditions 
//  required   required   optional   optional   required   required   optional   required   optional   required   required   optional   optional   optional   optional   optional 
//  This will throw NoSuchLockException (even though it's the   transaction we've closed) because that will have deleted the lock. 
//  Doesn't clear underlying hashtable 
//  1. Simulate emitting all records in closeRecordProcessor(). 
//  initialize the integer values 
//  PARENT_TABLE_NAME 
//  Check whether we are replicating 
// this is a checked expression use a different template for checked expressions 
//  CombinedSplit. 
//  If there are no new segments, we can just bail out 
//  Suppress headers and all objects below. 
//  process join filters 
//  The lock for ZK updates. 
//  This is solely for testing.  It checks if the test has set the looped value to false,   and if so remembers that and then sets it to true at the end.  We have to check here 
//  replace original VAR_SAMP(x) with       (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x))       / CASE COUNT(x) WHEN 1 THEN NULL ELSE COUNT(x) - 1 END 
// "delete from tab1" txn 
//  This is the last RG for which this buffer will be used. Remove the initial refcount 
//  one of the tables that is not in memory 
//  should not be happened. ignore remaining 
//  Don't want to attempt to grab more memory than we have available .. percentage is a bit arbitrary 
//  could be the min_uncommitted_txnid if lesser than NEXT_TXN_ID.ntxn_next. 
//  then we throw an exception 
//  This is not bulletproof but should allow us to close session in most cases. 
//  Create random test string 
//  Arithmetic operations reset the results. 
//  Set up the session for driver. 
//  Create the corresponding hive expression to filter on partition columns. 
//  bit packing disabled 
//  If a single quote is seen and the index is not inside a double quoted string and the previous character   was not an escape, then update the hasUnterminatedSingleQuote flag 
//  No compatible MapWork. 
//  on top. We will add it. 
//  use deep hashcode for arrays 
//  nothing to do - we are not running in local mode - only submitting   the job via a child process. in this case it's appropriate that the   child jvm use the same memory as the parent jvm 
//  introducing explicit aliases for tbl. 
//  10000000....000 
//  check if all parent statistics are available 
// Environment Variables short values 
//  mergeable in next loop iteration. 
//  Mark a fragment as completing, but don't actually complete it yet.   The wait queue should now have capacity to accept one more fragment. 
//  Add the support for read variations in Vectorized Text. 
//  second batch to last but one batch will be actualBatchSize   actualBatchSize is same as batchSize when no exceptions are expected 
//  big tables that should be streamed 
//  1. Get alias from topOps 
//  End HiveRemoveGBYSemiJoinRule 
//  required   required   required   required   optional 
//  get all the dependencies to delete 
//  this is the constructor to use for SMB. 
//  mGby1 ---already contains group by key, we need to remove distinct column 
//  check if the join operator encountered is a candidate for being converted 
//  if all children are done, this operator is also done 
//  test with null or empty randomly 
//  VectorDeserializeRow produces "sparse" VRB when includes are used; we need to write the   "dense" VRB to ORC. Ideally, we'd use projection columns, but ORC writer doesn't use them.   In any case, we would also need to build a new OI for OrcWriter config.   This is why OrcWriter is created after this writer, by the way. 
//  used to store each value's length 
//  Assumes no ranges passed to cache to read have data. 
//  We will split the block at headerIx [splitWays] ways, and take [toTake] blocks,   which will leave [lastSplitBlocksRemaining] free blocks of target size. 
//  create a new InputFormat instance if this is the first time to see 
//  new connections goes to miniHS2_1 now 
//  Convert integer related types because converters are not sufficient 
//  the session hook should set the property 
//  STRING_STATS 
//  end of month behavior 
//  FULL_RESOURCE_PLAN 
/*    * Waits for other threads to join and returns with its Id.    */
//  refer paper 
//  for ExprNodeGenericFuncDesc, it should be deterministic and stateless 
//  Setting a non important configuration should return the same client only 
//  Load to an empty database 
// null server url means local mode 
/*      * Setup the overflow batch.      */
//  get list of dynamic partitions 
// 1. get the ColumnStatsSemanticAnalyzer 
//  Only two elements expected in partExprParts partition column name and partition value 
//  Optimize revoke/grant list, remove the overlapping 
//  A TS can have multiple branches due to DPP Or Semijoin Opt.   USe DFS to traverse all the branches until RS is hit. 
//  2. Collect child projection indexes used 
//  n != batch.size when isRepeating 
//  no col names in parent 
//  be a CM uri in the from path. 
//  We don't want to check types already checked 
//  optional .EntityDescriptorProto io_descriptor = 2; 
//  End SubQueryRemoveRule.java 
//  service not started yet 
//  Set host 
//  The first argument, just set the return to be the standard   writable version of this OI. 
//  Handle hint based semijoin 
//  Evict all blocks. 
//  2) Copy and fixup the parent list of the original child instead of just assuming a 1:1 
//  minimum required. 
//  Not used value. 
//  This will also take care of the queries if query parallelism changed. 
/*    * The current kinds of column vectors.    */
//  One row per value. 
//  check this project only projects one expression, i.e. scalar 
//  If null is returned, then help message was displayed in parseCommandLine method 
//  4. Same as 2. Also emit extra records from a separate thread. 
//  Create an environment variable that uniquely identifies this script 
//  we print the vertex that has more rs before the vertex that has fewer rs. 
/*  Creates an empty union object.  */
//  For non-generic UDF, type info isn't available. This poses a problem for Hive Decimal. 
//  constant map projection of known length 
//  LOG.info("getJobTrackerDelegationToken("+conf+","+userName+")"); 
//  VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + " MATCH isSingleValue " + equalKeySeriesIsSingleValue[equalKeySeriesCount] + " currentKey " + currentKey); 
//  converts partNames,partVals into "partName1=val1, partName2=val2" 
//  Marker to track if there is starting single quote without an ending double quote 
//  Register Plain SASL server provider 
//  Currently 3 known tasks. 1, 2, 5 
//  This happens when the ReduceSink's edge has been removed by cycle   detection logic. Nothing to do here. 
//  the MapJoin's parents may have been replaced by dummy operator. 
//  Blindly add this as a non settable list of list of integers,   should be sufficient for the test case.   Use the standard list object inspector. 
//  roundPower < 0     Negative scale means we start rounding integer digits.     The result will integer result will have at least abs(roundPower) trailing digits.     Examples where the 'r's show the rounding digits:          round(12500, -3) = 13000           // BigDecimal.ROUND_HALF_UP                rrr     Or,  ceiling(12400.8302, -2) = 12500     // BigDecimal.ROUND_CEILING                   rr rrrr     Notice that any fractional digits will be gone in the result. 
//  Fake dead session 
//  EmptyBuckets = false 
//  if we have records left in the group we push one of those 
//  the location string will be of the form:   <database name>.<table name> - parse it and   communicate the information to HCatInputFormat 
//  We cannot merge 
//  This is not an actual list; see intermList. 
//  serialize again 
//  Only publish stats if this operator's flag was set to gather stats 
//  3. Prepare for next iteration (if any) 
//  Dates are stored as long, so convert and compare 
//  broken configuration from mapred-default.xml 
//  else fall through and add this condition as nonEqui condition 
//  reset exec context so that initialization of the map operator happens   properly 
//  Determine distKeyLength (w/o distincts), and then add the first if present. 
//  No truncate (ASCII) -- same maximum length. 
//  Create new value 
//  let it create 57 partitions without any triggers 
//  Turns out partition columns get marked as virtual in ColumnInfo, so we need to 
// Create the metastore client as the clientUgi. Doing so this  way will give the client access to the token that was added earlier 
//  Remove the directories for aborted transactions only 
// test will be in local mode 
//  all the insane DST variations, where we actually end up is anyone's guess. 
//  using reflection and update the MDC. 
//  Random column name to reduce the chance of conflict 
//  Determine the partition columns using the first partition descriptor. 
//  Partitions do not exist for this table 
//  We pass all the checks, we can rewrite 
//  Table is valid 
//  NOT NULL constraint could be enforced/enabled 
//  1.2. We extract the information that we need 
//  new hs2 instance session 
//  no replacement, the existing database state is newer than our update. 
/*        * Trigger kill threads and verify we get InterruptedException and expected Message.        */
/*        * Validate and vectorize the Map operator tree.        */
//  Don't wait if empty - go to take() above, that will wait for us. 
//  Operator with single child 
//  Skip header lines. 
//  There should still now be 6 directories in the location 
//  if cbo is enabled, orderby position will be processed in genPlan 
//  Should never happen... ctor is just assignments. 
//  Already consistent. Can happen w/null lSG. 
//  for NULL 
//  GroupBy generates a new vectorized row batch... 
//  We made sure the references are for different join inputs 
/*    * Helper function to create Vertex for given ReduceWork.    */
//  Counters with vertex name as suffix   desiredCounter = INPUT_FILES   counters: {INPUT_FILES_Map_1 : 5, INPUT_FILES_Map_4 : 10}   outcome: INPUT_FILE : 15 
//  Add file paths of the files that will be moved to the destination if the caller needs it 
//  Need to preserve enabled flag 
//  get service host 
// this simulates the completion of "Update tab2" txn 
//  Get the cookie name 
//  Register information about pushed predicates 
//  the pruning needs to preserve the order of columns in the input schema 
/*          * push filters only for this QBJoinTree. Child QBJoinTrees have already been handled.          */
/*      * if the user has specified a queue name themselves, we create a new session.     * also a new session is created if the user tries to submit to a queue using     * their own credentials. We expect that with the new security model, things will     * run as user hive in most cases.      */
//  Hadoop Configuration Properties   Properties with null values are ignored and exist only for the purpose of giving us   a symbolic name to reference in the Hive source code. Properties with non-null 
//  it is not a terrible thing even if the data is not deleted 
//  "length" of sync entries   number of bytes in hash   escape + hash 
//  Don't set lineage on delete as we don't have all the columns 
//  Now, take the serialized keys we just wrote into our scratch column and look them   up in the IN list. 
/*        * substitute OutputFormat name based on HiveFileFormatUtils.outputFormatSubstituteMap        */
//  string. 
//  no 0-sized block 
//  Tests for the List<Partition> exchange_partitions(Map<String, String> partitionSpecs, String   sourceDb, String sourceTable, String destdb, String destTableName) method 
//  that recognizes parenthesis as a delimiter. 
//  TTL check 
//  Attempt to find maxAppendAttempts possible alternatives to a filename by   appending _a_N and seeing if that destination also clashes. If we're   still clashing after that, give up. 
// now we have delta_0003_0003_0000 with inserts only (ok to push predicate) 
//  Ignore error, just return the valid tables that are found. 
//  Wait queue could have been re-ordered in the mean time because of concurrent task   submission. So remove the specific task instead of the head task. 
//  First, handle the actual thing we found. 
//  look for matches in file system counters 
//  Check fast0. 
//  Verify if both groupset and aggrfunction are empty) 
//  reset temp list index 
//  The toDigitsOnlyBytes stores digits at the end of the scratch buffer. 
//  Give the caller context for future errors. 
//  If value is not a constant, we bail out 
//  Display all non-vectorized leaf objects unless ONLY. 
//  Separate client to create the catalog 
//     conf.setVar(HiveConf.ConfVars.METASTORE_CONNECTION_DRIVER, "com.mysql.jdbc.Driver");      conf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY,          "jdbc:mysql://localhost:3306/metastore_db");      conf.setVar(HiveConf.ConfVars.METASTORE_CONNECTION_USER_NAME, "");      conf.setVar(HiveConf.ConfVars.METASTOREPWD, ""); 
//  If there are no txns which are open for the given ValidTxnList snapshot, then just return it. 
//  All rows should be in the in-memory hashmap 
/*      * Determine the 3 binary words like what SerializationUtils.readBigInteger does.      */
//  Test that fetching a non-existent partition yields ObjectNotFound. 
//  For caching column stats for an unpartitioned table 
//  test repeating on right 
//  grouping id should be pruned, which is the last of key columns 
//  use sub-dir as inputpath. 
//  We should expect a semantic exception being throw as this partition   should not be present. 
//  dfs is AutoCloseable 
//  RO     LEFT\RIGHT   skip  filtered   valid   skip        --(1)     -+(1)   -+(1)   filtered    --(1)     -+(1)   -+(4*)   valid       --(1)     -+(1)   ++(2)     * If left alias has any pair for right alias, continue (3) 
//  NOTE: pig-0.8 sets client system properties by actually getting the client   system properties. Starting in pig-0.9 you must pass the properties in.   When updating our pig dependency this will need updated. 
//  run the script using Beeline 
//  The ptned table should be there in both source and target as rename was not successful 
//  this indicates if corr var is left operand of rex call or not   this is used in decorrelate(logical correlate) to appropriately   create Rex node expression 
//  this is the data copy 
//  Picks topN K:V pairs from input. 
//  This is the old logic which assumes that the filenames are sorted in   alphanumeric order and mapped to appropriate bucket number. 
/*    * Job status request executor to get status of a job.    */
//  list of dynamic partitions 
//  Add partitions with new schema. 
/*    * Set the buffer that will receive the serialized data.  The output buffer will NOT be reset.    */
//  Introduce top project operator to remove additional column(s) that have   been introduced 
//  make the new aggRel 
//  We could have multiple sources restrict the same column, need to take   the union of the values in that case. 
//  such as "%abc%" 
//  prepare plan for submission (building DAG, adding resources, creating scratch dirs etc.) 
//  1 - check that the table is Acid 
//  Don't load defaults.   NOTE: hive-site.xml is only available on client, not AM. 
//  Its a new column 
//  Calculate the number of bytes in the split that are local to each 
//  neg-infinity to start exclusive 
//  Initial MM write ID for CTAS and import. 
//  UDFs 
//  max number of threads we can use to check non-combinable paths 
// note that originalFiles are all original files recursively not dirs 
//  Note: totalDeleteEventCount can actually be higher than real value. 
/*  (non-Javadoc)   * In order to update a Decimal128 fast (w/o allocation) we need to expose access to the   * internal storage bytes and scale.    */
//  netty4  netty3  arrow-vector  arrow-memory  arrow-format  flatbuffers  hppc 
//  Another special case, because timestamp is not implicitly convertible to numeric types. 
// perform simple checksum here; make sure nothing got turned to NULL 
//  and must have locations outside the table directory. 
//  Add the grouping set key to the group by operator.   This is not the first group by operator, but it is a subsequent group by operator   which is forwarding the grouping keys introduced by the grouping sets.   For eg: consider: select key, value, count(1) from T group by key, value with rollup.   Assuming map-side aggregation and no skew, the plan would look like:     TableScan --> Select --> GroupBy1 --> ReduceSink --> GroupBy2 --> Select --> FileSink     This function is called for GroupBy2 to pass the additional grouping keys introduced by 
//  5. hold a lock file in HDFS session dir to indicate the it is in use 
//  rs is semijoin optimization branch, which should look like <Parent>-SEL-GB1-RS1-GB2-RS2 
//  Run a reverse DNS lookup on the URL 
//  AM can not do Kerberos Auth so will do the input split generation in the HS2 
//  First, drop all the dependencies. 
//  we just use view name as location. 
//  We only apply this rule if Union.all is true.   And Sort.fetch is not null and it is more than 0. 
//  Make sure that if the session is returned to the pool, it doesn't live in the global. 
//  Handle to cancel loop 
//  Regardless of the following exception 
//  There may be speculative tasks waiting. 
//  need to do the work to detangle this 
//  If the task hasn't started, and it is killed - report back to the AM that the task has been killed. 
//  We do mnot test this 
//  Possible since either container / task can be unregistered. 
//  A value of 0 for n indicates that the mapper processed data that does not meet   filter criteria, so merge() should be NO-OP. 
//  Spend at most HIVE_PREWARM_SPARK_TIMEOUT to wait for executors to come up. 
// tries to get S lock on T7, S on T7.p=1 and S on T7.p=2 
//  Currently returned bootDumpBeginReplId as we don't consolidate the events after bootstrap 
// tezJsonParser 
//     want query level fairness, and don't want the get in queue to hold up a session. 
//  For PARTIAL2 and FINAL 
//  ID 5 committed, no open IDs 
// and do a Load Data into the same table, which should now land in a delta_x_x. 
//  3. We try to merge the join with the right child 
//  Read db via ObjectStore 
//  Create the object inspector for the input columns and initialize the UDTF 
//  the hmsc is not shared across threads. So the only way it could get closed while we are doing healthcheck   is if removalListener closes it. The synchronization takes care that removalListener won't do it 
// create a table with bad avro uri 
//  PASSWORD 
//  No transaction for the compaction for now. 
//  If existing table is valid but the partition spec is different, then ignore partition   validation and create new partition. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getNCharacterStream(int)    */
//  convert to equivalent time in UTC, then get day offset 
//  Input record iterator, not used 
//  Setup the bloom filter once 
//  we store big keys in one table into one dir, and same keys in other   tables into corresponding different dirs (one dir per table).   this map stores mapping from "big key dir" to its corresponding mapjoin   task. 
//  Process --hiveconf   Get hiveconf param values and set the System property values 
//  Combo 3: url set, literal set to none 
/*  * The interface for a single long key hash multi-set contains method.  */
//  filter disabled, injection disabled, exception not expected 
//  16 
//  Set sasl qop 
//  print the results 
//  The THEN expression is either IdentityExpression (a column) or a ConstantVectorExpression 
//  Test that timestamp arithmetic is done in UTC and then converted back to local timezone,   matching Oracle behavior. 
//  No additional data type specific setting. 
//  Leave requestedHostWillBecomeAvailable as is. If some other host is found - delay,   else ends up allocating to a random host immediately. 
//  Constant if operator is deterministic and all operands are   constant. 
//  The metastore shouldn't care what txn manager Hive is running, but in various tests it 
//  No point separating IOException vs YarnException vs others 
//  We bypass the OR clause and select the first disjunct 
//  be removed, and the size before and after the genRootTableScan will be different. 
//  Process the last byte if necessary. 
//  dummy parent operators as well. 
//  deleteRecord >= firstRecordInBatch or until we exhaust all the delete records. 
// we don't prevent using non-acid resources in a txn but we do lock them 
//  We compare class name/method name using ObjectInspectorUtils.compare(...), to avoid   any object conversion (which may cause object creation) in most cases, when the class 
// since TAB2 is empty  update stmt has p=blah, thus nothing is actually update and we generate empty dyn part list 
//  no replacement, the existing table state is newer than our update. 
//  Build the status message for the /status call. 
//  test if we need partition/global order, SHUFFLE_SORT should only be used for global order 
//  This should not happen, but we ignore for safety 
//  2 - check if partitionvals are legitimate 
/*  If the function has an explicit name like func(args) then call a   * constructor that explicitly provides the function name in the   * funcText argument.    */
//  Nothing special needs to be done for grouping sets if   this is the final group by operator, and multiple rows corresponding to the   grouping sets have been generated upstream.   However, if an addition MR job has been created to handle grouping sets, 
//  18 
//  if this is the last element 
//  4. Now copy the data into cache buffers. 
//  Per JDBC spec, if the connection is closed a SQLException should be thrown. 
// Adding Query specs to be used by org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat 
//  'equals' and 'compareTo' are not compatible with HiveDecimals. We want   compareTo which returns true iff the numbers are equal (e.g.: 3.14 is   the same as 3.140). 'Equals' returns true iff equal and the same scale   is set in the decimals (e.g.: 3.14 is not the same as 3.140) 
//  we are translating Calcite operators into Hive operators. 
//  We will not try partial rewriting for rebuild if incremental rebuild is disabled 
//  Update fetchSize if modified by server 
//  We don't create sessions for empty entries. 
//  Estimate that there will be 16 bytes per entry 
//  Run using environment context with cascade 
//  Get the key positions 
//  test for invalid group name 
//  estimate size of key from column statistics 
//  This is the time zone for VM in test. 
/*    * (non-Javadoc)   *   * @see javax.sql.CommonDataSource#setLogWriter(java.io.PrintWriter)    */
//  Get the top Nodes for this task 
//  Validation is the same as for map join, since the 'small' tables are not vectorized 
//  Check for completed transactions 
//  @@protoc_insertion_point(builder_scope:GetTokenResponseProto) 
//  outputs are ready 
//  enable escaping 
//  accept to start dag (schedule wait time, resource wait time etc.) 
//  Mapping of reducesink to mapjoin operators 
//  make sure the correlated reference forms a unique key check   that the columns referenced in these comparisons form an 
//  Add list bucketing location mappings. 
// ---------------------------------------------------------------------------   Inner big-table only join specific members.   
//  There will be no DDL task created in case if its CREATE TABLE IF   NOT EXISTS 
//  optional bytes token = 1; 
//  HOST_NAME 
//  left border is the max 
//  close + commit 
//  Check if the value is in bloom filter 
//  large 
// 2)  test reordering 
//  SCHEMA_TEXT 
//  ENTITY_TYPE 
//  The output of FINAL and COMPLETE is a full aggregation, which is a   list of DoubleWritable structs that represent the final histogram as   (x,y) pairs of bin centers and heights. 
//  this corresponds to a map<string,?> 
//  1: 
//  initialize the forward operator 
// Now cancel the delegation token 
//  this offer will be accpeted and r1 evicted 
//  Run without round-off 
//  This is a subquery and must have an alias 
//  no values array 
//  Set of UDF classes for type casting data types in row-mode. 
//  If root object is an array, map or collection, add estimators as for fields 
//  Register only if the attempt is known. In case an unregister call   came in before the register call. 
//  Define the SerDe Parameters 
//  check the config used very often! 
//  Perform the same URI normalization as create_database_core. 
//  create a new external table 
//  is and continue processing the children 
//  2^(62 + 63) 
//  mr or spark 
//  remove ' 
//  it's easier then because we simply do division and then scale   down. 
//  these aggregations should be updated only once. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getBlob(int)    */
//  delegate back to the "local" serializeRowId method 
//  Bound skip by beginning and end of the source 
//  Not public since we must have the field count and other information. 
//  If help option is requested, then display help and exit 
//  VectorReduceSinkOperator is not native vectorized. 
// this was 1/4 acid 
//  way! 
//  need to clone the plan. 
//  should have null for non-specified comments 
//  VectorMapJoinOperator is not native vectorized. 
// insert of merge lands in part (3,4) - no updates land there 
//  When we have multiple values, we save the next value record's offset here. 
//  removed. 
/*      * Output Columns in the following order     * - the columns representing the output from Window Fns     * - the input Rows columns      */
//  How to get config paths and AmInfo 
//  For ORC, there is no Tez Job for table stats. 
//  This happens if register calls getMetrics. 
//  1) Obtain input and all related data structures 
//  Text file comparison 
//  initializeOp can be overridden   Initializing data structures for vectorForward 
//  Master node will serialize writercontext and will make it available at slaves. 
//  Transactions 
//  WARNINGS 
// list of last keys for each stripe 
//  If the function is deterministic and the children are constants, 
/* String s = "SELECT COLUMN_NAME FROM " + (ci.partName == null ? "TAB_COL_STATS" :          "PART_COL_STATS")         + " WHERE DB_NAME='" + ci.dbname + "' AND TABLE_NAME='" + ci.tableName + "'"        + (ci.partName == null ? "" : " AND PARTITION_NAME='" + ci.partName + "'"); */
//  Default is double, but if one of the sides is already in decimal we   complete the operation in that type. 
//  Calcite year-month literal value is months as BigDecimal 
//  verify proper null output data value 
//  Assert class-invariant. 
//  we can tolerate this as this is the previous behavior 
//  Older version of hadoop should have had this field 
//  Eliminate MR plans with more than one TableScanOperator. 
//  7. Convert Hive projections to Calcite 
//  Create callables with different queries. 
//  This constructor appeared in 1.4 and specifies that we do not want to   line-wrap or use any newline separator 
//  no sub-directories 
//  Get the serialized value for the column 
/*  isVectorMapJoin  */
//  Make sure the UGI contains the token too for good measure 
// Update max counter if new value is greater than max seen so far 
//  Do not allow view to be defined on temp table or other materialized view 
//  Evaluate the expression tree. 
/*  @bgen(jjtree) Throws  */
//  It is possible that some operators add records after closing the processor, so make sure 
//        query is being killed until both the kill, and the user, return it. 
//  The uri requested 
//  check if its a simple cast expression. 
//  Last item -- ok to be at end. 
//  (nulls, etc.). vice versa. 
//  default is using long types 
//  if top is null then there are multiple parents (RS as well), hence   lets use parent statistics to get data size. Also maxSplitSize should 
//  Need to be in consistent with that VectorizedPrimitiveColumnReader#readBatchHelper 
//  LocalDate must be present 
// now run as if it's a minor Compaction so we don't collapse events 
/*      * The default number of threads will be 0. That means thread pool is not used and     * operation is executed with the current thread.      */
// Reset the PerfLogger 
//  Now make a copy. 
//  check result now 
//  The schedule loop will be triggered again when the deallocateTask request comes in for the   preempted task. 
//  Step 3 : parse the query   Set dynamic partitioning to nonstrict so that queries do not need any partition 
//  test when second argument has nulls 
//  base configuration   active configuration 
//  create table, db 
//  property names needed to keep internal structure of serde 
// to allow cross join from 'teeCurMatch' 
//  Get failed attempts 
//  local resources are session based. 
//  Populate the driver context with the scratch dir info from the repl context, so that the temp dirs will be cleaned up later 
//  The first entry with accumulated count (lower+1) corresponds to the lower position. 
//  It's difficult to impossible to pass global things to compilation, so we have a static cache. 
//  queries like select * from t1 where 'foo';   Calcite's rule PushFilterThroughProject chokes on it. Arguably, we   can insert a cast to   boolean in such cases, but since Postgres, Oracle and MS SQL server   fail on compile time   for such queries, its an arcane corner case, not worth of adding that   complexity. 
//  optional buffer to use when actually copying in data   next free position in buffer 
//  table name has to be present so min child 1 and max child 4 
//  Cache to use during optimization 
//  convert partition to partition spec string 
//  WRITE_ID_HIGH_WATER_MARK 
//  12 is timeout and 255 is unspecified error 
//  -f <file> 
//  1. Find our bearings in the stream. Normally, iter will already point either to where we   want to be, or just before. However, RGs can overlap due to encoding, so we may have 
//  This will only be set if the metastore is being accessed from a metastore Thrift server,   not if it is from the CLI. Also, only if the TTransport being used to connect is an 
//  Event 2 
//  Test the idempotent behavior of Abort Txn 
//  void, boolean, byte, short, int, float   long, double 
/*  @bgen(jjtree) TypeBool  */
//  Forward any remaining selected rows. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getMetaData()    */
//  This logic assumes one dag at a time; if it was not the case it'd keep rewriting it. 
//  required   required   required   required   required   required   required   required   required   required   optional   optional 
//  timestamp - timestamp   interval_day_time +/- interval_day_time 
//  Keys are always primitive, respect the binary 
//  Event 1 
//  TODO: add a section like the restricted configs for overrides when there's more than one. 
//  dryRun = true, immutable = true 
//  each column's length in the value 
// Query the hbase table and check the key is valid and only 5  are present 
/*  Invalid FileSystem schemes  */
//  Insert some data -> this will again generate only insert deltas and no delete deltas: delta_2_2 
//  If we have some sort of expression tree, try JDOQL filter pushdown. 
//  Regression test for defect reported in HIVE-6243 
//  Many old tests depend on this. 
//  List of non-distinct aggrs. 
//  dataSchema can be obtained from partitionInfo.getPartitionSchema() 
//  Gather information about the DPP table scans and store it in the cache 
/*    * Context for reading using the regular partition deserializer to get the row object and   * assigning the row object into the VectorizedRowBatch with VectorAssignRow    */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBytes(int, byte[])    */
//  Default run 
// This doesn't throw MetaException when setting to high max part count 
//  Project operator, we can continue 
//  RESULTS 
//  Insert entries to TXN_TO_WRITE_ID for aborted write ids 
//  Store the user name in the open request in case no non-sasl authentication 
//  i.e. Is the partition outside the table-dir? 
//  can be inherited from a base class. 
//  load them into a set 
//  Clip off seconds portion.   Bring nanoseconds into integer portion. 
// / ALTER TABLE scenarios 
//  even without type params, return a default OI for varchar 
//  remember the mapping in case we scan another branch of the mapjoin later 
//  for DENSE encoding, use bias table lookup for HLLNoBias algorithm 
//  Void can be converted to any type 
//  Verify that the node is blacklisted 
//  Write the "suffix" of the cq 
//  unequal maps 
//  We're told to use some port but it's occupied, fail 
//  remember the branching ops we have visited 
//  Remove the join operator from the query join context   Data structures coming from QBJoinTree 
//  Special handling for decimal because decimal types need scale and precision parameter.   This special handling should be avoided by using returnType uniformly for all cases. 
//  Lookup list bucketing pruner 
//  after some other submission has evicted it. 
//  Once we move to a Hadoop-2.8 dependency, the following paramteer can be used.   conf.set(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETRY_POLICY_SPEC); 
//  Buffers at offset 2 & 3 exist; 1 exists and is stale; 4 doesn't 
//  Go through the set of key columns, and find their representatives in the values 
//  We need to work a little harder for our comparison.  Note we round down for   integer conversion so anything below the next min/max will work. 
//  Vectorized implementation of Hex(long) that returns string 
//  enable trash so it can be tested 
//  In non-test mode, emit to a log file,   which can be different from the normal hive.log.   For example, using NoDeleteRollingFileAppender to   log to some file with different rolling policy. 
//  Double.compare() treats -0.0 and 0.0 as different 
//  tempTable is only set when load is rewritten. 
//  1. We iterate through all the operators that have candidate FKs and   choose the FK that has the minimum selectivity. We assume that PK and this FK   have the PK-FK relationship. This is heuristic and can be 
// remove the locks in Waiting state 
//  STAT 
//  work.checkFileFormat is set to true only for Load Task, so assumption here is   dynamic partition context is null 
//  No need to check catalog for null as parseDbName() will never return null for the catalog. 
//  they are not the same constant. for example, union all of 1   and 2. 
//  There should be 1 new base directory: base_0000001   Original bucket files, delta directories, delete_delta directories and the 
//  Not supported for temp tables. 
//  performance problem: ObjectStore does its own new HiveConf() 
//  required   optional   required   required   optional 
// keep track of corresponding col in partCols 
//  CTAS should NOT create a VOID type 
//  compute the number of tasks 
//  Put it all in the Mutation 
//  Test double 
/*    * Initialize one column's source conversion related arrays.   * Assumes initTargetEntry has already been called.    */
//  It passes the test, it is valid 
//  Privilege matrix:                      user1  user2  group_a  group_b  public   testdb1:            S             S     testtable1.*:     SU     S     testtable2.*:                   S     testtable3.*:                                     S     testtable4.*:                            S   testdb2:            S 
//  Register the new dag identifier, if that's not the one currently registered. 
//  ACCEPTED 
//  Capture system out and err 
//  Allow implicit String to varchar conversion, and vice versa 
//  Set the run frequency low on this test so it doesn't take long 
//  call-1: open to read data - split 1 => mock:/mocktable4/0_0 
/*    * Executes job request operation. If thread pool is not created then job request is   * executed in current thread itself.   *   * @param jobExecuteCallable   *          Callable object to run the job request task.   *    */
//  Keep track of view alias to read entity corresponding to the view   For eg: for a query like 'select * from V3', where V3 -> V2, V2 -> V1, V1 -> T   keeps track of aliases for V3, V3:V2, V3:V2:V1.   This is used when T is added as an input for the query, the parents of T is 
//  1. Handle kill query results - part 1, just put them in place. We will resolve what 
//  worth it. 
//  overflow means this is smaller 
//  All zeroes -- we should have handled this earlier. 
//  before any of the other core hive classes are loaded 
/*    * DATE.    */
//  Need to extend the tenancy if we saw a newer file with the same content 
//  Count all rows. 
//  Fix temp path for alter table ... concatenate 
//  conf and then the children 
//  But if hive supports assigning bucket number for each partition, this can be vary 
//  new filter; currently we support comparison functions, in and between 
//  if both are last day of the month then time part should be ignored 
//  Sort cost 
//  no begin + abort 
//  Legacy file, see if it's a bucket file 
//  Call the real method instead of the mock 
//  Replication done, we now do the following verifications: 
//  TODO: these would need to be propagated from AM via progress.   @Metric("Number of allocated guaranteed executors in use"),   @Metric("Number of speculative executors in use") 
//  6.2 Convert UDAF Params to ExprNodeDesc 
//  generate absolute path relative to home directory 
//  string to encrypt 
//  2: Create an unpartitioned table T1 => 1 event 
//  write addition payload required for orc 
//  such as "a_b"   such as "a%bc" 
//  MY_STRUCTLIST 
//  a db owner can be a user or a role 
//  Whole batch is spilled. 
//  tblProps will be null if user didnt use tblprops in his CREATE   TABLE cmd. 
//  read that many chars 
//  Count zeros until first non-zero digit is encountered. 
//  if this table has an associated index table then attempt to build   index mutations 
//  An update needs to select all of the columns, as we rewrite the entire row.  Also,   we need to figure out which columns we are going to replace. 
//  holds restored (from disk) big table rows 
//  for the big table, we only need to promote the next group to the current group. 
//  now it succeeds. 
//  Convert the fields 
//  try using jdbc metadata api to get column list as user2 - should fail 
//  Data structure to control whether a certain reference is present in every 
//  Find the value object   Update the timestamp of the key,value if value matches the criteria 
//  Replace INSERT OVERWRITE by INSERT INTO   AST tree will have this shape:   TOK_QUERY     TOK_FROM        ...     TOK_INSERT        TOK_DESTINATION <- THIS TOKEN IS REPLACED BY 'TOK_INSERT_INTO'           TOK_TAB              TOK_TABNAME                 default.cmv_mat_view        TOK_SELECT           ... 
// if we are waiting for connection for a long time, something is really wrong  better raise an error than hang forever  see DefaultConnectionStrategy.getConnectionInternal() 
//  join from multiple relations: 
// Map<?,?> c7Value = (Map<?,?>) rowValues[6];  assertEquals(0, c7Value.size()); 
//  key:column output name, value:tag 
//  Get the sign of the decimal. 
//  lazy mode => we only list files, and expect that the eventual copy will pull data in.   default is that the import mode is insert overwrite   WriteIds snapshot for replicating ACID/MM tables.   DEFAULT means REPL_LOAD or BOOTSTRAP_DUMP or EXPORT 
//    see if the filename matches and we can read it 
//  Create db1/t1/dt=20160101/part                /dt=20160102/part                /dt=20160103/part   Test: recycle single file (dt=20160101/part)         recycle single partition (dt=20160102)         recycle table t1 
//  When union is followed by a multi-table insert 
//  getAllFunctions() 
//  If this operator has been visited already by the rule, 
//  When we were inserting the key, we would have inserted here; so, there's no key. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getFunctions(org.apache.hive.service.cli.SessionHandle, java.lang.String)    */
//  Don't recheck with next, only 2 lists each w/o collisions. 
//  instances is likely incorrect. 
//  1 + 2 + 4 + 8 + 4 + 8 + 5 + 2 + 4 + 3 + 4 + 4 + 4 + 4 + 4 + 3 = 64 
//  Try marking the query as complete if this is an external submission 
//  TODO: UNIQUE JOIN 
//  then it's a dynamic partitioning case and we shouldn't check the table itself. 
//  Simple task registration and un-registration. 
//  #getColumn(2) should return the instance passed in: 
//  AWS settings 
//  Walk through all the source vertices 
// can happen in retrying deleting the zLock after exceptions like InterruptedException  or in a race condition where parent has already been deleted by other process when it  is to be deleted. Both cases should not raise error 
// --------------------------------------------------------------------------------------------- 
//  For now, throw away file. 
//  Override to do nothing, as the this test is not related with vectorization.   The parent class creates a temporary table in this test and alters its properties.   To not override this test, that temporary table needs to be renamed. However, as   mentioned this does not serve any purpose, as this test does not relate to vectorization. 
// should only get here if retrying this op 
//  if any filters are present in the join tree, push them on top of the   table 
//  rFile = new RandomAccessFile(tmpFile, "rw"); 
// JSON key is always a String 
//  All the objects must be different. 
/* {"writeid":0,"bucketid":536870912,"rowid":0}     0       2/000000_0{"writeid":0,"bucketid":536870912,"rowid":1}     0       4/000000_0{"writeid":1,"bucketid":536870912,"rowid":0}    4       4/delta_0000001_0000001_0000/000000_0{"writeid":1,"bucketid":536870912,"rowid":1}    5       5/delta_0000001_0000001_0000/000000_0 */
//  0. Generate a Select Node for Windowing 
//  Empty queue. But no capacity available, due to waitQueueSize and additionalElementsAllowed   Return the element. 
//  start_date is Wed, full timestamp, full day name 
//  The name of this column in the Hive schema 
//  6 highword digits. 
//  moved...this may change 
/*    * Get the index separating the user name from domain name (the user's name up   * to the first '/' or '@').   *   * @param userName full user name.   * @return index of domain match or -1 if not found    */
//  Uses a pattern and specifies a DB 
/*        * b. Build Reduce Sink Details (keyCols, valueCols, outColNames etc.) for this ptfDesc.        */
//  change it to local 
/*  256 files x 100 size for 9 splits  */
//  Different number of field names 
//  process the second child,if exists, node to get partition spec(s) 
/*    * Initialize using one target data type info.    */
//  if there are multiple stats for the same scheme (from different NameNode), this   method will squash them together 
//  Can't use CacheLoader because SearchArguments may be built either from Kryo strings, 
//  Since EXISTS/NOT EXISTS are not affected by presence of   null keys we do not need to generate count(*), count(c) 
//  move file would require session details (needCopy() invokes SessionState.get) 
//  Check fast1. 
//  We hold one refcount. 
//  Not MM. 
//  Disallow changing temp table location 
//  NULLs are handled by each individual base writer setter   We could handle NULLs centrally here but that would result in spurious allocs 
//  In a simple storage-based auth, we have no information about columns   living in different files, so we do simple partition-auth and ignore   the columns parameter. 
//  prepare the tmp output directory. The output tmp directory should   exist before jobClose (before renaming after job completion) 
//  Puts, gets, hits, unused, unused. 
//  Union type is not supported in Calcite. 
//  TODO: see if we can get rid of this... used in one place to distinguish archived parts 
//  We failed to update this task. Instead of retrying for this task, find another.   To change isGuaranteed and modify maps, we'd need the epic lock. So, we will not 
//  Modify TableScanOperator in-place so it knows to operate vectorized. 
//  strip off the delimiter 
//  A new row is also inserted in the usual delta file for an update event. 
//  Finish the scheduled compaction for ttp2 
//  Current installed Configuration 
//  If config is set, table is not temporary and partition being inserted exists, capture   the list of files added. For not yet existing partitions (insert overwrite to new partition   or dynamic partition inserts), the add partition event will capture the list of files added.   Generate an insert event only if inserting into an existing partition 
/*    * This is used during translation to decide if the internalName -> alias mapping from the Input to the PTF is carried   * forward when building the Output RR for this PTF.   * This is used by internal PTFs: NOOP, WindowingTableFunction to make names in its input available in the Output.   * In general this should be false; and the names used for the Output Columns must be provided by the PTF Writer in the   * function getOutputNames.    */
//  Wait for all writes to finish before we actually close. 
//  type of target column 
//  partitioned table 
//  The dispatcher fires the processor corresponding to the closest matching rule 
//  Host 
//  Floor on date: special handling since function in Hive does   include <time_unit>. Observe that <time_unit> information   is implicit in the function name, thus translation will   proceed correctly if we just ignore the <time_unit> 
//  FKTABLE_DB 
//  prefer right most alias 
//  lrfuThreshold is +inf in this case 
// reached End Of Split 
//  if there is any confict, then we do not generate it in the new select   otherwise, we add it into the calciteColLst and generate the new select 
//  Ignore nullscan-optimized paths. 
/*    * Setup our inner big table only join specific members.    */
//  Always want to re-create pm as we don't know if it were created by the 
//  an blocking operator (e.g. GroupByOperator and JoinOperator) can 
//  FOREIGN_CATALOG_NAME 
/*      * Iterate over the Symbol Functions in the Chain:     * - If we are not at the end of the Iterator (i.e. row != null )     * - match the current componentFn     * - if it returns false, then return false     * - otherwise set row to the next row from the Iterator.     * - if we are at the end of the Iterator     * - skip any optional Symbol Fns (star patterns) at the end.     * - but if we come to a non optional Symbol Fn, return false.     * - if we match all Fns in the chain return true.      */
//  -Xmx not specified 
//  Set longPollingTimeout to a custom value for different test cases 
//  Note that reset also resets the data buffer for bytes column vectors. 
//  If we're here, we'll proceed down the next while loop iteration. 
//  Create data buffers for value bytes column vectors. 
//  compare every groupbyMapAggrInterval rows 
//  Create an object inspector 
//  Lock types 
//  Find the old database id 
//  The sorting order of the parent RS is more specific or they are equal.   We will copy the order from the child RS, and then fill in the order   of the rest of columns with the one taken from parent RS. 
/*    * This method is mainly intended for debug display purposes.    */
//  Define summary metrics for each column 
//  in this case, we have to scale up _BEFORE_ division. otherwise we   might lose precision. this is costly, but inevitable. 
//  but we want it to, so just re-set it if it's null. 
//  We optimize performance by only looking up the first key in a series of equal keys. 
//  generic options parser doesn't seem to work! 
//  Allocate t4 at higher priority. t3 should not be allocated, 
/*  256 files x 1000 size for 9 splits  */
//  This is done at pre-read stage where there's nothing special w/refcounts. Just release. 
//  set the comparison in the IOContext and the type of the UDF 
//  Create the HFile writer 
//  Try extended deduplication 
//  the name of the field - not optional 
//  Need to preserve authorizer flag 
//  Built-in functions shouldn't go in the session registry,   and temp functions shouldn't go in the system registry.   Persistent functions can be in either registry. 
//  required   required   required   required   optional   optional   optional   optional   optional   optional 
//  force BI to avoid reading footers 
//  If the result is already present in the cache, return it. 
/*      * Setup for 3 different kinds of vectorized reading supported:     *     *   1) Read the Vectorized Input File Format which returns VectorizedRowBatch as the row.     *     *   2) Read using VectorDeserializeRow to deserialize each row into the VectorizedRowBatch.     *     *   3) And read using the regular partition deserializer to get the row object and assigning     *      the row object into the VectorizedRowBatch with VectorAssignRow.      */
// "warehouse/t/HIVE_UNION_SUBDIR_15/000000_0" is a meaningful path for nonAcid2acid 
// no records will be emited from Hive 
//  Filter.g cannot parse a quoted date; try to parse date here too. 
//  verify whether the sql operation log is generated and fetch correctly in async mode. 
//  to get access to the queryInfo instance. 
//  Set the row values 
//  Handle MergeJoin specially and check for all its children 
//  Push filter on top of children for discardable 
//  leftFast1 != 0. 
//  Run initiator to clean the row fro the aborted transaction from TXNS. 
//  If there is a single column, return the number of distinct values 
//  5/ test serialization and deserialization with different schemas 
//  Use the REWRITTEN AST 
//  for writing out single byte 
//  then search from parent 
//  If the top-level object inspector is non-settable return false 
//  The keyEvaluate reuses storage.  That doesn't work with SMB MapJoin because it   holds references to keys as it is merging. 
//  Event 6 
//  give HMS time to handle close request 
//  Used for showJobFailDebugInfo 
//  cKey is not present in parent 
//  If a replacement exists for this code point, emit out the replacement and append it to the 
//  Try scheduling again. 
//  optional string myString = 2; 
//  set bit to 1 if a field is not null 
//  is retried. "2000,1" means 3 retries - each with 1 retry with a random 2000ms sleep. 
//  Prune partitions 
//  Test that existing exclusive db with new shared_read coalesces to 
//     assertEquals(null,stats.getSum()); 
//  mapjoin should not affected by join reordering 
//  hadoop 2.4 and earlier way of finding the sasl property settings   Initialize the SaslRpcServer to ensure QOP parameters are read from   conf 
//  Drop table with partitions 
//  thus we run the field trimmer again to push them back down 
//  Datanucleus propagates some pointless exceptions and rolls back in the finally. 
//  Get of non-existent key should terminate.. 
//  double scalar/long column IF 
//  If/else chain arranged in likely order of frequency for performance 
//  check if required privileges is subset of available privileges 
//  SettableListObjectInspector 
//  Event 5 
//  print operators 
//  We've already dropped testDbName in constructor & we also drop it in tearDownAfterClass 
//  Multiply by inverse (2^-N) to do the 2^N division. 
//  no data for bucket 3 -- expect 0 length bucket file 
//  Similarly, this is map of which vectorized row batch columns are the big table value columns.   Since we may have value expressions that produce new scratch columns, we need a mapping. 
//  To detect incorrect lists. 
//  no stripes satisfies the condition 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#isValid(int)    */
//  2: 
//  A user could encounter this if a stored view definition contains   an old SQL construct which has been eliminated in a later Hive   version, so we need to provide full debugging info to help   with fixing the view definition. 
//  Restore the old value. 
// now that we are using LEFT OUTER JOIN to join inner count, count(*)   with outer table, we wouldn't be able to tell if count is zero   for inner table since inner join with correlated values will get rid   of all values where join cond is not true (i.e where actual inner table   will produce zero result). To  handle this case we need to check both   count is zero or count is null 
//  default to -1 means we leave it up to Tez to decide 
//  Restrict the set of columns that we want to read from the Accumulo table 
//  [A: 0, B: 1, B.x: 0, B.y: 0, C: 0] 
// previous insert+union creates 3 data files (0-3) 
//  if rowCnt < 1 than its either empty table or table on which stats are not    computed We assume the worse and don't attempt to optimize. 
//  For CalcitePlanner, store qualified name too 
//  list need to be refactored out to be done only once. 
//  Try the base config 
//  affects some less obscure scenario. 
//  For primary keys, we retrieve the column descriptors if retrieveCD is true (which means   it is an alter table statement) or if it is a create table statement but we are 
//  Hive jars on the Accumulo classpath which we don't want 
//  The DPP operator/branch are equal 
//  Event 3 
//  Add the input with project on top 
//  Make sure we handle unary + and - correctly. 
//  Make sure that the unwanted key is not present in the map 
//  Setup our hash table specialization.  It will be the first time the process   method is called, or after a Hybrid Grace reload. 
//  Create a lock and trigger a heartbeat. With heartbeat, the lock won't expire. 
//  For the FetchTask, the limit optimization requires we fetch all the rows   in memory and count how many rows we get. It's not practical if the 
//  Create the mapping corresponding to the grouping set 
//  out the escaped byte in the block above already. 
//  Otherwise, only order by expressions. 
//  Gosh, really both scaling up and down.   unscaledValue = significand * 5**(scale) /   2**(twoScaleDown-scale)   To check overflow while preserving precision, we need to do a   real multiplication 
// We may be parsing a delta for Insert-only table which may not even be an ORC file so  cannot have ROW_IDs in it. 
//  if get exception in finding partition   it could be DESCRIBE table key   return null   continue processing for DESCRIBE table key 
//  test the mapping of empty string to all columns 
//  be updated to bytes per reducer (1GB default) 
/*  Two of the optimization rules, ConvertJoinMapJoin and RemoveDynamicPruningBySize, are put into     stats dependent optimizations and run together in TezCompiler. There's no guarantee which one     runs first, but in either case, the prior one may have removed a chain which the latter one is     not aware of. So we need to remember the leaf node(s) of that chain so it can be skipped.     For example, as ConvertJoinMapJoin is removing the reduce sink, it may also have removed a     dynamic partition pruning operator chain. However, RemoveDynamicPruningBySize doesn't know this     and still tries to traverse that removed chain which will cause NPE.     This may also happen when RemoveDynamicPruningBySize happens first.     */
//  Db name for materialization to rebuild   Name for materialization to rebuild 
//  3 Convert OB expr (OB Expr is usually an input ref except for top 
//  Create a semantic analyzer for the query 
// which was modified by the T1 update stmt or choose a non-conflicting one 
//  Do not release beyond current stream (we don't know which RGs that buffer is for). 
//  Move offset to point to start of next input. 
//  All are null, so all must be selected. 
//  The last char is an escape char, read the actual char.   The serialization format escape \0 to \1, and \1 to \2,   to make sure the string is null-terminated. 
// assumes bucket_NNNNN format of file name 
//  get column name 
//  This is what the Vectorizer class does. 
//  We transform the BETWEEN clause to AND clause (with NOT on top in invert is true).   This is more straightforward, as the evaluateExpression method will deal with 
//  convert to a lazy object and return 
//  used to determined whether the merge can happen 
//  Succeed - trying to set "transactional" to "true", and satisfies bucketing and Input/OutputFormat requirement 
//  NOTE: we should gather stats in MR1 rather than MR2 at merge job since we don't 
// initial state is one connection 
// Inner classes 
//  Should go through in a single process call 
//  either schema literal, schema url or serialization class must   be provided 
//  20seconds to wait for app to be visible 
//  given the current input row, the mapping for input col info to dp columns, and # of dp cols,   return the relative path corresponding to the row. 
//  If sort contains a limit operation, we bail out 
//  None of the operators is changing the positions 
//  first operator of the reduce task. (not the reducesinkoperator, but the 
// This is just a simple way to generate test data 
//  Create enough keyConverters/valueConverters   NOTE: we have to have a separate key/valueConverter for each key/value,   because the key/valueConverters can reuse the internal object.   So it's not safe to use the same key/valueConverter to convert multiple   key/values. 
//  We use the trick mentioned in "Less Hashing, Same Performance: Building a Better Bloom Filter"   by Kirsch et.al. From abstract 'only two hash functions are necessary to effectively   implement a Bloom filter without any loss in the asymptotic false positive probability' 
//  Validate the metastore client call validatePartitionNameCharacters to ensure it throws   an exception if partition fields contain Unicode characters or commas 
//  first item 
//  Check for DPP and semijoin DPP 
//  any value will become zero. even no possibility of rounding 
//  will cause overflow for result at position 0, must yield NULL 
// This error code should really be produced by Hive 
//  metastore calls timing information 
//  When truncated included is used, its length must be at least the number of source type infos.   When longer, we assume the caller will default with nulls, etc. 
//  if we are in close op phase, we have definitely exhausted the big table input 
//  Check if array is null or empty or value is null 
//  Pick the formatter to use to display the results.  Either the   normal human readable output or a json object. 
//  The DecimalColumnVector set method will quickly copy the deserialized decimal writable fields. 
//  find the file on the include path 
//  Primarily to avoid multiple shutdowns. 
//  Temporary tables created during the execution are not the input sources 
//  null principal 
//  replace original VAR_POP(x) with       (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x))       / COUNT(x) 
//  We use a StringBuilder and then call printError only once as   printError will write to both stderr and the error log file. In   situations where both the stderr and the log file output is   simultaneously output to a single stream, this will look cleaner. 
//  private boolean isOuterJoin; 
//  We don't send messages to pending tasks with the flags; they should be killed elsewhere. 
//  Recursive flush to flush all the tree operators 
/*    * Test updateCredentialProviders does not corrupt existing values of   * Mapred env configs    */
//  sort the works so that we get consistent query plan for multi executions(for test verification). 
//  Process the report 
//  for wordshift 
//  Thursday 1st August 1985 12:00:00 AM 
//  neither dir should get created. 
//  statistics annotation fetches column statistics for all required columns which can 
//  We just had leading zeroes.   Value is 0. 
//  find the ancestor which exists to check its permissions 
//  load properties from spark-defaults.conf. 
//  Prepare aggs for updating 
/*    * This method updates the input expr, changing all the   * ExprNodeColumnDesc in it to refer to columns given by the   * colExprMap.   *   * For instance, "col_0 = 1" would become "VALUE.col_0 = 1";   * the execution engine expects filters in the Join operators   * to be expressed that way.    */
//  new n-gram 
//  1. If the input node is not an IN operator, we bail out. 
//  We should find an alias of this insert and do (alias).*. This however won't fix e.g.   positional order by alias case, cause we'd still have a star on the top level. Bail. 
//  Set to 1 so insert doesn't set it off but update does 
//  First generate the expression for the partition and sort keys   The cluster by clause / distribute by clause has the aliases for   partition function 
//  Because there is a 1-N relationship between CDs and SDs,   we must set the SD's CD to null first before dropping the storage descriptor   to satisfy foreign key constraints. 
// see bucket_num_reducers.q bucket_num_reducers2.q 
// we are in CTAS, so we know there are no partitions 
//  If the character set for decoding is constant, we can optimize that 
//  Retry the next port 
//  deltas and base and leave them up to the cleaner to clean up 
//  We have already locked the table, don't lock the partitions. 
//  Root tran, it must be MapInput 
//  For dpp case, dpp sink will appear in Task1 and the target work of dpp sink will appear in Task2.   Task2 is the child task of Task1. Task2 will be traversed before task1 because TaskGraphWalker will first   put children task in the front of task queue.   If a spark work which is equal to other is found and removed in Task2, the dpp sink can be removed when Task1   is traversed(More detailed see HIVE-16948) 
//  appended once all the session list are added to the url 
//  1. Insert two rows to an ACID table 
//  If left and right aliases are all valid, two values will be inner joined, 
// doesn't vectorize (uses neither of the Vectorzied Acid readers) 
//  -e 'quoted-query-string' 
/*      * Tracks the last non-OOB heartbeat number at which counters were sent to the AM.       */
//  AggKey in StatsWork is used for stats aggregation while StatsAggPrefix   in FileSinkDesc is used for stats publishing. They should be consistent. 
//  LLAP object cache, unlike others, does not use globals. Thus, get the existing one. 
//  Start HS2 with SSL 
//  No boolean value match for other lengths. 
// cannot be acid 
// if writing to a partitioned table, then pigSchema will have more columns than tableSchema  partition columns are not part of tableSchema... e.g. TestHCatStorer#testPartColsInData()          HCatUtil.assertNotNull(hcatFieldSchema, "Nothing matching '" + fSchema.alias + "' found " +                  "in target table schema", LOG); 
//  e.g. -(17#).e-###   see http://download.oracle.com/javase/6/docs/api/constant-values.html#java.lang.Double.MAX_EXPONENT 
//  Collect bucket and/or partition information for object hashing. 
//  every slot is a long 
//  We will try to combine multiple clauses into a smaller number with compatible keys. 
//  A flag for each byte to indicate if escape is needed. 
//  check if all of the bit vectors can merge 
//  invalid number 
//  initialize buffer to read the entire stripe 
//  add_partitions(5,4) : err = duplicate keyvals on mpart4 
//  Dispatch current node 
//  Queries rejected from being cached due to non-deterministic functions, temp tables, or other conditions. 
/*    * - called during translation.   * - invokes createEvaluator which must be implemented by a subclass   * - sets up the evaluator with references to the TableDef, PartitionClass, PartitionMemsize and   *   the transformsRawInput boolean.    */
//  Handle rename and other changes. 
//  Rule cannot be applied if there are GroupingId because it will change the   value as the position will be changed. 
//  The length of the long array that needs to be passed to serializationUtilsWrite. 
//  Allow accessing a field of list element structs directly from a list 
//  Print the key 
//  if kill query is null then session might have been released to pool or closed already 
//  This can be easily merged into 1 union 
//  If we have Kerberos credentials, we should obtain the delegation token 
//  first we check if we *can* run in llap. If we need to use   user code to do so (script/udf) we don't. 
//  We should always get a different object, and cluster fraction should be propagated. 
//  Set isManaged to false as this is not load data operation for which it is needed. 
// check status of compaction job 
//  While there are still nodes to dispatch... 
//     CacheChunks, so the list is just CacheChunk-s from that point on. 
//  to have more control. 
//  for rest of the join type we will take min of the reduction. 
//  verify that ptned table rename succeded. 
//  bucket map join the big table's bucketing version is considered. 
// ZK Stuff 
//  Dump and load insert after truncate (1 record) 
// branches. 
//  Copy the digits in the right side of the array 
//  We are replacing the current big table with a new one, thus 
//  Set the actual events for the tasks. 
//  required   required   required   required   required   required   required   required   required   required   required   required   optional   optional   optional   optional   optional   optional 
// operationManager.closeOperation() is expected to be invoked once 
//  Register taskAttempt, unregister container. TaskAttempt should also be unregistered 
//  If neither cred provider or conf have entry, return null; 
//  and "skewed columns != skewed keys" in selectOpClone 
//  ditto 
//  free the memory for the column vectors 
//  It is in the cache and up to date 
//  take integer part of it 
// This happens when the code inside the JMX bean (setter?? from the java docs)  threw an exception, so log it and skip outputting the attribute 
//  Used to make sure that waiting getSessions don't block update. 
//  Example for using cluster configuration xml-s 
//  If schemas do not match, we currently do not merge 
//  Fail the query if the stats are supposed to be reliable 
//  Create a list of topop nodes and walk! 
//  Simulate a 2s delay before finishing the task. 
//  in Operator since we want to individually track the number of rows from different inputs. 
// get actual number of rows from metastore 
//  update no skew task 
//  Make sure the compactor has a chance to run once 
//  which are going to return values from the input batch vector expressions 
//  TEST - repeating NULL & selection 
//  Now delete a node for this key's list 
//  Hash code logic from original calculateLongHashCode 
//  GSS credentials for server 
//  For Hybrid Grace Hash Join, during the 1st round processing,   we only keep the LEFT side if the row is not spilled 
//  Dump the drop events and check if tables are getting dropped in target as well 
/*  Walk through all found table locations to get the most encrypted table  */
//  This is the adjusted index after nested column pruning.   For instance, given the struct type: s:<struct<a:int, b:boolean>>   If only 's.b' is used, the pruned type is: s:<struct<b:boolean>>.   Here, the index of field 'b' is changed from 1 to 0.   When we look up the data from Parquet, index needs to be adjusted accordingly.   Note: currently this is only used in the read path. 
//  No matching methods found 
//  optimize for common case - just one row for a key, container acts as iterator 
//  since map key for Pig has to be Strings 
//  All data and partition columns. 
//  Runs an instance of DisallowUnicodePreEventListener   Returns whether or not it succeeded 
//  CTRL-D 
//  In this method, we must only process non-Decimal64 column vectors.   Convert Decimal64 columns to regular decimal. 
//  construct a mapping of (Partition->bucket file names) and (Partition -> bucket number) 
//  2. Generate tags 
//  A mapping from a directory which a FileSinkOperator writes into to the columns by which that 
//  Create a CuratorFramework instance to be used as the ZooKeeper client   Use the zooKeeperAclProvider to create appropriate ACLs 
//  should share cte contexts 
//  Assumes that the catalog has already been set. 
//  Restart if there's an internal error. 
//  NDV(expr) = max(NDV( expr args)) 
//  an error in creation, and we want to delete it anyway. 
//  -w (or) --password-file <file> 
//  Open an accumulo connection 
/*  'greg' < first_name  */
//  required   optional   optional   optional 
//  After this the KeyWrappers are properly set and hash code is computed 
//  1. Sum of input cardinalities 
//  the user has specified to ignore mapjoin hint 
//  NEW TAI LUE LETTER HIGH MA U+1996 (3 bytes) 
//  TODO: if using in multiple places, e.g. SerDe cache, pass this in. 
//  Empty array 
// EK: it's not obvious that this is the right logic, if we don't record the 'callback'  for example and never notify the client of job completion 
//  issue warning for missing file and throw exception 
//  in the absence of SORTED BY clause, the sorted dynamic partition insert 
//  If timer is null, start a new one.   If timer has completed during previous invocation, start a new one.   If timer already started and is not completed, leaving it running without resetting it. 
//  Test that CliDriver does not strip comments starting with '--' 
//  Create dest table partitions with custom locations 
//  2nd substring index refers to the 6th index (last char in the array) 
//  @@protoc_insertion_point(class_scope:LlapManagementProtocol) 
/*        * a. add Map-side PTF Operator if needed        */
//  Clear integer portion; keep fraction. 
//  0 XOR 1 yields 1, 1 XOR 1 yields 0 
//  Temp functions are not allowed to have qualified names. 
//  2. Sanity check 
//  Alter an existing partition ("aaa") via ObjectStore 
//  TYPE_QUALIFIERS 
//  abortedBits should be all true as everything in exceptions are aborted txns 
//  merge work. Else create a merge work, add above work to the merge work 
//  Find the writeId high water mark based upon txnId high water mark. If found, then, need to   traverse through all write Ids less than writeId HWM to make exceptions list. 
// MR stuff 
//  we've already gone beyond the specified range 
//  a state that the driver enters after destroy() is called and it is the end of driver life cycle 
//  For example, 2 partitions (1 sequencefile and 1 rcfile) will have 2 different splits 
//  check if all the ColumnStatisticsObjs contain stats and all the ndv are   bitvectors 
//  to find target for fetch task conversion optimizer (not allows subqueries) 
//  Handle date-string common category and numeric-string common category 
// ========================== 10000 range starts here ========================// 
//  LOG.debug("VectorMapJoinFastKeyStore equalKey no match on bytes"); 
//  We are not Throwing an exception since it might be a transient issue that is blocking loading 
//  Consumes whole key. 
//  store table descriptor in map-targetWork 
// this is db.table 
//  check whether substitution is allowed 
//  We have failed to reserve a single header. Do not undo the previous ones here,   the caller has to handle this to avoid races. 
/* Future thought: checkForCompaction will check a lot of file metadata and may be expensive.              * Long term we should consider having a thread pool here and running checkForCompactionS              * in parallel */
//  Hardcoded from a private field in ZKDelegationTokenSecretManager.   We need to check the path under what it sets for namespace, since the namespace is   created with world ACLs. 
//  Project the big table key into the small table result "area". 
// RecordReader.getRowNumber() produces a file-global row number even with PPD 
//  Subtract the spills to get all match and non-match rows. 
//  Stats bookkeeping 
//  use the specified database if specified 
// Increments one HMS connection 
/* rename(A, B) has "interesting" behavior if A and B are directories. If  B doesn't exist,        * it does the expected operation and everything that was in A is now in B.  If B exists,        * it will make A a child of B...  thus make sure the rename() is done before creating the        * meta files which will create base_x/ (i.e. B)... */
//  middle items in order 
//  1. Sanity check 
// certain queries like select count(*) from table do not have  any projected columns and still have isReadAllColumns as false  in such cases columnReaders are not needed  However, if colsToInclude is not empty we should initialize each columnReader 
//  Note: incomplete CBs are always an exact match. 
/*    * Helper function to create an edge property from an edge type.    */
/*  10 files x 1000 size for 111 splits  */
//  Insert time-zone for timestamp type 
//  Handle different types of CREATE TABLE command   Note: each branch must call addDbAndTabToOutputs after finalizing table properties. 
//  LATIN SMALL LETTER GAMMA U+0263 (2 bytes) 
// Is this an expression that should perform a comparison for sorted searches 
//  Skipping because of hint. Mark this info, 
//  Divide down just before scaleDown to get round digit. 
//  invalid time part 
//  Project projects the original expressions 
//  default serde for rcfile 
//  JAVA32_META + JAVA32_REF   JAVA32_ARRAY_META + JAVA32_REF 
//  test getTableObjectsByName 
//  We rewrite it 
//  The count of the elements of the above that are set. 
//  create the vertex 
//  init the RNG for breaking ties in histogram merging. A fixed seed is specified here   to aid testing, but can be eliminated to use a time-based seed (which would   make the algorithm non-deterministic). 
//  newDir(true) => stats not updated 
//  Do the V1 fields of older and newer match? 
//  Method signature changed in Hadoop 2.7. Cast provider to KeyProvider 
//  decimal 
//  Transaction for which the list of tables valid write Ids are populated 
//  set hive provider path in hiveConf if setHiveProviderPath is true   simulates hive.server2.job.credstore.location property set in hive-site.xml/core-site.xml of 
//  case 1 
//  Set to false to block the next loop. This must be called before draining the lists,   otherwise an add/completion after draining the lists but before setting it to false,   will not trigger a run. May cause one unnecessary run if an add comes in before drain.   drain list. add request (setTrue). setFalse needs to be avoided. 
//  extract all the inputFormatClass names for each chunk in the 
//  Remove semijoin optimization if SMB join is created. 
// TINYINT 
//  We use a separate metastore client for heartbeat calls to ensure heartbeat RPC calls are 
//  no encoded values, we can push directly to row. 
//  4. Construct GB Keys (ExprNode) 
//  "abc"   "abc%"   "%abc"   "%abc%"   all other cases, such as "ab%c_de" 
//  key is only string 
//  Drop an existing partition ("bbb") via ObjectStore 
//  The getter should remove the escape character for us 
//  Byte 
//  2nd Txn 
//  3. We populate the filters structure 
//  Multi-byte trims. 
//  option to bypass job setup and cleanup was introduced in hadoop-21 (MAPREDUCE-463) 
//  case 2 
//  LSB p bits 
//  Return value modulo n but always in the positive range (0..n-1).   And with the mask to zero the sign bit to make the input to mod positive   so the output will definitely be positive. 
//  the database directory 
// so that get(i) returns null rather than ArrayOutOfBounds 
//  we need not to do any instanceof checks for following. 
//  Filtering is handled in the input batch processing 
/*  * This class provides support to collect mapreduce stderr/stdout/syslogs * from jobtracker, and stored into a hdfs location. The log directory layout is: * <ul compact> * <li>logs/$job_id (directory for $job_id) * <li>logs/$job_id/job.xml.html * <li>logs/$job_id/$attempt_id (directory for $attempt_id) * <li>logs/$job_id/$attempt_id/stderr * <li>logs/$job_id/$attempt_id/stdout * <li>logs/$job_id/$attempt_id/syslog  * Since there is no API to retrieve mapreduce log from jobtracker, the code retrieve * it from jobtracker ui and parse the html file. The current parser only works with * Hadoop 1, for Hadoop 2, we would need a different parser  */
//  to take care of. 
//  Also, null down.   UNDONE 
//  this is file system counter, valid and create counter 
//  We're passing client credentials as null since we want them to be read from the Subject. 
//  This is the case where distinct cols are part of GB Keys in which case   we still need to add it to out put col names 
// important so we get an exception on name collision 
//  validate is true by default if we enable the constraint 
//  Dummy insert into command to mark proper last repl ID after dump 
//  We need to check whether this transaction is valid and open 
//  case 3 
//  Make a random array of longs 
//  Use SourceStateUpdatedRequestProto.newBuilder() to construct. 
//  3. Return result 
// the destf. in this case, the replaced destf still preserves the original destf's permission 
//  The regexes to look for in the log files 
//  that long to see an abandoned session 
//  This will be null at slaves. 
//  invalid zone 
//  Package permission so that HadoopThriftAuthBridge can construct it but others cannot. 
//  get databases for schema pattern 
// no DP, so it's populated from lock info 
//  If the retry logic is reached after copy error, then include the copied file as well.   This is needed as we cannot figure out which file is incorrectly copied.   Expecting distcp to skip the properly copied file based on CRC check or copy it if CRC mismatch. 
//  Change the table partition for collecting stats 
//  case 4 
//  Call describe 
//  if the primary isn't done, push it back into the readers 
// run Cleaner 
//  The Writable to return in serialize 
/*      * Connect using the delegation token passed via configuration object      */
//  We got an exception that is not IOException   (typically OOM, IndexOutOfBounds, InternalError).   This is most likely a corruption. 
//  all of the OR branches need to be bucket-leaves 
//  flatten key/value pairs into row object for use in Serde. 
//  TODO: type conversion 
//  No query. Either it is a CTAS, or we need to create a Druid meta data Query 
//  Obtain additional information if we should try incremental rewriting / rebuild   We will not try partial rewriting if there were update/delete operations on source tables 
//  Set the range on the deleteEventReaderOptions to 0 to INTEGER_MAX because 
//  All values should pass test 
//  enable/disable bias correction using table lookup 
//  first 2 stripes will satisfy the predicate and merged to single split, last two stripe will 
//  2. Finally create PTF 
//  lock manager 
//  Expected number of partitions dropped in each of those calls 
//  Try to find the method 
//  Create query 
//  If this DemuxOperator directly connects to a MuxOperator,   that MuxOperator must be the parent of a JoinOperator.   In this case, that MuxOperator should be initialized   by multiple parents (of that MuxOperator). 
//  loop over all the tasks recursively 
/*    * Tests with queries which cannot be executed with directSQL, because of type mismatch. The type   * of the num column is string, but the parameters used in the where clause are numbers. After   * falling back to ORM, the number of partitions can be fetched by the   * ObjectStore.getNumPartitionsViaOrmFilter method.    */
// This means hive type doesn't refer this field that comes from file schema.  i.e. the field is not required for hive table. It can occur due to schema  evolution where some field is deleted. 
/*  code copied over from UDFWeekOfYear implementation  */
//  We must parse to get the escape count. 
//  need to make sure names are set for tez to connect things right 
//  set data column count as 1. 
//  None of the cases above matched and everything is selected. Hence, we will use the   same values for the selected and selectedInUse. 
//  write ids 
//  unfortunately, the metastore api revokes all privileges that match on   principal, privilege object type it does not filter on the grator   username.   So this will revoke privileges that are granted by other users.This is   not SQL compliant behavior. Need to change/add a metastore api   that has desired behavior. 
// ~ Methods ---------------------------------------------------------------- 
//  Now make sure that we can re-use the re-encoder against a completely   different record to save resources 
//  All others 
//  target exists 
//  assign values in vector 
//  session creation should fail since the schema didn't get created 
//  Likely a malformed query eg, select hash(distinct c1) from t1; 
//  Reject default partitions if we couldn't determine whether we should include it or not.   Note that predicate would only contains partition column parts of original predicate. 
//  always should be in this order (see PTFDeserializer#initializeWindowing) 
//  ensure if destination is not empty only for regular import 
/*  (non-Javadoc)   * @see org.apache.hadoop.io.Writable#readFields(java.io.DataInput)    */
/*  Is any operator present, which prevents the conversion  */
//  db exists 
//  Logger the lineage info 
//  Two known tasks left. r2 and r5. (r1 complete, r3 evicted, r4 rejected) 
//  final vertices need to have at least one output 
//  the qualified table aliases, etc. 
//  Add implicit type conversion if necessary 
//  remove the entries so we don't get confused later and think we should 
// TODO: No Of buckets is not same as no of splits 
//  save logging message for log4j output latter after log4j initialize properly 
//  rowId >= 'h' 
//  Format <txnId>$<table_name>:<hwm>:<minOpenWriteId>:<open_writeids>:<abort_writeids>$<table_name>... 
//  STARTED 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setClob(int, java.sql.Clob)    */
//  DROP TABLE [IF EXISTS] table_name; 
//  For grouping sets, add a dummy grouping key   This dummy key needs to be added as a reduce key   For eg: consider: select key, value, count(1) from T group by key, value with rollup.   Assuming map-side aggregation and no skew, the plan would look like:     TableScan --> Select --> GroupBy1 --> ReduceSink --> GroupBy2 --> Select --> FileSink     This function is called for GroupBy1 to create an additional grouping key 
//  Note: the first and last element of the byte[] are NOT used 
// now actually write to table to generate some partitions 
//  compare data header with signature 
//  join(left, join.getRight) 
//  Might be from before the new resource plan. 
//  optional bytes vertexBinary = 2; 
//  STAGE_TYPE 
//  Either a non-MM query, or a load into MM table from an external source. 
//  Retrieve job conf into logDir 
//  insertIntoTables/insertOverwriteTables map a table's fullName to its ast; 
//  synthetic conditions there. 
//  TYPES 
//  [-trace|--trace] 
//  swap x and t2[h2(x)] 
//  T | F | F 
//  next byte is a null byte if there are more bytes to go 
//  When there is no explicit foreign key name associated with the constraint and the key is composite,   we expect the foreign keys to be send in order in the input list.   Otherwise, the below code will break.   If this is the first column of the FK constraint, generate the foreign key name   NB: The below code can result in race condition where duplicate names can be generated (in theory).   However, this scenario can be ignored for practical purposes because of   the uniqueness of the generated constraint name. 
//  a skew key now. 
//  Last row starting at the end of the split would be read. 
//  disable memory checking 
// check if the rest command specified explicitly to use hcatalog 
// these must be after non-partition cols 
/*    * Connects using the command line arguments. There are two   * possible ways to connect here 1. using the cmd line arguments like -u   * or using !properties <property-file>    */
//  Restore everything to default setup to avoid discrepancy between junit test runs 
//  If numAttr is 1, this means we join on one single key column. 
//  allocate memory for the histogram bins 
//  Add an empty checksum string for filesystems that don't generate one 
//  Remove cyclic dependencies for DPP 
//  there is no distinct aggregation,   update all aggregations 
//  we have created an HBase table, so we delete it to roll back; 
//  Locked for defrag 
//  Put each check in a separate try/catch, so if that particular   cycle fails, it'll try again on the next cycle. 
//  Create two input paths so that two map tasks get triggered. There could be other ways   to trigger two map tasks. 
//  We'll count misses as we iterate 
//  IS_TRANSACTIONAL 
//  SW.SR.wait Lock we are examining is waiting.  In this case we keep   looking, as it's possible that something in front is blocking it or 
//  Can't do remainder on NULL. 
//  build the routing table. 
//  Therefore, the maximum total size of a serialized timestamp is 4 + 5 + 4 = 13. 
//  avoid intial spike when using multiple HS2 
//  we only need to calculate it once, it'll be the same for other partitions in this job. 
/*        * checkif current row belongs to the current accumulated Partition:       * - If not:       *  - process the current Partition       *  - reset input Partition       * - set currentKey to the newKey if it is null or has changed.        */
//  We don't expect any nesting in most cases, or a lot of it if it is present; union and LB   are some examples where we would have 1, or few, levels respectively. 
//  The value is before the list record offset.  Make byte segment reference absolute. 
//  BI strategy requested through config 
//  If the id is a generated unique ID then this could affect .q file golden files for tests that run EXPLAIN queries. 
// if here then there are no Open txns and  highestAllocatedTxnId must be  resolved (i.e. committed or aborted), either way  there are no open txns with id <= highestAllocatedTxnId  the +1 is there because "delete ..." below has < (which is correct for the case when  there is an open txn  Concurrency: even if new txn starts (or starts + commits) it is still true that  there are no currently open txns that overlap with any committed txn with   commitId <= commitHighWaterMark (as set on next line).  So plain READ_COMMITTED is enough. 
//  Optimizer 
//  sources represent vertex names 
//  hadoop group mapping that maps user to same group 
//  corresponding with bucket number and hence their OIs 
/*          * for CBO provided orderings, don't attempt to reorder joins.         * only convert consecutive joins into n-way joins.          */
//  inbuilt assumption that the testdir has only one output file. 
//  increment/set input counters 
//  Create placeholder entry with PENDING state. 
//  used by PTFs 
// -p with the next argument being for BeeLineOpts 
//  max threshold for CNF conversion. having >8 elements in andList will be   converted to maybe 
//  call-5: open(mock:/mocktable8/delta_0000001_0000001_0000/bucket_00001) 
//  3: 
//  rule and passes the context along 
//  The mapping from a newTag to the index of the corresponding child 
//  First, actually give it a duck. 
//  print name 
// checking for delete_delta is only so that this functionality can be exercised by code 3.0  which cannot produce any deltas with mix of update/insert events 
//  Generate the result for the windowing ending at the current row 
//  Pick trust store config from the given path 
//  for updates first column is _rowid 
//  for STRUCTS(multiple-add-calls) and LISTS(single-add-call) 
//  Add NOT NULL constraint check 
//  trim off ending ",", if any 
//  call-1: listLocatedStatus - mock:/mocktbl   call-2: check existence of side file for mock:/mocktbl/0_0 
//  there could be more scheme after execution as execution might be accessing a   different filesystem. So if we don't find a matching scheme before execution we   just use the after execution values directly without computing delta difference 
//  Http mode 
//  Currently, the algorithm flushes 10% of the entries - this can be   changed in the future 
//  Avro only allows maps with string keys 
//  We support both List<Object> and Object[]   so we have to do differently. 
//  need a map from the reducer to the corresponding ReduceWork 
//  Data prematurely ended. Return start - 1 so we don't move our field position. 
//  looks like a db operation 
//  after decoding we can push to value. 
/*            * Do the {vector|row} deserialization of the one row into the VectorizedRowBatch.            */
//  Add dummy instances to all slots where LLAPs are MIA... I can haz insert_iterator?  
/*    * Update aggregations. If the aggregation is for distinct, in case of hash   * aggregation, the client tells us whether it is a new entry. For sort-based   * aggregations, the last row is compared with the current one to figure out   * whether it has changed. As a cleanup, the lastInvoke logic can be pushed in   * the caller, and this function can be independent of that. The client should   * always notify whether it is a different row or not.   *   * @param aggs the aggregations to be evaluated   *   * @param row the row being processed   *   * @param rowInspector the inspector for the row   *   * @param hashAggr whether hash aggregation is being performed or not   *   * @param newEntryForHashAggr only valid if it is a hash aggregation, whether   * it is a new entry or not    */
//  convert the first methodParameterTypes.length - 1 entries 
//  Some rows have already been assigned values. Assign the remaining.   We cannot use copySelected method here. 
//  SCHEMA_TYPE 
//  setup appropriate UGI for the session 
//  delegate to the group's converters 
//  http://dev.mysql.com/doc/refman/5.1/en/string-functions.html#function_locate 
//  this = this.mag / 10**this.scale   right = right.mag / 10**right.scale   this / right = (this.mag / right.mag) / 10**(this.scale -   right.scale) 
// all other ops using S4U on TXNS row. 
//  The following data should be changed, other data should be the same 
//  numeric primitive type. 
//  this.isOuterJoin = isOuterJoin;   PrimitiveTypeInfo[] primitiveTypeInfos = { TypeInfoFactory.stringTypeInfo };   keyBinarySortableDeserializeRead = new BinarySortableDeserializeRead(primitiveTypeInfos);   readStringResults = keyBinarySortableDeserializeRead.createReadStringResults();   bytesWritable = new BytesWritable(); 
/*     The number of writers seems to be based on number of MR jobs for the src query.  todo check number of FileSinks    warehouse/t/.hive-staging_hive_2017-09-13_08-59-28_141_6304543600372946004-1/-ext-10000/000000_0/delta_0000001_0000001_0000/bucket_00000 [length: 648]    {"operation":0,"originalTransaction":1,"bucket":536870912,"rowId":0,"currentTransaction":1,"row":{"_col0":1,"_col1":2}}    {"operation":0,"originalTransaction":1,"bucket":536870912,"rowId":1,"currentTransaction":1,"row":{"_col0":2,"_col1":4}}    ________________________________________________________________________________________________________________________    warehouse/t/.hive-staging_hive_2017-09-13_08-59-28_141_6304543600372946004-1/-ext-10000/000001_0/delta_0000001_0000001_0000/bucket_00001 [length: 658]    {"operation":0,"originalTransaction":1,"bucket":536936448,"rowId":0,"currentTransaction":1,"row":{"_col0":5,"_col1":6}}    {"operation":0,"originalTransaction":1,"bucket":536936448,"rowId":1,"currentTransaction":1,"row":{"_col0":6,"_col1":8}}    {"operation":0,"originalTransaction":1,"bucket":536936448,"rowId":2,"currentTransaction":1,"row":{"_col0":9,"_col1":10}}     */
//  [A: 1, B: 0, B.x: 0, B.y: 0, C: 0] 
/*        * Last group batch.       *       * Take the (non-streaming) group aggregation values and write output columns for all       * rows of every batch of the group.  As each group batch is finished being written, they are       * forwarded to the next operator.        */
//  @@protoc_insertion_point(builder_scope:IOSpecProto) 
//  Implementation of List<Object> and assorted methods 
// falling back 
//  Otherwise sleep, and try again. 
/*    * do nothing.    */
//  second of three was selected 
//  Not using method column.getIsVirtualCol() because partitioning columns   are also treated as virtual columns in ColumnInfo. 
// show throw 
// if here, do the slow path so that we can return info txns which were not in expected state 
//  Print out the sizes, if pretty is set, print it out in a human friendly format,   otherwise print it out as if it were a row 
//  Set up col expressions for the dynamic values using this input 
//   Cached values, to save on round trips to database. 
//  of Ranges implies that there are no possible Ranges to lookup. 
// ************************************************************************************************   Decimal Comparison. 
//  append additional virtual columns for storing statistics 
//  When an AND has no children (some conjunction over a field that isn't the column   mapped to the Accumulo rowid) and when a conjunction generates Ranges which are empty   (the children of the conjunction are disjoint), these two cases need to be kept separate.     A null `andRanges` implies that ranges couldn't be computed, while an empty List 
//  Make the following condition: all the values match for all the columns 
//  We only initialize once the tasks that need to be run periodically 
//  Testing setting length larger than array length, which should cap to the length itself 
//  If newBucketColList had a null value it means that at least one of the input bucket   columns did not have a representative found in the output columns, so assume the data   is no longer bucketed 
//  tableNamePattern, String columnNamePattern) 
//  The original lists do not contain collisions, so only one is 'old'. 
//  Prepare data.  Good for ANY implementation variation. 
//  remove the partition paths we know about 
/*    * This method gets called multiple times by Hive. On some invocations, the properties will be empty.   * We need to detect when the properties are not empty to initialise the class variables.   *   * @see org.apache.hadoop.hive.serde2.Deserializer#initialize(org.apache.hadoop.conf.Configuration, java.util.Properties)    */
// empty strings are marked by an invalid utf single byte sequence. A valid utf stream cannot 
//  if this is a skew key, we need to handle it in a separate map reduce job. 
//  add the merge job 
// should not happen here - this is for replication 
//  We are going to do something useful now. 
//  Create one dummy lock so we can go through the loop below, though we only  really need txnId 
//  Test string column to VARCHAR literal comparison 
//  used to log memory usage periodically 
//  get the size of cache BEFORE 
//  rebuild the tree since original is empty 
//  Get the groupby aliases - these are aliased to the entries in the   select list 
//  We assume that since we are joining on the same key, all tables would have either   optimized or non-optimized key; hence, we can pass any key in any table as reference.   We do it so that MJKB could determine whether it can use optimized keys. 
//  Small table indices has more information (i.e. keys) than retain, so use it if it exists... 
//  Set up test data 
//  compute deltas and write the values as varints 
//  change in future. 
//  invalid load path 
//  Used to queue up requests while the SparkContext is being created. 
//  2nd task requested host2, got host3 as host2 is full 
/*            * Multi-Key get key.            */
//  Don't do this optimization with updates or deletes 
//  optional   required   optional 
//  SessionState/Driver needs to be restarted with the Tez conf settings. 
//  Set temp file containing results to be sent to HiveClient 
//  Ignore updates that occured before this cached query was created. 
//  used to determine whether child tasks can be run. 
//  Making sure we treat dynamic partitioning jobs as if they were immutable. 
//  MY_STRING 
//  each element in the array: startPosition[i+1] - startPosition[i] - 1 
//  Remove the reduceSinkOperator. 
//  I32_VAL 
//  Java cruft; pair of long. 
//  For alter table exchange partition, we need select & delete on input & insert on output 
//  Traverse through the from string, one code point at a time 
//  This record produced a result this time, remove it from the storage   as it will not need to produce a result with NULL values anymore 
//  Reduce Sink contains the columns needed - no need to aggregate from   children 
//  setup the new map work 
//  start to generate multiple map join tasks 
//  Connect parent/child work with a brodacast edge. 
//  [A: 1, B: 1] 
//  Create and attach the filesink for the merge. 
// correlated vars across subqueries within same query needs to have different ID 
//  columns which are bucketed/sorted 
//  If we are left with a single child, return the child 
//  The array index can't be reserved. 
//  Sending a kill message to the AM right here. Don't need to wait for the task to complete. 
//  KILL always takes priority over MOVE 
//  Remove cast of BOOLEAN NOT NULL to BOOLEAN or vice versa. Filter accepts   nullable and not-nullable conditions, but a CAST might get in the way of   other rewrites. 
// above get() doesn't set it 
//  Perform SPNEGO login using the hadoop shim API if the configuration is available 
//  Uninitialized vertices will report count as 0. 
//  We do not need to do anything, it is in the OR expression   so probably we pushed previously 
//  n-way join, all later small tables   For all later small tables, follow the same pattern of the previously loaded tables. 
/*  3-replica ssd  */
// validate noscan 
//  By setting the comparison to equal, the search should use the block [0, 50] 
//  Grab round digit from lowest word. 
//  [A: 1, B: 2] 
//  adding 16KB constant memory for stringCommon as the rabit hole is deep to implement   MemoryEstimate interface, also it is constant overhead 
//  Enable retries to work around BONECP bug. 
//  if this record is larger than maxKey, we need to stop 
//  EventConsumer to invalidate cache entries based on metastore notification events (alter table, add partition, etc). 
//  "cause" was a useless intermediate cause and was replace it   with its own cause. 
//  convert to lazy object 
//  The table is missing either due to drop/rename which follows the operation. 
//  Compare as strings. Char comparison semantics may be different if/when implemented. 
//  A simple thread to wait until the server has started and then signal the other threads to   begin 
//  last resort 
// bail out can not infer unit 
//  If the table is partitioned, we need to select the partition columns as well. 
// 5. get the first SEL after TS 
//  Mark the write ids state as per the txn state. 
//  if value is null or not found, exception would get thrown 
//  Audience can't exist on its own 
//  Reopen happens even when close fails, so there's not much to do here. 
//  Add any required resources 
// I don't think this can have any FileSinkOperatorS - more future proofing 
//  remember the connections between ts and event 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#wasNull()    */
//  draw 1 and replace 
//  10^32 
//  Need to preserve currentTimestamp 
//  For kafka Streaming tables it is mandatory to set a kafka topic. 
//  setString can override this null propagation 
//  Check if the default values are set for all unfilled attributes 
//  for each struct subfield, create equivalent ResourceFieldSchema 
//  For MM tables, directory structure is   <table-dir>/<partition-dir>/<delta-dir>/ 
//  get sum for all columns to reduce the number of queries 
//  If we have a conflict on the number of reducers, we will not optimize   this plan from here. 
//  Checking for status of a db 
/*  Prepare the constant for use when the function is called. To be used   * during initialization.    */
//  there is no overlap between columns and partitioning columns 
//  Ensure Pig can read data correctly. 
// make it an Acid table and make sure we assign ROW__IDs correctly 
//  convert all of the children to CNF 
//  If it is a MapJoin or MergeJoin, we make sure that they are on   the reduce side, otherwise we bail out 
//  There should already be an instance of the session pool manager.   If not, ignoring is fine while stopping HiveServer2. 
//  this operator cannot be converted to mapjoin cause output is expected to be sorted on join key 
/*  there are NULLs in our column  */
/*      * The Vectorizer class enforces that there is only one TableScanOperator, so     * we don't need the more complicated multiple root operator mapping that MapOperator has.      */
//  This code is pretty much completely based on Hadoop's   SaslRpcServer.SaslDigestCallbackHandler - the only reason we could not   use that Hadoop class as-is was because it needs a Server.Connection object   which is relevant in hadoop rpc but not here in the metastore - so the 
//  After this, the non-initial refcounts are the responsibility of the consumer. 
//  Unpack the output. 
//  setting empty list results in reading none 
//  INT_VALUE 
//  We compress the owids into CompressedOwid data structure that records 
// if here, someone must have added new Work object - should it be walked to find FileSinks? 
//  let the schema and version be auto created 
//  all column types 
//  10^31 
//  Check the list of where expressions already added so they aren't duplicated 
//  4/ test serialization and deserialization with different schemas 
//  Only preempt if the task being preempted is "below" us in the dag. 
//  require view ownership for alter/drop view 
//  create map join task for the given big table position 
//  in this case, the result will be surely more than 128 bit even   after division 
//  end of loop over pending tasks 
//  get the output schema 
//  we have to check if we receive prefix of partition keys so in table   scheme like table/ds=2011-01-02/hr=13/   ARCHIVE PARTITION (ds='2011-01-02') will work and   ARCHIVE PARTITION(hr='13') won't 
//  parameters == null means the input table/split is empty 
//  Wait for up to 3 seconds before checking if any init error.   Init should be fast if no error, no need to make this configurable. 
//  extend any repeating values and noNulls indicator in the input 
//  Matching result for attemptId + input. 
//  URI 
//  Test DROP DATABASE. 
//  We SerDe the Throwable as String, parse it for the root cause 
//  Nope, so look to see if our conf dir has been explicitly set 
// invalid case 
//  Serialize rest of the field in the AggBuffer 
//  Total number of input rows is needed for hash aggregation only 
//  This probably doesn't need to be sync, but nobody calls this, so it doesn't matter. 
//  clone joinCond 
//  Note, we assume production LLAP always runs under YARN. 
//  if the aggregation type is avg, we use the average on the existing ones. 
//  9. Get rid of sq_count_check if group by key is constant (HIVE-) 
//  False positives probability we are ready to tolerate for the underlying bloom filter 
//  replaces the join operator with a new CommonJoinOperator, removes the 
//  Ignore for now. HS2 will probably try to send us the count we already have again.   We are assuming here that if we can't talk to ZK we will eventually fail. 
//  get the field out of struct 
//  skip incompatible file, files that are missing stripe statistics are set to incompatible 
//  Current Cookie Name, Current Cookie Value 
//  Ignore agg calls which are not distinct or have the wrong set   arguments. If we're rewriting aggs whose args are {sal}, we will   rewrite COUNT(DISTINCT sal) and SUM(DISTINCT sal) but ignore   COUNT(DISTINCT gender) or SUM(sal). 
// skip the big tables 
//  create partitions 
//  2. Insert Overwrite. 
// e.g. map z->expr for a 
//  Allocate next NULL byte. 
/*              * Common inner big-only join result processing.              */
//  read sync bytes 
//  Batch is full or using too much space. 
//  Test will be in local mode. 
//  Check for overflow 
//  Hash table memory usage allowed; used in case of non-staged mapjoin. 
//  Set up the foreign key constraints properly in the TAB_COL_STATS data 
//  If current state is a final state, notify of Spark job IDs before notifying about the   state transition. 
/*  id in (34,50)  */
//  set the backup task from curr task 
//  Get all the TS ops. 
/*    * Truncate a slice of a byte array to a maximum number of characters and   * return the new byte length.    */
//  7.convert Join + GBy to semijoin 
// Because LLAP arrow output depends on the ThriftJDBCBinarySerDe code path  this is required for 0 row outputs  i.e. we need to write a 0 size batch to signal EOS to the consumer 
//  1st close:   closing of open scope should be ok. 
//  blocks of a particular size, we'll try to split yet larger blocks, until we run out. 
//  Parse out the context and 'k' if we haven't already done so, and while we're at it,   also parse out the precision factor 'pf' if the user has supplied one. 
//  Fill up host1 with p2 tasks.   Leave host2 empty   Try running p1 task on host1 - should preempt   Await preemption request.   Try running another p1 task on host1 - should preempt   Await preemption request. 
//  for development, can add   "decision=<<"+nvae.grammarDecisionDescription+">>"   and "(decision="+nvae.decisionNumber+") and   "state "+nvae.stateNumber 
//  not a power of two, add one more 
//  The set object containing the IN list. This is optimized for lookup   of the data type of the column. 
// numEntriesSinceCheck is the number of entries added to the hash table   since the last time we checked the average variable size 
// this makes the jars available to Sqoop client 
//  No checks, the caller must ensure the offsets are correct. 
//  The class name of the generic UDF being used by the filter 
//  One child 
//  Cleaner would remove the obsolete files. 
// { "comment":"test", "columns": [ { "name": "col1", "type": "string" } ], "format": { "storedAs": "rcfile" } } 
/*  @bgen(jjtree) Header  */
//  Given that we do not delete, an empty slot means no match. 
//  If stats are already present and forceRecompute isn't set, nothing to do 
/*    * @throws HCatException   *   * @see org.apache.hive.hcatalog.api.HCatClient#closeClient()    */
//  If either argument is a string, we convert to a double or decimal because a number   in string form should always be convertible into either of those 
//  T | F | T 
//  1st level GB: create a GB (col0, col1, count(1) as c) for each branch 
//  check for max input size 
// INSERT [OVERWRITE] path 
// HIVE-17322: remove parallelism to check if the BeeLine test flakyness gets fixed  int numThreads = Runtime.getRuntime().availableProcessors(); 
//  enable extended nesting levels 
//  BIGINT 
//  F | unknown | F 
//  If there is a sort-merge join followed by a regular join, the SMBJoinOperator may not   get initialized at all. Consider the following query:   A SMB B JOIN C 
//  This means the DB name is empty 
//  Ensure child size. 
//  this is the constructor to use for the Bucket map join case. 
//  Text string 
//  We need to use the current cluster for the scan operator on views, 
//  tagToInput for reduce work 
//  Merge the files in the destination table/partitions by creating Map-only merge job   If underlying data is RCFile or OrcFile, RCFileBlockMerge task or   OrcFileStripeMerge task would be created. 
//  4) Transform first INSERT branch into an UPDATE 
//  nothing to do, soft references will clean themselves up 
//  set the output record by fiddling with the pointers so that we can 
//  and finally we're ready to create and start the session 
//  Check if the partitions don't exist in the sourceTable 
//  Verify fields were altered during the alterTable operation 
// In UpdateDeleteSemanticAnalyzer, after super analyze   3 ReadEntity: [default@acidtblpart, default@acidtblpart@p=p1, default@acidtblpart@p=p2]   1 WriteEntity: [default@acidtblpart TABLE/INSERT]  after UDSA   Read [default@acidtblpart, default@acidtblpart@p=p1, default@acidtblpart@p=p2]   Write [default@acidtblpart@p=p1, default@acidtblpart@p=p2] - PARTITION/UPDATE, PARTITION/UPDATE  todo: Why acquire per partition locks - if you have many partitions that's hugely inefficient. 
//  Serialize keyRow into key bytes. 
//  Read the fields. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getNCharacterStream(java.lang.String)    */
//  need to enforce nullability by applying an additional   cast operator over the transformed expression. 
//  cursor for the "inList" array.   cursor for an element list per an 'IN'/'NOT IN'-clause.   cursor for in-clause lists per a query. 
//  If maps, recursively compare the key and value types 
//  decimal_1_1.txt 
//  something is preventing metastore from starting 
//  Test select named_struct from named_struct:struct<a:boolean,b:double> 
//  create partition key schema 
//  list of map join 
//  for a while 
//  authorize drops if there was a drop privilege requirement 
//  to set this multiple times. 
//  Call the corresponding handler to evaluate this row and   forward the result 
/*  (non-Javadoc)   * @see org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClient#sendUpdateQuery(org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto, java.lang.String, int, org.apache.hadoop.security.token.Token, org.apache.hadoop.hive.llap.AsyncPbRpcProxy.ExecuteRequestCallback)    */
//  current code version   data's version   data's FC version   No exceptions expected 
//  the big table; 
//  a map to keep track of what reduce sinks have to be hooked up to 
//  If all child expressions of deterministic function are constants, evaluate such UDF immediately 
//  Must be deterministic order map - see HIVE-8707 
//  Some part of the requested range is not cached - the cached offset is past the requested. 
//  to decide whether to rewrite RR of subquery 
//  Get own Kerberos credentials for accepting connection 
//  True when the (random access) readField method of DeserializeRead are being used. 
//  Next we do this for tables and partitions 
//  If create view has LIMIT operator, this can happen   Fetch parent operator 
//  Throws InvalidOperationException if the new column types are not   compatible with the current column types. 
//  In this case, we have to find out which columns can be merged. 
//  Mock inputs 
//  Update the partitions for a table in cache 
//  If partial is NULL, then there was an overflow and myagg.sum will be marked as not set. 
//  session already has a violation 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getObject(int)    */
//  Might have been deleted already 
//  This is FS only mode, just initialize the dfs root directory. 
//  Replicate all the events happened so far 
//  Default case 
//  Couldn't find the parent insert; replace with ALLCOLREF. 
//  add to path set 
//  test that underflow produces null 
//  execution mode not set, null is returned 
//  timestamp column/scalar IF where scalar is really a CAST of a constant to timestamp. 
//  begin + write + commit 
//  Encountered a numeric value; Extract out the entire number 
//  0. Recreate cluster 
//  Only ">" predicate is supported right now, this has to be extended to support   expression tree when multiple conditions are required. HIVE-17622 
//  The mapping from a newTag to its corresponding oldTag.   oldTag is the tag assigned to ReduceSinkOperators BEFORE Correlation Optimizer   optimizes the operator tree. newTag is the tag assigned to ReduceSinkOperators   AFTER Correlation Optimizer optimizes the operator tree.   Example: we have an operator tree shown below ...          JOIN2         /     \     GBY1       JOIN1      |         /    \     RS1       RS2   RS3   If GBY1, JOIN1, and JOIN2 are executed in the same Reducer   (optimized by Correlation Optimizer), we will have ...   oldTag: RS1:0, RS2:0, RS3:1   newTag: RS1:0, RS2:1, RS3:2   We need to know the mapping from the newTag to oldTag and revert   the newTag to oldTag to make operators in the operator tree 
//  we allow multiple foreign keys (snowflake schema)   csfKs.size() + 1 == parents.size() means we have a single PK and all 
//  in the absence of column statistics, the estimated number of rows/data size that will 
//  populate pathToPartitionInfo and pathToAliases w/ DP paths 
//  disruptor   log4j-api   log4j-core   log4j-slf4j   log4j-1.2-API needed for NDC 
//  Ensure there's no more invocations. 
//  Some version upgrades often don't change schema. So they are equivalent to   a version   that has a corresponding schema. eg "0.13.1" is equivalent to "0.13.0" 
//  PRIVILEGE 
//  The set routine enforces precision and scale. 
//  test that setting read all resets column ids 
//  add the expr to reduceKeys if it is not present 
//  must be inside tx together with queries 
//  still exist 
//  set to ADMIN role, if user belongs there. 
/*    * Forwarding.    */
//  The response will have one entry per table and hence we get only one OpenWriteIds 
//  None of the expressions are constant. Nothing to do. 
//  Create scratch dir without lock files 
//  falling off, and the executor service being ready to schedule a new task. 
//  LlapIoImpl.LOG.info(prefix(ix) + " removing " + header(log[ix + 2]) + " from "     + getSecondInt(log[ix]) + " head " + log[ix + 3]); 
// To change body of implemented methods use File | Settings | File Templates. 
//  Already contains stats => stats not updated when forceRecompute isn't set 
//  Just check name/type for equality, don't compare comment 
//  TOK_TABLE_PARTITION 
//  Just loop through all values. We do not need to store anything though.   This is just for test purposes 
//  stats from writer 
//  Configuration for async thread pool in SessionManager 
//  We successfully converted agg calls into projects. 
//  We go by RG and not by column because that is how data is processed. 
//  FileSystem.CACHE.map 
//  If the same size. Sort on file name followed by startPosition. 
//  If older committed state is equivalent to newer state, then there should be no committed IDs   between oldHWM and newHWM, and newInvalidIds should have exactly (newHWM - oldHWM) 
//  Non-hash aggregation 
//  then we return immediately. 
// should not happen since the input were verified before passed in 
// ----------------------------------------------------------------------------------------------- 
//  Created using hint, skip it 
//  It is not at all clear how to flatten these last two out in a useful way, and no one uses 
/*    * Allocate the various arrays.    */
//  update max register value 
//  The user has already been notified of completion by SessionInitContext. 
//  drop view. ignore error. 
//  Need to also push projections by calling setOutputSchema on   HCatInputFormat - we have to get the RequiredFields information   from the UdfContext, translate it to an Schema and then pass it   The reason we do this here is because setLocation() is called by   Pig runtime at InputFormat.getSplits() and   InputFormat.createRecordReader() time - we are not sure when   HCatInputFormat needs to know about pruned projections - so doing it   here will ensure we communicate to HCatInputFormat about pruned   projections at getSplits() and createRecordReader() time 
//  add metric with a non-null value: 
//  Get all the failure execution hooks and execute them. 
//  Note, this is a "real" query that depends on one of the metastore tables 
//  statistics not implemented currently 
//  Binary mode 
//  First entry. 
/* at this point we have 2 delta files, 1 for insert 1 for update      * we should push predicate into 1st one but not 2nd.  If the following 'select' were to      * push into the 'update' delta, we'd filter out {3,5} before doing merge and thus     * produce {3,4} as the value for 2nd row.  The right result is 0-rows. */
//  Remaining column expressions would be a candidate for an RS value 
//  3) Sort columns 
//  Only one of the tasks should ever be added to resTsks 
//  Completion notifier vars 
//  Now set the response headers. 
//  Make sure non-null-valued ConfVar properties *do* override the Hadoop Configuration 
//  Used by kryo 
//  Mark this entry as being in use. Caller will need to release later. 
//  Requires conditional evaluation for good performance. 
//  In case of LB, we might get called repeatedly. 
//  validate the second parameter, which is the number of histogram bins 
//  Test that write blocks two writes 
//  Note: this assumes that the pattern where the same session object is reset with a different         Tez client is not used. It was used a lot in the past but appears to be gone from most         HS2 session pool paths, and this patch removes the last one (reopen). 
//  Assert class invariant. 
//  Disable dictionary encoding for the writer. 
//  Verify dirs 
//  Why are we still using writables in 2017? 
// some query attempted to lock (thus LOCK_WAITING state) but is giving up due to timeout for example 
//  Current getInputSummary() returns -1 for each file found   Current getInputSummary() returns -1 for each file found 
//  For testing only. 
//  Some of the conversion methods throw this exception on numeric parsing errors. 
//  Currently, we only optimized the query the content of the FROM clause 
//  After compaction/cleanup, all entries from TXN_TO_WRITE_ID should be cleaned up as all txns are committed. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setObject(int, java.lang.Object, int)    */
/*    * Parse a string as a query hint.    */
/*  0 is false and 1 is true in the input vector, so a simple dictionary is used     * with two entries. 0 references FALSE and 1 references TRUE in the dictionary.      */
//  subtract out the bits we just added 
//  The expected number of distinct values when choosing p values   with replacement from n integers is n . (1 - ((n - 1) / n) ^ p).     If we have several uniformly distributed attributes A1 ... Am   with N1 ... Nm distinct values, they behave as one uniformly 
/*      * We are "committing" this vertex to be vectorized.      */
//  RUNAS 
//  Setup a scratch batch that will be used to play back big table rows that were spilled 
//  we have correlated column, build data type from outer rr 
//  LOG.info("Reading offset " + prevHeadOffset + " at " + lrPtrOffset); 
//  We killed something. 
//  We need to deserialize and serialize query so intervals are written in the JSON   Druid query with user timezone, as this is default Hive time semantics. 
//  Spot check only. null & repeating behavior are checked elsewhere for the same template. 
//  Have to register this up front right now. Otherwise, it's possible for the task to start 
//  NumHashFunctions (1 byte) + NumBits (4 bytes) 
//  hdfs warehouse 
//  We only get here if we could map all join keys to source table columns 
// ---------------------------------------------------------------------------   Process Single-Column Long Inner Join on a vectorized row batch.   
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.newBuilder() 
//  required for deserialization 
//  Since we allow write operations on cache while prewarm is happening:   1. Don't add databases that were deleted while we were preparing list for prewarm   2. Skip overwriting exisiting db object   (which is present because it was added after prewarm started) 
//  remove ending ';' and starting '@' 
//  Change the query to use part_vals instead of the name which is 
// If the DateFormat is not provided by the user or is invalid, use the default format YYYY-MM-dd 
//  Ignore the interrupt status while returning the session, but set it back   on the thread in case anything else needs to deal with it. 
/*    * Once we have decided on the map join, the tree would transform from   *   *        |                   |   *       Join               MapJoin   *       / \                /   \   *     RS   RS   --->     RS    TS (big table)   *    /      \           /   *   TS       TS        TS (small table)   *   * for tez.    */
// check that delta dir has a version file with expected value 
//  defer 
//  Pretend it's vectorized if the non-vector wrapped is enabled. 
//  Thanks, HBase Storage handler 
//  TYPE2 
//  for tables other than the big table, we need to fetch more data until reach a new group or   done. 
//  Start the heartbeat after a delay, which exceeds the HIVE_TXN_TIMEOUT 
/*  @bgen(jjtree) TypeDefinition  */
/*  * This hook is used for verifying the table access key information * that is generated and maintained in the QueryPlan object by the * TableAccessAnalyer. All the hook does is print out the table/keys * per operator recorded in the TableAccessInfo in the QueryPlan.  */
//  Launch hadoop command file on windows. 
// updates (2,1) -> (2,0)  deletes (4,3)  inserts (11,11) 
//  startRow is inclusive while stopRow is exclusive,   this util method returns very next bytearray which will occur after the current one   by padding current one with a trailing 0 byte. 
//  The hash map for this specialized class. 
//  e.g., INSERT OVERWRITE TABLE temp1 SELECT  c0,  c0 FROM temp2;   In such a case Select Op will only have one instance of c0 and RS would have two.   So, locating bucketCol in such cases will generate error. So, bail out. 
//  whether this RS is deduplicated 
//  Try to infer possible sort columns in the query   i.e. the sequence must be pRS-SEL*-fsParent 
//  we also need the expr for the partitioned table 
//  With the high message size limit this connection should work 
//  In the SEL operator of the semijoin branch, there should be only one column in the operator 
//  set up the operator plan. (after generating splits - that changes configs) 
//  in a set so we can add them to the list of input cols to check. 
/*               This will happen only when loading tables and we reach the limit of number of tasks we can create;              hence we know here that the table should exist and there should be a lastPartitionName           */
//  This does not work hence, opening and closing file for every event.   writer.hflush(); 
//  order of overlapping keys should be exactly the same 
//  At this entry point, we are going to assume that these are logical table columns.   Perhaps we should go thru the code and clean this up to be more explicit; for now, we   will start with this single assumption and maintain clear semantics from here. 
//  If we are dropping from unmanaged, unset the flag; and vice versa 
//  Register information about created predicates 
//  PARTITION_SPECS 
//  Set up backup task 
//  Need to verify that when reading a datum with an updated reader schema   that the datum then returns the reader schema as its own, since we   depend on this behavior in order to avoid re-encoding the datum 
//  set the first one to be active ZK; Others are backups 
// exceptions including InterruptException and other KeeperException 
//  required   optional 
//  Normal case, the last parameter is a normal parameter.   ConversionHelper can be called without method parameter length   checkings   for terminatePartial() and merge() calls. 
//  mocked session starts with default queue 
//  Is bucketJoin possible? We need correct bucketing 
//  Trigger the creation of LLAP registry client, if in use. Clients may be using a different 
//  meta store check command - equivalent to add partition command   no input objects are passed to it currently, but keeping admin priv   requirement on inputs just in case some input object like file 
//  Create a default socket factory based on standard JSSE trust material 
//  if database is not the one currently using 
//  Verify if no create table/function calls. Only add partitions. 
//  Max is disabled, we can safely return false 
/*      * Implemented as a navigable set protected by a     * single lock and using conditions to manage blocking.      */
//  ignored 
//  Create a file sink operator for this file name 
//  as RS will be required. 
//  add partition in metastore for dynamic partition. We make a metastore call for every new partition value that   we encounter even if partition already exists (exists check require a metastore call anyways). 
/*  * Directly serialize, field-by-field, the LazyBinary format.* * This is an alternative way to serialize than what is provided by LazyBinarySerDe.   */
//  We should not remove the dynamic partition pruner generated synthetic predicates. 
//  Created 5 tables under "db1" 
//  Create a new ColumnInfo, replacing STRUCT.COLUMN with STRUCT_COLUMN 
/*      * The order of the join condition expressions don't matter.     * A merge can happen:     * - if every target condition is present in some position of the node condition list.     * - there is no node condition, which is not equal to any target condition.      */
//  Always show an array. 
//  need to localize the additional jars and files 
/*  alternate2 = unused  */
//  TYPE1 
//  Int 
//  4. address the colExp, colList, etc for the SEL 
//  Assign all remaining rows. 
/*  Add a new request to be executed  */
//  The ptned table should miss in target as the table was marked virtually as dropped 
//  This may happen for schema-less tables, where columns are dynamically   supplied by serdes. 
//  If cache is disabled, don't use it. 
//  STAGE_ATTRIBUTES 
//  Metastore stuff. Be sure to update HiveConf.metaVars when you add something here! 
/*    * Simulates the set <command>;    */
/*  100 files x 100 size for 11 splits  */
//  These methods are required by serialization 
//  Use both args to ease development.  Delete this one on   May 1. 
//  -p <password> 
//  check entries beyond first 2 
//  Successfully perform compaction on a table/partition, so that we have successful records in COMPLETED_COMPACTIONS 
//  Do not change the initial bytes which contain NumHashFunctions/NumBits! 
//  Merge the two partials 
//  threshold to switch from SPARSE to DENSE encoding 
//  thread local conf from HMS 
/* trivially retry-able */
//  Casts to exact types including long to double etc. are needed in some special cases. 
//  This test assumes the hive-contrib JAR has been built as part of the Hive build.   Also dependent on the UDFExampleAdd class within that JAR. 
//  this is what allows the UI to do cross-domain reads of the contents   only apply to idempotent GET ops (all others need crumbs) 
//  NOTE: The default value for null fields in vectorization is 1 for int types, NaN for 
//  The pruning needs to preserve the order of columns in the input schema 
//  make in Path to ensure no slash at the end 
//  left key only needs to be adjusted if there are system 
//  Allocated by caller. 
//  Add a shutdown hook for catching SIGTERM & SIGINT 
//  object overhead + 4 bytes for bitCount + 4 bytes for bitLength   + 4 bytes for firstNonzeroByteNum + 4 bytes for firstNonzeroIntNum +   + 4 bytes for lowestSetBit + 5 bytes for size of magnitude (since max precision   is only 38 for HiveDecimal) + 7 bytes of padding (since java memory allocations   are 8 byte aligned) 
//  Get the column names and their corresponding types 
//  ======= VARIOUS UTILITY METHOD 
//  2. Walk through UDAF and add them to GB 
//  different tasks. 
//  optional int32 guaranteed_task_count = 1; 
//  Verify that getNextNotification(last) returns events after a specified event 
// this proves data is written in Acid layout so T was made Acid 
// to handle map can read list of struct data (i.e. list<struct<key, value>> --> map<key, 
/*       * Failed to set job status as COMPLETED which mean the main thread would have      * exited and not waiting for the result. Call cleanup() to execute any cleanup.       */
// map hue/foo.bar@something.com->hue since user group checks   and config files are in terms of short name 
/*  * An multi-key hash map based on the BytesBytesMultiHashMultiSet.  */
//  Create the route objects based on the Nodes 
//  Backtrack partition columns of cRS to pRS 
//  stored as directories 
//  construct column name list for reference by filter push down 
// 1 split since we only have 1 bucket file in base/.  delta is not flushed (committed) yet, i.e. empty 
//  test get stats on a column for which stats doesn't exist 
//  use the original bytes in case decoding should fail 
//  2019-01-02 00:00:00 GMT is 2019-01-01 16:00:00 in zone GMT-0800 (PST) 
//  Bootstrap Repl A -> B and then export table t1 
//  exact numbers (power of 2) can do the same 
//  join 
//  This must be a final map reduce task (the task containing the file sink   operator that writes the final output) 
//  10^38 
//  Note: sessions in toRestart are always in use, so they cannot expire in parallel. 
/*  Count of number of null values seen so far  */
//  DEFAULT_VALUE 
//  vars that are not used in the join key. 
//  Explicit avro serialization not supported yet. Revert to default 
//  TODO: Split count is not the same as no of buckets 
//  write the base 
//  If the first child is directory, then rest would be directory too according to HCatalog dir structure   recurse in that case 
//  if there is no path element other than "/", report it but not fail 
//  since maxDepth is not yet reached, we are missing partition   columns in currentPath 
// so update has 1 writer, but which creates buckets where the new rows land 
//  user from TestPamAuthenticator 
//  LOG.debug("VectorMapJoinFastKeyStore equalKey no match big length"); 
//  10^39 
/*  This function determines whether sparkpruningsink is with mapjoin.  This will be called     to check whether the tree should be split for dpp.  For mapjoin it won't be.  Also called     to determine whether dpp should be enabled for anything other than mapjoin.    */
//     tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_COL"}, 1); 
// happen since IO layer either knows how to produce ROW__ID or not - but to be safe 
//  We have to unset the env workarounds so they don't confuse each other between tests. 
//  setup some helper config variables. 
//  Create payload 
//  OPERATION_STARTED 
//  write the number of elements in sparse map (required for   reconstruction) 
//  For deserialization. 
// verify that there is data in the resultset 
//  perform casting using Hive rules 
//  we found all the operators that we are supposed to process. 
//  response is written (i.e, response headers + mapOutput). 
//  Hand reset the big table columns. 
//  Day granularity 
//  Process global init file: .hiverc 
//  Drop all tables 
//  creates the static cache 
//               cost of transferring map outputs to GBy operator 
//  MySQL returns 0 if the string is not a well-formed numeric value.   return Byte.valueOf(0);   But we decided to return NULL instead, which is more conservative. 
//  Make sure negative numbers comes before positive numbers 
//  1. Figure out what we have to read. 
//  Partial aggregation is not done for distincts on the mapper   However, if the data is bucketed/sorted on the distinct key, partial aggregation   can be performed on the mapper. 
//  compare to must compare with scaling up/down. 
//  The write count does not matter, as the map will fail in its first 
//  version of Guava on the classpath depending on the deploy mode). 
//  End RelBuilder.java 
//  10^37 
/*      * Sum input and output are DECIMAL.     *     * Any mode (PARTIAL1, PARTIAL2, FINAL, COMPLETE).      */
//  add our base to the list of directories to search for files in. 
//  The output of a partial aggregation is a struct containing 
//  Or, session is being killed, need to coordinate between that and the user.   These two cases don't need to be distinguished for now. 
//  will update current number of open txns to 3 
//  We shouldn't cast strings to other types because that can break original data in cases of   leading zeros or zeros trailing after decimal point. 
//  source table spec -- for TableScanOperator   same as MoveWork.loadTableDesc -- for FileSinkOperator   same as MoveWork.loadFileDesc -- for FileSinkOperator   aggregation key prefix   are stats completely reliable 
//  First row determines isGroupResultNull and double firstValue; stream fill result as repeated. 
// check result now 
//  Open a new client session 
//  now we've got a table, check that it works 
//  previous base directory should stay until Cleaner kicks in. 
//  allow SET and DFS commands to be used during testing 
//  When we've buffered the max allowed, spill the oldest one to make space. 
/*      * let function decide if it can handle this special case.      */
//  Remove backlink. 
//  For complex object, serialize to JSON format 
//  struct<col1:struct<a:boolean,b:double>, col2:double>); 
//  We only expect 5 here because we'll get whichever of the partitions published its stats   last. 
//  long column/scalar IF 
//  Check result 
// Convert to object types used by the authorization plugin interface 
//  Handle overflow precision issue. 
//  collect the dynamic pruning conditions 
//  requested logger not found. Add the new logger with the requested level 
//  TODO: the way we add hash fns does exhibit some irregularities.         Seems like the 3rd iter has a better distribution in many cases, even better         that the original hash. That trips the "above MA" criteria, even if the rest is flat. 
//  todo: nested LV 
//  Not specified. 
//  Skip the child unique key if part of it is not   projected. 
//  At this point, no one will take the write lock and update, so we can do the last check. 
//  initialize record writer with connection and write id info 
// boolean keysAreEqual = (currentKeys != null && newKeys != null)?    newKeyStructEqualComparer.areEqual(currentKeys, newKeys) : false; 
//  2048 MB memory for compaction map job   minor compaction if more than 4 delta dirs   major compaction if more than 47% 
//  LSB 3 bits is used to locate offset within the block 
/*        * Same reason as above. This is the case when we have the main work item after the merge work       * has been created for the small table side.        */
//  Add all input columns 
//  @@protoc_insertion_point(builder_scope:GetTokenRequestProto) 
//  in a preserved table 
//  y event to truncate, last repl ID: replDumpId+2x+y 
//  satisfying precondition means column statistics is available 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getSQLXML(int)    */
//  see comment at "Dumping rows via SQL..." for why this doesn't work   assertEquals("Comparing Pig to Hive", t.get(0), l.get(0)); 
//  query qualify for the optimization 
//  Open files 
//  add Hive operator level statistics. - e.g. RECORDS_IN, RECORDS_OUT 
//  Check that all the jars are added to the classpath 
//  Schema should be same 
//  This indicates there is a second VInt containing the additional bits of the seconds   field. 
//  union keys 
//  We translate the grouping set bit field into a boolean arrays. 
//  1. Get valid Window Function Spec 
// for the big table, we only need to promote the next group to the current group. 
//  Lock the buffer, validate it and add to results. 
//  this is not a join condition. 
//  We do not want to modify the writable provided by the object o since it is not a copy. 
//  This is where we cut the tree as described above. We also remember that 
// 1 partitions updated 
//  2nd query's session has compile lock timeout of 1 sec, so it should 
//  CONSTRAINTNAME 
//  Insert overwrite on one partition with multiple files 
//  LOCK_ID_INTERNAL 
//  Allow TCP Keep alive socket option for for HiveServer or a maximum timeout for the socket. 
//  UNDONE: Convert byte 0 or 1 to character. 
// now we have the rootTasks set up for Insert ... Select 
//  Try with parameterized varchar types 
//  Taken care of on higher level. 
//  If we are seeing this mapjoin for the first time, initialize the plan.   If we are seeing this mapjoin for the second or later time then atleast one of the   branches for this mapjoin have been encounered. Join the plan with the plan created 
// but the writeEntity is complete in DDL operations, instead DDL sets the writeType, so  we use it to determine its lockMode, and first we check if the writeType was set 
//  there should be 2 calls to create partitions with batch sizes of 5, 4 
//  to a single node may fail. 
//  not authorized by this implementation, ie operation is allowed by it 
//  From java.util.Calendar 
//  check config properties expected with embedded metastore client 
//  this could be replaced by bucketing on RS + bucketed fetcher for next MR 
//  TODO: extract interface when needed 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBlob(int, java.io.InputStream, long)    */
//  Find the common class for type conversion 
//  Rename unpartitioned table 
//  Initialize 
// stepTracker should now be 2 which indicates t2 has started 
// make the partition in Target not empty 
//  Drop cascade, functions dropped by cascade 
//  these will become v0-v3 
//  If destination file is present and checksum of source mismatch, then retry copy. 
//  we're cloning the operator plan but we're retaining the original work. That means   that root operators have to be replaced with the cloned ops. The replacement map 
//  Column doesn't appear to be a partition column for the table. 
//  This is not a mistake, catName is in the where clause twice 
//  Found UDF in metastore - now add it to the function registry. 
//  Rewrite CASE into NVL 
//  get the first record 
// return the CompressionCodec used for this file 
//  User might have only specified partial list of partition keys, in which case add other partition keys in partSpec 
//  The entire storage is 0x00s or 0xFFs   0x00s means is 0   0xFFs means is -1 
//  Add a new table via CachedStore 
//  copy with 8K buffer, not close 
//  test february of leap year, 2/31 is viewed as 3/2 due to 2 days diff from 
//  Restore original values 
//  Release this shared timer resource 
//  this is a mapjoin but not suited for a sort merge bucket map join. check outer joins 
//  remove old parents 
//  all partitions are altered atomically   all prehooks are fired together followed by all post hooks 
//  No op 
//  create table must fail. 
// fetchTransactionBatch() was never called, we want to start with first txn 
//  Create the column stats table 
//  nothing to do here, silently return. 
//  The copyToBuffer will reposition and re-read the input buffer. 
//  In case of test, do just close the log files, do not remove them. 
//  if our dbName is equal, but tableName is blank, we're interested in this db-level event 
//  if doAs is not enabled, we pass the principal/keypad to spark-submit in order to   support the possible delegation token renewal in Spark 
//  max we allow tez to pick 
//  MY_STRING_ENUM_MAP 
// this is to handle syn(pos) where pos < headerEnd. 
// cut prefix from hive's map key 
//  Tarjan's algo 
//  Reset just the value columns and value buffer. 
//  Retrieve settings in HiveConf that aren't also set in the JobConf. 
//  Setup VectorizationContext 
//  Must be spark branch 
//  Share the same write buffers with our value store. 
// set inner schema for dtype 
//  Either initialCapacity is too large, or nextHighestPowerOfTwo overflows 
//  replace the current task with the new generated conditional task 
//  security property names 
//  Case 3.2 - Max in list members: 1000, Max query string length: 10KB, and exact 1000 members in a single IN clause 
//  Non-empty java opts with bad -Xmx specification 
//  COUNT(*) 
//  If partition columns occur in data, we want to remove them.   So, find out positions of partition columns in schema provided by user.   We also need to update the output Schema with these deletions. 
//  merge work only needs input and output. 
//  Replace the task with the new task. Copy the children and parents of the old 
//  ie we are running 1.3   In 1.3, this constructor has the same behavior, but in 1.4 the default   was changed to add wrapping and newlines. 
//  At this point, the task has been added into the queue. It may have caused an eviction for   some other task. 
//  if we're visiting a terminal we've created ourselves,   just skip and keep going 
//  there could be interval where desired counter value is not populated by the time we make this check 
//  In test mode print the logs to the output 
//  After that, a heuristic is used to decide. 
//  Max tolerable variance for matches 
//  Used for sending information for scheduling priority. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setTypeMap(java.util.Map)    */
//  already encoded to thrift-able object in ThriftFormatter 
/*  * JavaCC - OriginalChecksum=7edd3e61472739e9fede55c18a336638 (do not edit this * line)  */
//  ACTION_EXPRESSION 
//  sorting the keys from the properties helps to create   a deterministic url which is tested for various configuration in 
//  The list is being drained, cannot increase the delta anymore. 
//  SERIALIZATION_LIB 
//  map-side work 
//  The alias may not be present in case of a sub-query 
//  This is for runtime min/max pushdown - don't need to do NOT BETWEEN 
//  Sigh... 
//  to appropriate locations 
//  See the comment in the other isSameKey 
//  second group 
//  Verify that when stats are already present and forceRecompute is specified they are recomputed 
//  Close the inner output stream before closing the outer output stream.   For chunked output this means we don't write end-of-data indicator. 
//  Include type name for precision/scale. 
//  This inspector is initialized, we still need 
//  This is SQL standard - average of zero items should be null. 
//  Key index can be null/"null" if there is only a single stripe. Just start fresh. 
//  set VertexManagerPlugin if whether it's a cross product destination vertex 
//  This is not transactional 
//  Negative number. 
//  Cannot slice compressed files. 
//  notifies when memory usage is 70% after GC 
//  Accumulate the counts. 
//  Set primary key name if null before sending to listener 
//  Unlike in MR, we may call this method multiple times, for each 
//  Our input is repeating (i.e. inputColNumber = 0). 
//  column authorization is checked through table scan operators. 
//  nothing needs to be done 
//  5. Run Cleaner. It should remove the 2 delta dirs and 1 old base dir. 
//  the local batch has been consumed entirely, reset it 
//  Try to eat trailing blank padding. 
//  Set up a common id hash for this job, so that when we create any temporary directory   later on, it is guaranteed to be unique. 
//  optional int64 signatureKeyId = 2; 
/*    * construct the ASTNode for the SQ column that will join with the OuterQuery Expression.   * So for 'select ... from R1 where A in (select B from R2...)'   * this will build (. (TOK_TABLE_OR_COL Identifier[SQ_1]) Identifier[B])   * where 'SQ_1' is the alias generated for the SubQuery.    */
//  make sure it isn't 0 
//  int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM); 
//  Delete data.   Ignore unknownDB.   Cascade. 
//  These two are used to indicate that we are running tests 
// here each batch has written data and committed (to bucket0 since table only has 1 bucket)  so each of 2 deltas has 1 bucket0 and 1 bucket0_flush_length.  Furthermore, each bucket0  has now received more data(logically - it's buffered) but it is not yet committed. 
//  if last batch is successful remove it from partsNotInMs 
//  UNDONE: Copied from VectorMapJoinOuterGenerateResultOperator. 
/*        * Validate and vectorize the Reduce operator tree.        */
//  any log specific settings via hiveconf will be ignored 
//  buffer2 is now in the heap, buffer1 is in the list. "Use" buffer1 again; 
// must be old client talking, i.e. we don't know if it's DP so be conservative 
//  Forward; reset key and value columns. 
//  No existing shard present in the database, use the current version. 
//  This method is reached when error occurs while sending msg, so the session must be bad 
// 2 partitions updated 
//  No need to release memory - cache eviction. 
//  Move the files back to original data location 
//  Throw an HiveSqlException when do async calls 
//  We need to patch the dest back to original into new query.   This makes assumptions about the structure of the AST. 
// Round with default decimal places 
//  a map to keep track of which root generated which work 
//  May need to merge with list of temp tables 
/*        * If the user didn't specify a SerDe, we use the default.        */
//  may be do some retry logic here. 
//           LOG.info(e.getKey() + "=>" +e.getValue());          }        } 
//  skip this, as validateColumnName always returns true 
//  the settings in conf overlay should not be part of session config 
//  Select true fields child, none as 2nd child, and none as 3rd. 
//  Do nothing by default. 
//  Checks the status of the RPC call, throws an exception in case of error 
//  Set Checkpoint task as dependant to add partition tasks. So, if same dump is retried for   bootstrap, we skip current partition update. 
//  Return false for null 
//  Error should not be a timeout. 
//  From metastore (for security) 
//  Hive doesn't support primary keys   using local schema with empty resultset 
//  Compaction doesn't work under a transaction and hence pass null for validTxnList 
//  01000 -> warning 
//  DOUBLE_VAL 
//  Note that pathToAlias will behave as if the original plan was a join plan 
/*   * (non-Javadoc)  *  * @see  * org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider  * #authorize(org.apache.hadoop.hive.ql.metadata.Table,  * org.apache.hadoop.hive.ql.metadata.Partition, java.util.List,  * org.apache.hadoop.hive.ql.security.authorization.Privilege[],  * org.apache.hadoop.hive.ql.security.authorization.Privilege[])   */
//  No reduction, but let's still test the original   predicate to see if it was already a constant,   in which case we don't need any runtime decision   about filtering. 
//  Must be deterministic order maps - see HIVE-8707 
/*          * Vectorize this parent's children.  Plug them into vectorParent's children list.         *         * Add those children / vector children to nextParentList / nextVectorParentList.          */
//  Put parameters to aggregations in reduceValues 
//  the overflow batch. 
//  ^A, ^B, ^C, ^D, \t 
//  No timestamp patterns, should default to normal timestamp format 
//  required   required   required   required   required   required 
//  Extra bytes at the end? 
//  Note: we assume that batchSize will be consistent with vectors passed in.   This is rather brittle; same in other readers. 
//  Default 
//  Allocate 1-1-1-1; free 0&2; allocate 2 
//  List the new files in destination path which gets copied from source. 
//  Argument handling 
//  if all the elements are representing null, then return true 
// ?? 
//  reset the log buffer, verify new dump without any api call does not contain func 
//  All are filtered 
//  Put session into the pool. 
//  First authorize the call 
//  child isn't flattened, because parent is repeating null 
//  rowId >= 'f' 
//  column statistics at index 0 contains only the number of rows 
//  from TextMetaDataFormatter 
//  Everyone has permission to write, but with sticky set so that delete is restricted.   This is required, since the path is same for all users and everyone writes into it. 
//  delimiters for SQL statements are any   non-letter-or-number characters, except   underscore and characters that are specified   by the database to be valid name identifiers. 
/*    * Favor Broad Plans over Deep Plans.    */
//  we need to keep the original list of operators in the map join to know 
//  Map from PrimitiveTypeInfo to AbstractPrimitiveJavaObjectInspector. 
//  Irrelevant from eventIds. This can be tracked in the AM itself, instead of polluting the task.   Also since we have all the MRInput events here - they'll all be sent in together. 
//  If input has not been rewritten, do not rewrite this rel. 
//  right's signum wins 
//  Read all to the world 
//  this means correated value generator wasn't generated 
//  we are done, since there are no keys to check for 
//  If the start of the split points into the middle of the cached slice, we cannot   use the cached block - it's encoded and columnar, so we cannot map the file 
//  this is to keep track if a subquery is correlated and contains aggregate   this is computed in CalcitePlanner while planning and is later required by subuery remove rule   hence this is passed using HivePlannerContext 
//  to finish for it to execute 
//  4. Convert Agg Fn args to Calcite 
//  Entire response is written out. Safe to enable timeout handling. 
//  See comment in ObjectStore.getDataSourceProps 
//  Lookup the delegation token. First in the connection URL, then Configuration 
//  Ensures all params are indented. 
//  1.20 * 3.30 
//  return the number of columns recorded in this file's header 
//  Substitution is only supported in non-beeline mode. 
//  e.g., the output columns does not contains the input table 
//  We need to disable join emit interval, since for outer joins with post conditions   we need to have the full view on the right matching rows to know whether we need   to produce a row with NULL values or not 
//  2 hosts. 2 per host. 5 requests at the same priority.   First 3 on host1, Next at host2, Last with no host.   Third should allocate on host2, 4th on host2, 5th will wait. 
// Mocks HS2 publishing logic. 
/*              * Multi-Key outer get key.              */
//  sqlState, errorCode should be set 
//  First need to find the min_uncommitted_txnid which is currently seen by any open transactions.   If there are no txns which are currently open or aborted in the system, then current value of 
// Acid LM doesn't maintain getOutputLockObjects(); this 'if' just makes logic more explicit 
//  Don't use FieldSchema.equals() since it also compares comments,   which is unnecessary for this method. 
//  Create the required command line options 
//  Parse out the individual parts 
//  we are aborting all txns in the current batch, so no need to heartbeat 
//  remove the existing partition columns from the field schema 
// test major compaction 
//  PartitionView does not have SD. We do not need update its column stats 
//  HLL algorithm shows stronger bias for values in (2.5 * m) range.   To compensate for this short range bias, linear counting is used   for values before this short range. The original paper also says   similar bias is seen for long range values due to hash collisions   in range >1/30*(2^32). For the default case, we do not have to   worry about this long range bias as the paper used 32-bit hashing   and we use 64-bit hashing as default. 2^64 values are too high to   observe long range bias (hash collisions). 
//  let the dummy op be the parent of mapjoin op 
//  We have extracted the count from the hash multi-set result, so we don't keep it. 
//  c1-c10   c11-c20 
//  all columns (select *, for example) 
//  Assuming the used memory is equally divided among all executors. 
//  turn bytes back into identifier for cache lookup 
/*    * fastSerializationUtilsRead high word multiplier:   *   *    Multiply by 2^(62 + 63)                      -- 38 digits or 3 decimal words.   *   *    (2^62)*(2^63) =   *      42535295865117307932921825928971026432 or   *     (12345678901234567890123456789012345678)   *     (         1         2         3        )   *      42,535,295,865,117,307,932,921,825,928,971,026,432 or   *      425352,9586511730793292,1825928971026432  (16 digit comma'd)    */
//  Create expressions for Project operators before and after the Union 
//  Make sure semijoin is not enabled. If it is, then do not remove the dynamic partition pruning predicates. 
//  Find databases which name contains _to_find_ or _hidden_ 
//  retain output column number from parent 
//  result grantor principal 
//  Create a database with a view 
//  Ensure partition is present 
//  Check if we already have initiated or are working on a compaction for this partition   or table.  If so, skip it.  If we are just waiting on cleaning we can still check,   as it may be time to compact again even though we haven't cleaned.  todo: this is not robust.  You can easily run Alter Table to start a compaction between 
//  set r.returnType 
/*  @bgen(jjtree) TypeByte  */
//  Interval year month comparisons 
//  Semantic error not possible. Must be a bug. Convert to   internal error. 
//  if value too large, should also be null 
//  create a valid table 
//  For use from HCatClient.addPartitions(), to construct from user-input. 
//  record info   metrics 
//  StatsSetupConst.StatDB 
//  If the last line didn't match the patterns either, the stack trace is definitely   over 
//  For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list 
//  Buffers in test are fakes not linked to cache; notify cache policy explicitly. 
//  If we did not change the DB location, there is no need to move the table directories. 
// Check if ExprNodeColumnDesc is wrapped in expr. 
/*  in specific order  */
//  Get table details from tabNameToTabObject cache 
// this should fail because txn aborted due to timeout 
// this will abort the txn 
//  For each task, set the key descriptor for the reducer 
//  dynamic partitions   If no dynamic partitions are generated, dpPartSpecs may not be initialized 
//  Alternate between returning deleted and not.  This is easier than actually   tracking operations. We test that this is getting properly called by checking that only   half the records show up in base files after major compactions. 
//  Above decimal. 
//  Test that existing shared_write partition with new exclusive coalesces to 
//  Initialize constant arguments 
/*  TODO: determine the progress.  */
//  multiply by p 
/*    * num rows whose output is evaluated.    */
//  if it is a dot operator, remember the field name of the rhs of the   left semijoin 
//  Use UDF in query 
//  We can now create a multijoin operator 
//  If this is a non-local warehouse, then adding resources from the local filesystem   may mean that other clients will not be able to access the resources.   So disallow resources from local filesystem in this case. 
// insert overwrite not supported for ACID tables 
//  Perform alters in A for incremental replication 
//  this is now a LIFO operation 
// get the current batch size 
//  class static variables 
//  The batchIndex for the rows that are for the THEN/ELSE rows respectively. 
//  COLUMNS 
//  These are only used for tests. 
// now we have an archive with 3 partitions 
//  Get the partition specs 
//  Validate the function name 
//  Prepare buffers 
/*  (first_name < 'owen' or 'foobar' = substr(last_name, 4)) and    first_name between 'david' and 'greg'  */
//  If the lock id is 0, then there are no locks in this heartbeat 
//  -Xmx specified in bytes 
//  The input to the select does not matter. Go over the expressions 
//  initialize the object inspectors 
//  production is: Async() FunctionType() NAME FieldList() Throws()   [CommaOrSemicolon] 
//  Wrap the client with a thread-safe proxy to serialize the RPC calls 
/*    * (non-Javadoc)   *   * @see javax.sql.DataSource#getConnection(java.lang.String, java.lang.String)    */
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getFetchSize()    */
//  colExprMap.size() == size of cols from SEL(*) branch 
/*    * Whether root tasks after materialized CTE linkage have been resolved    */
//  inside our own, so that we can also store requested quantile values between calls 
//  loop for middles 
//  go through each event, and dump out each event to a event-level dump dir inside dumproot 
//  End HiveAggregateProjectMergeRule.java 
//           debugStackTrace = e.getStackTrace(); 
//  Simple container registration and un-registration without any task attempt being involved. 
//  Test with null args 
//  The column number for this one column join specialization. 
//  check if any left pair exists for right objects 
//                 newInput 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#executeUpdate(java.lang.String)    */
/*  * HiveCommand is non-SQL statement such as setting a property or * adding a resource. * */
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setQueryTimeout(int)    */
//  For all the source tables that have a lateral view, attach the 
//  by default partition and bucket columns are sorted in ascending order 
//  when creating the reader below there are 2 read ops per bucket file (listStatus and open). 
//  Create new Project-Union-Project sequences 
// find operators which are the children of specified filterOp and there are SparkPartitionPruningSink in these 
//  And one partition 
//  Now allow the users to specify any pools. 
//  Update the object. 
//  10^-39  (rounds) 
//  Now vary isRepeating   nulls possible on left, right 
//  anything else including boolean and string is null 
//  The task has been terminated and the duck accounted for based on local state.   Whatever we were doing is irrelevant. The metrics have also been updated. 
//  This test case currently fails, since add partitions don't inherit anything from tables. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNull(int, int)    */
//  get the lowValue 
//  we should cancel hcat token if it was acquired by hcat   and not if it was supplied (ie Oozie). In the latter 
//  in 2 longs) produces a interval_day_time. 
/*        End of additional steps     */
/*    * The fix up is delayed so that the parent operators aren't modified until the entire operator   * tree has been vectorized.    */
/*    * represents the schema exposed by a QueryBlock.    */
//  block 
//  If it's a move task, get the path the files were moved from, this is what any   preceding map reduce task inferred information about, and moving does not invalidate   those assumptions   This can happen when a conditional merge is added before the final MoveTask, but the 
//  But for native tables, we need to do a prefix match for   subdirectories.  (Unlike non-native tables, prefix mixups don't seem   to be a potential problem here since we are always dealing with the   path to something deeper than the table location.) 
//  Data Type Conversion Needed?     VECTORIZED_INPUT_FILE_FORMAT:      No data type conversion check?  Assume ALTER TABLE prevented conversions that      VectorizedInputFileFormat cannot handle...     VECTOR_DESERIALIZE:      LAZY_SIMPLE:          Capable of converting on its own.      LAZY_BINARY          Partition schema assumed to match file contents.          Conversion necessary from partition field values to vector columns.   ROW_DESERIALIZE      Partition schema assumed to match file contents.      Conversion necessary from partition field values to vector columns.   
//  Use non-settable struct object inspector. 
// can handle only case trunc date type 
//  Raw splits 
//     of an aggregate expression from the rest of nodes 
//  We are also not supposed to call setDone, since we are only part of the operation. 
//  PKTABLE_DB 
//  This should never happen with linked list queue. 
// need to create the table "manually" rather than creating a task since it has to exist to   compile the insert into T... 
//  repeated string tablesRead = 10; 
// JDOException wrapped in MetaException wrapped in InvocationException 
//  get tmp file URI 
//  only one HS2 instance available (cannot failover) 
//     Trigger moveTrigger1 = new ExecutionTrigger("move_big_read", moveExpression1,        new Action(Action.Type.MOVE_TO_POOL, "ETL"));      Trigger killTrigger = new ExecutionTrigger("big_write_kill", moveExpression2,        new Action(Action.Type.KILL_QUERY));      setupTriggers(Lists.newArrayList(moveTrigger1), Lists.newArrayList(killTrigger));      String query = "select t1.under_col, t1.value from " + tableName + " t1 join " + tableName +        " t2 on t1.under_col>=t2.under_col order by t1.under_col, t1.value";      List<String> setCmds = new ArrayList<>();      setCmds.add("set hive.tez.session.events.print.summary=json");      setCmds.add("set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter");      setCmds.add("set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter");      List<String> errCaptureExpect = new ArrayList<>();      errCaptureExpect.add("Workload Manager Events Summary");      errCaptureExpect.add("\"eventType\" : \"GET\"");      errCaptureExpect.add("\"eventType\" : \"MOVE\"");      errCaptureExpect.add("\"eventType\" : \"KILL\"");      errCaptureExpect.add("\"eventType\" : \"RETURN\"");      errCaptureExpect.add("\"name\" : \"move_big_read\"");      errCaptureExpect.add("\"name\" : \"big_write_kill\"");      // violation in BI queue      errCaptureExpect.add("\"violationMsg\" : \"Trigger " + moveTrigger1 + " violated");      // violation in ETL queue      errCaptureExpect.add("\"violationMsg\" : \"Trigger " + killTrigger + " violated");      errCaptureExpect.add("\"subscribedCounters\" : [ \"HDFS_BYTES_READ\", \"HDFS_BYTES_WRITTEN\" ]");      errCaptureExpect.add("Event: GET Pool: BI Cluster %: 80.00");      errCaptureExpect.add("Event: MOVE Pool: ETL Cluster %: 20.00");      errCaptureExpect.add("Event: KILL Pool: null Cluster %: 0.00");      errCaptureExpect.add("Event: RETURN Pool: null Cluster %: 0.00");      runQueryWithTrigger(query, setCmds, killTrigger + " violated", errCaptureExpect);    } 
//  in recent hadoop versions, use deleteOnExit to clean tmp files. 
//  the remainder. 
//  Generate all expressions from lateral view 
//  STATUS 
//  should be either SELECT or SELECT DISTINCT 
//  jc is effectively final, but it has to be volatile since it's accessed by different 
//  a concatenation of dbname, tablename and partition keyvalues. 
//  production is: struct this.name { FieldList() } 
//  Ensure task1 is preempted based on time (match it's allocated containerId) 
//  using init(..) instead of this(..) because the EventUtils.getDbTblNotificationFilter   is an operation that needs to run before delegating to the other ctor, and this messes up chaining   ctors 
//  if UDAFs are present, new columns needs to be added 
//  Signature restriction to NSOE, and NSOE being a flat exception prevents us from   setting the cause of the NSOE as the MetaException. We should not lose the info   we got here, but it's very likely that the MetaException is irrelevant and is   actually an NSOE message, so we should log it and throw an NSOE with the msg. 
// use the UGI object that got added 
//  NUM_NULLS 
//  Lets try to store original column name, if this column got folded   This is useful for optimizations like GroupByOptimizer 
//  set the new hive-site.xml 
//  1. Insert MapSide GB 
//  Drain any calls which may have come in during the last execution of the loop. 
/*      * Handling of SubQuery Expressions:     * if "Where clause contains no SubQuery expressions" then     *   -->[true] ===CONTINUE_FILTER_PROCESSING===     * else     *   -->[false] "extract SubQuery expressions\n from Where clause"     *   if "this is a nested SubQuery or \nthere are more than 1 SubQuery expressions" then     *     -->[yes] "throw Unsupported Error"     *   else     *     --> "Rewrite Search condition to \nremove SubQuery predicate"     *      --> "build QBSubQuery"     *        --> "extract correlated predicates \nfrom Where Clause"     *        --> "add correlated Items to \nSelect List and Group By"     *        --> "construct Join Predicate \nfrom correlation predicates"     *     --> "Generate Plan for\n modified SubQuery"     *     --> "Build the Join Condition\n for Parent Query to SubQuery join"     *     --> "Build the QBJoinTree from the Join condition"     *     --> "Update Parent Query Filter\n with any Post Join conditions"     *     --> ===CONTINUE_FILTER_PROCESSING===     *   endif     * endif     *     * Support for Sub Queries in Having Clause:     * - By and large this works the same way as SubQueries in the Where Clause.     * - The one addum is the handling of aggregation expressions from the Outer Query     *   appearing in correlation clauses.     *   - So such correlating predicates are allowed:     *        min(OuterQuert.x) = SubQuery.y     *   - this requires special handling when converting to joins. See QBSubQuery.rewrite     *     method method for detailed comments.      */
//  Operator with 2 children 
//  No CRLF substitution -- return original line. 
//  task is processed. 
//  Copied from AcidUtils so we don't have to put the code using this into ql. 
//  Tracks existing requests which are cycled through. 
//  If a partition has been spilled to disk, its size will be 0, i.e. it won't be picked 
//  Evaluate the function result for each row in the partition 
// TException 
//  Create LFD even for MM CTAS - it's a no-op move, but it still seems to be used for stats. 
//  remove ending ';' 
//  Create the reduce sink operator 
//  column index, stream type, buffers 
//  column is interpreted as the row key. 
//  SortBy ASC 
//  call-1: listLocatedStatus - mock:/mocktbl   call-2: check existence of side file for mock:/mocktbl/0_0   call-3: open - mock:/mocktbl/0_0   call-4: check existence of side file for  mock:/mocktbl/0_1 
//  optional int32 app_attempt_number = 3; 
//  make a clone of existing hive conf 
//  supportsKeyTypes 
//  Necessary to clean up the transaction in the db. 
//  Use the UnparseTranslator to resolve unqualified table names. 
//  No op. Shouldn't actually be called, if it is, the test will fail. 
//  SSL support 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getClob(int)    */
//  This is the initial state for a lock 
//  See the class comment - HIF handles MM for all input formats, so if we try to handle it   again, in particular for the non-recursive originals-only getSplits call, we will just   get confused. This bypass was not necessary when MM tables didn't support originals. Now   that they do, we use this path for anything MM table related, although everything except   the originals could still be handled by AcidUtils like a regular non-txn table. 
//  CTAS; make the movetask's destination directory the table's destination. 
//  check the root expression for final candidates 
//  4. Now decompress (or copy) the data into cache buffers. 
//  Step 2: Merge mapJoinTask into the Map-side of its child. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#executeQuery()    */
//  Initializing a stats publisher 
//  not come from the local branch 
//  file for local 
//  Verify the next write id 
//  Repeating with other as nullable 
//  the output of the CommonJoinOperator to the input columnInfo. 
//  Our limit is max precision integer digits + "leading" zeros below the dot.   E.g. 0.00021 has 3 zeroes below the dot.   
//  Re-run constant propagation so we fold any new constants introduced by the operator optimizers 
//  Request task2 (task1 already started at previously set time) 
//  Run the cleaner thread when the cache is maxFull% full 
//  trim the trailing "," 
//  no locality-requested, randomly pick a node containing free slots 
//  And UDF 
//  default return the urlParam passed in as-is. 
//  ORC files can be converted to full acid transactional tables 
//  Tests for dropPartition(String db_name, String tbl_name, List<String> part_vals,   boolean deleteData) method 
//  LATIN SMALL LETTER S WITH HOOK U+0282 (2 bytes) 
//  remove the table folder 
//  point (old releases) 
//  Add root Tasks to runnable 
//  2^(56 + 56) 
//  Remains to copy from current read buffer. Less than wbSize by def. 
//  LOCATION 
//  To make sure host/port pair is valid, the status of the location   does not matter 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#executeQuery(java.lang.String)    */
//  Here we just reuse the THREAD_COUNT configuration for   METASTORE_FS_HANDLER_THREADS_COUNT since this results in better performance   The number of missing partitions discovered are later added by metastore using a   threadpool of size METASTORE_FS_HANDLER_THREADS_COUNT. If we have different sized   pool here the smaller sized pool of the two becomes a bottleneck 
//  You can blame this on Owen. 
//  If the table could not cached due to memory limit, stop prewarm 
//  Create the batch we will use to return data for this RG. 
//  foo >= 'f' 
//  The master thread and various workers. 
//  long colulmn   text column 
//  has NOT keyword 
//  Avoid "/tmp", directories closer to "/" 
//  And got a duck. 
//  Create the work for moving the table 
//  efficiently add computed values to the last batch of a group key. 
//  how many jobs have been started 
//  We don't do proper overlap checking because it would cost cycles and we   think it will never happen. We do perform the most basic check here. 
//  by default assume no filter tag so we are good 
//  Reset database location. 
/*          * The partition columns are set once for the partition and are marked repeating.          */
//  Align the THEN/ELSE types. 
/*    * A task and its child task has been converted from join to mapjoin.   * See if the two tasks can be merged.    */
//  test double->double version 
// these 2 values are equal when TXN entry is made.  Should never be equal after 1st heartbeat, which we 
/*  But that will be specific to hdfs. Through storagehandler mechanism,    */
//  Methods to set/reset getNextNotification modifier 
//  construct value table Desc 
//  any exact match? 
//  Repl policy should be created based on the table name in context. 
//  this depends on o 
//  6. Add Schema(RR) to RelNode-Schema map 
//  We don't check explicit pool match for apps; both are specified on the jdbc string 
//  We always return something from getAclForPath so this should not happen. 
//  No boolean value match for 5 char field. 
//  3. Insert RS on reduce side with Reduce side GB as input 
//  if it is a dynamic partition throw the exception 
//  add the move task for those partitions that do not need merging 
//  Extract partition desc from sorted map (ascending order of part dir) 
//  1) The timestamp column 
//  split[0] = DP, split[1] = LB 
//  Class variables 
//  container 
//  Use only 1 reducer in case of cartesian product 
//  add jars as we find them to a map of contents jar name so that we can   avoid 
//  generate a DDL task and make it a dependent task of the leaf 
//  Limit to milliseconds only... 
//  MapJoinOperator 
//  Make sure it doesn't already exist 
//  Convert non-skewed table to skewed table. 
//  The last field:       single_value(true) 
//  Remove table entry from SessionState 
//  ////// 2. Generate GroupbyOperator 
//  Test that listPartitionsByFilter() throws HCatException if the partition-key is incorrect. 
//  COMPLEX pattern 
//  4. Construct AggInfo 
// We use partition schema properties to set the partition descriptor properties   if usePartSchemaProperties is set to true. 
//  get the semijoin rhs table name and field name 
//  all column names referenced including virtual columns. used in ColumnAccessAnalyzer 
//  See FastHiveDecimalImpl for more details on these fields. 
//  Make sure nothing escapes this run method and kills the metastore at large, 
//  The update fails because the task has terminated on the node. 
//  Some Spark plans cause Hash and other modes to get this.  So, ignore it. 
//  inputSplitInfo = MRInputHelpers.generateInputSplitsToMem(jobConf, false, 0); 
//  sending out status/DONE/KILLED/FAILED messages before TAImpl knows how to handle them. 
//  (will need to handle an alternate work-dir as well in this case - derive from branch?) 
// since we have an open transaction, only 4 values above are expected  
//  Just the primitive types. 
//  Cannot simplify, we bail out 
//  Evaluator results are first. 
//  Get the column / table aliases from the expression. Start from 1 as   0 is the TOK_FUNCTION 
//  1. Gather GB Expressions (AST) (GB + Aggregations) 
/*  * Directly deserialize with the caller reading field-by-field the LazyBinary serialization format. * * The caller is responsible for calling the read method for the right type of each field * (after calling readNextField). * * Reading some fields require a results object to receive value information.  A separate * results object is created by the caller at initialization per different field even for the same * type. * * Some type values are by reference to either bytes in the deserialization buffer or to * other type specific buffers.  So, those references are only valid until the next time set is * called.  */
//  if the columns in row schema is not contained in column   expression map, then those are the aggregate columns that   are added GBY operator. we will estimate the column statistics   for those newly added columns 
//  Check that the correlated variables referenced in these 
//  This ensures the incremental dump doesn't get all events for replication. 
// not relevant - creating new partition 
//  For outer join, remember our input rows before ON expression filtering or before   hash table matching so we can generate results for all rows (matching and non matching) 
//  dummyOps is a reference to all the HashTableDummy operators in the   plan. These have to be separately initialized when we setup a task.   Their function is mainly as root ops to give the mapjoin the correct 
//  and if necessary load the JARs in this thread. 
//  NEW TAI LUE LETTER HIGH LA U+199C (3 bytes) 
//  determine which input rel oldOrdinal references, and adjust   oldOrdinal to be relative to that input rel 
//  not selected 
//  expected exception due to lexer error 
//  Jump to the field we want and read it. 
//  used by Windowing 
//  Look for hive-site.xml on the CLASSPATH and log its location if found. 
//  Want to start sufficiently "high" enough in the iterator stack 
//  an array of partitions holding the triplets   total number of small table rows in memory   the max memory limit that can be allocated   the actual memory used   row size of the small table   whether there's any spilled partition   the partition into which to spill the big table row; 
//  remove filterMap for outer alias if filter is not exist on that 
/*    * Check that SubQuery is a top level conjuncts.   * Remove it from the Where Clause AST.    */
//  Check if the parent is coming from a table scan, if so, what is the version of it. 
//  for each joining table, set dir for big key and small keys properly 
//  Figure out who we should run the file operations as 
//  Calcite literal is in millis, we need to convert to seconds 
//  create a new  MapredLocalWork 
//  -1 indicates malformed version. 
//  To support nested column pruning, we need to track the path from the top to the nested 
//  hash-based aggregations 
/*        * Restriction.1.h :: SubQueries only supported in the SQL Where Clause.        */
//  if the table is in a different dfs than the partition,   replace the partition's dfs with the table's dfs. 
// Class.forName().newInstance() does not work if the underlying  InputSplit has package visibility 
//  common class between char/varchar is string? 
// We don't want to not exit because of an issue with logging 
// updates p=1/q=3  deletes from p=1/q=2, p=2/q=2  insert p=1/q=2, p=1/q=3 and new part 1/1 
//  v[2] 
//  Http transport mode.   We set the thread local proxy username, in ThriftHttpServlet. 
//  hive native 
//  Infer sort columns from operator tree 
//  long IN 
// complete 1st txn 
//  Leading spaces 
//  one VInt without nanos 
/*  Routines for stubbing into Writables  */
//  ExecDriver has no plan path, so we cannot derive VRB stuff for the wrapper. 
//  copy the properties from storageHandler to jobProperties 
//  should never be executed 
//  1) Replace INSERT OVERWRITE by INSERT 
//  I32_VALUE 
//  For example in the case of "select * from V join T ..." T would be direct dependency 
//  check compatibility with subsequent files 
//  Return false otherwise 
//  We killed something, but still got rejected. Wait a bit to give a chance to our   previous victim to actually die. 
/*      * Setup the OI based on the:     * - Input TableDef's columns     * - the Window Functions.      */
//  If we reached here, we did not find a replication   spec in the node or its immediate children. Defaults   are to pretend replication is not happening, and the   statement above is running as-is. 
//  Change to new table and append stats for the new table 
//  Kryo will set this; or so we hope. 
//  If the newPosition is the same as the previousPosition, we've reached the end of the   binary search, if the new position at least as big as the size of the split, any 
// origPathStr="hdfs://host:99" for example 
//  select is UDTF 
//  delete the parent temp directory of all custom dynamic partitions 
//  this is best effort optimization, bail out in error conditions and   try generate and execute slower plan 
//  Column Types of all partitioned columns.  Used for generating partition specification 
//  The fifth could be combined again. 
//  12. Save state for future iterations. 
//  The buffer can only be removed after the removed flag has been set. If we are able to   lock it here, noone can set the removed flag and thus remove it. That would also mean   that the header is not free, and noone will touch the header either. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBlob(int, java.sql.Blob)    */
//               partitionCols.add(schema.getName()); 
//  Don't log the exception, people just get confused. 
//  if its not an internal name, this is what we want. 
//  We will adjust start and end so that we could record the metrics; save the originals. 
//  Prevent excessive logging in case of deadlocks or slowness. 
//  Allow implicit String to Date conversion 
//  close the underlying connection pool to avoid leaks 
//  v[8] -- since left integer #5 is always 0, some products here are not included. 
//  write byte to say whether to skip pruning or not 
//  That Union[T, NULL] is converted to just T, within a Map 
//  Tracks HiveQueryId by QueryIdentifier. This can only be set when config is parsed in TezProcessor.   all the other existing code passes queryId equal to 0 everywhere.   If we switch the runtime and move to parsing the payload in the AM - the actual hive queryId could 
//  make right a child of left 
//  Tests for Partition appendPartition(String tableName, String dbName, List<String> partVals) method 
//  SCHEMA_GROUP 
//  locality information before those without locality information 
//  Test basic right trim and truncate to vector. 
//  check if map side aggregation is possible or not based on column stats 
//  Vectorization enabled 
//  Collect operator to observe the output of the script 
//  UDTF 
//  we've already set this one up. Need to clone for the next work. 
//  ZooKeeper property name to pick the correct JAAS conf section 
//  body 
//  newOutput is the index of the cor var in the referenced   position list plus the offset of referenced position list of 
/*  Convert an integer value representing a timestamp in nanoseconds to one   * that represents a timestamp in seconds, with fraction, since the epoch.    */
//  position in file where the first row in this block starts 
//  there should be no adjustments for leap seconds 
//  FIFO policy doesn't care. 
//  Note this behavior may have to change if we ever implement a vectorized merge join 
//  LOG.debug("VectorMapJoinFastLongHashTable add key " + key + " slot " + slot + " pairIndex " + pairIndex + " empty slot (i = " + i + ")"); 
//  We keep the columns only the columns that are part of the final output 
//  Verify mergeOnlyTask is NOT optimized 
//  Bail out, nothing to do 
//  if it is not an external table then create one 
//  2.1 Construct JoinLeafPredicateInfo 
//  first change the filter condition into a join condition 
//  no need to make a metastore call 
/*          * Don't drop NOTIFICATION_LOG, SEQUENCE_TABLE and NOTIFICATION_SEQUENCE as its used by other         * table which are not txn related to generate primary key. So if these tables are dropped         *  and other tables are not dropped, then it will create key duplicate error while inserting         *  to other table.          */
//  Division with overflow/zero-divide check. Error produces NULL output. 
//  Create a FileSinkOperator for the file name of taskTmpDir 
/*    *    VectorAggregateExpression()   *    VectorAggregateExpression(VectorAggregationDesc vecAggrDesc)   *   *    AggregationBuffer getNewAggregationBuffer()   *    void aggregateInput(AggregationBuffer agg, VectorizedRowBatch unit)   *    void aggregateInputSelection(VectorAggregationBufferRow[] aggregationBufferSets,   *                int aggregateIndex, VectorizedRowBatch vrg)   *    void reset(AggregationBuffer agg)   *    long getAggregationBufferFixedSize()   *   *    boolean matches(String name, ColumnVector.Type inputColVectorType,   *                ColumnVector.Type outputColVectorType, Mode mode)   *    assignRowColumn(VectorizedRowBatch batch, int batchIndex, int columnNum,   *                AggregationBuffer agg)   *    */
//  This is called either with an error that was queued, or an error that was set into the   atomic reference in this class. The latter is best-effort and is used to opportunistically   skip processing of a long queue when the error happens. 
//  Check for aborted txns 
// make sure they are the same before and after compaction 
//  NOTE:      and power = -3 is multiply by 10^-3 
//  Setting success to false to make sure that if the listener fails, rollback happens. 
//  TODO factor security manager into pipeline   TODO factor out encode/decode to permit binary shuffle   TODO factor out decode of index to permit alt. models 
//  Check for a nested message. If found, set the schema, else return. 
//  ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS;   The plan consists of a simple MapRedTask followed by a StatsTask.   The MR task is just a simple TableScanOperator 
//  Refering to SQLOperation.java, there is no chance that a HiveSQLException throws and the   async background operation submits to thread pool successfully at the same time. So, Cleanup   opHandle directly when got HiveSQLException 
// todo: define groups in regex and use parseInt(Matcher.group(2)).... 
//  2) We propagate 
//  usually controlled by bucketing 
/*  Private constructor  */
/*  * The interface adds the single long key hash multi-set contains method.  */
//  write the plan out 
//  test for CHAR type 
//  request and validate the request cookies. 
// Test that changing column data type fails 
//  References to external fields for async SplitInfo generation. 
//  1. Get Select Expression List 
//  Case when this is an expression 
//  outputRel is the generated augmented select with extra unselected 
/*      * initialize Event Processor and dispatcher.      */
//  1. Get the constant value associated with the current element in the struct. 
//  "successful". 
//  for HashMap 
//  Some of the columns' stats are missing   This implies partition schema has changed. We will merge columns   present in both, overwrite stats for columns absent in metastore and   leave alone columns stats missing from stats task. This last case may   leave stats in stale state. This will be addressed later. 
//  Clone all the operators between union and filescan, and push them above   the union. Remove the union (the tree below union gets delinked after that) 
//  The wait queue should be able to fit at least (waitQueue + currentFreeExecutor slots) 
// no point making an acid table if these other props are not set since it will just throw  exceptions when someone tries to use the table. 
//  mGby3 is a follow up of mGby2. Here we start to count(key). 
//  Not registered for this node.   Register and send state if it is successful. 
// give me more 
//  Create partition 
//  collect the newly added partitions. connection.commitTransaction() will report the dynamically added   partitions to TxnHandler 
//  2. Based on the statement, generate the selectOperator 
// found parent mapjoin operator.  Its size should already reflect any other mapjoins connected to it. 
//  Total rows to emit during the whole iteration,   excluding the rows emitted by the separate thread. 
//  DELETE_DATA 
//  This test assumes the hive-contrib JAR has been built as part of the Hive build. 
//  Collect table access keys information for operators that can benefit from bucketing 
// Verify 
//  If the predicate matches, then return true.   Otherwise visit the next set of nodes that haven't been seen. 
//  We require the use of recursive input dirs for union processing 
//  should have severed the ties 
//  Multiply by 1/2^56 or 1.387778780781445675529539585113525390625e-17 to divide by 2^56.   As 16 digit comma'd 13877787,8078144567552953,9585113525390625     Scale down: 56 = 39 fraction digits + 17 (negative exponent or number of zeros after dot).     3*16 (48) + 8 --> 56 down shift. 
//  Log and ignore 
//  verify that the number of events since we began is at least 25 more 
//  the master thread. 
//  NOTE: The current implementation does not allow importing to an "EXTERNAL" location.   This is intentional, since we want the destination tables to be "managed" tables.   If this assumption should change at some point in the future, ImportSemanticAnalyzer   will need some of its checks changed to allow for "replacing" external tables. 
//  Don't worry about setting raw data size diff.  I have no idea how to calculate that   without finding the row we are updating or deleting, which would be a mess. 
//  change the newly created map-red plan as if it was a join operator 
//  check physical path 
//  partition columns   virtual columns 
//  Select all with the first expression and expect the other 2 children to not be invoked. 
//  Read the record with the same record reader ID 
//  If the group by expression is anything other than a list of columns, 
// if another thread adds an entry before the check in this one 
//  This is required for writing null as key for file based tables. 
//  struct<a:boolean,b:double> 
//  Do all checks on short names 
//  Update sum length with the new length 
//  if this is a replication spec, then replace-mode semantics might apply.   if we're already asking for a table replacement, then we can skip this check.   however, otherwise, if in replication scope, and we've not been explicitly asked   to replace, we should check if the object we're looking at exists, and if so,   trigger replace-mode semantics. 
//  NOTE: The reason we use a string name of the hive hbase handler here is   because we do not want to introduce a compile-dependency on the hive-hbase-handler   module from within hive-hcatalog.   This parameter was added due to the requirement in HIVE-7072 
//  original path and not available in CM as well. 
//  by the original MapredTask and this new generated MapredLocalTask. 
//  Check the ndv/rows from the SEL vs the destination tablescan the semijoin opt is going to. 
//  positive numbers, flip just the first bit 
//  build error message 
/*      * input      */
//  Schedule 3 tasks. Give out two ducks - two higher pri tasks get them. Give out 2 more   - the last task gets it and one duck is unused. Give out 2 more - goes to unused.   Then revoke similarly in steps (1, 4, 1), with the opposite effect. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setTransactionIsolation(int)    */
//  Drop table will clean the table entry from the compaction queue and hence worker have no effect 
//  and finally we hook up any events that need to be sent to the tez AM 
//                                12345678901234567890123456789012345678 
//  merge 
//  Tracks appMasters to which heartbeats are being sent. This should not be used for any other   messages like taskKilled, etc. 
// the jars in libJars will be localized to CWD of the launcher task; then -libjars will 
//  Locks newRequestList   Locks completedNodes 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.newBuilder() 
//  If this is an insert, update, or delete on an ACID table then mark that so the 
// since there is no tx, we only have locks for current query (if any) 
//  Set the mapjoin hint if it needs to be disabled. 
//  Ignore. Not required as this will be never   serialized/deserialized. 
//  user provided configs 
//  this test 
//  Ignore closing quote 
//  Create the equality condition 
//  add spark job metrics. - e.g. metrics collected by Spark itself (JvmGCTime, 
//  Boring scenario #2 - two concurrent revokes. Same as above. 
/*  delete_delta_21_23 and delete_delta_25_33 which are created as a result of compacting */
//  TODO: Now we assume the key Object supports hashCode and equals functions. 
//  For information only. 
//  Verify the accumulated logs 
//  Get completed attempts 
//  Update credential provider location   the password to the credential provider in already set in the sparkConf 
//  Count the number of tasks, and kill application if it goes beyond the limit. 
//  create a the context for walking operators 
/*  virtualColumnCount  */
//  don't break old callers that are trying to reuse storages 
// create 4 rows in a file 000000_0_copy_1 
//  MY_ENUM 
//  if there are no nulls then the iteration is the same on all cases 
//  Add embedded rawstore, so we can cleanup later to avoid memory leak 
//  HIVE-19061 introduces UPDATE event which will capture changes to allocation % after GET 
//  (if any) 
//  Add a new table via ObjectStore 
//  Try next available server in zookeeper, or retry all the servers again if retry is enabled 
//  Double 
//  Drop a foreign key 
//  Set if partition sorted or partition bucket sorted 
//  Verbose Logs should contain everything, including execution and performance 
// so we don't accidentally cache the value; shouldn't 
//  check most significant part first 
//  Update value 
//  number n of elements   average of x elements   average of y elements   n times the variance of x elements   n times the variance of y elements   n times the covariance 
//  internal input format used by CombineHiveInputFormat 
//  Test select pow(root.col1.b, root.col2) from table test(root 
//  If the code point from from string already has a replacement or is to be deleted, we   don't need to do anything, just move on to the next code point 
// return true; 
//  boolean to signal whether tagging will be used (e.g.: join) or 
//  right repeats and is null 
//  VRB was created from VrbCtx, so we already have pre-allocated column vectors.   Return old CVs (if any) to caller. We assume these things all have the same schema. 
//  handle the logical operators 
//  no begin + commit 
//  Use context.mapJoinParentMap to get the original RS parents, because 
//  There are some elements that were cached in parallel, take care of them. 
//  TABLE_NAME 
//  which case stats need not be updated 
// do not support task level progress, do nothing here. 
//  to hold output if needed   to hold boolean output 
//  we are here either unpartitioned table or partitioned table with no   predicates 
//  The arg is part of another list. 
/*  Test decimal column to decimal scalar addition. This is used to cover all the   * cases used in the source code template ColumnArithmeticScalarDecimal.txt.    */
//  required   required   required   required   required   required   optional   optional 
//  StorageDescriptor has an empty list of fields - SerDe will report them. 
//  update service registry configs - caveat: this has nothing to do with the actual settings   as read by the AM   if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between   instances 
//  file dump should write to session state console's error stream 
//  We need to obtain an intersection of all the privileges 
//  min txn id is incremented linearly within a transaction batch. 
/*  major version number should match for backward compatibility  */
//  We need to check that datasource was not specified by user 
//  add added files 
//  5. Get Calcite Agg Fn 
//  FOOTER_SUMMARY 
//  http://svn.apache.org/viewvc/commons/proper/dbcp/branches/DBCP_1_4_x_BRANCH/doc/ManualPoolingDataSourceExample.java?view=markup 
//  we always serialize the String type using the escaped algorithm for LazyString 
// ---------------------------------------------------------------------------   Pass-thru constructors.   
//  Should not be treated like it needs data. 
//  skip one 
//  The "most preemptable" task is still too important for us to kill. Put it back. 
//  itself is missing, then throw error. 
//  Duplicate to avoid modification. 
//  from IndexPredicateAnalyzer 
/*        * Single-Column Long check for repeating.        */
//  Use GLOBAL when no key for Reduce. 
//  Uses all 3 decimal longs. 
//  Handle MapJoin specially and check for all its children 
//  setEntryValid() already increments the reader count. Set usedCacheEntry so it gets released. 
//  is it a partitioned table ? 
//  Not present 
//  If partition already exists and we aren't overwriting it, then respect   its current location info rather than picking it from the parent TableDesc 
//  @@protoc_insertion_point(builder_scope:UpdateQueryRequestProto) 
//  Must be a class. 
//  Process partition pruning sinks 
//  Get should fail now (since TTL is 2s) and we've snoozed for 3 seconds 
//  Move the original parent directory to the intermediate original directory 
//  This initializes currentFileRead. 
//  Read single value. 
//  handle repeating 
//  If in test mod create a test log file which will contain only logs which are supposed to   be written to the qtest output 
//  a new vertex. 
//  While threads are blocked on A, we should still be able to get and return a B session. 
//  0 implies no limit 
//  Choosing the 2nd function, since the 1st one is duplicated in the dummy database 
//  used by spark mode to decide whether global order is needed 
//  Conf for non-llap 
//  Location 
//  convert constant back to RexNode 
//  ACLs for znodes on a non-kerberized cluster   Create/Read/Delete/Write/Admin to the world 
//  get all parents 
/*  Valid FileSystem schemes  */
//  now diff the lists 
//  Is this a union type? 
//  Project projects the original expressions, 
//  In non-strict mode and there is no predicates at all - get everything. 
/*    * Creates a job request object and sets up execution environment. Creates a thread pool   * to execute job requests.   *   * @param requestType   *          Job request type   *   * @param concurrentRequestsConfigName   *          Config name to be used to extract number of concurrent requests to be serviced.   *   * @param jobTimeoutConfigName   *          Config name to be used to extract maximum time a task can execute a request.   *    */
//    throw new HiveException("selected1 is not in sort order and unique");   } 
//  don't visit multiple times 
//  Remove old table object's sd hash 
//  Don't attempt scheduling for additional priorities 
//  Verify if entries added in COMPACTION_QUEUE for each table/partition 
//  Specifically necessary for DPP because we might have created lots of "and true and true" conditions 
//  6. Local session path 
//  the sort and bucket cols have to match on both sides for this 
//  with this root operator. 
//  If the table is bucketed, and bucketing is enforced, do the following:   If the number of buckets is smaller than the number of maximum reducers,   create those many reducers.   If not, create a multiFileSink instead of FileSink - the multiFileSink will   spray the data into multiple buckets. That way, we can support a very large   number of buckets without needing a very large number of reducers. 
// keys.add(key);  would need a list of (stmt,rs) pairs - 1 for each key 
//  When the first buffer is loaded ResultSet.next() should be called "incrementalBufferRows" times 
// 1,2,3 
//  Groups the clause names into lists so that any two clauses in the same list has the same   group by and distinct keys and no clause appears in more than one list. Returns a list of the 
//  We should be having a tree which looks like this    TS -> * -> RS -                    \                     -> JOIN -> ..                    /    TS -> * -> RS -     We are in the join operator now. 
//  By the time either success / failed are called, the task itself knows that it has terminated,   and will ignore subsequent kill requests if they go out. 
//  Close the session which should free up the TxnHandler/locks held by the session.   Done in the finally block to make sure we free up the locks; otherwise   the cleanup in tearDown() will get stuck waiting on the lock held here on ACIDTBL. 
//  Spark property.   for now we don't support changing spark app name on the fly 
//  if last batch is successful remove it from partsNotInFs 
// update runtime 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#registerOutParameter(java.lang.String, int,   * int)    */
//  one of the branches is definitely a bucket-leaf 
//  open till limit but not exceed 
//  Find the biggest small table; also calculate total data size of all small tables 
/*    * Initialize using an StructObjectInspector and a column projection list.    */
//  Avoids creating Tez Client sessions internally as it takes much longer currently 
//  Handle the rejection outside of the lock 
//  What we need is a way to get buckets not splits 
//  WHERE clause and pick up the second disjunct from the OR operation. 
//  object overhead + 8 bytes for intCompact + 4 bytes for precision   + 4 bytes for scale + size of BigInteger 
//  Should initialize the value for createValue 
//  more entries than oldInvalidIds. 
//  separator 
//  There will not be any Tez job above this task 
//  forward as arg   forward as arg   forward as arg 
//  enable assertion 
// No big table candidates. 
//  Invoke the InputFormat entrypoint 
//  Not using expressions. 
//  OR(=($0, 1), AND(=($0, 0), =($1, 8)))   transformation creates 7 nodes AND(OR(=($0, 1), =($0, 0)), OR(=($0, 1), =($1, 8)))   thus, it is triggered 
//  from the original SparkWork 
/*          * If this is a Not In SubQuery Predicate then Join in the Null Check SubQuery.         * See QBSubQuery.NotInCheck for details on why and how this is constructed.          */
//  Vector reduce key (i.e. partition) columns are repeated -- so we test element 0. 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.newBuilder() 
//  If there's no memory available, fail 
//  NOTE: Calling last flush length below is more for future-proofing when we have   streaming deletes. But currently we don't support streaming deletes, and this can 
//  use the columnNames to initialize the reusable row object and the columnBuffers. reason this is being done is if buffer is full, we should reinitialize the 
//  IndexStore is trying to tell us something. 
//  MSD and SD should be same objects. Not sure how to make then same right now 
//  "<expr>.FIELD" is constant iff "<expr>" is constant. 
//  TODO HIVE-12255. Make use of SplitSizeEstimator.   The actual task computation needs to be looked at as well. 
//  Failover didn't succeed - log error and exit 
//  execute the setup queries 
//  Try the output as a primitive object 
//  Verify the buffer was reset (real output doesn't happen because it was mocked) 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareCall(java.lang.String, int, int)    */
//  double BETWEEN 
//  for keeping track of the number of elements read. just for debugging 
//    Special handling is needed at times for DATE, TIMESTAMP, (STRING), CHAR, and VARCHAR so they can   be named specifically as argument types.     LongColumnVector -->      INT_FAMILY      DATE      INTERVAL_FAMILY     DoubleColumnVector -->      FLOAT_FAMILY     DecimalColumnVector -->      DECIMAL     BytesColumnVector -->      STRING      CHAR      VARCHAR     TimestampColumnVector -->      TIMESTAMP     IntervalDayTimeColumnVector -->      INTERVAL_DAY_TIME   
//        created when the task is executed. So, we don't care about the correct MM state here. 
//  
/*    * AddSplitsForGroup collects separate calls to setInputPaths into one where possible.   * The reason for this is that this is faster on some InputFormats. E.g.: Orc will start   * a threadpool to do the work and calling it multiple times unnecessarily will create a lot   * of unnecessary thread pools.    */
//  Check if the table should be skipped. 
// this is a leaf so add exportTask to follow it 
//  use the same filesystem as input file if backup-path is not explicitly specified 
//  Initialization fails with retry, no resource plan change. 
//  TODO: Key and Values are Object[] which can be eagerly deserialized or lazily deserialized. To accurately   estimate the entry size, every possible Objects in Key, Value should implement MemoryEstimate interface which   is very intrusive. So assuming default entry size here. 
//  In tez we use a different way of transmitting the hash table.   We basically use ReduceSinkOperators and set the transfer to   be broadcast (instead of partitioned). As a consequence we use   a different SerDe than in the MR mapjoin case. 
//  BEGIN pattern 
//  an unique key of the filterInputRel 
//  lookup the field corresponding to the given field ID and return 
//  The dependencies should include V at depth 0, and T at depth 1 (inferred). 
//  Locked by someone to move or force-evict.   Evicted. This is cache-specific.   Removed from allocator structures. The final state.   The memory was released to memory manager.   New allocation before the first use; cannot force-evict. 
//  Only used in ACID writer. 
/*    * The current version of jetty server doesn't have the status   * HttpStatus.TOO_MANY_REQUESTS_429. Hence, passing this as constant.    */
//  In milliseconds 
//  not allocated yet 
//  The following data members are only required to support the deprecated constructor (and builder). 
//  for partial specifications we need partitions to follow the scheme 
// we create FILTER (sq_count_check(count()) <= 1) instead of PROJECT because RelFieldTrimmer 
//  assertEquals("course",Kfields.get(3).schema.getFields().get(0).schema.getFields().get(0).alias.toLowerCase());   commented out, because the name becomes "innerfield" by default - we call it "course" in pig,   but in the metadata, it'd be anonymous, so this would be autogenerated, which is fine 
/*    * In case of outer joins, we need to push records through even if one of the sides is done   * sending records. For e.g. In the case of full outer join, the right side needs to send in data   * for the join even after the left side has completed sending all the records on its side. This   * can be done once at initialize time and at close, these tags will still forward records until   * they have no more to send. Also, subsequent joins need to fetch their data as well since   * any join following the outer join could produce results with one of the outer sides depending on   * the join condition. We could optimize for the case of inner joins in the future here.    */
// byte 
//  required   required   optional   required 
//  Full table name of format <db_name>.<table_name> 
//  adding same property key twice should throw unique key constraint violation exception 
//  Assume it is almost always a performance win to fill all of isNull so we can   safely reset noNulls. 
//  if subquery is in FILTER 
//  If pool didn't exist, checkAndRemoveSessionFromItsPool wouldn't have returned OK. 
//  and table the table is not sorted 
//  ignore client only queries 
//  Optimization: if the first child is file, we have reached the leaf directory, move the parent directory itself   instead of moving each file under the directory. See HCATALOG-538   Note for future Append implementation : This optimization is another reason dynamic   partitioning is currently incompatible with append on mutable tables. 
//  For caching partition objects 
//  change SerDe to LazySimpleSerDe if it is columnsetSerDe 
//  Add uncovered ACID delta splits. 
//  Use the basic STRING bytes read to get access, then use our optimal truncate/trim method   that does not use Java String objects. 
//  should be set by child class 
//  Restore original state 
//  Could not resolve all of the function children, fail 
//  everything ok. try normal shutdown 
//  discard possibly type related sorting order and replace with alphabetical 
//  all calls fail 
//  validate the plan 
//  NULLABLE   REMARKS   COLUMN_DEF   SQL_DATA_TYPE   SQL_DATETIME_SUB   CHAR_OCTET_LENGTH   ORDINAL_POSITION   IS_NULLABLE   SCOPE_CATALOG   SCOPE_SCHEMA   SCOPE_TABLE   SOURCE_DATA_TYPE   IS_AUTO_INCREMENT 
//  handle the isNull array first in tight loops 
// verify all scopes are closed. 
//  ignore dummy inputs 
//  Test if both are not configured 
// add new column with no cascade option 
//  -create- should not return a ResultSet 
// DFS Stuff 
//  queryTimeout == 0 means no timeout 
//  generate a full partition specification 
//  We store some hash bits in ref; for every expansion, we need to add one bit to hash.   If we have enough bits, we'll do that; if we don't, we'll rehash.   LOG.info("Expanding the hashtable to " + capacity + " capacity"); 
//  1. Build Rel For Src (SubQuery, TS, Join) 
//  We expect that there's only one field schema. 
//  Lag on the whole partition not the iterator range 
//  Replace the buffer in our big range list, as well as in current results. 
//  Categorize the partitions returned, and confirm that all partitions are accounted for. 
//  The manner in which the values in this column are de/serialized from/to Accumulo 
//  1 1 
//  We allow decimals and will return a truncated integer in that case.   Therefore we won't throw an exception here (checking the fractional   part happens below.) 
//  opening allowed after closing: 
//  The encoding method is simple, e.g., replace   all the special characters with the corresponding number in ASCII.   Note that unicode is not supported in table names. And we have explicit   checks for it. 
// http://www.postgresql.org/docs/7.3/static/queries-limit.html 
//  Set columns list for temp table. 
//  Try pulling directly from URL 
//  diffAfterSleep < total sleepTime 
//  Scale down left and compare. 
// In UpdateDeleteSemanticAnalyzer, after super analyze   Read [default@acidtblpart, default@acidtblpart@p=p1]   Write default@acidtblpart TABLE/INSERT  after UDSA   Read [default@acidtblpart, default@acidtblpart@p=p1]   Write [default@acidtblpart@p=p1] PARTITION/UPDATE  todo: this causes a Read lock on the whole table - clearly overkill 
//  table is partitioned   user did NOT specify partition 
//  Multiply by 10^(-scale) to normalize.  We do not use negative scale in our representation.     Example:      4.172529E+20 has a negative scale -20 since scale is number of digits below the dot.      417252900000000000000 normalized as scale 0.   
//  All field names are of the form "key." or "value." 
//  Only evaluate +ve/-ve or cast on constant or recursive casting. 
//  Permissions for metric directory 
//  to marshal/unmarshal znode data 
//  Replication destination will not be external 
/*  * An multi-key hash set optimized for vector map join. * * The key is stored as the provided bytes (uninterpreted).  */
//  1 0 
//  find the number of reducers such that it is a divisor of totalFiles 
//  Node1 has free capacity but is disabled. Node 2 has capcaity. Delay > re-enable tiemout 
//  Assume that for an MM table, or if there's only the base directory, we are good. 
//  This is not called by ConsecutiveChunk stuff in Parquet.   If this were used, it might make sense to make it faster. 
//  Note that we create the cluster name from user conf (hence, a user can target a cluster),   but then we create the signer using hiveConf (hence, we control the ZK config and stuff). 
//  validate 
//  omit nulls 
//  4. Create encoded data reader. 
//  check whether the part exists or not in fs 
//  interim row count can not be less due to containment   assumption in join cardinality computation 
//  Make a MockInstance here, by setting the instance name to be the same as this mock instance 
//  Inner join specific. 
// creates more files in that partition 
//  No match for entire batch. 
//  NOTE: Originally we named this isEmpty, but that name conflicted with another interface. 
//  add all dependencies (i.e.: edges) to the graph 
//  TODO: expose non-primitive as a structured object while maintaining JDBC compliance 
// Environment Variables name 
//  Calculate the expected timeout based on the elapsed time between waiting start time and polling start time 
//  3.3 Add column info corresponding to virtual columns 
//  Test that exclusive lock blocks read and write 
//  null in tests 
//  Create, initialize, and test 
//  This test will make sure that every entry in hive.conf.restricted.list, has a test here 
//  remove the previous renewable jars 
//  the 30TB TPCDS scale set. This way the optimizer will generate plans for a 30 TB set. 
//  As of Hadoop 2.8 - this timeout spec behaves in a strnage manner. "2000,1" means 2000s with 1 retry.   However it does this - but does it thrice. Essentially - #retries+2 is the number of times the entire config 
//  Test inputformat with column prune 
//  Stub OutputFormat actions 
//  For now only alter name, owner, parameters, cols, bucketcols are allowed 
//  fetch the counters 
//  We don't know the acceptable size for Java array, so we'll use 1Gb boundary. 
//  Check if the file format of the file matches that of the partition 
//  Ensure we find the single row which matches our timestamp (where field 1 has value 1) 
//  if at least a partition does not contain row count then mark basic stats state as PARTIAL 
// Read should get 10 + 20 + 10 + 10 + 20 rows 
/*    * This code that creates the result for the granularity functions has been brought from Druid    */
//  Last row of last batch determines isGroupResultNull and double lastValue. 
//  If partition level statistics is requested, add predicate and group by as needed to rewritten   query 
//  Insert overwrite MM table from source table 
// https://issues.apache.org/jira/browse/HIVE-13212 
//  We should send a message to undo what we just did. 
//  Do not clean up the writers - the callback should do it. 
//  Only INLINE followed by ARRAY supported in CBO 
//  We only store longs in our LongColumnVector. 
/*    * Captures how an Input should be Partitioned. This is captured as a   * list of ASTNodes that are the expressions in the Distribute/Cluster   * by clause specifying the partitioning applied for a PTF invocation.    */
//  it is possible that nullscan can fire, we skip this rule. 
//  localize llap client jars 
//  The next byte should be the marker 
//  v[3] 
//  no nulls, not repeating 
//  Partitions to be dropped 
//  All tables are to be cached - this is not possible. In future, we can   support this by randomly 
//  -help 
/*    * TODO: use TableSnapshotRegionSplit HBASE-11555 is fixed.    */
//  Reconnect was successful 
//  cut the operator tree so as to not retain connections from the parent RS downstream 
//  The second one should be combined into the first. 
//  Fill high long from lower long. 
/*      * @param Tuple /* @return null /* @throws IOException     *     * @see org.apache.pig.EvalFunc#exec(org.apache.pig.data.Tuple)      */
//  We start here with at least one byte. 
//  Throw a special exception since it's usually a well-known misconfiguration. 
//  char text value is already stripped of trailing space 
//  Constants for 32 bit variant 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#execute(java.lang.String, int[])    */
//  Set parameters in the n-gram estimator object 
//  Compare long value with HiveDecimal#longValue 
//  temp tables exempted from checks. 
//  If the optimization has been stopped for the reasons like being not qualified,   or lack of the stats data. we do not continue this process. For an example,   for a query select max(value) from src1 union all select max(value) from src2   if it has been union remove optimized, the AST tree will become   TS[0]->SEL[1]->GBY[2]-RS[3]->GBY[4]->FS[17]   TS[6]->SEL[7]->GBY[8]-RS[9]->GBY[10]->FS[18]   if TS[0] branch for src1 is not optimized because src1 does not have column stats 
//  key & value are already read. 
//  clear most members 
//  all the parent SparkTasks that this new task is depend on, if they don't already exists. 
//  [-h|--help] 
// 0123456789012345678901234567890 
// delete output file on exit 
/*    * Copy the current object contents into the output. Only copy selected entries   * as indicated by selectedInUse and the sel array.    */
//  Use different separator values. 
// but preserve table name in SQL 
//  It is already verified that the join can be converted to a bucket map join 
//  Clear rounding portion in lower longword and add 1 at right scale (roundMultiplyFactor). 
//  if zk is disabled or if HA service discovery is enabled we return the already populated params.   in HA mode, params is already populated with Active server host info. 
//  nonFinalCandidates predicates should be empty 
// default version is -1 
//  version of schema for this version of hive 
//  Create HiveConf once, since this is expensive. 
//  Already called in doAs, so no need to doAs here. 
//  there should be 1 call to create partitions with batch sizes of 13 
//  we have just ensured the item is not in the list, so we have a definite state now. 
//  Test class to write a series of values to the designated output stream 
//  Move the data files of this newly created partition to a temp location 
//  Note: we're not creating a copy of the list for saving memory 
//  There are original format files 
//  Returns whether or not two lists contain the same elements independent of order 
//  no hook by default 
//  sort is pushed, we bail out 
//  TODO: return createNullLiteral(literal); 
//  check that the defaults did not remain. 
/*    * (non-Javadoc)   *   * @see   * org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer#   * getHivePolicyProvider()    */
//  If any operator in the stack does not support a auto-conversion, this join should   not be converted. 
//  This is min number of reducer for deduped RS to avoid query executed on   too small number of reducers. For example, queries GroupBy+OrderBy can be executed by 
//                    12345678.901234567890123456789012345678 
//  No room for optimization since we cannot create an empty   Project operator. 
//  We need to support field names like KEY.0, VALUE.1 between   map-reduce boundary. 
/*  Run distcp if source file/dir is too big  */
//  If table is already transactional, no migration needed. 
//  string, char, varchar 
//  configured limit for reducers 
/*  id < 15 or  */
//  If we have no space in the cache, run cleaner thread 
//  4 * grouping(c1) + 2 * grouping(c2) + grouping(c3) 
//  write stmt + ";" + System.getProperty("line.separator") 
//  2. Check if the input is an IN operator with struct children 
//  collect all DPP sinks 
/*      * Use common binary to decimal conversion method we share with fastSetFromBigIntegerBytes.      */
//  Aggregate functions 
//  3. Transform if we have created a new filter operator 
//  If we reached here, then we were successful at finding an alternate internal   column mapping, and we're about to proceed. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getResultSetType()    */
//  count 
//  used by external cache   used by local cache 
//  return the row only if it's not corrupted 
//  TODO: Embedded MetaStore changes the table object when client.createTable is called  Assert.assertNull("Original table storage descriptor location should be null",      table.getSd().getLocation()); 
//  violation in BI queue 
/*      * Add the support of the VectorizedInputFileFormatInterface.      */
/*      * Bloom filter *any* input and output is BYTES.     *     * Just modes (PARTIAL1, COMPLETE).      */
//  Ref 
//  Allows mocking in testing. 
//  set all child tasks 
//  if alias to CTE contains the table name, we do not do the translation because 
//  Find all of the agg expressions. We use a LinkedHashSet to ensure determinism. 
//  spaces at the end of the line. 
//  This node will likely be activated after the task timeout expires. 
//  CONCERN : the way this mapping goes, the order *needs* to be   preserved for table.getPartitionKeys() and ptn.getValues() 
//  releasing the locks. 
//  Must reset the isNull, could be set from prev batch use 
//  aggregation columns (HIVE-10627) 
/*   these are sessionState objects that are copied over to work to allow for parallel execution.  based on the current use case the methods are selectively synchronized, which might need to be  taken care when using other methods.   */
//  Set the size of the struct 
//  delete sample jars 
//  Code Sections:     Initialize (fastSetFrom*).     Take Integer or Fractional Portion.     Binary to Decimal Conversion.     Decimal to Binary Conversion.r     Emulate SerializationUtils Deserialization used by ORC.     Emulate SerializationUtils Serialization used by ORC.     Emulate BigInteger Deserialization used by LazyBinary and others.     Emulate BigInteger Serialization used by LazyBinary and others.     Decimal to Integer Conversion.     Decimal to Non-Integer Conversion.     Decimal Comparison.     Decimal Rounding.     Decimal Scale Up/Down.     Decimal Precision / Trailing Zeroes.     Decimal Addition / Subtraction.     Decimal Multiply.     Decimal Division / Remainder.     Decimal String Formatting.     Decimal Validation.     Decimal Debugging. 
//  if specified generate alias using func name 
//  Group mapping:   group_a: user1, user2 
// SqlStdOperatorTable.SUM, 
//  Call AppMasterEventOperator with new input inspector. 
//  work.removePathToPartitionInfo(p); 
//  create map 
//  verify that there are two calls because of two instances of the authorization provider 
//  Testing using != is good enough, because we use ObjectInspectorFactory   to   create ObjectInspectors. 
//  If filter condition is NULL, transform to FALSE 
//  Removing job credential entry/ cannot be set on the tasks 
//  For PARTIAL1 and COMPLETE 
//  Return the new list 
//  Allow the user to set the ORC properties without getting an error. 
//  High word gets integer rounding. 
//  Do some changes (optional) 
//  Use the hive table name, ignoring the default database 
//  returns the bucket number to which the record belongs to 
/*  * The interface for a single byte array key hash map lookup method.  */
//  Update catalogs 
//  Test that exclusive blocks exclusive and read 
//  2. Add TOK_WINDOW as child of UDAF 
//  Get scheme from FileSystem 
//  assert cvb.cols.length == batch.getColumnIxs().length; // Must be constant per split. 
// this should use VectorizedOrcAcidRowReader 
//  Process the records in the input iterator until    - new output records are available for serving downstream operator,    - input records are exhausted or 
//  Return the new expression containing only partition columns 
/*    * Helper method to create a yarn local resource.    */
//  field present in both. validate type has not changed 
//  Get one top level TS Op directly from the stack 
//  Iterate through the children nodes of the IN clauses starting from index 1,   which corresponds to the right hand side of the IN list. 
//  if table is being modified to be external we need to make sure existing table   doesn't have enabled constraint since constraints are disallowed with such tables 
//  requested host died or unknown host requested, fallback to random selection. 
//  Old logic. 
//  initialize and evaluate 
//  required   required   required   optional   optional   optional 
//  Null-safe isSame for lists of ExprNodeDesc 
//  we choose to keep the invalid stats and only change the setting. 
//  Otherwise convert t to RawType so we will fall into the following if 
//  date/timestamp is higher precedence than String_GROUP 
//  com.esotericsoftware.kryo.io.Output getHybridBigTableSpillOutput(int partitionId); 
//  make the offset non-zero to keep things interesting. 
//  ID 8 was committed, all others open 
//  Negative number 
//  Check for empty partitions 
//  update only the basic statistics in the absence of column statistics 
//  prepare output buffer to accept results 
//  to the conf using the connection hook 
/*      * Restriction 17.s :: SubQuery cannot use the same table alias as one used in     * the Outer Query.      */
//  The lock may have been released. Ignore and continue 
//  A little strange that we forget the dummy row on read. 
//  ask default fs first 
// now start concurrent txn 
//  First, look in the classpath 
//  metastore schema only allows maximum 255 for constraint name column 
// process records until done 
//  We've prewarmed this database, continue with the next one 
//  This method will scale down and round to fit, if necessary. 
//  We will use decimal if all else fails. 
//  maps from a work to the DPPs it contains 
//  4. We create the join operator with its descriptor 
//  execute in child jvm 
/*    * - invoked during FROM AST tree processing, on encountering a PTF invocation.   * - tree form is   *   ^(TOK_PTBLFUNCTION name partitionTableFunctionSource partitioningSpec? arguments*)   * - setup a PTFInvocationSpec for this top level PTF invocation.    */
//  Add the attemptDir to the watch set, scan it and add to the list of found files 
//  Now copy missing chunks (and parts of chunks) into cache buffers. 
//  If the partitions were not added due to memory limit, return false 
//  If the input is sorted, and we are executing a search based on the arguments to this filter, 
//  joined with multiple small tables on different keys 
//  add tez counters for task execution and llap io 
//  We are revoking from an updating task. 
//  Project everything from the LHS and then those from the original 
//  test without nulls 
//  empty string delim 
//  getFunction() 
//  Reserve blocks in this arena that would empty the sections of requisite size. 
//  Looks like it doesn't exist.  Lock so that two threads don't create it at once. 
//  3. Materialized view based rewriting   We disable it for CTAS and MV creation queries (trying to avoid any problem 
//  I expect we'll only see NOT_ACQUIRED here? 
/*  * Root interface for a vector map join hash table (which could be a hash map, hash multi-set, or * hash set).  */
//  empty HSchema construct 
//  go through all small tables and get the mapping from bucket file name 
//  No room above for rounding. 
//  Get explain plan for the query. 
//  Modify Table schema at the source. 
//  the valueList will save all data for ListColumnVector temporary. 
//  Use TerminateFragmentResponseProto.newBuilder() to construct. 
//  supposed to get 500 rows if maxRows isn't set 
//  Report the row if its the first time 
//  Select one in 1st child, none as 2nd child, and none as 3rd. 
//  For a non-list (i.e. single value), the offset is for the variable length long (VLong)   holding the value length (followed by the key length). 
//  Slice before the start of the split. 
//  in case of an overflow return -1 
//  If both schema information are provided, they should be the same. 
//  narrow down the possible choices based on type affinity 
//  break loop on equal comparator 
//  s.getLength() and will never resize the buffer down. 
//  TODO: if this method is ever called on more than one jar, getting the dir and the 
//  JoinOperator assumes the key is backed by an list.   To be consistent, the value array is also converted 
//  Test with just high water mark 
//  Already registered to send updates to this node for the specific source.   Nothing to do for now, unless tracking tasks at a later point. 
//  Without the round, this conversion fails. 
/*  Partial aggregation result returned by TerminatePartial. Partial result is a struct     * containing a long field named "count".      */
// since now we have scalar subqueries we can get subquery expression in having   we don't want to include aggregate from within subquery 
//  Allow analyze the whole table and dynamic partitions 
//  This is a negative because we want the positive to be the default when nothing is specified. 
//  It should also be possible to calculate this based on ts.getTime() only. 
//  summary of aliasBucketFileNameMapping for test result 
// Base time 
//  Union 
//  Repeat the expression on the same batch,   the result must be unchanged. 
//  args as child of func? 
//  we need a constant on one side. 
//  Return if either of the arguments is null 
//  because HIVE doesn't support null type it is appropriately typed boolean 
/*    * Called to generate the taks tree from the parse context/operator tree    */
//  Verify resulting dirs. 
//  create a walker which walks the tree in a DFS manner while maintaining   the operator stack. 
/*    * hasAllFieldsSettable without any caching.    */
//  instead fall back to default behavior for determining input records. 
//  very small heap, 14 elements 
//  only tasks that cannot finish immediately are pre-emptable. In other words, if all inputs 
//  in the serde. 
/*        * this 'if' is pretty lame - QTestUtil.QTestUtil() uses hiveSiteURL to load a specific       * hive-site.xml from data/conf/<subdir> so this makes it follow the same logic - otherwise       * HiveConf and MetastoreConf may load different hive-site.xml  ( For example,       * HiveConf uses data/conf/spark/hive-site.xml and MetastoreConf data/conf/hive-site.xml)        */
//  make current task depends on this new generated localMapJoinTask 
//  case the statement is a CREATE MATERIALIZED VIEW AS 
//  interim row count can not be less due to containment   assumption in join cardinality computation   interimNumRows represent number of matches for join keys on two sides.   newNumRows-interimNumRows represent number of non-matches. 
//  for ex: count(*) 
//  result principal 
//  in case all files in locations do not exist 
//  may be the table is getting created in this load 
//  0 seconds, for first retry assuming fs object was closed and open will fix it. 
//  As we read, we can unlock initial refcounts for the buffers that end before 
//  test for VARCHAR type 
/*          * Single value.          */
/*    * Serializes decimal64 up to the maximum 64-bit precision (18 decimal digits).   *   * NOTE: Major assumption: the fast decimal has already been bounds checked and a least   * has a precision <= DECIMAL64_DECIMAL_DIGITS.  We do not bounds check here for better   * performance.    */
//  for persistent function 
//  Last chance, look in the old Hive config value.  Still avoiding defaults. 
//  Rename the event directories such a way that the length varies.   We will encounter create_table, truncate followed by insert.   For the insert, set the event ID longer such that old comparator picks insert before truncate   Eg: Event IDs CREATE_TABLE - 5, TRUNCATE - 9, INSERT - 12 changed to   CREATE_TABLE - 5, TRUNCATE - 9, INSERT - 100   But if TRUNCATE have ID-10, then having INSERT-100 won't be sufficient to test the scenario.   So, we set any event comes after CREATE_TABLE starts with 20.   Eg: Event IDs CREATE_TABLE - 5, TRUNCATE - 10, INSERT - 12 changed to 
//  Set the destination for the SELECT query inside the CTAS 
/*    * We cannot use a base file if its range contains an open write id.   * @param writeId from base_xxxx    */
//  No includes - use the standard batch. 
//  To get vertex status we can use DAGClient.getVertexStatus(), but it will be expensive to   get status from AM for every refresh of the UI. Lets infer the state from task counts. 
//  Select DISTINCT + windowing; GBy handled by genSelectForWindowing 
//  Secondly, we extract information about the part of the tree that can be merged   as well as some structural information (memory consumption) that needs to be 
// this will also handle copy_N files if any 
//  we just negate it to get the size. 
// both commit & rollback clear ALL locks for this tx 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.newBuilder() 
//  Only for char/varchar return types 
//  Open txn 
//  PRINCIPAL_TYPE 
//  handle the close up 
//  else skip this one 
//  Invalidate 
//  3. Perform a delete. 
//  Retrieve log from task tracker 
//  Wait for the events to be processed. 
//  if not the first, put a blank separator in 
//  Get the result in leftInspectableObject 
//  disable auto parallelism for bucket map joins 
//  2. Trigger transformation 
//  First request for host. 
//  Round using the "half-even" method used in Hive. 
//  unsupported if null 
//  Should generate [f,+inf) 
//  We only need to update the work with the hashtable   sink operator with the same mapjoin desc. We can tell   that by comparing the bucket file name mapping map   instance. They should be exactly the same one due to   the way how the bucket mapjoin context is constructed. 
//  No tasks should have been started yet. Checked by initial state 
//  Test a set of random adds at high precision. 
// writing both acid and non-acid resources in the same txn  txnid:1 
// request came from old version of the client  this matches old behavior 
//  If a partition value is not there, then it is dynamic partition key. 
/*    * Utility to visit all nodes in an AST tree.    */
//  getFunctions(String catalog, String schemaPattern, String functionNamePattern)   getSchemas()   getTables(String catalog, String schemaPattern, String tableNamePattern, String[] types)   getTableTypes()   getTypeInfo() 
//  Conf for llap 
/*    * Get detailed read position information to help diagnose exceptions.    */
//  We query for minimum values in all the queries and they can only increase by any concurrent 
//  dummy impl 
/* may not be Idempotent but is safe to retry */
//  Trailing spaces are not significant 
//  Results cache directory should be cleaned up at process termination. 
//  sorting the list in the descending order so that deletes happen back-to-front 
//  create a new columnstatistics desc to represent partition level column stats 
//  Validate there the new insertions for column c. 
//  We were combining SS-es and the time has expired. 
//  Compute value and hashcode - we'd either store or forward them. 
//  add udtf aliases to QB 
//  find out all equivalent works in the Set. 
//  PARENT_SCHEMA_NAME 
//  Handle case with nulls. Don't do function if the value is null, to save time,   because calling the function can be expensive. 
//  3. Translate projection indexes to join schema, by adding offset. 
//  Evaluate the result given a partition 
//  Denominator is zero, convert the batch to nulls 
//  Disabled in HIVE-19509 
//  Add back to the queue for the next heartbeat, and schedule the actual heartbeat 
//  Check if materialization defined its own invalidation time window 
// this ensures that "SHOW LOCKS" prints the locks in the same order as they are examined 
//  Test basic right trim of bytes slice. 
//  reach beginning of the row group. This is required for IS_PRESENT stream. 
//  Also check hashCode() 
//  By default allow only ADDPROPS and DROPPROPS.   alterOpType is null in case of stats update. 
//  already initialized 
//  Semijoin created using hint or marked useful, skip it 
//  Doing String comps here as value objects in Hive in Pig are different so equals()   doesn't work. 
//  getWindowFunctionInfo() cannot be called during map/reduce tasks. So cache necessary   values during query compilation, and rely on plan serialization to bring this info   to the object during the map/reduce tasks. 
//  read the stopping point for the first flush and make sure we only see 
//  Add it to the list of work to decompress. 
//  the next item will be a new root. 
//  optional string class_name = 1; 
// this contains base_xxx or delta_xxx_yyy 
//  Use the original fsOp path here in case of MM - while the new FSOP merges files inside the   MM directory, the original MoveTask still commits based on the parent. Note that this path   can only be triggered for a merge that's part of insert for now; MM tables do not support 
//  non-partitioned 
//  ENVIRONMENT_CONTEXT 
//  Initialize acidOperationalProperties based on table properties, and   if they are not available, see if we can find it in the job configuration.   We have to look at these two places instead of just the conf, because Streaming Ingest   uses table properties, while normal Hive SQL inserts/updates/deletes will place this 
//  Complete fractional digits shear off.  Zero result. 
//  NDV of the join can not exceed the cardinality of cross join. 
//  this test is done 
//  Select none in 1st child, one as 2nd child, and none as 3rd. 
/*          * Multi-Key specific variables.          */
//  change the parent of the original SMBjoin operator to point to the map 
//  We need to check if the other input branches for union is following the first branch   We may need to cast the data types for specific columns. 
//  Specifying username/password/driver explicitly will override the values from the url;   make sure we don't override the values present in the url with empty values. 
//  3. Add Part Spec & Range Spec as child of TOK_WINDOW 
//  This test checks that if we have a minor compacted delta for the txn range [40,60]   then it will make any delete delta in that range as obsolete. 
//  Note: rarely called (unless buffers are very large or we evict a lot, or in LFU case). 
//  Check that in the path between cRS and pRS, there are only Select operators 
//  Generate special repeated case. 
//  Does this make sense? 
//  But the registry was fully initialized, thus we need to add it 
//  For TypeInfoFactory use only 
//  Get the key column names, and check if the keys are all constants 
//  only explain uses it 
//  DP in the form of T partition (ds, hr) 
//  get columns for SEL(*) from LVJ 
//  test date string 
//  INSERT OVERWRITE command 
//  Non-partition expressions are converted to nulls. 
//  Verify if Rename after bootstrap is successful 
//  open record reader to read next split 
/*  side files are only created by streaming ingest.  If this is a compaction, we may          * have an insert delta/ here with side files there because the original writer died. */
//  If the user asked for a formatted output, dump the json output   in the output stream 
// adds delta and delete_delta 
//  Test 0 is nul character 
//  We do the cross product of the N big table equal key row's values against the   small table matching key which has M value rows into overflow batch. 
//  The encoding name/codes don't contain pound signs 
// for cases where different Rel nodes are referring to   same correlation var (e.g. in case of NOT IN)   avoid generating another correlation var   and record the 'rel' is using the same correlation 
//  openTransactionCalls > 1 means this is an interior transaction   We should already have a transaction created that is active. 
//  if the specified path is directory, iterate through all files 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getURL(int)    */
//  although technically its unbounded, its unlikely we will ever see ndv > 20 
//  Need to necessarily override this method since default impl assumes HDFS   based location string. 
//  ObjectStore methods to be overridden with injected behavior 
//  TableScan will also be followed by a Select Operator. Find the expressions for the 
//  resolve futures used for testing 
//  Call the regular method since it does error checking. 
//  The column number and type information for this one column string reduce key. 
//  is the registry dynamic (i.e refreshes?) 
//  The number of times eh has returned non-null errors 
//  only process partition which is skewed and list bucketed 
// builds partition spec so we can build suitable WHERE clause 
//  iterator the reducer operator tree 
//  Initialize to satisfy compiler finals. 
//  As setNumDistributionKeys is a subset of keycols, the size should   be 0 too. This condition maybe too strict. We may extend it in the   future. 
//  All inserts are committed and hence would expect in TXN_TO_WRITE_ID, 3 entries for acidTbl   and 2 entries for acidTblPart as each insert would have allocated a writeid. 
//  temporary functions don't have any database 'namespace' associated with it, 
//  check equivalent versions, should be compatible 
//  The startTime may not be set if the sparkTask finished too fast,   because SparkJobMonitor will sleep for 1 second then check the state,   right after sleep, the spark job may be already completed.   In this case, set startTime the same as submitTime. 
//  /////////////////////////////   Exception handling routines   ///////////////////////////// 
//  v[0] 
//  More required. 
// Comments are separated by "\0" in columnCommentProperty, see method getSchema  in MetaStoreUtils where this string columns.comments is generated 
//  -----------------------------------------------------------------------------------------------     ----------------------------------------------------------------------------------------------- 
//  Initially, no children or inputs; set later with setInput* methods. 
//  Now try to evict with locked buffer still in the list. 
//  Identical strings should be equal 
//  The SIMD optimized form of "a <= b" is "((b - a) >>> 63) ^ 1" 
//  Find databases which name contains _to_find_ 
// Unicode case. 
//  To find if a given (owid, rowId) pair is deleted or not, we perform   two binary searches at most. The first binary search is on the   compressed owids. If a match is found, only then we do the next   binary search in the larger rowId vector between the given toIndex & fromIndex. 
//  Stolen from Hive's MetricsTestUtils.  Probably should break it out into it's own class. 
//  doesn't have a notion of small, and saves the full value as an int, so no overflow   expected:<null> but was:<32768> 
//  Construct using org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto.newBuilder() 
//  Run initialization statements on the connection 
//  Avro should always use the table properties for initialization (see HIVE-6835). 
//  there is more than one FK 
//  MY_STRUCTSET 
//  8 bytes per long in the refs, assume data will be empty. This is just a sanity check. 
//  The bucket value should be same for all the records. 
//  NONE pattern 
//  We want to have only one auth bridge.  In the past this was handled by ShimLoader, but since   we're no longer using that we'll do it here. 
//  Check that the data is removed 
//  partition 
//  Write any remaining bytes to the out stream. 
//  test getMaterializedViewsForRewriting 
//  If isExternalQuery -> the call is from within hte daemon, so no permission check required 
//  The output of a partial aggregation is a struct containing   a long count, two double averages, two double variances,   and a double covariance. 
//  We will cache if we have the entire part. 
//  Parse until union separator (currentLevel). 
//  The query should be completed by now 
//  No input parameters. 
//  This is our way of documenting that we are MUTATING the contents of   this writable's internal timestamp. 
/*    * (non-Javadoc)   *   * @see org.apache.hadoop.hive.ql.exec.Operator#processOp(java.lang.Object,   * int) this processor has a push-pull model. First call to this method is a   * push but the rest is pulled until we run out of records.    */
//  Backward-compatibility interfaces for functions without a user-visible name. 
// when last txn finished (abort/commit) the currentTxnIndex is pointing at that txn  so we need to start from next one, if any.  Also if batch was created but  fetchTransactionBatch() was never called, we want to start with first txn 
//  Statistics 
//  sum(c) 
//  No need to check guaranteed here; if it was false we would already be in the queue. 
//  SHARED_SDPARTITION_SPEC 
//  For ORC and Parquet, all the following statements are the same   ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS   ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan; 
//  Create limit desc with limit value 
//  become available. 
//  Call the real preCreateTable method 
//  the general column statistics 
//  test that schema was loaded correctly 
// assertEquals(expected, row); 
//  This should make the search linear, sync to the beginning of the block being searched   [50, 100], set the comparison to be null, and the flag to reset the range should be unset 
//  All columns of the expression must be partitioned columns 
//  Iterate over all expression (either after SELECT, or in SELECT TRANSFORM) 
//  If the values are equal, the queue limit is fixed. 
//  Only store the latest error, if there are multiple. 
// check if admin option has been specified 
//  Stack methods 
//  Test that existing shared_write db with new shared_write coalesces to 
//  verify that udf in whitelist can be executed 
//  error with driver 
//  In case of lateral views followed by a join, the same tree   can be traversed more than one 
//  Operator allPath = op; 
//  store post-exec hooks calls so we can look at them later 
//  Only meant for use by the QueryTracker 
//  We have to add this bit of exception handling here, because Function.apply does not allow us to throw   the actual exception that might be a checked exception, so we wind up needing to throw a RuntimeException   with the previously thrown exception as its cause. However, since RuntimeException.getCause() returns   a throwable instead of an Exception, we have to account for the possibility that the underlying code   might have thrown a Throwable that we wrapped instead, in which case, continuing to throw the   RuntimeException is the best thing we can do. 
// List<String> rs = runStatementOnDriver("select a,b from " + Table.ACIDTBL + " order by a,b");  Assert.assertEquals("Data didn't match in autocommit=true (rs)", stringifyValues(rows1), rs); 
//  Add the rest of the metadata keys. 
//  [path2, shared] 
//  For getDetailedReadPositionString. 
//  Originally the mvTask and the child move task of the mrAndMvTask contain the same   MoveWork object.   If the blobstore optimizations are on and the input/output paths are merged   in the move only MoveWork, the mvTask and the child move task of the mrAndMvTask   will contain different MoveWork objects, which causes problems.   Not just in this case, but also in general the child move task of the mrAndMvTask should 
//  2. analyze create table command 
// now copy over the data when isNull[index] is false 
/*    * Tests if LOCATION_3 is returned when the first file is found is later in lookup order    */
//  convert the rest and put into the last entry 
//  retrieved from object cache 
//  add virtual columns for ANALYZE TABLE 
//  The "not vectorized" information has been stored in the MapWork vertex. 
/*      * Count null input which is for COUNT(*) and output is LONG.     *     * Just modes (PARTIAL1, COMPLETE).      */
//  Then, if the buffer was in the list, remove it. 
//  cast on single column 
//  Remove semijoin optimization if it creates a cycle with mapside joins 
//  we need to first join and flush out data left by the previous file. 
//  local mode 
//  REPL LOAD is not partition level. It is always DB or table level. So, passing null for partition specs.   Also, REPL LOAD doesn't support external table and hence no location set as well. 
//  The token file location and mapreduce job tag should be right after the tool argument 
//  Fail - trying to set "transactional" to "false" is not allowed 
//  Global used when setting errors, etc. 
//  long BETWEEN 
//  Not closing this at the moment at shutdown, since this could be a shared instance. 
/*    * findRoots returns all root operators (in ops) that result in operator op    */
//  Not really much we can do here. 
//  Always use localhost for hostname as some tests like SSL CN validation ones   are tied to localhost being present in the certificate name 
/*  256 files x 1000 size for 11 splits  */
//  Schedule outside of the scheduleLock - which should only be used to wait on the condition. 
//  Since we store references to HiveDecimalWritable instances, we must use the update method instead   of plain assignment. 
//  todo: invalid + valid = invalid 
//  If there is an end node that not the limit0/wherefalse.. 
//  We need to fetch the table before it is dropped so that it can be passed to   post-execution hook 
//  This can happen if all values in stream are nulls or last row group values are all null. 
//  float/double. String types have no default value for null. 
/*    * Dependency is a class used for explain    */
//  Note that this uses short user name without consideration for Kerberos realm.   This seems to be the common approach (e.g. for HDFS permissions), but it may be   better to consider the realm (although not the host, so not the full name). 
//  First we need to check if it is valid to convert to MERGE/INSERT INTO.   If we succeed, we modify the plan and afterwards the AST.   MV should be an acid table. 
//  hcat.py will become the first argument pass to command "python" 
//  modify it in the middle for view rewrite. 
//  Check if the status of all the columns of all the partitions exists 
//  De-serialization code 
//  +20 from the duplicate publish 
// x=y 
//  It cannot contain a non-deterministic function 
//  Inherit Java system variables 
//  Note: we could just do what we already do above from disk data, except for the validation   that is not strictly necessary, and knownTornStart which is an optimization. 
//  [null, null] not allowed, so this check is ok. 
//  Drop all the tables 
//  Do not set an environment context. 
//  we have to set ndv 
//  column exprmap. 
//  GRANT_TIME 
/*  * An single LONG key hash set optimized for vector map join.  */
//  Literal decimal 
//  For current schema evolution. 
// method in HiveMetaStoreClient 
//  Compare timestamp to integer seconds or double seconds with fractional nanoseonds. 
//  ignore, it will be generated by SEL op 
//  v[1], v[0] 
//  Generate sortCols and order 
//  Create a dummy task if no move is needed. 
//  This correlator was generated by a previous invocation of   this rule. No further work to do. 
// volatile because heartbeat() may be in a "different" thread 
//  Lock entire heap; heap is still full; we should not be able to evict or insert. 
//  of the nullable side of the OJ. 
//  Check if the database location is in the default location based on the old warehouse root.   If so then change the database location to the default based on the current warehouse root. 
//  Time zone file was written in, from metadata 
//  changes the value of a variable, the corresponding change will be made in this mapping. 
// Constructor used by HiveRexExecutorImpl 
//        This only lives for the duration of the service init. 
//  There could be races here, e.g. heartbeat delivered us the old value just after we have   received a successful confirmation from the API, so we are about to overwrite the latter.   We could solve this by adding a version or smth like that; or by ignoring discrepancies   unless we have previously received an update error for this task; however, the only effect 
//  Don't clear the attempt ID, or the stuff will be cleared. 
/*          * Single-Column Long specific variables.          */
//  projections from child? 
//  first group 
//  get [local time at toZone] 
//  If retrieveCD is false, we do not need to do a deep retrieval of the Table Column Descriptor. 
//  execute cli driver work 
//  Construct a column statistics object from the result 
//  If there are more than 1 children at any level, don't do anything 
//  Now do a general lookup 
//  nothing to do when the optimization is off 
//  LAST_ACCESS_TIME 
//  The underlying SSLSocket object is bound to host:port with the given SO_TIMEOUT and   SSLContext created with the given params 
// Given jar to add is stored as key  and all its transitive dependencies as value. Used for deleting transitive dependencies. 
//  Test from server to client too. 
// This can be set for old behavior of nulls printed as empty strings 
//  SESSION_HANDLE 
// make non-blocking 
/*    * - A Window Frame that has only the start boundary, then it is interpreted as:   *     BETWEEN <start boundary> AND CURRENT ROW   * - A Window Specification with an Order Specification and no Window Frame is   *   interpreted as: RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW   * - A Window Specification with no Order and no Window Frame is interpreted as:   *     ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING    */
//  separate split 
//  add -s and --scratchdir to specify a non-default scratch dir 
//  We handle TableScanOperator here as we can safely ignore table alias   and the current comparator implementation does not. 
/*    * Check if the input line is a multi-line command which needs to read further    */
//  2) We extract the collation for this operator and the collations 
//  set output format parameters (these are not supported by QL but only 
//  See if a custom CompositeKey class was provided 
//  Doing characters comparison directly instead of regular expression   matching for simple patterns like "%abc%". 
//  anything but a " in "" 
//  Swap column vectors, but keep selected vector unchanged 
//  These members are used as out-of-band params   for the inner-loop supper.processOp callbacks 
//  - There cannot exist any (distinct) aggregate. 
//  Don't actually create the key 
//  Can the join operator be converted to a sort-merge join operator ? 
//  can retrieve it later. 
//  Ignore files eliminated by PPD, or of 0 length. 
//  Above should have thrown NoSuchObjectException if there is no such catalog 
//  idempotent case for destdb db 
//  There may be a race for this slot so re-query after a delay with some probability. 
//  serialize the row as BytesWritable 
//  Don't propagate errors from close() since this will lose the original error above. 
//  There are not enough failed compactions yet so checkFailedCompactions() should return false. 
//  so figure out if we can lock it too. 
//  optional bytes user_payload = 1; 
//  calculate key once; lookup once. 
//  Invoke the method 
//  JDBC 1 driver error 
//  Check whether this is a list or a map 
//  Augment conf with the settings from the started llap configuration. 
//  map of original column id -> index among selected columns 
//  1.2 Now generate RS operator 
//  Update out_rwsch 
//  make new generated task depends on all the parent tasks of current task. 
// convert the table to Acid  //todo: remove trans_prop after HIVE-17089 
// random md5 
/*  Patterns that are included in performance logging level.     * In performance mode, show execution and performance logger messages.      */
/*      * Now determine the small table results.      */
//  loop over all the operators recursively 
//  TODO: use fileId right from the list after HDFS-7878; or get dfs client and do it 
/*    * Generate the ReduceSinkOperator for the Group By Query Block   * (qb.getPartInfo().getXXX(dest)). The new ReduceSinkOperator will be a child   * of inputOperatorInfo.   *   * It will put all Group By keys and the distinct field (if any) in the   * map-reduce sort key, and all other fields in the map-reduce value.   *   * @param numPartitionFields   *          the number of fields for map-reduce partitioning. This is usually   *          the number of fields in the Group By keys.   * @return the new ReduceSinkOperator.   * @throws SemanticException    */
//  data should not be visible 
//  Drop two files so they are moved to CM 
//  current map join is null means it has been handled by CurrentMapJoin   process. 
//  It is a cartesian product, row count is easy to infer 
//  max of numBitVectors = 1024, 2 bytes is enough. 
//  This will be null at master. 
// since LM is using non strict mode we get shared lock 
//  6. Set synthetic flag, so that we would push filter below this one 
//  check if argument is a string or an array of strings 
// remove trailing comma 
//  Workaround for HIVE_DEFAULT_PARTITION - ignore it like JDO does, for now. 
//  If sizes of at least n-1 tables in a n-way join is known, and their sum is smaller than 
//  Get the return ObjectInspector. 
// since this is on conversion from non-acid to acid, NEXT_WRITE_ID should not have an entry  for this table.  It also has a unique index in case 'should not' is violated 
//  Recheck to make sure someone didn't create it while we waited. 
//  assert that the table created has no hcat instrumentation, and that we're still able to read it. 
//  Check if table is transactional 
//  We have to manually re-set it in the JobConf to make sure it gets picked up. 
//  Property defined in hive-site.xml only 
//  for now this should be true... 
// currently multi-insrt doesn't allow same table/partition in > 1 output branch 
//  TYPE_CLASS_NAME 
//  Timestamp is represented as long internally no need to any thing here 
/*  RandomAccessFileManager.DEFAULT_BUFFER_SIZE  */
//  We are now "sending" a message... update again, "return" both callbacks. 
//  Mark this task as a final map reduce task (ignoring the optional merge task) 
//  intermediate outputs of joins/groupbys 
//  Used by all flavors. 
//  construct column name list and types for reference by filter push down 
//  For partition-less table, initialize partValue to empty string.   We can have partition-less table even if we have partition keys   when there is only only partition selected and the partition key is not   part of the projection/include list. 
//  Magic value usage is invalid with nanoTime, so once in a 1000 years we may log extra. 
//  close writer 
//  call-6: file stat - split 2 => mock:/mocktable6/0_0 
//  if exception happens after doCopyOnce, then need to call getFilesToRetry with copy error as false in retry. 
//  these many values to reach beginning of the row group 
//  write the blob 
//  simple tree with single parent 
//  this session should never be a default session unless something has messed up. 
//  When done, handleUpdate.. may break the iterator, so the order of these checks is important. 
//  Load the hash table 
//  Get rid of spills before we start modifying the batch. 
//  violating which can cause data loss 
//  2 elements: "key", "value" 
/*    * Compare the two map objects for equality.    */
//  if database itself is null then we can not filter out anything. 
//  parameter value is still false in 1st connection.  The alter still goes through. 
//  Can be null since the task may have completed meanwhile. 
//  The SIMD optimized form of "a > b" is "(b - a) >>> 63" 
//  Test setter for configuration object. 
//  Unless at least one of '/' or '@' was not found, in 
//  Remove DPP based on expected size of the output data 
//  extract stage plans 
//  Verify that session wasn't closed on transport close. 
//  Move data from temp directory the actual table directory   No metastore operation required. 
//  Constants for 128 bit variant 
//  Need to check the original schema to see if this is actually a Fixed. 
//  Get all the driver run hooks and pre-execute them. 
//  tab1, tab2 
//  check aggOutputProj projects only one expression 
// run("CREATE MATERIALIZED VIEW " + dbName + ".mat_view2 AS SELECT * FROM " + dbName + ".unptned", driver);  verifySetup("SELECT * from " + dbName + ".mat_view2", unptn_data, driver); 
//  This should never happen, at least for now. Throw? 
//          FilterInputRel 
/*    * a subclass must indicate whether it will transform the raw input before it is fed through the   * partitioning mechanics.    */
//  Tracks instances known by both YARN Service and llap. 
//  let's don't fail on future timeout since we have a timeout for pre-warm 
//  the join 
/*  10 files x 100 size for 111 splits  */
//  DummyMataStoreInitListener's onInit will be called at HMSHandler   initialization, and set this to true 
//  This method will return only after the cache has updated once 
//  Test basic truncate of bytes slice. 
//  task (typically, a task gets re-run up to 4 times if it fails. 
// http://hadoop.apache.org/docs/r1.1.1/api/org/apache/hadoop/security/authentication/server/AuthenticationFilter.html 
//  not null constraint name   default constraint name 
//  we need extrapolation 
//  also match for this to be converted to a map-only job. 
//  Test with open transactions 
//  Event 12, 13, 14 
//  For all other kinds of operators, assume the output is as big as the 
//  set min NDV value to both columns involved in join 
//  indicating that the previous cookie has expired. 
//  HIVE-19096 - disable for explain analyze 
//  We have a class-level annotation that says whether the UDF's vectorization expressions 
//  Read database, table, partition via CachedStore 
//  No support for DECIMAL_64 input.  We must convert. 
//  we use the basic or the extended version of the optimizer. 
/*    * Setup our 2nd batch with the same "column schema" as the big table batch that can be used to   * build join output results in.    */
//  make sure we initialize if necessary 
//  In 2 cases out of 3, we could pass the path and type directly to metastore... 
//  create a new conf file, using contents from current one 
//  A mapping from a tableName to a table object in metastore. 
// change the table name back 
/*    * Job callable task for job list operation. Overrides behavior of execute() to list jobs.   * No need to override behavior of cleanup() as there is nothing to be done if list jobs   * operation is timed out or interrupted.    */
//  write the orc file to the mock file system 
//  reset keyValueSeparatorPosition 
//  Unlock the previous lock 
// test WritableBinaryObjectInspector 
//  Create warehouse with 777, so that user impersonation has no issues. 
//  @@protoc_insertion_point(builder_scope:UserPayloadProto) 
//  touch the next file 
//  Step 2 : create the Insert query 
//  Now calculate which rows were filtered out (they are logically no matches). 
//  PATTERN 
//  Write key to buffer to compute hashcode and compare; if it's a new key, it will 
//  @@protoc_insertion_point(class_scope:GetTokenRequestProto) 
//  If this function is called, the parent should only include constant 
//  Method to get the Valid write ids list for the given table 
//  Worker threads stuff 
//  Create a test table 
//  to no_auto_compact, we need to check it in both cases. 
//  We will touch all blocks in random order. 
//  updates key with sequence number 
//  This thread should throw an exception 
/*  2*3 =  */
//  Successfully insert some data into ACID tables, so that we have records in COMPLETED_TXN_COMPONENTS 
// delta_xxxx_yyyy format 
//  Real column name - on which the operation is being performed 
//  IGNORE 
//  partition spec is not specified but column schema can have partitions specified 
//  expression for the table. 
// Ignored the mbean itself was not found, which should never happen because we  just accessed it (perhaps something unregistered in-between) but if this  happens just don't output the attribute. 
//  For local src file, copy to hdfs 
//  GroupBy1 into the reduce keys. 
//  EncodedColumnBatch is already decompressed, we don't really need to pass codec.   But we need to know if the original data is compressed or not. This is used to skip   positions in row index properly. If the file is originally compressed,   then 1st position (compressed offset) in row index should be skipped to get   uncompressed offset, else 1st position should not be skipped. 
//  add all except the right side to the bad positions 
//  The else clause 
//  Perform compaction. Join result after compaction should still be the same 
/*        * Multi-Key Long check for repeating.        */
//  Write the escaped byte. 
//  6.4 Build ExprNode corresponding to colums 
//  3. If the new conjuncts are already present in the plan, we bail out 
//  The value doesn't matter 
//  Regardless whether it was removed successfully or after failing to remove, restart it.   Since we just restart this from under the user, mark it so we handle it properly when 
//  Mark one of the transactions as an exception to test that invalid transactions   are being handled properly.   Exclude transaction 5 
//  Commit the txn under HWM. 
//  Spot check decimal column modulo decimal column 
//  special char 
//  We cannot obtain a better estimate without CustomPartitionVertex providing it   to us somehow; in which case using statistics would be completely unnecessary. 
//  The target data is in TextInputFormat. 
//  Definitely not a byte. 
//  Fits in two longwords. 
//  Hint to disable runtime filtering. 
//   we need to evaluate result for every pruned partition 
//  Not a transactional op, nothing more to do 
//  Go through the argClasses and for any string, void or date time, start   looking for doubles 
//  out   of   range   due   to   time!=0 
//  number of bits to store the number of zero runs 
//  Get the latest timestamp of all the cells as the row timestamp   from hbase-0.96.0 
//  Whether this operator is an outer join. 
// to see it working in UTs 
//  All DML should fail with DummyTxnManager on ACID table 
//  The table alias should exist 
//  Second INSERT round with new inserts into previously existing partition 'yesterday'. 
//  Override external stuff. These could also be injected as extra classes. 
//  set a small time unit as cookie max age so that the server sends a 401 
// no need to pass virtual columns to reader. 
//  This is intentionally duplicated because of HIVE-3179 
//  in the future could allow users to specify a quote character that doesn't   need escaping but for now ... 
//  if we did not see a skew key in this table, continue to next   table 
//  Assumption: top portion of tree could only be   (limit)?(OB)?(Project).... 
//  for now, expose non-primitive as a string 
//  reallocate only if any filters pruned 
//  1. get all the stats for colNames in partNames; 
//  3rd close: 
//  EVENT_ID 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#rollback(java.sql.Savepoint)    */
//  Read enough data for just the first message to be decoded. 
//  MAX_DECIMAL 9's WITH NO ROUND (longer than 38 digits) 
//  Validate the first parameter, which is the expression to compute over. This should be a 
//  verify that non whitelist params can't be set 
//  Hadoop 1 doesn't support credential merging, so this will fail. 
//  Initialize the rowId array when we have some delete events. 
//  TODO RS+JOIN 
//  Find the index of the least significant bit that is 1 
//  Testing with nulls 
//  SR.SW: Lock we are examining is shared write 
//  Use the remapped arguments for the (non)distinct aggregate calls 
//  Caching instances only in case of the YARN registry. Each host based list will get it's own copy. 
//  overflow batchs... 
//  Write json to the temp file 
//  left border is the min 
/*      * If we have more than one group key batch, we will buffer their contents.     * We don't buffer the key columns since they are a constant for the group key.     *     * We buffer the non-key input columns.  And, we buffer any streaming columns that will already     * have their output values.      */
//  There are some txns in the list which does not have write id allocated and hence go ahead and do it.   Get the next write id for the given table and update it with new next write id. 
//  get the databases for the desired pattern - populate the output stream 
//  FLAG 
//  we're dealing with an array of strings 
/*    * - a partitionTableFunctionSource can be a tableReference, a SubQuery or another   *   PTF invocation.   * - For a TABLEREF: set the source to the alias returned by processTable   * - For a SubQuery: set the source to the alias returned by processSubQuery   * - For a PTF invocation: recursively call processPTFChain.    */
//  Implement if needed. 
//  walk through existing map to truncate path so that test won't mask it   then we can verify location is right 
//  Should we propagate the error message properly? 
/*    * Where in the inverse multiplication result to find the quotient integer decimal portion.   *   * Please see comments for doDecimalToBinaryDivisionRemainder.    */
//  revert back to local fs 
//  Resolve for the method based on argument types 
//  "CREATE DATABASE" is specifically not replicated across, per design, since if a user   drops a database and recreates another with the same one, we want to distinguish   between the two. We will replicate the drop across, but after that, the goal is   that if a new db is created, a new replication definition should be created in   the replication implementer above this. Thus, we extend NoopReplicationTask and   the only additional thing we do is validate event type. 
//  Order of KEY columns   1) Partition columns   2) Bucket number column 
/*         * Job request got interrupted. Job kill should have started. Return to client with        * with QueueException.         */
//  verify that udf in default whitelist can be executed 
//  Simply create 
//  restore state of repeating and non nulls indicators 
//  For now - decrement the count to avoid accounting errors. 
//  So, the starting position of grouping set need to be known 
//  @@protoc_insertion_point(class_scope:LlapPluginProtocol) 
//  Additional conf settings specified on the command line 
//  we need to translate the ExprNodeFieldDesc too, e.g., identifiers in   struct<>. 
//  Let the VectorAssignRow class do the conversion. 
//  The call succeeded, so presumably the API is there. 
//  @@protoc_insertion_point(class_scope:UpdateFragmentRequestProto) 
//  Insert some data -> this will generate only insert deltas and no delete deltas: delta_3_3 
//  Because we need to revert the tag of a row to its old tag and   we cannot pass new tag to this method which is used to get   the old tag from the mapping of newTagToOldTag, we bypass   this method in MuxOperator and directly call process on children   in process() method.. 
//  Should not be getting invoked, configureInputJobProperties or configureOutputJobProperties   should be invoked instead. 
//  we want to have project after join since sq_count_check's count() expression wouldn't   be needed further up 
//  Called by LazyMap 
/*  Print the per Vertex summary  */
/*   * (non-Javadoc)  *  * @see  * org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider  * #authorize(org.apache.hadoop.hive.ql.metadata.Table,  * org.apache.hadoop.hive.ql.security.authorization.Privilege[],  * org.apache.hadoop.hive.ql.security.authorization.Privilege[])   */
//  has nothing to be cached 
//  Implementation of AbstractRowContainer and assorted methods 
//  check if left is valid 
//  stateful implies non-deterministic, regardless of whatever   the deterministic annotation declares 
//  Convert the elements 
//  delete db and all tables in it 
//  Thread cancelling the query 
//  beginning with distcp.options. should be honoured 
//  Create the object inspector for the input columns and initialize the 
//  remember the JobConf cloned for each MapWork, so we won't clone for it again 
//  Values of accumulators can only be read on the SparkContext side. This field is used when   creating a snapshot to be sent to the RSC client. 
//  Don't create context for the 0-s column. 
//  table1 and view1 as second read entity 
//  Serialize as time 0 
//  Set on the server side.   @see org.apache.hive.service.cli.operation.SQLOperation#prepare 
//  Initially zero. 
//  For every field 
//  For constructing HCatPartitions afresh, as an argument to HCatClient.addPartitions(). 
//  but whether the table itself is partitioned is not know. 
//  try again with a null value 
//  Implicit -- use batchIndex. 
//  check partitioning column order and types 
//  log exception, but ignore inability to start 
//  Requires schema change 
/*      * Small table information.      */
//  binary mode   For embedded mode, the JDBC uri is of the form:   jdbc:hive2:///dbName;sess_var_list?hive_conf_list#hive_var_list   and does not contain host:port string.   As a result port is parsed to '-1' per the Java URI conventions 
//  Carefully handle NULLs... 
//  Not null constraint should reference a single column 
//  we're dealing with an array of arrays of strings 
/*  check the forward and backward compatibility  */
//  op is not a DemuxOperator, so it should have   a single child. 
//  0 is function name 
//  Irrelevant. See comment above.  Irrelevant. See comment above. 
//  For primitive type, add directly. 
//  create new mapping 
//  Try to make something reasonable to pass up to the base class 
//  Prepare updated partition columns for small table(s).   Get the positions of bucketed columns 
//       LOG.info("Returning final parent : "+ptnRootLocation); 
/*                * Single-Column String specific save key.                */
//  Test dryrun of schema initialization 
//  Set them back 
//  don't save maxwidth: it is automatically set based on   the terminal configuration 
//  will not be deleted. The user will run ARCHIVE again to clear this up 
//  Find out the segment with latest version and maximum partition number 
//  set num of threads to 0 so that single-threaded checkMetastore is called 
//  bucket 0   bucket 1   bucket 2 
//  We always call init because the hook name in the configuration could   have changed. 
//  TRUNCATE_TABLE EVENT on unpartitioned table 
//  For the filtered out rows that didn't (logically) get looked up in the hash table,   we need to generate no match results for those too... 
//  If we have not added to this column desc before, we bail out 
//  HIVE_SUPPORT_SPECICAL_CHARACTERS_IN_TABLE_NAMES in HiveConf as well. 
//  the "work" needs to know about the dummy operators. They have to be separately initialized 
//  find all aggregate calls without distinct 
/*    * this is not a real bloom filter, but is a cheap version of the 1-memory   * access bloom filters   *   * In several cases, we'll have map-join spills because the value columns are   * a few hundred columns of Text each, while there are very few keys in total   * (a few thousand).   *   * This is a cheap exit option to prevent spilling the big-table in such a   * scenario.    */
//  We need to add select since order by schema may have more columns than result schema. 
//  getName 
//  TODO: Remove in Hive 0.16. 
//  Make sure this isn't one of the partitioning columns, that's not supported. 
//  Output: get the evaluate method 
//  A second connection should not be able to see the table 
//  result 
/*  Get all locks for a particular object  */
//  no cleanup thread 
//  The client has to wait and retry. 
//  shared to all session functions 
//  after each test 
//  Accurate long value cannot be obtained. 
//  If this is in acid format always read it recursively regardless of what the jobconf says. 
// table is empty, so can only lock the table 
//  This basically means stop has been called. 
// string comparisons 
//  1.2 Recurse over all the source tables 
//  read just the first column 
//  first row - the process should only be started if necessary, as it may   conflict with some 
/*  Convert a long to a string. The string is output into the argument   * byte array, beginning at character 0. The length is returned.    */
//  Execute another query 
//  Check the output of FixAcidKeyIndex - it should indicate nothing required fixing. 
//  The output of a partial aggregation is a struct containing   a long count, two double averages, and a double covariance. 
//  operator stack. The dispatcher generates the plan from the operator tree 
//  Replace default keystore with keystore for www.example.com 
//  Get the abortedWriteIds which are already sorted in ascending order. 
// -----------------------------------------------------------------------------------------------   Validation methods.  ----------------------------------------------------------------------------------------------- 
//  If we do not need this format of accessor using ObjectNode, this is a candidate for removal as well 
// set auth privileges 
// Update partition schema to have 3 fields 
//  The varchar type info need to be set prior to initialization,   and must be preserved when the plan serialized to other processes. 
//  hive vars 
//  Since we're reusing the compiled plan, we need to update its start time for current run 
//  IMetaStoreClient is needed to access token store if DBTokenStore is to be used. It   will be got via Hive.get(conf).getMSC in a thread where the DelegationTokenStore   is called. To avoid the cyclic reference, we pass the Hive class to DBTokenStore where   it is used to get a threadLocal Hive object with a synchronized MetaStoreClient using   Java reflection.   Note: there will be two HS2 life-long opened MSCs, one is stored in HS2 thread local   Hive object, the other is in a daemon thread spawned in DelegationTokenSecretManager   to remove expired tokens. 
// create a some of delta directories 
/* the implementation of HCatFieldSchema is a bit messy since with the addition of parametrized types (e.g. char(7)) we need to represent something richer than an enum but for backwards compatibility (and effort required to do full refactoring) this class has both 'type' and 'typeInfo';similarly for mapKeyType/mapKeyTypeInfo  */
//  On success, but with nothing to return, we can return an empty list. 
//  @@protoc_insertion_point(class_scope:SignableVertexSpec) 
//  Remove RS and SEL introduced by enforce bucketing/sorting config 
//  ADD_CONSTRAINT EVENT 
//     our preliminary mapping won't work out. We'll handle that below. 
//  We assume that if put/lock throws in the middle, it's ok to treat buffers as not being   locked and to blindly deallocate them, since they are not going to be used. Therefore   we don't remove them from the cleanup list - we will do it after sending to consumer.   This relies on sequence of calls to cacheFileData and sendEcb.. 
//  No capacity left on node1. The next task should be allocated to node2 after it times out. 
//  guava stores the hashcodes in little endian order 
//  Resolve column expression to input expression by using expression mapping in current operator 
//  Optimization copied from BigDecimal. 
//  root operator is union (can happen in reducers) 
// UDFYear 
//  remove trailing empty split(s) 
/*    * Right trim and truncate a slice of a byte array to a maximum number of characters and   * return the new byte length.    */
//  create a partitioned table 
//  4. We build the new predicate and return it 
//  the dummy option should not have made it either - only options 
//  1. We try to transform possible candidates 
//  75% for 4 executors 
// if the error message is changed for REPL_EVENTS_MISSING_IN_METASTORE, then need modification in getNextNotification 
// ************************************************************************************************   Decimal to Non-Integer conversion. 
//  3. Query Hints 
//  Populating the Empty string bytes. Putting it as static since it should be immutable and can   be shared. 
//  next file in the path 
/*  Byte bit patterns of the form 10xxxxxx are continuation       * bytes. All other bit patterns are the first byte of       * a character.        */
//  Unique rows 
//  Instantiate Driver to compile the query passed in.   This UDF is running as part of an existing query, which may already be using the   SessionState TxnManager. If this new Driver also tries to use the same TxnManager   then this may mess up the existing state of the TxnManager.   So initialize the new Driver with a new TxnManager so that it does not use the 
//  shared plan utils for tez 
//  sometimes RowSchema is empty, so fetch stats of columns in exprMap 
/*  batchIndex  */
//  The writeHWM = min(NEXT_WRITE_ID.nwi_next-1, max(TXN_TO_WRITE_ID.t2w_writeid under txnHwm)) 
//  I think this is wrong, the alter table statement should come on the table topic not the 
//  write partitionInfo into output 
//  Return the MockInstance's Connector 
// make sure it works with nothing to expire 
//  If we reach here, we succeed. 
//  this tests the case where older data has an ambiguous structure, but the   correct interpretation can be determined from the repeated name 
//  for current query. 
//  the RHS table columns should be not be output from the join 
// copy if across file system or encryption zones. 
/*  We write many records because sometimes the RecordWriter for the format to test     * behaves different with one record than a bunch of records  */
//  GroupBy query 
//  add log links and other diagnostics from YARN Service 
//  the following two are used for join processing 
//  Also, reading beyond our byte range produces NULL. 
//  For ACID non-bucketed case, the filenames have to be in the format consistent with INSERT/UPDATE/DELETE Ops,   i.e, like 000000_0, 000001_0_copy_1, 000002_0.gz etc.   The extension is only maintained for files which are compressed. 
//  if this is a nested sql script then flatten it 
//  ////// Generate GroupbyOperator3 
/*  Indicates a request has completed on a node  */
//  With local spark context, all user sessions share the same spark context. 
//  TimestampColumnArithmeticTimestampColumn.txt   TimestampScalarArithmeticTimestampColumn.txt   TimestampColumnArithmeticTimestampScalar.txt 
//  Create a clone of the operator 
//  No input data 
//  copy the src to the destination and create local resource. 
//  the jobtracker setting to its initial value 
//  getRows will call estimateRowCount 
//  List of configurations. Currently the list consists of hadoop version and execution mode only 
//  If the properties does not define any transactional properties, we return a default type. 
//  We'll wait for 120s for node creation 
//  At least a single item in project is required. 
//  Get number of partitions by doing count on PART_ID. 
//  Test that existing exclusive table with new shared_read coalesces to 
//  we need to replace the dummy operators in the work with the cloned ones. 
//  Assumes the reader count has been incremented automatically by the results cache by either   lookup or creating the cache entry. 
//  assumes un partitioned table 
//  just set a really small lower bound 
//  admin check 
//  Done grouping partitions within table-dir. 
//  initialize the merge operators first. 
//  This session is bad, so don't allow reuse; just convert it to normal get. 
//  no nulls, is repeating 
//  Special handling for Druid rules here as otherwise   planner will add Druid rules with logical builder 
//  some of the partitions miss stats. 
//  we know rowSet has only one element 
//  Note: there are so many different onSuccess/onFailure callbacks floating around that         this will probably be called twice for the done state. This is ok given the sync. 
// import static org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.AVRO_SERDE_SCHEMA;  import static org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.SCHEMA_LITERAL; 
//  2019-01-02 00:00:00 GMT is 2019-01-01 19:00:00 GMT-0500 (America/New_York / EST) 
//  A symlink file, contains first file from first dir and second file from   second dir. 
//  TBL_NAME 
//  Decrement only if an element was removed. 
//  no child map join 
//  This many bytes are necessary to store the reversed nanoseconds. 
//  Not order preserving 
//  2. Gen OP Tree from resolved Parse Tree 
//  pattern 
//  for auto reduce parallelism - max reducers requested 
//  Get locations again, and make sure they're the same. 
//  statistics stored in metastore 
//  optional .SourceStateProto state = 3; 
//  We can invalidate the entry now, but calling removeEntry() requires a write lock   and we may already have read lock taken now. Add to entriesToRemove to delete later. 
//  limit factor is too big 
/*  useExactBytes  */
//  This is going to be slow... hold on. 
//  optional int32 vertex_parallelism = 13; 
//  column name is the second group from current match 
/*      * Return how the list of columns passed in match.     * Return NO_MATCH if either of the list is empty or null, or if there is a mismatch.     * For eg: ([], []), ([], ["a"]), (["a"],["b"]) and (["a", "b"], ["a","c"]) return NO_MATCH     *     * Return COMPLETE_MATCH if both the lists are non-empty and are same     * Return PREFIX_COL1_MATCH if list1 is a strict subset of list2 and     * return PREFIX_COL2_MATCH if list2 is a strict subset of list1     *     * For eg: (["a"], ["a"]), (["a"], ["a", "b"]) and (["a", "b"], ["a"]) return     * COMPLETE_MATCH, PREFIX_COL1_MATCH and PREFIX_COL2_MATCH respectively.      */
//  there could be some spilled partitions which needs to be cleaned up 
//  require delete privilege if this is an insert-overwrite 
// test_param_2 = "75" 
//  12.1. Merge join into multijoin operators (if possible) 
//  create a walker which walks the tree in a DFS manner while maintaining   the operator stack. The dispatcher   generates the plan from the operator tree 
// if the partition does not have partition level privilege, go to table level. 
//  copy all the properties 
//  The cache entry has just been invalidated, no need for the scheduled invalidation. 
//  Also set this to the Thread ContextClassLoader, so new threads will   inherit   this class loader, and propagate into newly created Configurations by   those 
//  Read configuration for the target path, first from jobconf, then from table properties 
// empty list, non partitioned 
//  mark the MapredWork and FileSinkOperator for gathering stats 
//  Allow numeric to string 
//  valiade 
//  bother about generating a schema only if a schema retriever class wasn't provided 
//  Quarter granularity 
// Read should get 10 rows if immutable, 30 if mutable 
//  Prepare the bloom filter 
//  all rows qualify 
/*    * The = sign in the string for TOKEN_FILE_ARG_PLACEHOLDER is required because   * org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows() prepares   * arguments expecting an = sign. It will fail to prepare the arguments correctly   * without the = sign present.    */
//  Now let's load this file into a new Hive table. 
//  end ReadOnlySubList 
//  Initialize UDF which will output the return type for the UDF. 
//  since this is special cased when it is rewritten in SubqueryRemoveRule 
//  Query is the hive query string i.e. "SELECT * FROM src;" associated with   this set of tasks logs 
//  Ignore non-directory files 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getSchemas(org.apache.hive.service.cli.SessionHandle, java.lang.String, java.lang.String)    */
//  FK_NAME 
//  Populating the Empty string bytes. Putting it as static since it should be immutable and can be   shared 
/*      * This member has information for data type conversion.     * Not defined if there is no conversion.      */
//  if the destination file exists for some reason delete it 
//  For the file size check. 
//  The reducer contains a groupby, which needs to be restored. 
//  Replace pRS with cRS and remove operator sequence from pRS to cRS 
//  Set up periodic progress reporting in case the UDTF doesn't output rows 
//  swap the fields with the passed in orcStruct 
//  No-op for session killed by WM. 
//  First infer the type of object 
//  non-constant or non-primitive constants 
// don't want set autocommit true|false to get mixed with set hive.foo.bar... 
//  Add jar to current thread class loader dynamically, and add jar paths to JobConf as Spark   may need to load classes from this jar in other threads. 
//  A pound # statement (IF/ELSE/ENDIF). 
/*  construct one location map if not exists.  */
//  LOG.debug("VectorMapJoinFastBytesHashMap findWriteSlot slot " + slot + " tripleIndex " + tripleIndex + " empty"); 
//  Some other operation in progress using the same lock.   A subsequent fragmentComplete is expected to come in. 
//  But, this is tricky to implement, and we'll leave it as a future work for now. 
//  key = (key << 15) - key - 1; 
//  A slow way to get the number of decimal digits. 
//  Is an EXTERNAL table 
//  Nothing to do - e.g. two messages have canceled each other before we could react. 
//  Previous row was for a large bytes value ( > MAX_SIZE_FOR_SMALL_BUFFER).   Use smallBuffer if possible. 
//  For use by VectorKeySeriesMulti so that the minimum equal key can be advanced. 
//  Had the put succeeded for our new buffer, it would have refcount of 2 - 1 from put,   and 1 from notifyReused call above. "Old" buffer now has the 1 from put; new buffer   is not in cache. releaseBuffer will decref the buffer, and also deallocate. 
//  Form key object array 
//  Execute the UDF 
//  after column is not null, but we did not find it. 
//  Assumes the lists are sorted. 
/*         * Change the current thread name to include parent thread Id if it is executed        * in thread pool. Useful to extract logs specific to a job request and helpful        * to debug job issues.         */
// subject to list of 'exceptions' in 'writeIdList' (not show in above example). 
//  We don't bail on failure - try in detail below. 
//  tez needs its own scratch dir (per session)   TODO: De-link from SessionState. A TezSession can be linked to different Hive Sessions via the pool. 
//  Common name constants for event messages 
//  Used by hash-based GroupBy: Mode = HASH, PARTIALS 
//  Even if the cleanup throws some exception it will continue. 
//  The sql should be completed now. 
//  those top layer ReduceSinkOperators. 
//  curr value becomes old and vice-versa 
//  test repeating on left 
//  None of the partitions will be dumped as the partitions list was empty 
//  Does vectorization use stripped char values? 
//  this messageType only has one optional field, whose name is mapCol, original Type is MAP 
//  Create enough elementConverters   NOTE: we have to have a separate elementConverter for each element,   because the elementConverters can reuse the internal object.   So it's not safe to use the same elementConverter to convert multiple   elements. 
//  RemoteException with AlreadyBeingCreatedException will be thrown   if the file is currently held by a writer 
// set new db and verify get 
//  2.1 Convert AST Expr to ExprNode 
//  any name, it does not matter. 
// reduce-side join, use MR-style shuffle 
//  Do insert overwrite to create some invalid deltas, and import into a non-MM table. 
//  Check if we can process it or not by the index of distinct 
//  Preserve partitioning and ordering 
//  row group position within stripe 
//  do nothing. 
//  Read state. 
//  Sort the list as requested 
//  { 100_000_000 }, { 1_000_000_000 } 1B passed but is super slow 
//  required   required   required   required 
//  Clear gWorkMap 
// convert the table to Acid 
//  Most of the method got skipped but we still need to handle the duck. 
//  Setup serDe. 
//  type is MAJOR since there's no base yet 
//  3. Populate other data structures 
//  A call to increaseBufferSpace() or ensureValPreallocated() will ensure that buffer[] points to   a byte[] with sufficient space for the specified size. 
//  Create the dummy aggregation. 
/*    * used to give a unique name to each SubQuery QB Currently there can be at   * most 2 SubQueries in a Query: 1 in the Where clause, and 1 in the Having   * clause.    */
//  Load a BytesColumnVector by copying in large data, enough to force   the buffer to expand. 
//  These are suffixes attached to intermediate directory names used in the 
//  configureTableJobProperties shouldn't be getting called by Hive, but, if it somehow does,   we should just set all of the configurations for input and output. 
//  1. Add GB Keys to reduce keys 
// checkTGT calls ugi.relogin only after checking if it is close to tgt expiry  hadoop relogin is actually done only every x minutes (x=10 in hadoop 1.x) 
//  Caches disabled nodes for quicker lookups and ensures a request on a node which was skipped   does not go out of order. 
// rowIdOffset could be 0 if all files before current one are empty 
//  Since enforcing precision and scale can cause a HiveDecimal to become NULL,   we must read it, enforce it here, and either return NULL or buffer the result. 
//  Send only if the state has changed. 
//  Perf times 
//  MSerdeInfo *& SerdeInfo should be same as well 
//  now positions contains all the distinct positions, i.e., $5, $4, $6   we need to first sort them as group by set   and then get their position later, i.e., $4->1, $5->2, $6->3 
//  The operator tree till the sink operator has already been processed while   fetching the next row to fetch from the priority queue (possibly containing   multiple files in the small table given a file in the big table). Now, process   the remaining tree. Look at comments in DummyStoreOperator for additional   explanation. 
//  the event will not be sent to ATS if there are too many outstanding work submissions. 
//  RENEWER_KERBEROS_PRINCIPAL_NAME 
//  The Avro deserializer would deserialize our object and return back a list of object that   hive can operate on. Here we should be getting the same object back. 
//  a susbset of files for the partition are sufficient for the optimization 
//  NOTE: It is critical to do this here so that log4j is reinitialized   before any of the other core hive classes are loaded 
//  nodeMap registration. 
//  Create all the files - this is required because empty files need to be created for   empty buckets   createBucketFiles(fsp); 
//  End HiveRelMdCost.java 
//  if the only difference is numeric types, pick the method   with the smallest overall numeric type. 
// An empty batch will appear at the end of the stream 
//  If this is true, then there is no data in the batch -- we have hit the end of input. 
//  Now, we could get previous and next day, figure our how many hours were inserted or removed,   and from which of the days, etc. But at this point our gun is pointing straight at our foot, 
//  Valid range is "range/rows between 10 preceding and 2 preceding" for preceding case 
//  add empty line 
//  all good 
//  Binary 
//  interestingly decimal(2) means decimal(2,0) 
//  Look for databases without pattern 
//  The sortMerger is a heap data structure that stores a pair of   (deleteRecordKey, deleteReaderValue) at each node and is ordered by deleteRecordKey.   The deleteReaderValue is the actual wrapper class that has the reference to the   underlying delta file that is being read, and its corresponding deleteRecordKey   is the smallest record id for that file. In each iteration of this loop, we extract(poll)   the minimum deleteRecordKey pair. Once we have processed that deleteRecordKey, we   advance the pointer for the corresponding deleteReaderValue. If the underlying file   itself has no more records, then we remove that pair from the heap, or else we   add the updated pair back to the heap. 
//  Multi-Key specific imports. 
//  strictly not required, just for consistency 
//  NEW TAI LUE LETTER LOW KVA U+19A8 (3 bytes) 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getObject(int, java.util.Map)    */
//  during the tests we run with 100K prealloc in the logs.   on windows systems prealloc of 64M was seen to take ~15seconds   resulting in test failure (client timeout on first session).   set env and directly in order to handle static init/gc issues 
/*  partialCount  */
//  Make a decimal batch with three columns, including two for inputs and one for the result. 
//  at this point we've found the fork in the op pipeline that has the pruning as a child plan. 
//  Create the default properties object 
//  Can't be set in constructor due to circular dependency. 
//  Get the distribute by aliases - these are aliased to the entries in   the   select list 
//  process location one-by-one 
//  The planner seems to pull this one out. 
//  Simple pattern: D H:M:S.nnnnnnnnn 
//  signature and generate field expressions for those 
//  Skip if the transaction under evaluation is already committed. 
//  copy loginTimeout from driver manager. Thrift timeout needs to be in millis 
//  all evaluation should be processed here for valid aliasFilterTags     for MapJoin, filter tag is pre-calculated in MapredLocalTask and stored with value. 
//  The arg is self. 
//  both sides. 
//  Parse validReaderWriteIdList from creation metadata 
// normal insert 
//  Since a key expression can be a calculation and the key will go into a scratch column,   we need the mapping and type information. 
//  we make sure that we do not change anything if there is anything   wrong. 
//  Set the correct last repl id to return to the user 
//  Expand, and write result 
//  Now insert the new buffer in its place and restore heap property. 
//  in this case, we have to scale up _BEFORE_ division. otherwise we   might lose precision. 
//  Now try to reuse with no other sessions remaining. Should still work. 
//  if verifySetup is set to true, all the test setup we do will perform additional   verifications as well, which is useful to verify that our setup occurred   correctly when developing and debugging tests. These verifications, however   do not test any new functionality for replication, and thus, are not relevant   for testing replication itself. For steady state, we want this to be false. 
//  Test that we are cleaning aborted transactions with no components left in txn_components.   Put one aborted transaction with an entry in txn_components to make sure we don't   accidently clean it too. 
//  copy entire string value 
//  puts long in little endian order 
//  This is the only public constructor of FileSplit 
//  Save prev val of the key on threadLocal 
//  Persist the column statistics object to the metastore   Note, this function is shared for both table and partition column stats. 
//  We create a new op if it is the first time we fire the rule 
//  subsequent instances when it's taken off the queue. 
//  First entry.   Count. 
//  age >= '10' 
//  the sorting property is not obeyed 
//  If the code point exists in deletion set, no need to emit out anything for this code point. 
//  create tables, verify query 
//  Was able to execute something before the last blacklist. Reset the exponent. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getCharacterStream(int)    */
//  Convert to date value (in days) 
//  in hivemetastore-site.xml 
//  Test that the whole things works when there's nothing in the queue.  This is just a   survival test. 
//  No need to propagate to this to the responder 
//  If there is a distinctFuncExp, add all parameters to the reduceKeys. 
//  Validate all integer type values are stored correctly 
//  Output record readers 
//  2. Regen OP plan from optimized AST 
//  For each aggregation 
//  Array full? 
//  We have just removed the session from the same pool, so don't check concurrency here. 
//  Field node, e.g. get a.myfield1 from a 
// add new 'synthetic' columns for projections not provided by Select 
//  Aggregate does not change input ordering so corVars will be 
//  null 
//  Restore broken links between operators, and remove the branch from the original tree 
//  compile internal will automatically reset the perf logger 
//  Dump and load only second insert (2 records) 
//  if here, we may be checking a DB level lock against a Table level lock.  Alternatively,   we could have used Intention locks (for example a request for S lock on table would   cause an IS lock DB that contains the table).  Similarly, at partition level. 
/*  id >= 10 and not (10 > id)  */
//  TODO: Handle quoted tablenames 
//  Zero and above numbers indicate a big table key is needed for   small table result "area". 
// TODO 
//  traversing origin, find ExprNodeDesc in sources and replaces it with ExprNodeDesc   in targets having same index. 
//  Equivalent aliases for the column 
//  See if we can load all the delete events from all the delete deltas in memory... 
//  from Key and Value TableDesc 
/*      * FLOAT: Min and max.      */
//  In case of cross join, we disable hybrid grace hash join 
//  ColumnStatisticsObj with info about its db, table, partition (if table is partitioned) 
//  Write the terminating NULL byte 
// 42X65/3000 means index doesn't exist 
//  cast 
//  VectorizedBatchUtil.debugDisplayOneRow(overflowBatch, overflowBatch.size, "generateHashMapResultMultiValue overflow"); 
//  Extract the hex digits of num into value[] from right to left 
/*     explicitly remove the setting of last.repl.id from the db object parameters as loadTask is going    to run multiple times and explicit logic is in place which prevents updates to tables when db level    last repl id is set and we create a AlterDatabaseTask at the end of processing a database.      */
//  2. rewrite into query    TOK_QUERY       TOK_FROM          join       TOK_INSERT          TOK_DESTINATION             TOK_DIR                TOK_TMP_FILE          TOK_SELECT 
//  3. We need to fix it, we create the two replacement project 
/*  all tests are identical to the other seek() tests  */
//  batches will be sized 17, 8, 4, 2, 1 
// Reuse the re-encoder  Evolved schema?  Create and store new encoder in the map for re-use 
//  40 + 50 
//  Try with chunked streams 
//  keep it as-is 
//  TODO: this boolean flag is set only by RS stats annotation at this point  clone.setRuntimeStats(runtimeStats); 
//  table locks for this db. 
//  The operators specified by depth and removed from the tree. 
//  Key out of range for whole hash table. 
//  is now different from HiveDecimal.precision() 
//  We have to be mindful of order during filtering if we are not returning all partitions. 
//  We add Hive function names   For functions that aren't infix operators, we add an open 
//  However, these expressions should not be considered as valid expressions for separation. 
// fall through to ACQUIRE 
//  Always re-schedule the next callable - irrespective of task count,   in case new tasks come in later. 
//  Since user names need to be valid unix user names, per IEEE Std 1003.1-2001 they cannot   contain comma, so we can safely split above string on comma. 
//  For all practical purposes a code point is a fancy name for character. A java char data type   can store characters that require 16 bits or less. However, the unicode specification has   changed to allow for characters whose representation requires more than 16 bits. Therefore we   need to represent each character (called a code point from hereon) as int. More details at   http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html 
//  This map defines the progression of up casts in numeric types. 
//  Look for tables without pattern 
//  Columns are output from the join from the different reduce sinks in the order of their 
//  Leading space is significant 
//  This appears to leave the remove transaction in an inconsistent state but the heartbeat is now   cancelled and it will eventually time out 
//  if column name is not contained in needed column list then it   is a partition column. We do not need to evaluate partition columns   in filter expression since it will be taken care by partitio pruner 
//  only single subquery expr is supported 
//  CHAR and VARCHAR types can be specified with maximum length. 
//  close the existing ctx etc before compiling a new query, but does not destroy driver 
//  ideally we should just call FileInputFormat.setInputPaths() here - but   that won't work since FileInputFormat.setInputPaths() needs   a Job object instead of a JobContext which we are handed here 
//    trim=false 
//  we have checked all the parents for the "index" position. 
//  Note: we get query ID here, rather than in the caller, where it would be more correct         because we know which exact query we intend to kill. This is valid because we         are not expecting query ID to change - we never reuse the session for which a 
/*      * Calculate the std result when count > 1.  Public so vectorization code can     * use it, etc.      */
/*    * make sure Aborted txns don't red-flag a base_xxxx (HIVE-14350)    */
//  Transformation :   Outer Query Left Join (inner query) on correlated predicate 
//  make SSL connection 
//  Previous batch was the last of a group of batches.  Remember the next is the first batch   of a new group of batches. 
//  do nothing because "And" and "Or" and "Not" supports null value   evaluation   NOTE: In the future all UDFs that treats null value as UNKNOWN (both   in parameters and return   values) should derive from a common base class UDFNullAsUnknown, so   instead of listing the classes   here we would test whether a class is derived from that base class.   If All childs are null, set unknown to true 
//  unix_timestamp(args) -> to_unix_timestamp(args) 
//  Druid only support appending more partitions to Linear and Numbered ShardSpecs. 
//  First we check if the two table scan operators can actually be merged 
//  Create an empty output object which will be populated when convert() is invoked. 
//  Iterative through the children in a DFS manner to see if there is more than 1 table alias 
//  Obtain list of col stats, or use default if they are not available 
//  Position doesn't make sense for async reader, chunk order is arbitrary. 
//  Pre-allocated member for remembering the big table's selected array at the beginning of   the process method before applying any filter.  For outer join we need to remember which   rows did not match since they will appear the in outer join result with NULLs for the 
//  Validate that we can add partition without escaping. Escaping was originally intended   to avoid creating invalid HDFS paths; however, if we escape the HDFS path (that we   deem invalid but HDFS actually supports - it is possible to create HDFS paths with   unprintable characters like ASCII 7), metastore will create another directory instead   of the one we are trying to "repair" here. 
//  it should be the same as the MoveWork's sourceDir. 
//  There is no need for the user to specify mapjoin for it to be 
//  Compaction doesn't work under a transaction and hence pass 0 for current txn Id 
//     Release initial refcounts. 
//  the registers 
//  check the specified partitions 
//  STATS_DESC 
/*    * Use when merging variance and partialCount > 0 and mergeCount > 0.   *   * NOTE: mergeCount and mergeSum do not include partialCount and partialSum yet.    */
//  Evaluate THEN expression (only) and copy all its results. 
/*     use loadTask as dependencyCollection    */
//  If there's no fraction part, return immediately to avoid the cost of a divide. 
//  Either the slice comes entirely after the end of split (following a gap in cached   data); or the split ends in the middle of the slice, so it's the same as in the   startIx logic w.r.t. the partial match; so, we either don't want to, or cannot,   use this. There's no need to distinguish these two cases for now. 
//  Random 
//  SemanticAnalyzer 
//  This regex is a bit lax in order to compensate for lack of any escaping   done by Amazon S3 ... for example useragent string can have double quotes 
//  Replace the filter expression to reference output of the join 
//  Java Primitive Type? 
//  skipping columns since partition level field schemas are the same as table level's   skipping partition keys since it is the same as table level partition keys 
//  getTable is invoked after fetching the table names 
//  3. Add Child Project Rel if needed, Generate Output RR, input Sel Rel 
//  Create the new RowSchema for the projected column 
//  create 10 dummy partitions 
//  child 0 is the name of the column 
//  38 * 2 or 76 full decimal maximum - (64 + 8) digits in 4 lower longs (4 digits here). 
/*    * Use this copy method when the source batch is safe and will remain around until the target   * batch is finished.   *   * Any bytes column vector values will be referenced by the target column instead of copying.    */
//  Finally SUBMIT the JOB! 
//  read in the first 1K characters from the URL 
//  and initiates the SASL handshake. 
//  Convert decimal into the scratch buffer without allocating a byte[] each time   for better performance. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#addBatch(java.lang.String)    */
//  if it is two way left outer or right outer join take selectivity only for 
//  column projection 
//  MySQL returns 0 if the string is not a well-formed numeric value.   But we decided to return NULL instead, which is more conservative. 
//  REPL STATUS 
//  GBY Operator of the RS Operator. 
//  hash map overhead 
//  No scratch dir initially 
//  Run the Worker explicitly, in order to get the reference to the compactor MR job 
//  Now start with everything and test losing stuff. 
//  By default, assume we can user directSQL - that's kind of the point. 
//  Only TezTask sets this, and then removes when done, so we don't expect to see it. 
//  incorrect precision: expected:<0 xxxxx yyy 5.2[]> but was:<0 xxxxx yyy 5.2[0]> 
//  verify when third argument is repeating 
//  Metrics system will get this via reflection 0_o 
//  like type. 
//  Don't clear the hash table - reuse is possible. GC will take care of it. 
//  same, but repeating value is null 
//  we should get back the latest, reader schema: 
//  drop a table without saving to trash by setting the purge option 
//  SUCCESS 
/*        the currentPartitionDesc cannot be inlined as we need the hasNext() to be evaluated post the       current retrieved lastReplicatedPartition       */
// Test for null partition value map 
// clone to make sure new prop doesn't leak 
//  We put the query user, not LLAP user, into the message and token. 
//  If we got an error attempting to ss.close, then it's not likely that   ss.err is valid. So we're back to System.err. Also, we don't change   the return code, we simply log a warning, and return whatever return   code we expected to do already. 
//  Following sequence of commit-abort-open-abort-commit. 
//  Note: the only sane case where this can happen is the non-pool one. We should get rid         of it, in non-pool case perf doesn't matter so we might as well open at get time         and then call update like we do in the else.   Can happen if the user sets the tez flag after the session was established. 
//  tried all, back to original code (for error message) 
//  if parents aren't in llap neither should the child 
//  will update current number of open txns back to 0 
//  Omitting zone or time part is allowed 
// no conflicting operations, proceed with the rest of commit sequence 
/*    * Returns false if there is a SelectExpr that is not a constant or an aggr.   *    */
//  the subquery 
//  For backward compatibility 
//  Go over all the keys and get the size of the fields of fixed length. Keep 
/*  (non-Javadoc)   * This provides a LazyShort like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//        because it should be impossible to get incompatible outputs. 
//  Collect columns we copy from the big table batch to the overflow batch. 
//  Escaping happened, we need to copy byte-by-byte.   1. Set the length first. 
//  it will be used to estimate num nulls 
//  test for null input strings 
//  if IF is self describing no need to send column info per partition, since its not used anyway. 
//  Don't emit user/timestamp info in test mode,   so that the test golden output file is fixed. 
//  List of SparkWork.Dependency 
//  It could not be renewed, return that information 
//  map that keeps track of the last operator of a task to the following work 
//  Test that not changing the database and the function name, but only other parameters, like 
//  Reset table params 
//  deep copy expr node desc 
//  Use RW, not PRIVATE because the copy-on-write is irrelevant for a deleted file 
//  [A: 0, B: 0, B.x: 1, B.y: 0, C: 0] 
//  Virtual trailing zeroes. 
//  Nothing to cleanup 
//  Then, scale up with 5 
//  Go through the Reduce keys and find the matching column(s) in the reduce values 
//  nesting not allowed! 
//  LOG.debug("VectorMapJoinFastLongHashTable add key " + key + " slot " + slot + " pairIndex " + pairIndex + " found key (i = " + i + ")"); 
//  For debug tracing: information about the map or reduce task, operator, operator class, etc. 
//  special handling. 
//  create a walker which walks the tree in a DFS manner while maintaining 
//  Cartesian product is not supported in strict mode 
//  The expected tags from the parent operators. See processOp() before 
//  mimicking behaviour in CreateTableDesc tableDesc creation   returning null table description for output. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNull(java.lang.String, int,   * java.lang.String)    */
//  call getHCatComparisonString on lhs and rhs, and and join the   results with OpType string 
// ---------------------------------------------------------------------------   Process Multi-Key Outer Join on a vectorized row batch.   
//  there should be only one MRInput 
// then check if it is distinct key 
//  Initialize a deleteEventWriter if not yet done. (Lazy initialization) 
//  Don't log exception here. 
//  The root might have changed because of tree modifications.   Compute the new root for this tree and set the astStr. 
//  Apply rest of the configuration only to HiveServer2 
//  Can't divide NULL. 
//  set up a java key provider for encrypted hdfs cluster 
//  try to get default value only if this is DEFAULT constraint 
//  The header to look for. We use "X-XSRF-HEADER" if this is null.   Methods to not filter. By default: "GET,OPTIONS,HEAD,TRACE" if null. 
//  VectorMapOperator. 
//  print header   -------------------------------------------------------------------------------           VERTICES     STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED 
//  The eventual goal is to monitor the progress of all the tasks, not only the   map reduce task.   The execute() method of the tasks will return immediately, and return a   task specific handle to   monitor the progress of that task.   Right now, the behavior is kind of broken, ExecDriver's execute method   calls progress - instead it should   be invoked by Driver 
//  of Accumulo Ranges 
//  Convert the join operator to a bucket map-join join operator 
//  common comparison class for char/varchar is string? 
//  Skip the same value if avgDistinct is true 
// can safely convert the join to a map join. 
//  We will retrieve stats from the metastore only for columns that are not cached 
//  We could make some assumptions given how the reader currently does the work (consecutive   chunks, etc.; blocks and columns stored in offset order in the lists), but we won't -   just save all the chunk boundaries and lengths for now. 
//  assert that there is one partition present, and it had hcat instrumentation inserted when it was created. 
//  Source operator to get the number of entries 
//  Use PurgeCacheResponseProto.newBuilder() to construct. 
//  Step 2.2: Replace the corresponding part childMRWork's MapWork. 
//  transaction batch size = 1 case 
//  Repeat the same check for dropTable. 
//  table exists 
//  Should not get here. 
//  Conditions. 
//  setRef is used below and this is safe, because the reference   is to data owned by this column vector. If this column vector   gets re-used, the whole thing is re-used together so there   is no danger of a dangling reference. 
//  Form result from lower, middle, and middle words. 
//  The new base dir now has two bucket files, since the delta dir has two bucket files 
//  we proceed only if we'd actually succeeded anyway, otherwise, 
// test_param_2 = "50" 
//  excludedProvidedBy Framework vs excludedConfigured 
/*  HCat Output Format related errors 2000 - 2999  */
//  Indicate last batch of current group. 
/*    * Helper function to retrieve the basename of a local resource    */
//  since currentReadBlock may assigned to currentWriteBlock, we need to store 
//  This will throw an expected exception since   client is communicating with the wrong http service endpoint 
//  throw a HiveException if the table/partition is archived 
//  find out database name and table name of target table 
//  We need some value that indicates NULL. 
//  Done, we have 3 bytes. Continue reading this buffer. 
//  update old data with values for the new schema columns 
//  Get the sort order 
//  We are just a relay; send unpause to encoded data producer. 
/*        * The TableScanOperator's needed columns are just the data columns.        */
//  For non-acid tables (or paths), all data files are in getOriginalFiles() list 
//  verify we found them all 
//  Close output stream if open 
//  TOK_ALTERVIEW_AS 
//  97 
//  the api that finds the jar being used by this class on disk 
//  number of columns pertaining to keys in a vectorized row batch 
//  Because every value will be NULL. 
//  input/output settings 
//  CTE 
//  that QH will not be too far from the correct digit later in D3 
//  INFO_VALUE 
//  Double-check the header under lock. 
//  Now add the corVars from the input, starting from   position oldGroupKeyCount. 
//  Use Junit's Assume to skip running this fixture against any storage formats whose   SerDe is in the disabled serdes list. 
//  hive has no max limit for strings 
//  For snapshot isolation, we don't care about txns greater than current txn and so stop here.   Also, we need not include current txn to exceptions list. 
//  have to emulate "distinct", otherwise tables with the same name may be returned 
//  true if it is insert overwrite. 
//  be able to report progress. 
//  Don't use Assert.fail, we are catching assertion errors. 
//  The object that determines equal key series. 
//  Collect column access information 
//  decide whether this is already in hashmap (keys in hashmap are deepcopied   version, and we need to use 'currentKeyObjectInspector'). 
//  No task lock. But acquires lock on the scheduler 
// no-op 
//  This is an internal error, something odd happened with reflection so   log it and don't output the bean. 
/*  Routines for copying between VectorizedRowBatches  */
/*        * if there are fewer than leadAmt values in leadWindow; start reading from the first position.       * Otherwise the window starts from nextPosInWindow.        */
//  At this point, we have seen the exponent letter E or e and have decimal information as:       isNegative, precision, integerDigitCount, nonTrailingZeroScale, and       fast0, fast1, fast2.     After we determine the exponent, we will do appropriate scaling and fill in fastResult. 
//  Map 1 .......... 
//  create the destination if it does not exist 
//  Special handling for time-zone 
//  LazyBinary seems to work better with an row object array instead of a Java object... 
//  dealing with views 
//  maybe valid - too expensive to check without a parse 
//  Calculate relative offset 
//  Rows are in a combination of the on-disk hashmap and the sidefile 
//  the vertex that this operator belongs to 
//  FUNC_NAME 
//  Since we don't have a non-native or pass-thru version of VectorPTFOperator, we do not   have enableConditionsMet / enableConditionsNotMet like we have for VectorReduceSinkOperator,   etc. 
//  Stateful? 
//  Check query results cache   In the case that row or column masking/filtering was required, we do not support caching. 
//  To handle the case like SUM(LAG(f)) over(), aggregation function includes   LAG/LEAD call 
//  @@protoc_insertion_point(builder_scope:SubmitWorkRequestProto) 
//  Build not null conditions 
//  2.2 Obtain col stats for partitioned table. 
//  check access columns from ColumnAccessInfo 
//  such as "abc\%" 
//  The key wasn't present in the mapping, and the function didn't   return a default value - ignore, and use our default. 
//  BINARY_VAL 
//  Wrapper extends ql.metadata.Partition for easy construction syntax 
// subquery either in WHERE <LHS> IN <SUBQUERY> form OR WHERE EXISTS <SUBQUERY> form  in first case LHS should not be bypassed 
// this is just for debug 
//  local path doesn't depend on drone variables 
//  Insert overwrite ACID table from source table 
//  We could not have removed the pool for this session, or we would have CANCELED the init. 
//  -e 'query' 
//  this table cannot be big table 
//  Map 1 - 28 splits   Map 3 - 28 splits 
//  Copy JAR to DFS 
//  @VisibleForTesting 
//  Test that jdbc does not allow shell commands starting with "!". 
//  If the join keys matches the skewed keys, use the table skewed keys 
//  If the first byte of the VInt is -1, the VInt itself is -1, indicating that there is a   second VInt but the nanoseconds field is actually 0. 
//  This is a test. The parameter hive.test.dummystats.aggregator's value   denotes the method which needs to throw an error. 
//  First, read the CB header. Due to ORC estimates, ZCR, etc. this can be complex. 
//  If you change this function, remove the @Ignore from TestTxnHandler.deadlockIsDetected()   to test these changes.   MySQL and MSSQL use 40001 as the state code for rollback.  Postgres uses 40001 and 40P01.   Oracle seems to return different SQLStates and messages each time,   so I've tried to capture the different error messages (there appear to be fewer different   error messages than SQL states).   Derby and newer MySQL driver use the new SQLTransactionRollbackException 
//  Now re-initialize batch1 to simulate batch-object re-use. 
//  remove all detached objects from the cache, since the transaction is   being rolled back they are no longer relevant, and this prevents them   from reattaching in future transactions 
//  We're pretty screwed if we can't load the default conf vars 
//  Current hashMap in use 
//  Call here because at this point the WindowTableFunctionDef has been set 
//  Start: tests that check values from Pig that are out of range for target column 
//  Form the expression node corresponding to column 
//                    123456789012345678901234567890123456789012345 
//  Truncate by re-opening FileOutputStream. 
//  First find the path to be searched 
//  Update the database in cache 
//  present in the child (& hence we add a child Project Rel) 
//  ClusterBy 
//  Add to signature 
/*            * Single-Column String get key.            */
//  parse the string to determine column level storage type for primitive types   's' is for variable length string format storage   'b' is for fixed width binary storage of bytes   '-' is for table storage type, which defaults to UTF8 string   string data is always stored in the default escaped storage format; the data types   byte, short, int, long, float, and double have a binary byte oriented storage option 
//  Append Mode 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getTables(org.apache.hive.service.cli.SessionHandle, java.lang.String, java.lang.String, java.lang.String, java.util.List)    */
//  right now they come from jpox.properties 
//  template, <ClassNamePrefix>, <ReturnType>, <FuncName> 
//  Use only 1 reducer for order by 
//  At this point we don't have to do anything special in this case. Just   run through the regular paces w/o creating a new task. 
//  connection above 
//  Continue on to the next code point 
//  open the client transport 
//  Euro sign (3 bytes) 
//  1. It is an IN operator, check if it uses STRUCT 
/*        * Restriction.9.m :: disallow nested SubQuery expressions.        */
//  can safely convert the join to a map join. 
// Extract the partitions keys segments granularity and partition key if any 
//  We do not use the new cache buffers for the actual read, given the way read() API is.   Therefore, we don't need to handle cache collisions - just decref all the buffers. 
/*          * this happens in case of map join operations.         * The tree looks like this:         *         *        RS <--- we are here perhaps         *        |         *     MapJoin         *     /     \         *   RS       TS         *  /         * TS         *         * If we are at the RS pointed above, and we may have already visited the         * RS following the TS, we have already generated work for the TS-RS.         * We need to hook the current work to this generated work.          */
//  Suppress useless evaluation. 
//  If Kerberos 
//  there is no predicate on partitioning column, we need all partitions   in this case. 
//  Driver not initialized 
//  @@protoc_insertion_point(builder_scope:SignableVertexSpec) 
// Hiveserver2 using "-hiveconf hive.hadoop.classpath=%HIVE_LIB%". This is to combine path(s). 
//  Since HiveDecimal now uses FastHiveDecimal which stores 16 decimal digits per long,   lets test edge conditions here. 
//  Walk through the projection list and replace the column names with the   expressions from the original update.  Under the TOK_SELECT (see above) the structure   looks like:   TOK_SELECT -> TOK_SELEXPR -> expr             \-> TOK_SELEXPR -> expr ... 
/*        * Clone the Search AST; apply all rewrites on the clone.        */
//  how many times we'll sleep before giving up 
//  A bunch of these are in HiveMetaStoreClient but not IMetaStoreClient.  I have marked these   as deprecated and not updated them for the catalogs.  If we really want to support them we   should add them to IMetaStoreClient. 
//  SELECT * FROM src1 LATERAL VIEW udtf() AS myTable JOIN src2 ...   is not supported. Instead, the lateral view must be in a subquery   SELECT * FROM (SELECT * FROM src1 LATERAL VIEW udtf() AS myTable) a   JOIN src2 ... 
//  load jars under the hive.reloadable.aux.jars.path 
//  proper children of the union 
//  change curr ops row resolver's tab aliases to subq alias 
//  nullIndicator after the transformation. 
//  Only seal those partitions that haven't been spilled and cleared,   because once a hashMap is cleared, it will become unusable 
//  Generate the hiveConfArgs after potentially adding the jars 
//  if the execution engine is MR set the map/reduce env with the credential store password 
//  ReplicationSpec.KEY scopeKey = ReplicationSpec.KEY.REPL_SCOPE; 
//  If a table is in bigTables then its output is big (2) 
//  Try to fold (key <op> 86) and (key is not null) to (key <op> 86)   where <op> can be "=", ">=", "<=", ">", "<".   Note: (key <> 86) and (key is not null) cannot be folded 
//  Server thread pool 
//  Process else statement 
/*  @bgen(jjtree) EnumDefList  */
//  validate reserved values 
//  Handle case with nulls. Don't do function if the value is null,   because the data may be undefined for a null value. 
//  This table is not yet loaded in cache   If the prewarm thread is working on this table's database,   let's move this table to the top of tblNamesBeingPrewarmed stack,   so that it gets loaded to the cache faster and is available for subsequent requests 
//  Copy an intervening non-CRLF characters up to but not including current 'index'. 
//  String.split returns a single empty result for splitting the empty 
/*  alternate1 = unused  */
//  try to get prime number table size to have less dependence on good hash function 
/*    * Represents a PTF Invocation. Captures:   * - function name and alias   * - the Partitioning details about its input   * - its arguments. The ASTNodes representing the arguments are captured here.   * - a reference to its Input    */
//  Remove from cache if it is a materialized view 
//  iF RS is found remove it and its child (EX) and connect its parent 
//  batches will be sized 11,3,1 
//  5. Run Cleaner. Shouldn't impact anything. 
// for case of conversion, convert both values to common type and then compare. 
//  each partition maintains a large properties 
//  Since warehouse path is non-qualified the table should be located on second filesystem 
//  Sanity check for overlap with regions already being expanded 
//  Construct a temp table name 
//  subsequent K hashes are used to generate K bits within a block of words 
// 4. because there is only one TS for analyze statement, we can get it. 
//  Only for live instances. 
//  to the tasks are not ready yet, the task is eligible for pre-emptable. 
//  Hive Variables 
//  This can happen for numbers less than 0.1   For 0.001234: rawPrecision=4, scale=6   In this case, we'll set the type to have the same precision as the scale. 
//  The percentage of maximum allocated memory that triggers GC   on job tracker. This could be overridden thru the jobconf. 
//  find the privileges that we are looking for 
//  Lazy binary value serializer. 
//  Test that existing shared_read db with new exclusive coalesces to 
// ok since previously opened txn was killed 
//  We may need to update the conditional task's list. This happens when a common map join   task exists in the task list and has already been processed. In such a case,   the current task is the map join task and we need to replace it with   its parent, i.e. the small table task. 
// Other fields are skipped for this case 
//  Don't fail execution due to counters - just don't print summary info 
//  Only check host/port pair is valid, wheter the file exist or not does not matter 
//  4: Add a partition P1 to T2 => 1 event 
//  Combo 2: Literal set, url set to none 
//  return empty string 
//  Bail on an exception - out of the loop. 
//  is called to stop the query if it is running, clean query results, and release resources. 
//  Find tables which name contains _to_find_ in the default database 
//  Key is aggregate of partition values, column name and the value is the col stat object 
//  Try this as a map 
//  String enclosed by single quotes. 
//  Requirements: for SMB, sorted by their keys on both sides and bucketed.   Get key columns 
//  Choose array size. We have two hash tables to hold entries, so the sum   of the two should have a bit more than twice as much space as the   minimum required. 
//  We do this before checking failedUpdate because that might break the iterator. 
//  the output needed for the qfile results. 
//  The zookeeper connection to use 
//  This version of the loop eliminates a condition check and branch   and is measurably faster (20% or so) 
//  Disable new tasks from being submitted 
//  Create the temporary file, its corresponding FileSinkOperaotr, and 
//  Remove failures for tasks that succeeded 
//  get the set of all partition columns in custom path 
//  strip the column name of the targetId 
//  Divide down just before round point to get round digit. 
//  if setAutoCommit is called and the auto-commit mode is not changed, the call is a no-op. 
//  Log(base, Col) is a special case and will be implemented separately from this template 
//  Catch the exceptions, so every other metastore could be stopped as well   Log it, so at least there is a slight possibility we find out about this :) 
//  is the table already present 
//  Remove additional elements if the list is reused 
//                   of 2 
//  Allocate the bean at the beginning - 
//  set the wrong type parameters for prepared sql. 
// Druid storage timestamp column name 
//  VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, "generateHashMapResultSingleValue big table"); 
//  Use path relative to dataDir directory if it is not specified 
//  check if input pruning is enough 
// columns being updated -> update expressions; "setRCols" (last param) is null because we use actual expressions 
//  Conversion to the target data type requires a "helper" target writable in a   few cases. 
//  If partition is null on either of these, then they are claiming to   lock the whole table and we need to check it.  Otherwise, 
//  The ColumnEncoding, column name and type are all irrelevant at this point, just need the   cf:[cq] 
//  If the offset was never added or offset < fileSize. 
//  filter tags for objects 
//  Now the other enum possibility 
//  The list of servers the database/partition/table can locate on   Hive username, for use when creating the user, not for connecting   Hive password, for use when creating the user, not for connecting   Hive database, for use when creating the user, not for connecting 
//  2, we need to connect this cloned parent work with the corresponding child work. 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.newBuilder() 
//  if big alias is partitioned table, it's partition spec + bucket number 
//  Most accurate domain cardinality would be source column NDV if available. 
//  it will be enabled in the CustomVertex. 
//  From https://msdn.microsoft.com/en-us/library/ms190476.aspx   e1 * e2   Precision: p1 + p2 + 1   Scale: s1 + s2 
// CTAS path or insert into file/directory 
//  The free list level the blocks from which we need to merge. 
//  if task.side.metadata is set, rowGroupOffsets is null 
// This is an update statement, thus at any Isolation level will take Write locks so will block 
//  In order to convert from integer to float correctly, we need to apply the float cast not the double cast (HIVE-13338). 
//  Logger debug message from "oproc" after log4j initialize properly 
//  The planner puts a constant field in for the dummy grouping set id.  We will overwrite it 
/*      * Use common decimal to binary conversion method we share with fastBigIntegerBytes.      */
//  compression buffer size should only be set if compression is enabled 
//  Start an instance of HiveServer2 which uses miniMR 
//  Invalidate the entry. Rely on query cleanup to remove from lookup. 
/*    * List    */
//  Collect the needed columns from all the aliases and create ORed filter 
//  check that a property that begins the same is also hidden 
//  We might not be able to assign all rows because of input NULLs.  Start tracking any   unassigned rows. 
//  For complex types like STRUCT, MAP, etc we do not support, we need a writer that   does nothing.  We assume the Vectorizer class has not validated the query to actually   try and use the complex types.  They do show up in inputObjInspector[0] and need to be 
//  Original scan only 
//  Timestamp column type in Druid is timestamp with local time-zone, as it represents   a specific instant in time. Thus, we have this value and we need to extract the   granularity to split the data when we are storing it in Druid. However, Druid stores   the data in UTC. Thus, we need to apply the following logic on the data to extract   the granularity correctly:   1) Read the timestamp with local time-zone value.   2) Extract UTC epoch (millis) from timestamp with local time-zone.   3) Cast the long to a timestamp.   4) Apply the granularity function on the timestamp value.   That way, '2010-01-01 00:00:00 UTC' and '2009-12-31 16:00:00 PST' (same instant)   will end up in the same Druid segment. 
//  Generate new cookie and add it to the response 
//  Guard because ASTNode.getChildren.iterator returns null if no children available (bug). 
//  reconstruct join tree 
//  -----------------------------------------------------------------------------------------------     Filter timestamp against timestamp, long (seconds), and double (seconds with fractional   nanoseconds).      Filter  TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn    Filter  TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   {Long|Double}Column  * Filter  {Long|Double}Col     {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn      Filter  TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampScalar    Filter  TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   {Long|Double}Scalar  * Filter  {Long|Double}Col     {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampScalar      Filter  TimestampScalar      {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn    Filter  TimestampScalar      {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   {Long|Double}Column  * Filter  {Long|Double}Scalar  {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn     ----------------------------------------------------------------------------------------------- 
//  Set the required field. 
//  Enable trash, so it can be tested 
//  if UNION ALL insert, for non-mm tables subquery creates another subdirectory at the end for each union queries   <table-dir>/<staging-dir>/<partition-dir>/<union-dir> 
//  we might have generated a dynamic partition operator chain. Since   we're removing the reduce sink we need do remove that too. 
//  Add the layout to the queryId appender 
//  Note - like vectorizer, this assumes partition columns go after data columns. 
//  Put sample data in the columns. 
//  Set the correct position 
//  5. Add the Partition expressions as the Order if there is no Order and validate Order spec. 
//  Use the example from HIVE-13423 where the integer digits of the result exceed the   enforced precision/scale. 
//  Handle cancellation of the promise. 
//  LlapIoImpl.LOG.info("Setting enc " + i + "; " + colIx + " to " + allEnc.get(i)); 
//  Track as you walk up the tree if there is an operator   along the way that changes the rows from the table through   joins or aggregations. Only allowed operators are selects   and filters. 
//  before 1970 or after 2038. 
//  Convert the join operator to a sort-merge join operator 
//  Per the javadocs on Condition, do not depend on the condition alone as a start gate   since spurious wake ups are possible. 
//  Set the index table information 
//  Conversion is needed? 
//         createdTable.getParameters().get("numFiles")); 
/*  Hash function should map the long value to 0...2^L-1.     * Hence hash value has to be non-negative.      */
//  explicitly disable bit packing 
//  No such database 
//  Based on user-specified parameters, check if the hash table needs to be 
//  For a complex type, a helper object that describes elements, key/value pairs,   or fields. 
//  logs 
//  We don't need to do the check for U[T,Null] here because we'll give the real type   at deserialization and the object inspector will never see the actual union. 
//  Ignore the tag passed in, which should be 0, not what we want 
//  pRS-pGBY-cRS-cGBY 
// ---------------------------------------------------------------------------   Process Single-Column String Outer Join on a vectorized row batch.   
//  Step 2.3: Fill up stuff in local work 
//  Best-effort check; see the comment in the method. 
/*        * Interrupt all threads and verify we get InterruptedException and expected       * Message. Also raise 2 kill operations and ensure that retries keep the time out       * occupied for 4 sec.        */
//  ConditionalTask 
//  Try with null VOID 
//  This one uses the arcsin method. Involves more multiplications/divisions.   pi=Sum (3 * 2n!/(16^n * (2n+1) * n! * n!))   =Sum (3 * ((n+1)(n+2)...2n)/n!*16^n/(2n+1))   =Sum (3 / (2n+1) * (n+1)/16 * (n+2)/32... * 2n/16(n+1))   (note that it is split so that each term is not overflown) 
//  mix of binary/non-binary args 
//  it's resulted from RS-dedup optimization, which removes following RS under some condition 
// doing major compaction - it's possible where full compliment of bucket files is not  required (on Tez) that base_x/ doesn't have a file for 'bucket' 
//  do generic lookup 
//  class Factory; 
//  For reasons I don't understand and am too lazy to debug at the moment the 
//  No partitioned specified for partitioned table, lets fetch all. 
//  Remainder := Dividend 
//  there is no need to continue processing TS[6] branch 
//  functions 
//  #1 - Read the column value 
//  If we have split-update turned on for this table, then the delta events have already been   split into two directories- delta_x_y/ and delete_delta_x_y/.   When you have split-update turned on, the insert events go to delta_x_y/ directory and all   the delete events go to delete_x_y/. An update event will generate two events-   a delete event for the old record that is put into delete_delta_x_y/,   followed by an insert event for the updated record put into the usual delta_x_y/.   Therefore, everything inside delta_x_y/ is an insert event and all the files in delta_x_y/   can be treated like base files. Hence, each of these are added to baseOrOriginalFiles list. 
//  add same jar multiple times and check that dependencies are added only once. 
//  REQUEST 
//  PARTITIONS specified - partitions inside tableSpec 
//  Generated earlier to get possible null(s). 
//  testConvertBooleanToInt() sets HCatConstants.HCAT_DATA_CONVERT_BOOLEAN_TO_INTEGER=true, and   might be the last one to call HCatContext.INSTANCE.setConf(). Make sure setting is false. 
//     LOG.info("har file : " + harFile); 
//  and fetch the sql operation log with FETCH_NEXT orientation 
//  get the highValue 
//  We should not get any rows. 
//  scale down 
//  And, for Complex Types, also leave the children types in place... 
//  The subquery identifier from QB. 
//  test URI with no dbName 
//  Sleep for 5ms and cancel again 
//  3. IO cost = cost of writing intermediary results to local FS +                cost of reading from local FS for transferring to GBy + 
//  If source is local, then source files won't be deleted, and we have to delete them here 
//  Go through the map and print out the stuff 
//  Null first (default for ascending order) 
//  Adapted from SQLStdHiveAuthorizationValidator, only check privileges for LOAD/ADD/DFS/COMPILE and admin privileges 
//  As we always use foreach action to submit RDD graph, it would only trigger one job. 
//  SOURCE_DB 
// builder.join(JoinRelType.INNER, builder.literal(true), variablesSet); 
//  SER_DE 
//  set union operator as child of each of leftOp and rightOp 
//  internal fields 
//  static partition without list bucketing 
// cast it to long to get rid of periodic decimal 
//  Remove from the list. 
//  Unexpected metric type. 
//  Get non null row count from root column, to get max vector batches 
//  comparisons do come from the correlatorRel. 
//  From Tez. Eventually changes over to the LLAP protocol and ProtocolBuffers 
// c6Value = (Map<?,?>) rowValues[5];  assertEquals(2, c6Value.size());  assertEquals("x", c6Value.get(Integer.valueOf(1)));  assertEquals("y", c6Value.get(Integer.valueOf(2))); 
//  test is executed 3 times in worst case 1 original + 2 retries 
//  3rd char starts at index 3, and with length 2 it is covering the rest of the array. 
//  If HIVE_HOME is not defined or file is not found in HIVE_HOME/conf then load default ivysettings.xml from class loader 
//  there should be 5 calls to create partitions with batch sizes of 17, 15, 7, 3, 1 
//  check whether log file is created on test running 
//  Handle skewed value.   if it is skewed value   add directory to path unless value is false 
//  Number of headers (smallest blocks) per target block.   Next free list from which we will be splitting. 
//  Create a default database inside the catalog 
//  Second Row 
/* perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);      basePlan = hepPlan(basePlan, true, mdProvider, executorProvider, SemiJoinJoinTransposeRule.INSTANCE,          SemiJoinFilterTransposeRule.INSTANCE, SemiJoinProjectTransposeRule.INSTANCE);      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER,        "Calcite: Prejoin ordering transformation, Push Down Semi Joins");  */
//  Add a filter to just do a scan on the keys so that we pick up everything 
//  Other counter sources (currently used in LLAP IO). 
//  Now we know where to put row 
//  Mapping from constraint name to list of unique constraints 
//  for managed tables, make sure the file formats match 
//  If semijoin is attempted then replace the condition with a min-max filter   and bloom filter else, 
//  Break if polling times out 
//  Create a GSSContext for authentication with the service. 
//  Notify to clear pending events, if any. 
//  Not a part. 
// assumes line would never be null when this method is called 
/*    * DECIMAL.    */
//  No output type information. 
// 1)  test dropping fields - first middle  & last 
//  Fail - "transactional" property is set to an invalid value 
//  Input metrics. 
//  create the table 
//  called by late-MapJoin processor (hive.auto.convert.join=true for example) 
//     bail out. 
//  FilterCorrelateRule rule mistakenly pushes a FILTER, consiting of correlated vars,   on top of LogicalCorrelate to within  left input for scalar corr queries   which causes exception during decorrelation. This has been disabled for now.  .addRuleInstance(FilterCorrelateRule.INSTANCE) 
//  Our aggregation buffer has nothing in it, so just copy over 'other'   by deserializing the ArrayList of (x,y) pairs into an array of Coord objects 
//  do nothing if this property is not specified or empty 
// process each level in parallel 
/*  Determine if there is a match between big table row and the corresponding hashtable     * Three states can be returned:     * MATCH: a match is found     * NOMATCH: no match is found from the specified partition     * SPILL: the specified partition has been spilled to disk and is not available;     *        the evaluation for this big table row will be postponed.      */
//  do nothing for null object 
//  9. Optimize Physical op tree & Translate to target execution engine (MR, 
//  Not sequential. 
//  Position at first row. 
// read friendly string: [ETX][STX]ak[EXT]av[STX]bk[ETX]bv[STX]ck[ETX]cv[STX]dk[ETX]dv 
//  A mapping from an operator to the columns by which it's output is sorted 
//  implementing applyRowFilterAndColumnMasking 
//  FUTURE: Decide how to ask an input file format what vectorization features it supports. 
//  It will be a BloomFilter in ByteWritable 
//  run given query and validate expected result 
//  For bytes type, it can be mapped to decimal. 
/*          * The transactional listener response will be set already on the event, so there is not need         * to pass the response to the non-transactional listener.          */
//  keys[i] -> ArrayList<exprNodeDesc> for the i-th join operator key list 
/*    * default behavior when neither hive.job.credstore location is set nor   * HIVE_JOB_CREDSTORE_PASSWORD is. In this case if hadoop credential provider is configured job   * config should use that else it should remain unset    */
//  Now, compact -> Compaction produces a single range for both delta and delete delta   That is, both delta and delete_deltas would be compacted into delta_3_5 and delete_delta_3_5 
//  TEST - reset 
//  Noone could have moved it, we have the heap lock. 
//  Populate the complete query with provided prefix and suffix 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setCharacterStream(java.lang.String,   * java.io.Reader, long)    */
//  skip escape 
//  No more places to get the schema from. Give up.  May have to re-encode later. 
/*    * 1. Join condition must be an Equality Predicate.   * 2. both sides must reference 1 column.   * 3. If needed flip the columns.    */
//  These are the output columns for the small table and the outer small table keys. 
//  Shortcut for HDFS. 
//  now we remove all the unions. we throw away any branch that's not reachable from   the current set of roots. The reason is that those branches will be handled in 
//  9. Resolve all the kill query requests in flight. Nothing below can affect them. 
//  tablescan with same alias. 
/*  PaB0  */
//  See if we can use re-encoding to read the format thru IO elevator. 
//  Pass along hashcode to avoid recalculation 
//  base  = JAVA32_OBJECT + PRIMITIVES1 * 2 + JAVA32_FIELDREF;   entry = JAVA32_OBJECT + JAVA32_FIELDREF * 2 
//  Mock out the predicate handler because it's just easier 
//  count(1), 1's position is input.getRowType().getFieldList().size() 
//  retrieve the tables from the metastore in batches to alleviate memory constraints 
//  Remember the condition variables for EXPLAIN regardless. 
//  At this point we've verified the types are correct. 
//  Start small table random generation   from beginning. 
//  create 13 dummy partitions 
//  add empty stats object for each column 
//  use remove instead of get so that it is not parsed again 
//    testLazyBinaryFast(         source, rows,         serde, rowStructObjectInspector,         serde_fewer, writeRowStructObjectInspector,         primitiveTypeInfos,         /* useIncludeColumns */ false, /* doWriteFewerColumns */ true, r); 
//  NOP as there's no caching 
//  Configuration 
//  4) We change the Join operator to reflect this info 
//  Local dirs   ConfVars.LOCALSCRATCHDIR - {test.tmp.dir}/localscratchdir 
//  this will only be available when we are doing table load only in replication not otherwise 
/*  * An multi-key hash map based on the BytesBytesMultiHashSet.  */
//  If it contains a LV 
//  We should never get here. 
//  for future use 
//  Insert transaction entries into MIN_HISTORY_LEVEL. 
//  Find the partition we will be working with, if there is one. 
/*  stage is started, but not complete  */
//  if the table scan is for big table; then skip it 
//  Print all results for standalone SELECT statement 
//  Just digits. 
//  the number of columns output by the UDTF 
//  Updates the references that are present in every operand up till now 
// Oozie does not change the service field of the token  hence by default token generation will have a value of "new Text("")"  HiveClient will look for a use TokenSelector.selectToken() with service 
//  Make sure we're locking the whole table, since this is dynamic partitioning 
//  Must set isNull[i] to false to make sure   it gets initialized, in case we set noNulls to true. 
// After catching an OOM java says it is undefined behavior, so don't  even try to clean up or we can get stuck on shutdown. 
/*      * Create table related objects      */
//  Max characters when auto generating the column name with func name 
//  DBNAME 
//  6) Now we set some tree properties related to multi-insert 
//  v[5] -- since integer #5 is always 0, some products here are not included. 
//  Assert.assertEquals(21, resultDec.integerDigitCount()); 
//  requires to calculate stats if new and old have different fast stats 
/*      * Create a new vectorization context to create a new projection, but keep     * same output column manager must be inherited to track the scratch the columns.      */
//  call further down, we rely upon op.abort(). 
//  Value 
/*    * Given a Work descriptor and the TaskName for the work   * this is responsible to check each MapJoinOp for cross products.   * The analyze call returns the warnings list.   * <p>   * For MR the taskname is the StageName, for Tez it is the vertex name.    */
//  store this in the udf context so we can get it later 
//  Make sure all of the partitions have the catalog set as well 
//  return if output is null because no additional work is needed 
//  complex types (map, list, struct, union) 
//  make an expression for default value 
/*  * This is a copy of GenericUDFNvl, which is built-in. We'll make it a generic * custom UDF for test purposes.  */
//  assertEquals(expectedPartition.getSd().getLocation(),       actualPartition.getSd().getLocation());   we don't compare locations, because the location can still be empty in   the pre-event listener before it is created. 
//  add_partition 
//  clone configuration before modifying it on per-task basis 
//  isolated from the other transaction related RPC calls. 
//  Run worker to delete aborted transaction's delta directory. 
//  ACQUIREDAT 
//  c.set doesn't reset millis 
//  output is sorted 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTimestamp(java.lang.String)    */
// we expect correlated variables in HiveFilter only for now.   Also check for case where operator has 0 inputs .e.g TableScan 
//  This table has no keys. 
//  Try to read from the cache first 
//  Support for schema evolution 
/*  Constructing the row ObjectInspector:     * The row consists of some set of primitive columns, each column will     * be a java object of primitive type.      */
//  2 CHAR test 
//  Generate GroupbyOperator 
//  It is possible that all the async methods returned on the same thread because the   session with registry data and stuff was available in the pool.   If this happens, we'll take the session out here and "cancel" the init so we skip 
//  this offer will be accepted and r1 evicted 
//  this test method is here to do an initial call to parsedriver; and prevent any tests with timeouts to be the first. 
//  Insert into appends to old version 
//  Druid query 
//  Substitution option --hivevar 
//  1. Separate required columns to Non Partition and Partition Cols 
//  Update cross size 
//  Send out the actual SubmitWorkRequest 
//  The sort order contains whether the sorting is happening ascending or descending 
//  Evaluate ELSE expression (only) and copy all its results. 
//  Filter may have sensitive information. Do not send to debug. 
//  Now, compact   One important thing to note in this test is that minor compaction always produces   delta_x_y and a counterpart delete_delta_x_y, even when there are no delete_delta events.   Such a choice has been made to simplify processing of AcidUtils.getAcidState(). 
//  Physical files are resides in local file system in the similar location 
//  7. It may happen that we know we won't use some cache buffers anymore (the alternative      is that we will use the same buffers for other streams in separate calls). 
//  the key is not found in MapColumnVector, set the output as null ColumnVector 
//  TXN_IDS 
//  It was deleted during the transaction 
//  No key, or no nodes in candidate list 
//  The extra parameters will be added on server side, so check that the required ones are   present 
//  Remove semijoin Op if there is any.   The semijoin branch can potentially create a task level cycle   with the hashjoin except when it is dynamically partitioned hash 
//  Search mapping for any strings and return their output columns. 
//  newData.isSetBitVectors() should be true for sure because we   already checked it before. 
//  Put now available buffered batch at end. 
//  Expressions are not supported currently without a alias. 
//  default -> utc   utc 
//  Get any new notification events that have been since the last time we checked,   And pass them on to the event handlers. 
//  Iterate over each clause 
//  Check non null 
//  4. return subquery 
//  Class to store necessary information for an attempt to log 
//  remember map joins as we encounter them. 
//  Verify that the table is created successfully. 
//  Set lambda to 1 so the heap size becomes 1 (LRU). 
/*    * All of the fastSetFrom* methods require the caller to pass a fastResult parameter has been   * reset for better performance.    */
//  sequence number is used to name vertices (e.g.: Map 1, Reduce 14, ...) 
//  Overlay hive-site.xml if it exists 
//  Create Remote MetaStore 
//  We may add NO_AND_STOP in future where combine is impossible and other should not be base. 
/*  * Specialized class for native vectorized reduce sink that is reducing on Uniform Hash * multiple key columns (or a single non-long / non-string column).  */
//     relationship. 
//  nulls in the join keys. 
//  add cStatsTask as a dependent of all the nonStatsLeafTasks 
// now 2  this should block until t1 unlocks 
//  put the mapping task to aliases 
//  This is only called for replication that handles MM tables; no need for mmCtx. 
//  Case 2: is repeating, has nulls 
//  check file system permission 
//  create a snapshot 
//  should we convert? 
//  Add all the public member classes that implement an evaluator 
//  We need to filter i) those that have been pushed already as stored in the join,   and ii) those that were already in the subtree rooted at child 
//  RENEWER 
//  1 running, 1 queued. 
//  Important - no sorting here! We retain order, it's used to match with values at runtime 
//  fetch across schemas 
//  ensure filters are not set from previous pushFilters 
//  The message from remote exception includes the entire stack.  The error thrown from   hive based on the remote exception needs only the first line. 
//  cluster than the default one, but at least for the default case we'd have it covered. 
//  Mapping from column name to Check expr 
//  Our original foo should be in the wrapper 
//  testcase.testWithColumnNumber(count, 25, checkCorrect, codec); 
//  Reset for filling. 
//  HiveServer2 configs that this instance will publish to ZooKeeper,   so that the clients can read these and configure themselves properly. 
//  basic test 
//  for each partition spec, get the partition 
//  Next we locate the aggregation buffer set for each key 
// txn started implicitly by previous statement 
// process user groups for which doAs is authorized 
//  The task will either be killed or is already in the process of completing, which will   trigger the next scheduling run, or result in available slots being higher than 0, 
//  normalize label row 
//  VIEW_ORIGINAL_TEXT 
//  Add LIMIT as Order by-s without limit can disabled for safety reasons 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTimestamp(java.lang.String,   * java.util.Calendar)    */
//  Install the JAAS Configuration for the runtime 
//  Create and set MD provider 
//  1st close: 
//  A node became available. Enable the node and try scheduling. 
//  Get all target paths first, because the number of total target paths   is used to determine number of splits of each target path. 
//  2 VInt 
//  Handle the special cases here. Perhaps we could have a more general structure, or even   a configurable set (like storage handlers), but for now we only have one. 
//  Attempt to make the path in case it does not exist before we check 
//  The values from Timestamp.getNanos(). 
//  function correctly. 
//  outerjoin-pos = other-pos:filter-len, other-pos:filter-len, ... 
//  hive depends on FileSplits, so wrap in HBaseSplit 
//  for AND condition cascadingly update stats 
//  Walk over all the sources (which are guaranteed to be reduce sink   operators).   The join outputs a concatenation of all the inputs. 
//  unknown | T | unknown 
//  If cookie based authentication is allowed, generate ticket only when necessary.   The necessary condition is either when there are no server side cookies in the   cookiestore which can be send back or when the server returns a 401 error code 
//  Do not delete for MM tables. We either want the file if we succeed, or we must   delete is explicitly before proceeding if the merge fails. 
/*  unpartitioned table + no filters  */
//  Create the delta directory.  Don't worry if it already exists,   as that likely means another task got to it first.  Then move each of the buckets.   it would be more efficient to try to move the delta with it's buckets but that is   harder to make race condition proof. 
//  adding this as a child to the Union later 
//  the schema after GB is like this   all keys + sum(c) as a + sum(VCol*c) as b   the column size is the same as unionColumnSize;   (1) for except distinct add a filter (b-a>0 && 2a-b=0)   i.e., a > 0 && 2a = b   then add the project   (2) for except all add a project to change it to   (2b-3a) + all keys   then add the UDTF 
//  use the session or the one supplied in constructor 
//  perform the data read asynchronously 
//  The default, unless SerDe overrides it. 
//  Connect after the lifetime, there should not be any failures 
//  Actual Batch size that will be used 
// close one connection, verify still one left 
//  store into configuration 
//  Change the selected vector 
// no records will be emitted from Hive 
//  Otherwise, expect the user is already logged in 
//  If there are any open txns, then the minimum of min_open_txnid from MIN_HISTORY_LEVEL table 
//  send task off to another jvm 
// Generate test jar files 
// input job properties 
//  and remember link between event and table scan 
//  we checked if partitions matching specification are marked as archived   in the metadata; if they are and their levels are the same as we would   set it later it means previous run failed and we have to do the recovery; 
//  2. Validate that SetOp is feasible according to Hive (by using type 
//  check same filter exists already 
//  stats publishing/aggregating key prefix 
//  No need to set type name, it should always be decimal 
//  The basic idea for CBO support of UDTF is to treat UDTF as a special   project.   In AST return path, as we just need to generate a SEL_EXPR, we just   need to remember the expressions and the alias.   In OP return path, we need to generate a SEL and then a UDTF   following old semantic analyzer. 
//       }        doHarCheck(fs,harFile); 
//  The implementation balks when this method is invoked multiple times 
//  TASK_STATUS 
//  Function to create subCache 
//  Once the conversion is done, we can set the partitioner to bucket cols on the small table 
//  We can just use setKeyProvider() as it is 
//  index==0 means this is key 
//  Transaction manager the Driver has been initialized with (can be null).   If this is set then this Transaction manager will be used during query   compilation/execution rather than using the current session's transaction manager.   This might be needed in a situation where a Driver is nested within an already   running Driver/query - the nested Driver requires a separate transaction manager   so as not to conflict with the outer Driver/query which is using the session 
//  cancel other tasks 
//  Reserve space for potential future list 
//  SCHEDULING_POLICY 
//  Choose max 
//  This better be a generic struct with constant values as the children. 
//  Vectorized doesn't adjust usage for the keys while processing the batch 
//  required for MDC based routing appender so that child threads can inherit the MDC context 
//  It is an outer join. We are going to add the inspector from the   inner side, but the key value will come from the outer side, so   we need to create a converter from inputOI to outputOI. 
//  just to be safe about numRows 
//  Hive has no concept of Avro's fixed type.  Fixed -> arrays of bytes 
//  worst-case, hash aggregation disabled 
//  because inverse[3] is 
//  partitions 
// { "comment":"Hello there", "location":"file:///tmp/warehouse", "properties":{"a":"b"}} 
// should only have one aggregate 
//  If ACID/MM tables, then need to find the valid state wrt to given ValidWriteIdList. 
//  Default to 100,000 partitions if hive.metastore.maxpartition is not defined 
//  Include specified, but this module is not in the set. 
//  The operator is not of RexCall type   So we fail. Fall through.   Add this condition to the list of non-equi-join conditions. 
//  we use the source ordering flavor for the mapping. 
//  insert current common join task to conditional task 
// equals 
// returns 0 if value is NULL 
//  2. Try factoring out common filter elements & separating deterministic   vs non-deterministic UDF. This needs to run before PPD so that PPD can   add on-clauses for old style Join Syntax   Ex: select * from R1 join R2 where ((R1.x=R2.x) and R1.y<10) or 
//  Test that existing shared_read partition with new exclusive coalesces to 
// COMPACTOR_HISTORY_RETENTION_FAILED failed compacts left (and no other since we only have failed ones here) 
//  Output the exit code 
//  Create a transactional table 
//  Try to allocate memory if we haven't allocated all the way to maxSize yet; very rare. 
//  Multi-byte characters with blank ranges. 
// only expect transactional components to be in COMPLETED_TXN_COMPONENTS 
//  BLOCKED_BY_EXT_ID 
//  Set the memory treshold so that we get 100Kb before we need to flush. 
//  name/method name is constant Java String, or constant Text (StringWritable). 
//  List of operation for which we log. 
//  All precision has been lost -- result is 0. 
//  embedded metastore mode 
/*    * Use when calculating intermediate variance and count > 1.   *   * NOTE: count has been incremented; sum included value.    */
// Get the file status up-front for all partitions. Beneficial in cases of blob storage systems 
//  No need to add BucketMapJoinOptimizer twice 
//  Now make sure it's an array of doubles or floats. We don't allow integer types here 
//  Object to receive results of reading a decoded variable length int or long. 
//  We are assuming the update-error AM is bad and just try to kill it. 
//  creating stats table if not exists 
//  generate the temporary file   it must be on the same file system as the current destination 
//  unique id set for operation when run from HS2, base64 encoded value of   TExecuteStatementResp.TOperationHandle.THandleIdentifier.guid 
//  Write the key out 
//  Get 2 different dates 
//  Return the desired VectorExpression if found. Otherwise, return null to cause 
//  Stay with multi-key. 
//  3. Perform a major compaction. Nothing should change. Both deltas and base dirs should have the same name. 
//  so, we need to scale down (this.scale - right.scale - newScale) 
//  verify - throws exception 
/*      * OI of object constructed from output of Wdw Fns; before it is put     * in the Wdw Processing Partition. Set by Translator/Deserializer.      */
/*    * @return A new hash map result implementation specific object.   *   * The object can be used to access the values when there is a match, or   * access spill information when the partition with the key is currently spilled.    */
//  Constructing the row ObjectInspector:   The row consists of some string columns, each column will be a java 
//  Get jobids from job status dir 
//  Output header 
//  SQL usage inside a larger transaction (e.g. droptable) may not be desirable because   some databases (e.g. Postgres) abort the entire transaction when any query fails, so 
//  Scalar subquery 
//  We have already locked the table in DDLSemanticAnalyzer, don't do it again here 
//  the sub-query alias. 
//  Bloom filter false positive probability 
//  If the table exists and we found a valid create table event, then need to drop the table first   and then create it. This case is possible if the event sequence is drop_table(t1) -> create_table(t1).   We need to drop here to handle the case where the previous incremental load created the table but 
//  No grant option in revoke, remove the whole role. 
//  Operator specific logic goes here 
//  TODO: change type to the one in the table schema 
//  Create new join 
//  operation with INSERT/UPDATE 
//  no-op - SBA does not attempt to authorize auth api call. Allow it 
//  do the same thing as setChildren when there is nothing to read.   the setChildren method initializes the object inspector needed by the operators   based on path and partition information which we don't have in this case. 
//  We are making what we are trying to do more explicit if there's a union alias; so 
// cancel other futures 
//  Cannot call class TestCliDriver since that's the name of the generated   code for the script-based testing 
//  sync   total record length   key portion length 
//  hard to know exactly for decimals 
//  Allow string to double conversion 
//  Check for rounding. 
//  We don't need to lookup order_column_id_by_name because we know it   must be "i". 
//  this will be used in RexNodeConverter to create cor var 
//  Check our config value first.  I'm explicitly avoiding getting the default value for now,   as I don't want our default to override a Hive set value. 
//  Append the deserialized standard object row using the current batch size 
//  Ideally, these properties should be part of LlapDameonConf rather than HiveConf 
//  3. Create new TS schema that is a subset of original 
//  Type-specific handling done here 
//  for example, original it is max 0, dist 1, min 2   rs1's schema is key 0, max 1, min 2 
//  Tracks various maps for dagCompletions. This is setup here since stateChange messages 
//  bugbug somewhat fragile below substring expression 
//  If the dbType is "hive", this is setting up the information schema in Hive.    We will set the default jdbc url and driver.   It is overriden by command line options if passed (-url and -driver 
//  Unlikely, but log the actual values in case one of the two was empty/null 
//  If the buffer was pointing to smallBuffer, then nextFree keeps track of the current state   of the free index for smallBuffer. We now need to save this value to smallBufferNextFree 
//  insert SparkHashTableSink and Dummy operators 
//  Create identity projection 
//  push first record of group 
//  called explicitly through FileRecordWriterContainer.close() if dynamic - return false by default 
// this simulates the completion of txnid:idTxnUpdate3 
/*    * @param baseDir if not null, it's either table/partition root folder or base_xxxx.   *                If it's base_xxxx, it's in dirsToSearch, else the actual original files   *                (all leaves recursively) are in the dirsToSearch list    */
//  if UDAF present and if column expression map is empty then it must   be full aggregation query like count(*) in which case number of 
//  detecting failed executions by exceptions thrown by the operator tree 
//  create a table with multiple partitions 
//  string tests 
//  There should now be 3 directories in the location 
//  Build the path from bottom up. pick up list bucketing subdirectories 
// 2) obtain metastore clients 
//  Do the V1 methods of older and newer match? 
//  nulls come first; otherwise nulls come last 
//  just access key and value to ensure they are correct 
//  Check if there are enough entries in the tree to constitute a hint. 
//  fix for sf.net bug 879422 
//  length of green is 5 
//  use the min/max instead of the byte range 
//  first 2 qualify 
//  Don't print full exception trace if DEBUG is not on. 
//  and check HDFS before and after. 
//  Decimal classes cannot be converted by printf, so convert them to doubles. 
//  If the offsets are the same, we assume our initial jump did not cross any DST boundaries, 
// create a lot of locks 
//  Collection methods 
// this simulates the completion of txnid:idTxnUpdate1 
//  Expected 
//  for non-MM tables, the final destination partition directory is created during move task via rename   for MM tables, the final destination partition directory is created by the tasks themselves 
//  move task will create dp final path 
//  Use FragmentRuntimeInfo.newBuilder() to construct. 
//  When there are no exceptions, this has to be called always to make sure incompatible files   are moved properly to the destination path 
//        Remains here as the legacy of the original higher-level interface (getInstance). 
//  Already processed, skip 
//  GROUP_NAMES 
//  Check that the change stuck. 
//  unsupported 
//  test using loadFileWork 
//  schema.setProperty(serdeConstants.SERIALIZATION_FORMAT, 
//  Update. 
//  Default type: all string 
//  PLAN 
//  some other task 
//  4. Make location hints. 
//  now grant all privs to admin 
//  Note: this does not work for embedded channels. 
//  Perform kerberos login using the hadoop shim API if the configuration is available 
//  Aggregate itself should not reference cor vars. 
//  Regardless of other criteria, ducks are always more important than non-ducks. 
// link queryId to txnId 
//  get the SerDe parameters 
//  Find all root TSs and add up all data sizes   Not adding other stats (e.g., # of rows, col stats) since only data size is used here 
//  load the test files into tables 
//  Union expr for distinct keys 
//  avoid calculating modulo 
//  set up the operator plan. (before setting up splits on the AM) 
//  add this filter for deletion, if it does not have non-final candidates 
//  2.3 Determine the index of ob expr in child schema   NOTE: Calcite can not take compound exprs in OB without it being 
//  or columns (not expressions). If yes, proceed. 
//  Test for only partNames being empty 
//  If the matched field is leaf which means all leaves are required, not need to go   deeper. 
//  and return all the dummy parent 
//  we are just converting to a common merge join operator. The shuffle   join in map-reduce case. 
//  a DynamicPartitionCtx to indicate that it needs to dynamically partitioned. 
//  some DDL task that directly executes a TezTask does not setup Context and hence TriggerContext.   Setting queryId is messed up. Some DDL tasks have executionId instead of proper queryId. 
//  date value to boolean doesn't make any sense. 
//  We are not using the key and value contexts, nor do we support a MapJoinKey. 
//  should not have more than 1 load file for CTAS. 
//  set input format information if necessary 
//  Set the configuration parameters 
/*  " => " + bb.hashCode() +  */
//  get the list of task 
//  Try qualifying with current db name for permanent functions 
//  Wait before sending another heartbeat. Otherwise consider as an OOB heartbeat 
//  May need to convert to common type to compare 
//  First column empty 
// Partitioned table 
//  If the queue does not have capacity, it does not throw a Rejection. Instead it will   return the task with the lowest priority, which could be the task which is currently being processed. 
//  Will be true if there are null entries 
//  caller must make sure product of inputs is not too big 
/*  copied over from VectorUDFTimestampFieldLong  */
//  Enforce Hive defaults. 
/*      * Consider a query like:     *     * select -- mapjoin(subq1) --  * from     * (select a.key, a.value from tbl1 a) subq1     *   join     * (select a.key, a.value from tbl2 a) subq2     * on subq1.key = subq2.key;     *     * aliasToOpInfo contains the SelectOperator for subq1 and subq2.     * We need to traverse the tree (using TableAccessAnalyzer) to get to the base     * table. If the object being map-joined is a base table, then aliasToOpInfo     * contains the TableScanOperator, and TableAccessAnalyzer is a no-op.      */
//  nothing that we can really do about it 
//  since the oldname table is not under its database (See HIVE-15059), the renamed oldname table will keep   its location after HIVE-14909. I changed to check the existence of the newname table and its name instead   of verifying its location   assertTrue(tbl.getSd().getLocation().contains("newname")); 
//  iterate through each token, and create appropriate object here. 
//  Assumes the query has already been compiled 
//  CASE CTAS statement 
//  -(2^32) 
//     "TOTAL", "COMPLETED", "RUNNING", "PENDING", "FAILED", "KILLED" 
//  Allow lookup by query string 
/*    * Right trim and truncate a byte array to a maximum number of characters and   * return a byte array with only the trimmed and truncated bytes.    */
//  Minimum 5000 rows per stripe. 
//  container prewarming. tell the am how many containers we need 
//  Ignore the exception, this may be caused by external jars 
// bucket count for test tables; set it to 1 for easier debugging 
//  This function serves as the wrapper of handleInsertStatementSpec in 
//  verify that we can create a table with IF/OF to some custom non-existent format 
//  When doWriteFewerColumns, try to read more fields than exist in buffer. 
//  If IDs 6,7,8 were all aborted and the metadata cleaned up, we would lose the record   of the aborted IDs. In this case we are not able to determine the new WriteIDList has   an equivalent commit state compared to the previous WriteIDLists. 
//  Add the checkpoint key to the Database binding it to current dump directory.   So, if retry using same dump, we shall skip Database object update. 
//  recurse into memoized decorator 
// this deletes the side file 
//  Insert them all before the get requests from this iteration. 
//           The first query has 2 full batches, and the second query only has 1 batch which only contains 1 member 
//  Scratch column information. 
//  If there is any unknown partition, create a map-reduce job for   the filter to prune correctly 
//  MUX operator with 1 parent 
//  Don't override ConfVars with null values 
//  At least one mr/tez/spark job 
//  number of objects in the block before it is spilled 
//  Filter the partitions to show based on on supplied spec 
//  replace original AVG(x) with SUM(x) / COUNT(x) 
//  Alter table "tbl" via ObjectStore 
//  the set of dynamic partitions 
//  Go on to middle word. 
//  Compare required privileges and available privileges for each hive object 
//  We compose the seconds field from two parts. The lowest 31 bits come from the first four 
//  We are in RecordWriter.close() make sense that the context would be   TaskInputOutput. 
//  existing thrift data 
//  by default the bounds checking for maximum number of   dynamic partitions is disabled (-1) 
//  suffix should be a timestamp 
//  find out the vertex for the big table 
//  This is the only place where isQuery is set to true; it defaults to false. 
//  Assumes serialized DAGs within an AM, and a reset of structures after each DAG completes. 
//  Calcite stores timestamp with local time-zone in UTC internally, thus   when we bring it back, we need to add the UTC suffix. 
//  Create the functions, and reload them from the MetaStore 
//  insert into values gets written into insert from select dummy_table   This table is dummy and has no stats 
// "set a=5, b=8" - rhsExp picks up the next char (e.g. ',') from the token stream 
//  Non-vectorized regular ACID reader. 
//  Make filter pushdown information available to getSplits. 
//  return true if this is any kind of float 
//  compute locally and assign 
/*  partitionColumnCount  */
//  each row 
//  minimum 3 seconds 
//  check that a change to the hidden list should fail 
//  The same for rolling the key; re-create the fsm with only the key #2. 
//  if we have traits, and table info is present in the traits, we know the   exact number of buckets. Else choose the largest number of estimated 
//                                                         12345678901234567890123456789 
//  In filter mode, the column must be a boolean 
//  for now make sure that serde exists 
//  "NULL" 
//  re-attach all registered listeners 
//  Serialize Table definition. Deserialize using the target HCatClient instance. 
/*    * This method tries to convert a join to an SMB. This is done based on   * traits. If the sorted by columns are the same as the join columns then, we   * can convert the join to an SMB. Otherwise retain the bucket map join as it   * is still more efficient than a regular join.    */
/*    * Given a TOK_SELECT this checks IF there is a subquery   *  it is top level expression, else it throws an error    */
//  The update is failed and could be retried. 
//  propagate constants 
//  Default: treat the table as a single column "col" 
//  Data types 
//  Set the credential provider passwords if found, if there is job specific password   the credential provider location is set directly in the execute method of LocalSparkClient 
//  Generic UDFs 
//  throw new RuntimeException("duplicate?!"); 
//  This may need to change as the implementation changes. 
//  Case 3: If there's delay for the heartbeat, and the delay is long enough to trigger the reaper,           then the txn will time out and be aborted.           Here we just don't send the heartbeat at all - an infinite delay. 
//  does not support timestamp   TypeInfoToSchema.createAvroPrimitive : UnsupportedOperationException 
//  base  = JAVA32_OBJECT + PRIMITIVES1 * 4 + JAVA32_FIELDREF * 3 + JAVA32_ARRAY;   entry = JAVA32_OBJECT + JAVA32_FIELDREF + PRIMITIVES1 
//  Total: 4/4 running. 
//  the aggregation buffers to use for each key present in the batch 
//  an array of structures containing the n-gram and its estimated frequency. 
//  read authorization does not work with default/legacy authorization mode   It is a chicken and egg problem granting select privilege to database, as the   grant statement would invoke get_database which needs select privilege 
//  For each SparkPartitionPruningSinkOperator, take the target MapWork and see if it is in a dependent SparkTask 
//  This hook verifies that the location of every partition in the inputs and outputs does not   start with the location of the table.  It is a very simple check to make sure the location is   not a subdirectory. 
//  propagate reporter and output collector to all operators 
//  Verify if the auth should fail 
//  Convert the complex LazyBinary objects to standard (Java) objects so downstream   operators like FileSinkOperator can serialize complex objects in the form they expect   (i.e. Java objects). 
//  Converts Date to TimestampTZ. 
//  The BinarySortable serialization of the current key. 
//  each of the ErrorHeuristics. Repeat for all the lines in the log. 
//  the Operator type 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setAsciiStream(int, java.io.InputStream)    */
//  When the repeated no match is due to filtering, we need to restore the   selected information. 
//  5) Modify INSERT branch condition. In particular, we need to modify the 
//  Operation may have been cancelled by another thread 
//  Fetch operator is not vectorized and as such turn vectorization flag off so that 
/*  Convert an integer value in seconds since the epoch to a timestamp value   * for use in a long column vector, which is represented in nanoseconds since the epoch.    */
//  UPDATE_RULE 
//  Track if we still have the entire part. 
//  Cookie based authentication when using HTTP Transport 
//  The table does not have any partitions 
//  Setup output stream to redirect output to 
//  Insert some data -> this will generate only insert deltas and no delete deltas: delta_1_1 
//  pick up unknown case and let and operator handle the rest 
//  We don't use this one. 
/*  256 files x 100 size for 1 splits  */
//  3.1 Create structs 
//  If it is not an inner join, we do not push the 
//  Make sure small table BytesColumnVectors have room for string values in the big table and 
//  use the positions to only pick the partitionCols which are required   on the small table side. 
//  to test whether that is held. 
//  For other registered patterns, find exact matches. 
//  Create a String Appender to capture log output 
//  skip processing has to be done first before continuing 
//  Sorting columns of the parent RS are more specific than those of the   child RS but Sorting order of the child RS is more specific than   that of the parent RS. 
//  instance of TSocket. This is also not set when kerberos is used. 
/*      * 5. Having      */
//  optional string unique_node_id = 2; 
//  We could be here either because its an unpartitioned table or because   there are no pruning predicates on a partitioned table. 
//  TODO: Enable caching for queries with masking/filtering 
//  Publish new segments to metadata storage 
//  Other exceptions which defaults to SPARK_CREATE_CLIENT_ERROR 
//  Not making it configurable for perf reasons (avoid checks) 
//  Prepare children 
//  This creates and publish new segment 
//  Somebody took away our unwanted ducks. 
//  num reduce sinks hardcoded to 0 because TS has no parents 
//  4/ write key-value pairs one by one 
//  WHEN indicator IS NULL 
//  project 
//  The output buffer used to serialize a value into. 
//  Note that db1 and db2 have a table with common name 
//  We collect information in VectorPTFDesc that doesn't need the VectorizationContext.   We use this information for validation.  Later when creating the vector operator   we create an additional object VectorPTFInfo. 
//  Not used 
// test table with db portion 
//  we want to signal an error if the function doesn't exist and we're 
//  mix functions for k1 
//  Now validate transactional_properties for the table. 
//  PTF need a SelectOp. 
//  so as to have their permissions mimic the table permissions 
//  local mode implies that scheme should be "file"   we can change this going forward 
//  if we reach here, it means it needs to do a table authorization   check, and the table authorization may already happened because of other 
//  modified.     
//  met, we are not going to try to merge. 
//  Clean anything from the txns table that has no components left in txn_components. 
/*      * The mapjoin operator will be encountered many times (n times for a n-way join). Since a     * reduceSink operator is not allowed before a mapjoin, the task for the mapjoin will always     * be a root task. The task corresponding to the mapjoin is converted to a root task when the     * operator is encountered for the first time. When the operator is encountered subsequently,     * the current task is merged with the root task for the mapjoin. Note that, it is possible     * that the map-join task may be performed as a bucketized map-side join (or sort-merge join),     * the map join operator is enhanced to contain the bucketing info. when it is encountered.      */
// For partitioned table we always track writes at partition level (never at table)  and for non partitioned - always at table level, thus the same table should never  have entries with partition key and w/o 
/*  256Kb in longs  */
/*          * if there is only one destination in Query try to push where predicates         * as Join conditions          */
/*       This is used to get hold of a reference during the current creation of tasks and is initialized      with "0" tasks such that it will be non consequential in any operations done with task tracker      compositions.        */
/*  @bgen(jjtree) Const  */
// package and compress all the hashtable files to an archive file 
//  No need to iterate more, when threshold is reached   (beneficial especially for object stores) 
//  also clone the colExprMap by default   we need a deep copy 
//  This operator has been removed, remove it from the list of existing operators 
//  Add a db via ObjectStore 
//  Test that fetching a non-existent table-name yields ObjectNotFound. 
/*    * (non-Javadoc)   *    * @see   * org.apache.hadoop.hive.ql.udf.generic.Collector#collect(java.lang.Object)    */
//  spill tables are 
//  Otherwise we have failed; the callback has taken care of the failure. 
/*    * Returns the node on the top of the stack, and remove it from the stack.    */
//  Ignore object, fail if not admin, succeed if admin. 
//  map that keeps track of the last operator of a task to the work 
//  set columns to read in conf 
//  Preserve the selected reference and size values generated 
//  Use the target directory if it is not specified 
// obtain a token by directly invoking the metastore operation(without going  through the thrift interface). Obtaining a token makes the secret manager  aware of the user and that it gave the token to the user  also set the authentication method explicitly to KERBEROS. Since the  metastore checks whether the authentication method is KERBEROS or not  for getDelegationToken, and the testcases don't use  kerberos, this needs to be done 
//  fieldIndex becomes so simple   Note that pos starts from 1 while fieldIndex starts from 0; 
//  For small table RS parents that have already been processed, we need to   add the tag to the RS work to the reduce work that contains this map join.   This was not being done for normal mapjoins, where the small table typically 
//  NOTE: A check for existence of deleteDeltaFile is required because we may not have 
//  Make sure we'll use a different plan path from the original one 
//  testtable1.*: 
//  In case of no partition we have to move each file 
//  Attempt to acquire write resources, waiting if they are not available. 
//  You store the materialized view 
// primitive types 
//  If we decided not to reposition and re-read the buffer to copy it with   copyToExternalBuffer, we we will still be correctly positioned for the next field. 
//  mix functions for k2 
//  DESCRIPTION 
//  Left child 
//  A BlockMissingException indicates a temporary error,   not a corruption. Re-throw this exception. 
//  scale up 
//  with ptfs, there maybe more (note for PTFChains: 
//  Add write hooks if needed. 
//  Tolerate repeated use of a big table column. 
//  check configs are hidden 
//  we weren't provided any actual qualifier name. Set these to 
//  ends up getting rid of Project since it is not used further up the tree 
//  Call Hive.closeCurrent() that closes the HMS connection, causes   HMS connection leaks otherwise. 
//  12. Run rules to aid in translation from Calcite tree to Hive tree 
//  in the form of T partition (ds="2010-03-03")   Not stripping quotes here as we need to use it as it is while framing PARTITION clause   in INSERT query. 
//  early exit, as getting file lengths can be expensive in object stores. 
//  newInstance should always be the same type of object as this 
//  Now notify the executorService that the task has moved to finishable state. 
//  Add some columns 
//  As of Hadoop 2.7 - this is what controls the RM timeout. 
//  implements java.util.Iterator<HCatPartition> { 
//  if the first argument is const then just set the flag and continue 
//  VRB mode - process the VRBs with cache data; the new cache data is coming later. 
//  Replicate the remaining INSERT OVERWRITE operation on the table. 
// creating Path is expensive, so cache the corresponding  Path object in normalizedPaths 
//  remove the condition by replacing it with "true" 
//  Catch the exception, log it and rethrow it. 
//  We need to enforce precision/scale here. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setAsciiStream(java.lang.String,   * java.io.InputStream, int)    */
//  We will estimate collection as an object (only if it's a field). 
//  Project any correlated variables the input wants to pass along. 
// 0  1  2  3  4  5  6  7  8  9  10  11  12  13 
//  Empty map case 
//  the owner can change, also owner might appear in user grants as well   so keep owner privileges separate from userGrants 
/*    * Whether the HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT variable   * (hive.vectorized.use.vectorized.input.format) was true when the Vectorizer class evaluated   * vectorizing this node.   *   * When Vectorized Input File Format looks at this flag, it can determine whether it should   * operate vectorized or not.  In some modes, the node can be vectorized but use row   * serialization.    */
//         LOG.warn("No partition found genereated by dynamic partitioning in ["              +loadPath+"] with depth["+jobInfo.getTable().getPartitionKeysSize()              +"], dynSpec["+dynPathSpec+"]"); 
//  called for each partition of big table and populates mapping for each file in the partition 
/*    * As per JDBC 3.0 Spec (section 9.2)   * "If the Driver implementation understands the URL, it will return a Connection object;   * otherwise it returns null"    */
//  get substring 
//  map-reduce job 
//  Report failure to the main thread. 
// initialize reporters 
//  the only conf allowed to have the metastore pwd keyname is the hidden list configuration   value 
//  We care only about open/aborted txns below currentTxn and hence the size should be determined   for the exceptions list. The currentTxn will be missing in openTxns list only in rare case like   txn is aborted by AcidHouseKeeperService and compactor actually cleans up the aborted txns.   So, for such cases, we get negative value for sizeToHwm with found position for currentTxn, and so, 
//  We need to stay out of the way of any sequences used by the underlying database.   Otherwise the next time the client tries to add a catalog we'll get an error.   There should never be billions of catalogs, so we'll shift our sequence number up 
//  No-op: testing events only 
//  Save the vector description for the EXPLAIN. 
//  Handle a table - populate aliases appropriately:   leftAliases should contain the first table, rightAliases should   contain all other tables and baseSrc should contain all tables 
//  Compute collations 
//  this is a lossy invert of the function above, which produces a hashcode   which collides with the current winner of the register (we lose all higher    bits, but we get all bits useful for lesser p-bit options) 
//  Stop tracking the fragment and re-throw the error. 
// Check if different FileSystems 
// -----------------------------------------------------------------------------------------------   Constructors are marked private; use create methods.  ----------------------------------------------------------------------------------------------- 
//  manufacture a StatsAggregator 
//  Only need to write out & close the delete_delta if there have been any. 
//  Deserialize the fields into the *overflow* batch using the buffered batch column map. 
//  Check the table directory. 
//  Some of the data is set on the server side, so reset those 
//  session identifier 
//  update mappings:   oldInput ----> newInput                    newProject                     |   oldInput ----> newInput     is transformed to     oldInput ----> newProject                     | 
//  we replace existing table. 
//  First time this is seen. Log it. 
//  ^(TOK_LATERAL_VIEW ^(TOK_SELECT ^(TOK_SELEXPR ^(TOK_FUNCTION Identifier["inline"] valuesClause) identifier* tableAlias))) 
//  linking these two operator declares that they are representing the same thing   currently important because statistincs are actually gather for newOp; but the lookup is done using oldOp 
//  0. Additional data structures needed for the join optimization 
//  template, <ClassName>, <ValueType>, <OperatorSymbol>, <DescriptionName>, <DescriptionValue> 
//  Successfully scheduled 
//  GroupBy query results as records 
//  Set appropriate owner/perms of the DB dir only, no need to recurse 
//  nothing to set 
//  It didn't seem useful to create another Constants class just for these though. 
//  Delete table data 
//  due to the way we use the allocation-free cast from HiveDecimalWriter to decimal128,   we do not have the luxury of a ByteBuffer... 
//  The output ObjectInspector is writableStringObjectInspector. 
//  Only print out one task because that's good enough for debugging. 
//  create a row per table name 
/* |  Use | Boundary2.type | Boundary2.amt | Sort Key | Order | Behavior                          || Case |                |               |          |       |                                   ||------+----------------+---------------+----------+-------+-----------------------------------||   1. | PRECEDING      | UNB           | ANY      | ANY   | Error                             ||   2. | PRECEDING      | unsigned int  | NULL     | DESC  | end = partition.size()            ||   3. |                |               |          | ASC   | end = 0                           ||   4. | PRECEDING      | unsigned int  | not null | DESC  | scan backward until row R2        ||      |                |               |          |       | such that R2.sk - R.sk > bnd.amt  ||      |                |               |          |       | end = R2.idx + 1                  ||   5. | PRECEDING      | unsigned int  | not null | ASC   | scan backward until row R2        ||      |                |               |          |       | such that R.sk -  R2.sk > bnd.amt ||      |                |               |          |       | end = R2.idx + 1                  ||   6. | CURRENT ROW    |               | NULL     | ANY   | scan forward until row R2         ||      |                |               |          |       | such that R2.sk is not null       ||      |                |               |          |       | end = R2.idx                      ||   7. | CURRENT ROW    |               | not null | ANY   | scan forward until row R2         ||      |                |               |          |       | such that R2.sk != R.sk           ||      |                |               |          |       | end = R2.idx                      ||   8. | FOLLOWING      | UNB           | ANY      | ANY   | end = partition.size()            ||   9. | FOLLOWING      | unsigned int  | NULL     | DESC  | end = partition.size()            ||  10. |                |               |          | ASC   | scan forward until row R2         ||      |                |               |          |       | such that R2.sk is not null       ||      |                |               |          |       | end = R2.idx                      ||  11. | FOLLOWING      | unsigned int  | not NULL | DESC  | scan forward until row R2         ||      |                |               |          |       | such R.sk - R2.sk > bnd.amt       ||      |                |               |          |       | end = R2.idx                      ||  12. |                |               |          | ASC   | scan forward until row R2         ||      |                |               |          |       | such R2.sk - R2.sk > bnd.amt      ||      |                |               |          |       | end = R2.idx                      ||------+----------------+---------------+----------+-------+-----------------------------------|    */
//  check that the agg is of the following type: 
//  is this a null?   only read the is-null byte for level > 1 because the top-level struct   can never be null. 
//  Variance check 
//  Iterate thru all the filecaches. This is best-effort.   If these super-long-lived iterators affect the map in some bad way, 
//  write it to file: 
// add the privileges not supported in V1  The list of privileges supported in V2 is implementation defined, 
//  The code inside the attribute getter threw an exception so log it,   and fall back on the class name 
//  initialize a complete map reduce configuration 
//  Current Hive parquet timestamp implementation stores timestamps in UTC, but other   components do not. In this case we skip timestamp conversion.   If this file is written by a version of hive before HIVE-21290, file metadata will   not contain the writer timezone, so we convert the timestamp to the system (reader)   time zone.   If file is written by current Hive implementation, we convert timestamps to the writer   time zone in order to emulate time zone agnostic behavior. 
//  If the table is sorted on a partition column, not valid for sorting 
//  After committing the initial txns, and updating current number of open txns back to 0, 
//  3.3.2 Get UDAF Info using UDAF Evaluator 
//  By default don't convert to unix 
//  Timeout for the iteration in case of asynchronous execute 
//  (2^128 - 1) * 10^-39 
//  non-cbo path retries to execute subqueries and throws completely different exception/error   to eclipse the original error message   so avoid executing subqueries on non-cbo 
//  Create an archived version of the partition in a directory ending in   ARCHIVE_INTERMEDIATE_DIR_SUFFIX that's the same level as the partition,   if it does not already exist. If it does exist, we assume the dir is good 
//  Noticed that we also suffer from the same issue as HIVE-3179   Only want to call a field init'ed when it's non-NULL   Check it twice, make sure we get null both times 
//  This can only happen in case of failure - we read some data, but didn't decompress   it. Deallocate the buffer directly, do not decref. 
//  Flush the print stream, so it doesn't include output from the last command 
//  test February of non-leap year, 2/31 is viewd as 3/3 due to 3 days diff   from 2/31 to 2/28 
//  Timeouts are bad... mmmkay. 
//  TOKEN_STR_FORM 
// convert the set into list 
//  If HIVE_LOCAL_TASK_CHILD_OPTS is set, child VM environment setting   HADOOP_CLIENT_OPTS will be replaced with HIVE_LOCAL_TASK_CHILD_OPTS.   HADOOP_OPTS is updated too since HADOOP_CLIENT_OPTS is appended   to HADOOP_OPTS in most cases. This way, the local task JVM can 
//  The channel listener instantiates the Rpc instance when the connection is established, 
//  assuming that this closes the underlying streams 
/*      * Restriction.16.s :: Correlated Expression in Outer Query must not contain     * unqualified column references.     * disabled : if it's obvious, we allow unqualified refs      */
//  column family is mapped to Map<string,string> 
//  Nothing we can do here, so just proceed normally from now on 
//  verify that hiveserver2 config is not loaded 
//  open transactions. 
//  If we do not reduce the input size, we bail out 
//  prevents a task from being processed multiple times 
//  ROOT_PATH 
//  Testing with multiByte String 
/*  @bgen(jjtree) ConstList  */
//  The current non-NULL key position. 
//  No implicit cast needed 
//  conf validator already checks this, so it will never trigger usually 
//  Ensure the session is open and has the necessary local resources. 
//  test conversion of long->string 
//  We might be visiting twice because of reutilization of intermediary results.   If that is the case, we do not need to do anything because either we have   already connected this RS operator or we will connect it at subsequent pass. 
//  Skip TOK_QUERY. 
//  6. We register both so we do not fire the rule on them again 
//  DAG might have been killed, lets try to get vertex state from AM before dying 
//  bigTableFound means we've encountered a table that's bigger than the 
//  has tag => need to set later 
//  mask this digit 
//  First, add all children of this work into queue, to be processed later. 
//  compare with 5 * 10**-tenScale   example: tenScale=-1. o will be zero after scaling if o>=5. 
//  2a. This is a decoded compression buffer, add as is. 
/*  Set total number of rows from all in memory partitions  */
//  In verbose mode, print an update per RECORD_PRINT_INTERVAL records 
//  map work starts with table scan operators 
//  At this point, everything in the list is going to have a refcount of one. Unless it   failed between the allocation and the incref for a single item, we should be ok.  
//  UNDERSCORE_INT 
//  exhausted all delete records, return. 
//  Special treatment for Filter operator that ignores the DPP predicates 
//  Create row related objects 
//  re-use existing text member in varchar writable 
//  For native vectorized map join, we require the key SerDe to be BinarySortableSerDe   Note: the MJ may not really get natively-vectorized later,   but changing SerDe won't hurt correctness 
//  Update the partition columns in small table to ensure correct routing of hash tables. 
//  Escape the escape 
//  Pre-allocated member for storing index into the hashMultiSetResults for each spilled row. 
//  Determine mapping between project input and output fields.    In Hive, Sort is always based on RexInputRef   We only need to check if project can contain all the positions that sort needs. 
//  ADJACENCY_LIST 
//  Close client session 
/*    * These parameters controls the maximum number of concurrent job submit/status/list   * operations in templeton service. If more number of concurrent requests comes then   * they will be rejected with BusyException.    */
//  output has noNulls set to false so set the isNull[] to false carefully 
//  recursively call the join the other rhs tables 
//  the actual size will be assigned in setChildrenInfo() after reading complete. 
//  It is not primitive; check if it is a struct and we can infer a common class 
//  Check if any of the txns in the list is committed. If yes, throw exception. 
//  Tried scheduling everything that could be scheduled in this loop. 
//  the MR job for compaction 
//  destf 
// Test for duplicate publish -- this will either fail on job creation time   and throw an exception, or will fail at runtime, and fail the job. 
//  There should be only 1 directory left: base_xxxxxxx. 
//  If the plan for this reducer does not exist, initialize the plan 
//  Test replicated drop, should drop this time, since repl.state.id < evid. 
//  Go on to high word. 
//  2 minutes. 
//  insert a select operator here used by the ColumnPruner to reduce 
//  create split for the previous unfinished stripe 
// could acquire 1 table level Shared_write intead 
//  first one will fail - count it in 
//  Create all nulls key. 
//  ORDERING 
//  struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint 
//  High and middle word must be zero.  Check for overflow digits in lower word. 
//  External LLAP clients would need to set LLAP_ZK_REGISTRY_USER to the LLAP daemon user (hive),   rather than relying on RegistryUtils.currentUser(). 
//  The vertex cannot be configured until all DataEvents are seen - to 
//  Use QueryCompleteResponseProto.newBuilder() to construct. 
//  swap debug options in HADOOP_CLIENT_OPTS to those that the child JVM should have 
//  which can affect the working of all downstream transformations. 
//  We estimate the same way for compressed and uncompressed for now. 
//  optional int32 num_self_and_upstream_tasks = 1; 
//  If any child work for this work is already added to the targetWork earlier,   we should connect this work with it 
//  Reader creation updates HDFS counters, don't do it here. 
//  Second granularity 
//  Try non-chunked stream. There should be no issues assuming we flushed the streams before closing. 
//  compose a query that select transactions containing an update... 
// delegate to the new api 
//  col > 1 
//  We use Spark RDD async action to submit job as it's the only way to get jobId now. 
//  invoke the right unpack method depending on data type of the column 
//  We couldn't do JDOQL filter pushdown. Get names via normal means. 
//  core pool size 
//  End HiveMetaHookLoader.java 
//  prefix used to auto generated column aliases (this should be started with '_') 
//  Nothing to compact, update expr with compacted children. 
//  Case when user has not specified any ingestion state in the current command   if there is a kafka supervisor running then keep it last known state is START otherwise STOP. 
//  This is a local file 
//  Not applicable. 
//  This mapping collects all the configuration variables which have been set by the user   explicitly, either via SET in the CLI, the hiveconf option, or a System property.   It is a mapping from the variable name to its value.  Note that if a user repeatedly 
//  avoid traversing the tree later. To save memory, this could be an array (of byte arrays?). 
// fetch the row inserted before schema is altered and verify 
//  Call the metastore to get the status of all known compactions (completed get purged eventually) 
// txnId=0 means it's a select or IUD which does not write to ACID table, e.g  insert overwrite table T partition(p=1) select a,b from T and autoCommit=true 
/*  allowComplex  */
//  Temporarily.... 
// See HCATALOG-499 
//  Casts 
//  Also print out the generic lineage information if there is any 
//  This tells the pending update (if any) that whatever it is doing is irrelevant,   and also makes sure we don't take the duck back twice if this is called twice. 
//  Save previous longword. 
//  initialize some variables, which used to be initialized in CommonJoinOperator 
//  the default fraction 
//  return the passed in string value 
//  cast int to double. 
//  remove ${ .. } 
//  Prepare output, set the projections 
// we don't deal with columns on RHS of SET expression since the whole expr is part of the  rewritten SQL statement and is thus handled by SemanticAnalzyer.  Nor do we have to  figure which cols on RHS are from source and which from target 
//  type interval_day_time (IntervalDayTimeColumnVector). 
//  this function is for internal use only 
//  Cache mkey.group(1) 
//  The current filters we use in ReplicationSemanticAnalyzer is as follows:      IMetaStoreClient.NotificationFilter evFilter = EventUtils.andFilter(          EventUtils.getDbTblNotificationFilter(dbNameOrPattern, tblNameOrPattern),          EventUtils.getEventBoundaryFilter(eventFrom, eventTo),          EventUtils.restrictByMessageFormat(MessageFactory.getInstance().getMessageFormat()));   So, we test each of those three filters, and then test andFilter itself. 
//  Define how to pass options to the child process. If launching in client (or local)   mode, the driver options need to be passed directly on the command line. Otherwise, 
//  cleanup pathToPartitionInfo 
//  adjust noconditional task size threshold for LLAP 
//  direct and not memory mapped 
//  rs1 --- remove distinctColIndices, set #reducer as -1, reset keys, 
//  Trigger post compilation hook. Note that if the compilation fails here then   before/after execution hook will never be executed. 
//  2 original files, 1 delta directory, 1 delete_delta directory and 1 base directory 
//  create an new operator: HashTableDummyOperator, which share the table desc 
//  finally create the vertex 
//  Do not use datetime in tests to avoid result changes. 
//  We keep track of all the contexts that are created by this query   so we can clear them when we finish execution 
//  contract on EOF differs between DataInput and InputStream 
//  Get the partition columns from the end of derivedSchema. 
//  to a smaller prefix (MD5hash/000000_0) and later will stored as such in staging stats table.   When stats gets aggregated in StatsTask only the keys that starts with "prefix" will be fetched.   Now that (prefix/ds=__HIVE_DEFAULT_PARTITION__) is hashed to a smaller prefix it will   not be retrieved from staging table and hence not aggregated. To avoid this issue   we will remove the taskId from the key which is redundant anyway. 
//  If the file is held by a writer, will throw AlreadyBeingCreatedException 
//  Cache uses allocator to allocate and deallocate, create allocator and then caches. 
//  This test with HDFS ACLs will only work if FileSystem.access() is available in the   version of hadoop-2 used to build Hive. 
// Block until all semaphore resources are released  by outstanding async writes 
//  try singular 
//  PARENT_TBL_NAME 
//  archiving / un-archiving process. 
//  the epoch 
//  If the V2 api of authorizer in use, the session state getAuthorizer return null.   Here we disable authorization if we use V2 api or the DefaultHiveAuthorizationProvider   The additional authorization checks happening in hcatalog are designed to   work with  storage based authorization (on client side). It should not try doing   additional checks if a V2 authorizer or DefaultHiveAuthorizationProvider is in use.   The recommended configuration is to use storage based authorization in metastore server.   However, if user define a custom V1 authorization, it will be honored. 
//  Don't allow swapping between virtual and materialized view in replace 
//  Server will create new threads up to max as necessary. After an idle   period, it will destroy threads to keep the number of threads in the 
// runStatementOnDriver("truncate table Tstage"); 
// we don't use the HadoopJobExecHooks for local tasks 
//  Try pre-empting a task so that a higher priority task can take it's place. 
//  Evaluate the column as a boolean, converting if necessary. 
//  We expect the start at 0 and count divisible by step. 
//  repeating case for first (boolean flag) argument to IF 
//  in both cases, we move the file under destf 
//  Stream offset in relation to the stripe.   1.1. Figure out which columns have a present stream 
//  Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll   write out formatted thrift objects to SequenceFile 
//  IS_EXTENDED 
//  Declare this method as final for performance reasons 
// ---------------------------------------------------------------------------   Multi-Key specific members.   
//  Copy the data to the buffer 
/*    * We use protected for the fields so the FastHiveDecimalImpl class can access them.  Other   * classes including HiveDecimal should not access these fields directly.    */
//  TODO pass on this exception 
//  Use the rowID directly 
//  Finally, start the server 
//  Don't record encodings for unneeded columns. 
//  2. Skip overwriting exisiting table object   (which is present because it was added after prewarm started) 
//  sanity checks 
//  ensure there is no operation related object leak 
//  not actually a getter 
//  this correlation to MuxOperators 
//  verify zero-divide result for position 0 
//  Parse until field separator (currentLevel). 
//  In spark local mode, we need to search added files in root directory. 
//  The following loop should create 20 stripes in the orc file. 
//  read split 
//  Put the key/value into the map 
//  if not using position alias and it is a number. 
//  Binary sortable key serializer. 
//  Must be deterministic order map for consistent q-test output across Java versions 
//  for unit tests 
//  Pass lineageState when a driver instantiates another Driver to run 
//  still nothing, Raise exception 
//  this operation. 
//  Needed to intercept readClassAndObject. 
//  production: this.name | BaseType() | MapType() | SetType() | ListType() 
//  PART_NAME 
//  currently getPrimaryKeys always returns an empty resultset for Hive 
//  ////// Generate GroupbyOperator for a map-side partial aggregation 
//  in the general case. This set restricts automatic type conversion to just these functions. 
// The default org.apache.hadoop.hive.ql.hooks.PreExecutePrinter hook 
//  // Currently avg(distinct) not supported in PartitionEvaluator 
//  Otherwise the registry has not been initialized, skip for the time being 
//  Lazy object inspectors for string/char/varchar will all be cached in the same map. 
//  See include/uapi/linux/stat.h 
//  getFunctions() 
//  skip the step to connect to the metastore. 
//  Grab round digit from middle word. 
//  This is a full outer join. This can never be a map-join   of any type. So return false. 
//  We include failedUpdate only after looking at all the tasks at the same priority. 
//  get a evaluator for a string concatenation expression 
//  issue as would happen is there was a tiny delay on the network, so we don't care. 
/*  (non-Javadoc)  * @see org.apache.hadoop.mapreduce.lib.output.FileOutputFormat#getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)   */
// For partitions, flag controlling whether the current  table specs are to be used 
//  generate the temporary file 
// All must be selected otherwise size would be zero. Repeating property will not change. 
// reset keyInited[mapSize] flag, since it may be set to true in the case of previous empty entry 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.newBuilder() 
//  Remove the dummy store operator from the tree 
//  Check if the function is really removed 
/*    * (non-Javadoc)   *   * @see javax.sql.CommonDataSource#getLogWriter()    */
//     addBaseFile(t, p, 20L, 20); 
//  We have to set up the bucketing columns differently for update and deletes, 
//  Here are some negative cases as below : 
//  Use LinkedHashSet to give predictable display order. 
//  Note: we could skip creating the table and just add table type stuff directly to the 
/*    * This is the same as the setChildren method below but for empty tables.   * It takes care of the following:   * 1. Create the right object inspector.   * 2. Set up the childrenOpToOI with the object inspector.   * So as to ensure that the initialization happens correctly.    */
//  varchar should take string length into account.   varchar(5), varchar(10) => varchar(10) 
//  confirm the batch sizes were as expected 
//  TXNS 
//  Discard the blocks. 
//  If partition columns occur in data, we want to remove them. 
//  Note: we pass in null factory because we allocate objects here. We could also pass a         per-call factory that would set fileKey; or set it after put. 
//  same as in getRecordReader? 
//  Check the cache first 
//  the deserializer is responsible for actually reading each record from   the stream 
//  in case of select(*) the data size does not change 
//  Init parse context 
//  Sort the objects first. You are guaranteed that if a partition is being locked,   the table has already been locked 
//  Revoke with grant option - only remove the grant option but keep the role. 
//  create output row ObjectInspector 
//  Generate (possibly get from a cached result) parent SparkTran 
//  SUCCEEDED state 
//  This function returns the grouping sets along with the grouping expressions   Even if rollups and cubes are present in the query, they are converted to 
//  Worthwhile only if more than 1 split, consistentGroupingEnabled and is a FileSplit 
//  exclude insert queries 
//  Add TINYINT values 
//  "key = 'val'" 
//  verify that the actual action also went through 
//  After one exception everything is expected to run 
//  skip the next child, since we already took care of it 
//  Excepted 
//  let's add a lot of constant rows to test the rle 
//  Set the cookie max age to a very low value so that 
//  change the key if need be 
//  nothing to do if there is not a index definition for this table 
//  First breaking up the filter conditions into equality   comparisons between rightJoinKeys(from the original   filterInputRel) and correlatedJoinKeys. correlatedJoinKeys   can be expressions, while rightJoinKeys need to be input 
//  Get most of the fields for the IDs provided. 
//  This should be called rarely enough; for now it's ok to just lock every time. 
//  The object [count LongWritable, sum ResultType] is reused during evaluating 
//  All parents should be reduce sinks. We pick the one we just walked   to choose the number of reducers. In the join/union case they will   all be -1. In sort/order case where it matters there will be only   one parent. 
//  clear out any parents as reducer is the   root 
//  final string 
//  wait for stream threads to finish 
//  It would be possible to support this, but this is such a pointless command. 
//  [-S|--silent] 
//  Release all the locks acquired for this object   This becomes important for multi-table inserts when one branch may take much more   time than the others. It is better to release the lock for this particular insert.   The other option is to wait for all the branches to finish, or set   hive.multi.insert.move.tasks.share.dependencies to true, which will mean that the   first multi-insert results will be available when all of the branches of multi-table 
//  We replace an earlier element, must have lower offset. 
//  validate Unset Non Existed Table Properties 
//  We first do exact match, and then do prefix matching. The latter is due to input dir   could be /dir/ds='2001-02-21'/part-03 where part-03 is not part of partition 
//  Set the relevant information in the Configuration for the AccumuloInputFormat 
//  TODO: vcpu settings - possibly when DRFA works right 
//  We allocate triples, so we cannot go above highest Integer power of 2 / 6. 
//  Use NumDistinctValues if possible 
//  replace the map join operator to local_map_join operator in the operator tree 
//  path format -- > .../dataSource/interval/version/partitionNum/xxx.zip 
//  Partition columns are repeated -- so we test element 0. 
//  swap x and t1[h1(x)] 
//  Test that existing shared_write table with new shared_write coalesces to 
//  alter partitioned table, rename partition 
// 1 row is sufficient to know we have to kill the query 
//  average value size will be sum of all sizes of aggregation buffers 
//  Call addTranslation just to get the assertions for overlap 
//  set up local work 
//  If normalize() was used, then day-hour-minute-second-nanos should have the same sign.   This is currently working with that assumption. 
//  Validate the IN items are only constants. 
// -1 indicates malformed version. 
//  For a query of the type:   insert overwrite table T1   select * from (subq1 union all subq2)u;   subQ1 and subQ2 write to directories Parent/Child_1 and   Parent/Child_2 respectively, and union is removed.   The movetask that follows subQ1 and subQ2 tasks moves the directory   'Parent' 
// convert to lower case in case we are getting from serde 
/*    * callback method used by subclasses to set the OutputOI on the Evaluator.    */
//  The real implementation for the instanceset... instanceset has its own copy of the   ZK cache yet completely depends on the parent in every other aspect and is thus unneeded. 
// from pre-acid insert 
//  There should be different txn IDs associated with each lock. 
//  ========= Master thread methods 
//  list of columns, comma separated 
//  \1 followed by each element 
//  Definitely a byte; most bytes fall here 
//        supports all types 
//  create a standard settable union object inspector 
//  see also - code in CliDriver.java 
//  This chicanery is to get around the fact that the table needs to be final in order to 
//  Nothing special needs to be done for grouping sets if   this is the final group by operator, and multiple rows corresponding to   the   grouping sets have been generated upstream.   However, if an addition MR job has been created to handle grouping sets,   additional rows corresponding to grouping sets need to be created here. 
//  Ignore other types for purposes of authorization 
//  Closed from ORC writer, we still need the data. Do not discard anything. 
//  Input #1 is type date (epochDays). 
//  2. CPU cost = sorting cost (for each relation) + 
//  Char 
//  TODO: If the DB name from the creation metadata for any of the tables has changed, 
//  This won't usually be called otherwise. 
// 'f' is the file whence this split is 
//  Pop (list, map) 
//  call-1: listLocatedStatus - mock:/mocktable   call-2: check side file for mock:/mocktbl1/0_0   call-3: open - mock:/mocktbl1/0_0   call-4: check side file for  mock:/mocktbl1/0_1 
//  find the right op 
//  operations will be lost once owning session is closed. 
//  Then try SERDEPROPERTIES 
//  As of now only used for Bucket MapJoin, there is exactly one event in the list. 
//  if any the fields of struct are representing null, then return true 
//  class HCatAddPartitionDesc; 
//  AST's are slightly different. 
//  Whether the method takes variable-length arguments   Whether the method takes an array like Object[],   or String[] etc in the last argument. 
// -------------------------------------------------------------------------------------------- 
//  For performance don't check that that the fieldRef isn't recId everytime,   just assume that the caller used getAllStructFieldRefs and thus doesn't have that fieldRef 
//  Recheck. 
//  (Currently none)   innerBigOnlyPerBatchSetup(batch); 
//  note that set basic stats false will wipe out column stats too. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBinaryStream(int, java.io.InputStream,   * int)    */
//  Subtraction with overflow check. Overflow produces NULL output. 
//  UGI for the hive/_HOST (kerberos) principal 
//  if archiving was done at this or at upper level, every matched   partition would be archived, so it not being archived means   no archiving was done neither at this nor at upper level 
//  enabled for an ACID case and the file format is ORC. 
// create LazyStruct with serialized string with expected separators 
//  outputStream == null means we need to process it for explain formatted 
//  Prefix used to specify module specific properties. Mainly to avoid conflicts with older unitTests properties 
//  The aggregation buffer already contains a partial histogram. Therefore, we need   to merge histograms using Algorithm #2 from the Ben-Haim and Tom-Tov paper. 
//  Incase of ACID, the file is ORC so the extension is not relevant and should not be inherited. 
//  New partition for example 
//  COL_VALS 
//     hiveConf.setBoolVar(HiveConf.ConfVars.MERGE_CARDINALITY_VIOLATION_CHECK, true); 
//  bucketed mapjoin cannot be performed 
//  constants for SPARSE encoding 
//  Another thread might have already created these tables. 
//  optional int32 version = 2; 
//  There's a bug in ZKDelegationTokenSecretManager ctor where seconds are not converted to ms. 
//  so clear timing in this thread's Hive object before proceeding. 
//  Assign tables without nested column pruning info to the default conf 
//  requires to calculate stats if new partition doesn't have it 
//  test repeating case for null value 
/*                * Single-Column Long specific save key.                */
//  If this table is working with ACID semantics, turn off merging 
//  3. Get Right Table Alias 
//  Initialize the list of event handlers 
//  Since a key expression can be a calculation and the key will go into a scratch column, 
//  A do nothing vectorized expression that passes all rows through. 
//  Load the current incremental dump and ensure it does nothing and lastReplID remains same 
//  IMPORTANT IMPORTANT IMPORTANT!!!!!  The keys used to store info into the job Configuration.  If any new keys are added, the HCatStorer needs to be updated. The HCatStorer  updates the job configuration in the backend to insert these keys to avoid  having to call setOutput from the backend (which would cause a metastore call 
//  if there is an equivalent version, return that, else return this version 
//  Now set one column nullable 
/*    * Test the unsecure base case when neither hadoop nor job-specific   * credential provider is set    */
//  REGULAR CREATE TABLE DDL 
//  Removing semijoin optimization when it may not be beneficial 
//  more then one part exist 
/* skip cardinality violation clause */
//  and it is not using this privilege mapping, but it might make sense to move it here 
//  Nanos converted to millis 
//  If sleep fails, we should exit now before things get worse. 
//  Parser enforces that table alias is added, but check again 
//  Make sure the basic query properties are initialized 
//  BRound without digits 
//  The hiveJarDir can be determined once per client. 
//  clear the mapredWork output file from outputs for CTAS   DDLWork at the tail of the chain will have the output 
//  3. Allocate the buffers, prepare cache keys. 
//  we compress a fastbitset to 4 bytes 
/*  PbB1  */
//  Cannot rebuild not materialized view 
//  Get notifications from metastore 
//  Clean up the cache 
// no need to (re-)initialize the currentUserName, currentRoles fields 
//  Cluster 
//  Create a table so we can work against it 
//  Ideally, this should never happen, and this should be an assert. 
//  but default partition 
//  should not be done for semijoin since it will change the semantics   Invert join inputs; this is done because otherwise the SemanticAnalyzer   methods to merge joins will not kick in 
//  no negative scale decimals 
//  required bytes input_event_proto_bytes = 1; 
//  get the VInt that represents the map size 
//  If SARG is present, get relevant stripe metadata from cache or readers. 
//  BytesWritable valueBytesWritable = (BytesWritable) valueWritable;   LOG.info("VectorReduceSinkCommonOperator collect keyWritable " + keyWritable.getLength() + " " +       VectorizedBatchUtil.displayBytes(keyWritable.getBytes(), 0, keyWritable.getLength()) +       " valueWritable " + valueBytesWritable.getLength() +       VectorizedBatchUtil.displayBytes(valueBytesWritable.getBytes(), 0, valueBytesWritable.getLength())); 
// here means last txn in the batch is resolved but the close() hasn't been called yet so  there is nothing to heartbeat 
//  tolerance to check float equality 
// compute stats before compaction 
//  2. We swap the join 
//  Position before the last written value. 
//  Followed by key and non-key input columns (some may be missing). 
//  non-bean fields needed during compilation 
//  Convert the remainders into a list that are AND'ed together. 
//  0 size means no-pooling case - passthru. 
//  NOTE: It is critical to do this here so that log4j is reinitialized 
//  Note that inputs and outputs can be changed when the query gets executed 
//  output string. If no such replacement exists, emit out the original input code point 
//  ignore if it is a constant 
//  100 <= x   start inclusive to infinity 
//  avoid concurrent modification 
//  create "virtual" row type for project only rename fields 
//  Do the removal before we change the element, to avoid invalid queue ordering. 
//  second copy of red, different object 
// nothing actually hashes to bucket0, so update/delete deltas don't have it 
// this is not likely to do the right thing for Compaction of "original" files when there are copyN files 
//  table is not partitioned 
//  We know we never went that far when we were inserting. 
//  in this case, we are actually scaling up.   we don't have to do complicated things because doing scaling-up   after   multiplication doesn't affect overflow (it doesn't happen or   happens anyways). 
//  try to find in this input rel the position of cor var 
// create input BytesWritable. This would have capacity greater than length)  
// check we have right delete delta files after minor compaction 
//  Remove the node if it has expired 
//  If it is bucketing version, skip it 
/*  Maximum length seen so far  */
//  convert to List as above Set was created using Sets.union (for reasons   explained there) 
//  FOREIGN_KEYS 
//  provide the path to the field in the error message 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.newBuilder() 
//  This is admittedly a bit simple, StatsObjectConverter seems to allow   old stats attributes to be kept if the new values do not overwrite them. 
//  Set non-hdfs tables to external, unless transactional (should have been checked before this). 
//  set the number of buckets here to ensure creation of empty buckets 
//  see ColumnPrunerGroupByProc 
//  corrupt last entry 
// TODO check whether rowGroupOffSets can be null   then we need to apply the predicate push down filter 
// volatile because heartbeat() may be in a "different" thread; updates of this are "piggybacking" 
//  by position in the row schema of the filesink operator. 
//  we can just stop all the sessions 
//  length of "mixedUp" 
//  Save original values 
/*  Changes the type of the input references to adjust nullability  */
/*  * Specialized class for doing a vectorized map join that is an outer join on a Single-Column Long * using a hash map.  */
//  For multi-insert query, currently we only optimize the FROM clause.   Hence, introduce multi-insert token on top of it.   However, first we need to reset existing token (insert).   Using qbp.getClauseNamesForDest().size() >= 2 would be   equivalent, but we use == to avoid setting the property   multiple times 
//  Verify ORC SARG still works. 
//  opening the META table ensures that cluster is running 
//  Simple case - no union.  
/*  * An single STRING key hash multi-set optimized for vector map join. * * The key will be deserialized and just the bytes will be stored.  */
/*      * Determine if there is only one TableScanOperator.  Currently in Map vectorization, we do not     * try to vectorize multiple input trees.      */
/*    * Gets new templeton controller objects.    */
//  Set not null constraint name if null before sending to listener 
//  given session, i.e. the name can be fixed across all invocations 
//  lets use the remaining space in column 1 as progress bar 
// if mapper can span partitions, make sure a splits does not contain multiple   opList + inputFormatClassName + deserializerClassName combination   This is done using the Map of CombinePathInputFormat to PathFilter 
/*  (non-Javadoc)   * This provides a LazyHiveDecimal like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  --- From here we evaluate the auto mode 
//  Update source info if the state is SUCCEEDED 
//  to these bottom layer ReduceSinkOperators. 
//  delink union 
//  No rounding. 
//  Need to pass all of the columns used in the where clauses as reduce values 
// opening the META table ensures that cluster is running 
//  Now we take a local TZ offset at midnight UTC. Say we are in -4; that means (surprise 
// Reject all paths to force it to continue.  When no more paths, should throw an exception. 
//  KERBEROS 
//  that are not in our wordlist (eg. table and column names) 
//  if we have too many results return null for a full scan 
//  Add a NOT operator in the beginning (this is for the cloned operator because we 
//  If start index is 0 in query, that is equivalent to using 1 in query.   So internal offset is 0. 
//  This happens when the code inside the JMX bean (setter?? from the   java docs) threw an exception, so log it and fall back on the    class name 
/*        * If Imported SerdeFormat is null, then set it to "1" just as       * metadata.Table.getEmptyTable        */
//  Bounded queue could be specified here but that will lead to blocking.   ConcurrentLinkedQueue is unbounded and will release soft referenced kryo instances under 
//  1. Build GB Keys, grouping set starting position 
//  If there are no sample cols and no bucket cols then throw an error 
//  raise error if we could not find the column 
//  Offset by 4 because the first 4 frames are just calls to get down here. 
/* In each group, walk from most recent and count occurences of each state type.  Once you        * have counted enough (for each state) to satisfy retention policy, delete all other        * instances of this status. */
//  Case-sensitive string found 
//  is created using field name 
//  Let caller set negative sign if necessary. 
//  Execute SELECT statement and verify the result set (should be two rows). 
//  Delete functions created by the tests   It is enough to remove functions from the default database, other databases are dropped 
/*     * Expected result 0th entry is the RecordIdentifier + data.  1st entry file before compact */
//  verify that the beginning entry is the only one that matches 
//  Assumes one instance of this + single-threaded compilation for each query. 
//  System registry should not be used to check persistent functions - see isPermanentFunc() 
//  Estimated number of bytes needed. 
//  Inactive nodes restart every call! 
//  framework 
// Binary arithmetic operator 
//  pick the length of the first ptn, we expect all ptns listed to have the same number of   key-vals. 
//  create a new vertex 
//  e.g. delta_0000001_0000001_0000 or base_0000022 
//  Non-skewed value, add it to list for later handle on default directory. 
//  Update for next iteration 
//  Copy to Java object because that saves object creation time.   Note that on average the number of copies is log(N) so that's not   very important. 
//  Iterate through the batch and for each (owid, rowid) in the batch   check if it is deleted or not. 
//  track of the variable length keys 
//  map from partID -> (statType->value) 
//  Map the newly allocated write ids against the list of txns which doesn't have pre-allocated 
//  round(longCol) returns a long and is a no-op. So it will not be implemented here. 
// ---------------------------------------------------------------------------   Process Multi-Key Left-Semi Join on a vectorized row batch.   
//  RESOURCE_PLAN 
/*  operators for which the optimization will be successful  */
//  For dummy partitions, only partition name is needed 
/*    * OI & Serde helper methods    */
//  set some of parameters for prepared sql, not all of them. 
//  GROUPBY operator in reducer may not be processed in parallel. Skip optimizing. 
//  parse, analyze, optimize and compile 
/*      * Again with correct output type...      */
//  specify the default log4j2 properties file. 
//  Loop over the given inList elements. 
//  1.1. Recurse over the subqueries to fill the subquery part of the plan 
//  we are only interested in ExprNodeColumnDesc 
//  Create a znode under the rootNamespace parent for this instance of the server   Znode name: serverUri=host:port;version=versionInfo;sequence=sequenceNumber 
//  Extrapolation is needed for some columns.   In this case, at least a column status for a partition is missing. 
//  is there a insert in the subquery 
//  Env interface to mock out dealing with Environment variables   This allows us to interface with Environment vars through   BeeLineOpts while allowing tests to mock out Env setting if needed. 
//  Update the time if it hasn't been specified. 
//  if it is a single item in an IN clause, transform A IN (B) to A = B   from IN [A,B] => EQUALS [A,B]   except complex types 
//  Remember we used this one in the query. 
//  For each dynamic partition, check if it needs to be merged. 
/*  If this conversion is frequently used, this should be optimized,       * e.g. by converting to decimal from the input bytes directly without       * making a new string.        */
//  This should only ever be called in testing scenarios.   There should not be any other users of the cache or its entries or this may mess up cleanup. 
//  If the given value is a type of LazyObject, then only try and convert it to that form. 
/*    * Build a Hive-to-X column mapping,   *    */
//  Timer that reports every 5 minutes to the jobtracker. This ensures that   even if the operator returning rows for greater than that   duration, a progress report is sent to the tracker so that the tracker   does not think that the job is dead. 
//  creating the context can create a bunch of files. So make   sure to clear it out 
//  Get and process the current datum 
//  Keep "toRead" list for future use, don't extract(). 
//  3. IO cost = cost of transferring small tables to join node * 
// primitives except binary 
//  same test as above but with 3 jars sharing dependencies 
//  This is where the spilling may happen again 
//  Note: we might want to be smarter if threadId-s are low and there more arenas than threads. 
//  NODE 
//  of this operator. 
//  COL 
//  add children, self to the front of the queue in that order 
//  HIVE_SERVER2_SESSION_CHECK_INTERVAL is set to 3 seconds, so we have to wait for at least 
//  Note that we cannot allow users to provide app ID, since providing somebody else's appId   would give one LLAP token (and splits) for that app ID. If we could verify it somehow   (YARN token? nothing we can do in an UDF), we could get it from client already running on   YARN. As such, the clients running on YARN will have two app IDs to be aware of. 
//  Negative max cache size means unbounded. 
//  Switch to iterate foreign keys 
// if there are un-compacted original files, they will be included in compaction, so 
//  Tests for List<Partition> add_partitions(List<Partition> partitions,   boolean ifNotExists, boolean needResults) method 
//  3/ test serialization and deserialization with different schemas 
/*            * If we are moving the partition across filesystem boundaries           * inherit from the table properties. Otherwise (same filesystem) use the           * original partition location.           *           * See: HIVE-1707 and HIVE-2117 for background            */
//  mutable thread-safe map to store instances 
//  1.2 Add column info corresponding to partition columns 
// default type is table 
//  Case 3.3 - Now with 2000 entries, try the above settings 
//  Blurb list. 
//  Process --deregister 
//  Case 6: column stats, NO hash aggregation, grouping sets 
//  Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy 
//  STRING type in Hive is represented as VARCHAR with precision Integer.MAX_VALUE.   In turn, the max VARCHAR precision should be 65535. However, the value is not   used for validation, but rather only internally by the optimizer to know the max   precision supported by the system. Thus, no VARCHAR precision should fall between 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.newBuilder() 
//  of column name within regex pattern with its corresponding value, if provided 
//  auth has been initialized 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTime(int, java.util.Calendar)    */
//  No such job. 
//  submit the job 
// try to replace a bucket map join with a sorted merge map join 
//  Other formats can be converted to insert-only transactional tables 
//  Do not update metrics - we didn't update on removal. 
//  HADOOP-13155, fixed version: 2.8.0, 3.0.0-alpha1 
//  Moving tables/partitions depend on the dependencyTask 
/*  * An single byte array value hash map based on the BytesBytesMultiHashMap. * * Since BytesBytesMultiHashMap does not interpret the key as BinarySortable we optimize * this case and just reference the byte array key directly for the lookup instead of serializing * the byte array into BinarySortable. We rely on it just doing byte array equality comparisons.  */
//  Parse the json map. 
//  Make a copy of the statement's result schema, since we may 
//  NOP 
//  the number of children slots used 
//  Remove from the runningTaskList 
//  present 
//  if embedded metastore is to be used as per config so far 
//  Heartbeats on the txn table.  This commits, so do not enter it with any state 
// Since the user running the test won't belong to a non-existent group  foo_bar_group, the call to getDelegationTokenStr will fail 
//  nothing to do here 
//  this is not a join condition. Will get handled by predicate pushdown. 
// hcatConf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname,  		HCatSemanticAnalyzer.class.getName()); 
//  A type date (LongColumnVector storing epoch days) minus a type date produces a   type interval_day_time (TimestampColumnVector storing nanosecond interval in 2 longs). 
//  Add procedures as they can be invoked by functions 
/*                * Single-Column Long specific lookup key.                */
//  Here we are disconnecting root with its parents. However, we need to save   a few information, since in future we may reach the parent operators via a   different path, and we may need to connect parent works with the work associated 
//  This is used for tests where there's always just one batch of work and we do the   checks after the batch, so the check will only come at the end of queueing. 
//  report or forward 
//  Found a semijoin branch.   There can be more than one semijoin branch coming from the parent 
//  We assume that NO_RGS value is only set from SARG filter and for all columns;   intermediate changes for individual columns will unset values in the array.   Skip this case for 0-column read. We could probably special-case it just like we do   in EncodedReaderImpl, but for now it's not that important. 
//  This will be null at slave nodes. 
//  At this point. 2 tasks running - both at priority 2. 
//  Create our vector map join optimized hash table variation *above* the   map join table container. 
//  4. HDFS session path 
//  QUALIFIERS 
//  This isn't really used for anything. 
//  find proxy user if any from query param 
//  Must be a struct 
//  @@protoc_insertion_point(class_scope:UserPayloadProto) 
//  Schedule low pri first. When high pri is scheduled, it takes away the duck from the   low pri task. When the high pri finishes, low pri gets the duck back. 
//  Compute join keys and store in reduceKeys 
//  add new item to the Spark work 
//  increase the row count 
//  repl export, has repl.last.id and repl.scope=all in it   import repl dump, table has repl.last.id on it (will likely be 0) 
//  update the log level for the specified logger 
//  * If expression is UDF it is not permanent UDF 
/*     we don't want to put any limits on this task as this is essential before we start    processing new database events.    */
//  Local file system path for spilled hashMap   Status of hashMap. true: on disk, false: in memory   When there's no enough memory, cannot create hashMap   Used to create an empty BytesBytesMultiHashMap   Same as above   Same as above   How many rows saved to the on-disk hashmap (if on disk) 
//  write the null byte every eight elements or   if this is the last element and serialize the 
//  Traverse the byte buffer containing the input string one code point at a time 
/*  UNDONE: Col Col, vs Scalar Col vs Col Scalar    testCodeGen.addColumnColumnOperationTestCases(          className,          inputColumnVectorType1,          inputColumnVectorType2,          "long");     */
//  Convert the key/value pairs 
//  try it again with an include vector 
//  Runs a 'stat' against the servers. 
//  One row (existence). 
//  the one created here will not be added. 
//  At least the header should fit. 
// we have MIN_TXN, MAX_TXN and IS_MAJOR in JobConf so we could figure out exactly what the dir  name is that we want to rename; leave it for another day 
//  Unfortunately, we cannot directly read a protected field of non-this object 
//  The block is being moved; the move will release memory. 
//  Put partial aggregation results in reduceValues 
//  Stored here only as async operation context. 
//  Use Calcite cost model for view rewriting 
//  cannot optimize any others 
//  BOOLEAN_STATS 
//  This was the only predicate, set filter expression to const 
//  Pre-allocated member for storing any non-spills, non-matches, or merged row indexes during a 
//  This hash function returns the same result as Double.hashCode()   while DoubleWritable.hashCode returns a different result. 
//  Max file num and size used to do a single copy (after that, distcp is used) 
//  non-null column alias 
//  Zero result. 
//  Synthetic predicates from semijoin opt should not affect stats. 
//  This is where counters are logged! 
//  See the comment in handleUpdateForSinglePriorityLevel. 
//  opNode's type is always either KW_EXISTS or KW_IN never NOTEXISTS or NOTIN    to figure this out we need to check it's grand parent's parent 
//  Clear rounding portion in high longword and add 1 at right scale (roundMultiplyFactor);   middle and lower longwords result is 0; 
//  If the function is distinct, partial aggregation has not been done on   the client side.   If distPartAgg is set, the client is letting us know that partial   aggregation has not been done.   For eg: select a, count(b+c), count(distinct d+e) group by a   For count(b+c), if partial aggregation has been performed, then we   directly look for count(b+c).   Otherwise, we look for b+c.   For distincts, partial aggregation is never performed on the client   side, so always look for the parameters: d+e 
//  Extract the sequence number of this ephemeral-sequential znode. 
//  The extra non existing values will be ignored. 
//  1. Additional data structures needed for the join optimization      through Hive 
//  We could just do toLowerCase here and let SA qualify it, but   let's be proper... 
//  if it is not CTAS, we don't need to go further and just return 
//  remember which reducesinks we've already connected 
/*   @Override  public com.esotericsoftware.kryo.io.Output getHybridBigTableSpillOutput(int partitionId) {    HybridHashTableContainer ht = (HybridHashTableContainer) mapJoinTableContainer;    HashPartition hp = ht.getHashPartitions()[partitionId];    return hp.getMatchfileOutput();  }   */
//  create a dummy database and a couple of dummy tables 
//  call-3: open - mock:/mocktbl2/0_0 
//  tracks only running portion of the query. 
//  object inspectors for input rows 
//  and finally let's check output sizes 
//  print vertexname 
//  Test "alter table" with schema change 
//  In case when user Ctrl-C twice to kill Hive CLI JVM, we want to release locks 
//  only if the column stats is available, update the data size from   the column stats 
//  read totalMonths from DataInput 
// spit is marked isOriginal but it's not an immediate child of a partition nor is it in a  base/ or delta/ - this should never happen 
//  is block-compressed? it should be always false. 
// no order by as it's just 1 row 
//  LEN_VALUE 
//  add children to tasksToVisit 
/*  Create a new distinctValueEstimator    */
/*    * Apply the rules in the Spec. to fill in any missing pieces of every Window Specification,   * also validate that the effective Specification is valid. The rules applied are:   * - For Wdw Specs that refer to Window Defns, inherit missing components.   * - A Window Spec with no Parition Spec, is Partitioned on a Constant(number 0)   * - For missing Wdw Frames or for Frames with only a Start Boundary, completely specify them   *   by the rules in {@link effectiveWindowFrame}   * - Validate the effective Window Frames with the rules in {@link validateWindowFrame}   * - If there is no Order, then add the Partition expressions as the Order.    */
//  We store the chunk indices by split file; that way if several callers are reading   the same file they can separately store and remove the relevant parts of the index. 
//  Invert words. 
//  sort the list to get sorted (deterministic) output (for ease of testing) 
// suppose it's the first Major compaction so we only have deltas 
//  Long masks and values. 
//  Join which are part of join keys 
//  current write ids are not valid. 
//  empty rows for each table 
//  no value for kale 
//  We need Partition-s for firing events and for result; DN needs MPartition-s to drop.   Great... Maybe we could bypass fetching MPartitions by issuing direct SQL deletes. 
//  Change lock manager, otherwise unit-test doesn't go through 
//  if not a control character, do nothing 
//   - FunctionType 
//  Emit the rest of word0. 
//  Run Cleaner to delete rows for the aborted transaction 
//  4. Add new rel & its RR to the maps 
//  table alias (small) --> input file name (big) --> target file names (small) 
//  This will happen in case of joins. The current plan can be thrown away   after being merged with the original plan 
//  long column/column IF 
//  Write back previous 8 field's NULL byte. 
//  2/ serialize the union 
// -1 is for internal (getAcidState()) purposes and means the delta dir  had no statement ID 
//  load into compressed buf first 
//  stores all the downloaded resources as key and the jars which depend on these resources as values in form of a list. Used for deleting transitive dependencies. 
//  Verify that Hive is caching the object inspectors for us. 
//  Explicitly using only the start offset of a split, and not the length. Splits generated on   block boundaries and stripe boundaries can vary slightly. Try hashing both to the same node.   There is the drawback of potentially hashing the same data on multiple nodes though, when a   large split is sent to 1 node, and a second invocation uses smaller chunks of the previous   large split and send them to different nodes. 
//  blank " " (1 byte)   NEW TAI LUE LETTER LOW BA U+19A5 (3 bytes) 
// ----------------------------------------------------------------------------------------------   VALIDATION  ---------------------------------------------------------------------------------------------- 
//  Create an http client from the configs 
//  Try opportunistically for the common case - the same-sized, allocated buddy. 
//  token expiration 
// so that "explain" doesn't "leak" tmp tables 
// if the exception is not 'NODEEXISTS', re-throw it 
//  determine the query qualifies reduce input size for LIMIT   The query only qualifies when there are only one top operator   and there is no transformer or UDTF and no block sampling   is used. 
//  finishes inside 
//  Then write chunk bytes 
//  dropFunction() 
//  number of buckets 
//  We're hijacking the big table evaluators an replace them with our own custom ones 
// not allowed in w/o tx 
//  If we're supporting dynamic service discovery, we'll add the service uri for this   HiveServer2 instance to Zookeeper as a znode. 
//  fill in colname 
/*        * Take what all input formats support and eliminate any of them not enabled by       * the Hive variable.        */
//  we always want to read all the delete delta files. 
//  Init input object inspectors 
//  How many blocks of this size comprise an arena. 
//  reset buffer 
//  If the loaded hash table is empty, for some conditions we can skip processing the big table rows. 
//  serialize work 
//  fill the temp list before merging to sparse map 
//  Null byte start 
//  the SerDe part is from TestLazySimpleSerDe 
//  At this point, hinted semijoin case has been handled already   Check if big table is big enough that runtime filtering is 
//  * NOT IN - always allow, but always return true because later subq remove rule will generate diff plan for this case 
//  rowId also found! 
//  this assumes no splitting 
//  6. Translate Window spec 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getResultSetConcurrency()    */
//  Get length word. 
/*    * True if same value repeats for whole column vector.   * If so, vector[0] holds the repeating value.    */
//  default tokenstore is MemoryTokenStore 
//  fetch table alias 
//  Retrieve not null constraints 
//  set output collector for any reduce sink operators in the pipeline. 
//  Preserve the old behavior of failing when we cannot write, even w/o deleteData,   and even if the table is external. That might not make any sense. 
/*      * Consider a query like:     *     * select count(*) from     *   (     *     select key, count(*) from     *       (     *         select --mapjoin(a)-- a.key as key, a.value as val1, b.value as val2     *         from tbl1 a join tbl2 b on a.key = b.key     *       ) subq1     *     group by key     *   ) subq2;     *     * The table alias should be subq2:subq1:a which needs to be fetched from topOps.      */
//  But if snapshot is not valid, we recompile the query. 
//  using init(..) instead of this(..) because the new HCatReplicationTaskIteratorNotificationFilter   is an operation that needs to run before delegating to the other ctor, and this messes up chaining   ctors 
//  Add FSD so that the LoadTask compilation could fix up its path to avoid the move. 
//  map from tablename to task (ColumnStatsTask which includes a BasicStatsTask) 
//  Reset to correct http path 
//  for the following method. 
//  This can change based on new splits. 
/*  dfs.  */
//  Proceed if AST contains partition & If Not Exists 
//  -------------------------------------------------------------------------------   VERTICES: 03/04            [=================>>-----] 86%  ELAPSED TIME: 1.71 s   -------------------------------------------------------------------------------   contains footerSummary , progressedPercentage, starTime 
/*     * The below loop may perform bad when the destination file already exists and it has too many _copy_    * files as well. A desired approach was to call listFiles() and get a complete list of files from    * the destination, and check whether the file exists or not on that list. However, millions of files    * could live on the destination directory, and on concurrent situations, this can cause OOM problems.    *    * I'll leave the below loop for now until a better approach is found.     */
//  repl-imports are replace-into unless the event is insert-into 
// Hive Date is representable as Pig DATETIME 
//  So, table "t1" and "t2" will exist and partition "india" will exist, rest failed as operation failed. 
//  In tests. 
//  Use threads to resolve directories into splits. 
//  process group-by pattern 
//  Must be called under the epic lock. 
//  call-2: open to read data - split 2 => mock:/mocktable2/0_1 
//  If we are doing an update or a delete the number of columns in the table will not   match the number of columns in the file sink.  For update there will be one too many   (because of the ROW__ID), and in the case of the delete there will be just the   ROW__ID, which we don't need to worry about from a lineage perspective. 
//  Print the results 
//  we don't allow turning on auto parallel once it has been   explicitly turned off. That is to avoid scenarios where   auto parallelism could break assumptions about number of   reducers or hash function. 
//  The bottom layer ReduceSinkOperators. These ReduceSinkOperators are used   to record the boundary of this sub-tree which can be evaluated in a single MR 
//  This shouldn't ever happen 
//  UNION_FIELD1 
//  reevaluate expression on current Row, to trigger the Lazy object   caches to be reset to the current row. 
// Reuse record reader ID 
/*  Order by clause  */
//  ~ Instance fields -------------------------------------------------------- 
/*    * Return true or false based on whether a bucketed mapjoin can be converted successfully to   * a sort-merge map join operator. The following checks are performed:   * a. The mapjoin under consideration is a bucketed mapjoin.   * b. All the tables are sorted in same order, such that join columns is equal to or a prefix   *    of the sort columns.    */
//  trailing blank field 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTime(java.lang.String,   * java.util.Calendar)    */
//  Convenient constructor for initial batch creation takes 
//  test that zero-divide produces null for all output values 
//  Check that write id is still valid 
//  evaluate the aggregators 
//  Matches only JoinOperators which are reducers, rather than map joins, SMB map joins, etc. 
//  Temp partition input path does not match exist temp path 
//  no munging inner-schemas 
//  approximate 
//  Reset for the new partition 
//  RG filtered. 
//  Fill high long from some of middle long. 
//  store the orc configuration from the first file. All other files should 
//  UNION_FIELD3 
//  1) If join is not a left or right outer, we bail out   2) If any sort column is not part of the input where the 
//  If there are no functions, it doesn't matter as much whether we   aggregate the inputs before the join, because there will not be   any functions experiencing a cartesian product effect.     But finding out whether the input is already unique requires a call   to areColumnsUnique that currently (until [CALCITE-1048] "Make   metadata more robust" is fixed) places a heavy load on   the metadata system.     So we choose to imagine the the input is already unique, which is   untrue but harmless.   
//  Export valid directories with a modified name so they don't look like bases/deltas.   We could also dump the delta contents all together and rename the files if names collide. 
//  Overridden and used in ProcessingModeReduceMergePartial mode. 
//  in case the empty grouping set is preset; but no output has done   the "summary row" still needs to be emitted 
//  If aborted - break out of the loop, and cancel all subsequent futures. 
//  MAX_COL_LEN 
//  The non-MM path only finds new partitions, as it is looking at the temp path.   To produce the same effect, we will find all the partitions affected by this txn ID.   Note: we ignore the statement ID here, because it's currently irrelevant for MoveTask         where this is used; we always want to load everything; also the only case where         we have multiple statements anyway is union. 
//  We only retrieve the materialization corresponding to the rebuild. In turn,   we pass 'true' for the forceMVContentsUpToDate parameter, as we cannot allow the   materialization contents to be stale for a rebuild if we want to use it. 
//  The value for the constant does not matter. It is replaced by the grouping set   value for the actual implementation 
//  Do the columns used by the join appear in the output of the aggregate? 
//  Test that locking a database prevents locking of tables in the database 
//  We must ensure the exactness of the double's fractional portion.   0.6 as the fraction part will be converted to 0.59999... and   significantly reduce the savings from binary serialization 
//  outputs is empty, which means this create table happens in the current 
// clean the staging table 
// this needs major compaction 
//  Allocate writeId to txn under HWM. This will get Id greater than a txn > HWM. 
// support for authorization on partitions needs to be added 
//  start the creation of znodes 
//  NOTE: We cannot use copySelected below since it is a whole column operation. 
//  Get the output ObjectInspector from the return type. 
//  and put it to WriteEntity for post-exec hook 
//  Derby commandline parser 
//  If destPath directory exists, rename call will move the srcPath   into destPath without failing. So check it before renaming. 
//  used for avoid extra byte copy 
//  updated only when a thread has failed. 
//  All columns have to be primitive. 
//  UNION_FIELD2 
//  TBL_VALID_WRITE_IDS 
//  Truncate a table 
//  Interleaved writes to both batches 
//  Governs remote-fetch-input behaviour   If set to true, we'll assume that the input has a _files file present which lists     the actual input files to copy, and we'll pull each of those on read.   If set to false, it'll behave as a traditional CopyTask. 
//  1) We extract the group by positions that are part of the collations and   sort them so they respect it 
//  Try to transform OR predicates in Filter into simpler IN clauses first 
/*  * The equality is implemented fully, the greater-than/less-than * values do not implement a transitive relation.   */
//  OUTPUT_FORMAT 
//  This join has already been processed 
//  End HiveReduceExpressionsRule.java 
//  nonblocking execute 
//  Do nothing for other modes 
//  parse a key 
//  the subtree to gather the references 
//  Now, for each entry in the queue, see if all of the associated locks are clear so we 
//  fully-specified partition 
//  twice or more, skip dedup. 
//  For now, there's nothing special to return in addedVals. Just return the footer. 
//  now create the new project 
//  for now require select WITH GRANT 
// now test that we don't timeout locks we should not 
//  Implementations may choose to override this 
//  Have to set it for each partition too 
//  output file system information 
//  [A: 0, B: 0, B.x: 0, B.y: 0, C: 0] 
// With nulls 
//  Forward the current keys if needed for sort-based aggregation 
//  Local time zone. Store separately because Calendar would clone it. 
//  Write out the first 63 bits worth of data. 
// should this be escaped string? 
//  Reset member variables so we don't get in a half-constructed state 
//  Note that the state of the failed service is still INITED and not   STARTED. Even though the last service is not started completely, still   call stop() on all services including failed service to make sure cleanup   happens. 
//  iterator cursor in the currBlock   size of current read block   append cursor in the lastBlock   serialization/deserialization for the row   object inspector for the row 
// if lock is part of txn, heartbeat info is in txn record 
//  make sure the null flag agrees 
//  alert if we already running low on memory (starting with low memory will lead to frequent auto flush) 
//  Add this condition to the list of non-equi-join conditions. 
//  2.1. Backtracking from RS 
//  List of TezWork.Dependency 
//  create the project before GB 
//  Now, we have written all information about the next value, work on the *new* value. 
//  tez task we're currently processing 
//  Set 'version' 
//  We allow stateful functions in the SELECT list (but nowhere else) 
//  Accurate byte value cannot be obtained. 
//  If we didn't find the Token, we can't proceed. Log the tokens for debugging. 
//  The key is missing - shouldn't be able to verify. 
//  Note. If the materialized view does not contain a table that is contained in the query,   we do not need to check whether that specific table is outdated or not. If a rewriting   is produced in those cases, it is because that additional table is joined with the   existing tables with an append-columns only join, i.e., PK-FK + not null. 
//  True if only one value is null 
//  Initialize 1.2.0 schema 
//  Exponent E or e. 
//  Type affinity does not help when multiple methods have the same type affinity. 
//  RETENTION 
//  Convert valueList to array for the ListColumnVector.child 
//  Map needs two separators (key and key/value pair). 
// Save compile-time PerfLogging for WebUI.  Execution-time Perf logs are done by either another thread's PerfLogger  or a reset PerfLogger. 
//  at this point the number of reducers is precisely defined in the plan 
//  Allowed operations:   IntervalYearMonth - IntervalYearMonth = IntervalYearMonth   Date - IntervalYearMonth = Date (operands not reversible)   Timestamp - IntervalYearMonth = Timestamp (operands not reversible)   IntervalDayTime - IntervalDayTime = IntervalDayTime   Date - IntervalYearMonth = Timestamp (operands not reversible)   Timestamp - IntervalYearMonth = Timestamp (operands not reversible)   Timestamp - Timestamp = IntervalDayTime   Date - Date = IntervalDayTime   Timestamp - Date = IntervalDayTime (operands reversible) 
//  For performance reasons we do not want to chase the values to the end to determine   the count.  Use hasRows and isSingleRow instead. 
//  initializeAlpha(DEFAULT_HASH_BITS); 
//  special handling for serde reader (text) in llap IO.   if file format version is null, then we are processing text IF in LLAP IO, in which case   we get vectors instead of streams. If vectors contain instance of Decimal64ColumnVector we   should use Decimal64StreamReader (which acts as a wrapper around vectors) 
//  Simulate a missing table scenario by renaming a couple of tables 
//  6. Generate table access stats if required 
//  will always excuse the first error. We can decide if single 
//  App never seen, or previous dag has been unregistered. 
//  Not affected or the op is not about transactional. 
//  we need the directory on hdfs to which we shall put all these files 
//  set third argument to IF 
//  Pre-allocated member for storing the (physical) batch index of rows that need to be spilled. 
//  Verify if all the aborted write ids are replicated to the replicated DB 
//  compare stats obj to ensure what we get is what we wrote 
//  add Token, only if it already doesn't exist 
//  If we allow estimated stats for the columns, then we shall set the boolean to true,   since otherwise we will throw an exception because columns with stimated stats are   actually added to the list of columns that do not contain stats. 
//  don't make a copy if we don't have to  noinspection unchecked 
//  cannot convert to map join; we've already chosen a big table   on size and there's another one that's bigger. 
// decimal place 
//  optional bool result = 1; 
//  wrapper class for reading and writing metadata about a dump   responsible for _dumpmetadata files 
//  do not throw exception if table does not exist 
//  cannot do delimited split for some commands like "dfs -cat" that prints the contents of file which may have   different delimiter. so we will split only when the resultSchema has more than 1 column 
//  ColumnizedDeleteEventRegistry loads all the delete events from all the delete deltas   into memory. To prevent out-of-memory errors, this check is a rough heuristic that   prevents creation of an object of this class if the total number of delete events   exceed this value. By default, it has been set to 10 million delete events per bucket. 
//  The id of the job this tracking node represents 
//  keys are the column names. basically this maps the position of the column   in 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setAsciiStream(java.lang.String,   * java.io.InputStream, long)    */
// get tokens for all other known FSs since Hive tables may result in different ones 
// for some reason this just locks the table; if I alter table to add this partition, then  we end up locking both table and partition with share_read.  (Plan has 2 ReadEntities)...?  same for other locks below 
//  LEVEL 
//  If input (GBY) is different than the source of columns, find the   same column in input. 
//  (conjuctive elements) 
//  key contains scheme (such as pfile://) and we want only the path portion fix in HIVE-6366 
//  empty array? 
// key is correct 
// lastAccessTime < 90 
//  no support for statistics 
//  With the fast hash table implementation, we currently do not support   Hybrid Grace Hash Join. 
//  Llap server depends on Hive execution, so the reverse cannot be true. We create the I/O   singleton once (on daemon startup); the said singleton serves as the IO interface. 
//  finally monitor will print progress until the job is done 
//  [0, 1, 2, 0] --> [T1.0, T1.1, T1.2, T2.0] (table columns mapping) 
//  Always generate a list with at least 1 value? 
//  Lock was outdated and it was removed (then maybe another transaction picked it up) 
//  "1234567" => unscaledValue=1234567, negative=false,   fractionalDigits=0   "-1234567.89" => unscaledValue=123456789, negative=true,   fractionalDigits=2   "12.3E7" => unscaledValue=123, negative=false, fractionalDigits=1,   exponent=7   ".123E-7" => unscaledValue=123, negative=false, fractionalDigits=3, 
//  DruidOutputFormat will write segments in an intermediate directory 
//  Get the first valid row in the batch still available. 
//  Create HepPlanner 
//  TODO: if this method is ever called on more than one jar, getting the dir   and the 
//  This is based on the existing valid write ID list that was built for a select query;   therefore we assume all the aborted txns, etc. were already accounted for.   All we do is adjust the high watermark to only include contiguous txns. 
//  add WriteEntity for each matching partition 
/*  It may happen that there's not enough memory to instantiate a hashmap for the partition.     * In that case, we don't create the hashmap, but pretend the hashmap is directly "spilled".      */
//  Update the property before offering. 
//  to this plan:     Project-A' (all gby keys + rewritten nullable ProjExpr)     Aggregate (groupby(all left input refs)                   agg0(rewritten expression),                   agg1()...)       Project-B' (rewriten original projected exprs)         Join (LOJ cond = true)           LeftInputRel           RightInputRel   
//  Tests that doing a table-level REPL LOAD updates table repl.last.id, but not db-level repl.last.id 
//  required   optional   optional   optional   optional 
//  Long and double are handled using descriptors, string needs to be specially handled. 
//  preserve the original configuration 
//  Try this as a list 
//  Passed the unparsed DB name here, as get_partitions_ps expects to parse it 
//  create a walker which walks the tree in a DFS manner while maintaining   the operator stack. The dispatcher 
// CTAS with ACID target table 
//  copy a null 
//  The current plan can be thrown away after being merged with the union   plan 
//  Update JobConf using MRInput, info like filename comes via this 
//  do filtering on the server, and have to fall back to client path. 
//  if the logger name is not found, root logger is returned. We don't want to change root logger level   since user either requested a new logger or specified invalid input. In which, we will add the logger   that user requested. 
//  ..end of conversion 
//  This is a normal insert delta, which only has insert events and hence all the files 
// looking for map = 100%,  reduce = 100% 
//  Someone else replaced/removed a parallel-added stale value, try again. Max confusion. 
// get options from arguments 
//  bd is less than 1 
//  Finally create the outer struct to contain the key, value structs 
//  3. Build Rel for GB Clause 
//  for writes (transaction batch not closed yet) 
//  from SQLStdHiveAccessController.applyAuthorizationConfigPolicy() 
//  suffix, reduce len 
//  FLOAT_TYPE is treated as DOUBLE_TYPE 
//  There may be more data after the gap. 
// Following fields for displaying queries on WebUI 
//  update them all. 
//  is the current task a root task 
/*        * Walk down expression to see which arguments are actually used.        */
//  set up WriteEntity for replication 
//  We have processed this on the previous run, after it has already queued the message. 
//  how much to we have   minimum size of ht completely in memory   blowout factor datasize -> memory size 
// Hadoop property names (set by templeton logic) 
//  do this for reconciling HBaseStorageHandler for use in HCatalog 
//  returns fileId for SMBJoin, which consists part of result file name 
//  "-foo bar -blah"  form 
//  In case last row was a large bytes value 
//  ROOTS 
//  3nd char starts from index 3 and total length should be 7 bytes as max is 10 
// "some inputs"; // Will probably never actually happen. 
//  will be KEY._COLx or VALUE._COLx 
//  create 2nd permanent function 
//  Request interceptor for any request pre-processing logic 
/*      * Similarly, we need a mapping since a value expression can be a calculation and the value     * will go into a scratch column.      */
//  has the permissions on the table dir 
/*  For UDFs that expect primitive types (like int instead of Integer or IntWritable),         * this will catch the the exception that happens if they are passed a NULL value.         * Then the default NULL handling logic will apply, and the result will be NULL.          */
//  entries in the VGBY are flushed. 
//  Next 6 bits are used to locate offset within a long/word 
//  Could we also join with ACID tables to only get tables with outdated stats? 
//  We return a garbage value if metrics haven't been initialized so that callers don't have   to keep checking if the resulting value is null. 
//  remove the comments 
//  check if groupby is empty and there is no other cols in aggr   this should only happen when newParent is constant. 
//  Replicate only one INSERT INTO operation on the table. 
//  We need to check the Druid metadata 
//  Timer that tops rpTimer after a long timeout, e.g. 1 hr 
//  Remove entire priority level if it's been emptied. 
//  We don't lock files or directories. We also skip locking temp tables. 
/*          * Initialize Single-Column String members for this specialized class.          */
//  Create a copy of the function descriptor 
//  warn the user if bytes per reducer is much larger than memory per task 
//  Nothing to do here. This is not invoked by the log4j framework. Should likely not be in   the log4j interface 
// Hive only supports primitive map keys:   https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-ComplexTypes 
//  parent stats are not populated yet 
//  allocate and initialize a new conf since a test can 
// match if there is filter (sq_count_check) as right input of a join which is left   input of another join 
//  splits are equal to number of files in worst case) 
/*    * A Unit Test convenience method for putting the key into the hash table using the   * actual type.    */
/*  1 files x 100 size for 1 splits  */
//  Accurate int value cannot be obtained. 
//  save some positional state 
//  If they gave a value but not a time unit assume the default time unit. 
//  Create the converters 
//  Scale down by a factor of 0.9 to account for approximate values 
//  Plugin interface for storage handler which supports input estimation 
//  We allocate pairs, so we cannot go above highest Integer power of 2 / 4. 
//  re-login with kerberos. This makes sure all daemons have the same login user. 
//  with nulls 
//  Assuming grouping enabled always. 
//  on a per split strategy basis and it has to be same for all the files in that strategy. 
/*  We read many records because sometimes the RecordReader for the format to test     * behaves different with one record than a bunch of records  */
//  TODO test dropping non-empty catalog 
//  draw 2 and return in order - further run should return last returned 
//  equal maps 
//  need to reset to true in case previous aggregate/project   has set it to false 
//  Evaluate the result given a partition and the row number to process 
//  error 
//  -d <driver class> 
//  BinarySortableDeserializeRead. 
//  work 
//  Code below shameless borrowed from Hadoop Streaming 
//  Each iteration cleans the file cache as a single unit (unlike the ORC cache). 
//  delimiter to check DOT delimited qualified names 
//  keeping minTxnId atomic as it is shared with heartbeat thread 
//  Setting the default batch size to 1000 makes the memory check at 5000   rows work the same as the row by row writer. (If it was the default 1024,   the smallest stripe size would be 5120 rows, which changes the output   of some of the tests.) 
//  @@protoc_insertion_point(builder_scope:FragmentRuntimeInfo) 
/*    * The hash table slots.  For a long key hash table, each slot is 2 longs and the array is   * 2X sized.   *   * The slot pair is 1) a non-zero reference word to the first value bytes and 2) the long value.    */
//  Corresponds to SemAnalyzer genGroupByPlanMapAggr2MR 
//  We have to check it here since invalid decref will overflow. 
//  batches will be sized 10, 5, 2, 1 
//  Found a stale value we cannot incRef; try to replace it with new value. 
//  since renewal is KERBEROS authenticated token may not be cached 
// check if this grant statement will end up creating a cycle 
//  we have to keep at least a branch before we support empty values() in   hive 
/*  @bgen(jjtree) EnumDef  */
//  update statistics based on column statistics.   OR conditions keeps adding the stats independently, this may   result in number of rows getting more than the input rows in   which case stats need not be updated 
//  Check that the table is valid under strict managed tables mode. 
//  The application-level name 
//  dedup file list 
//  Table was renamed. 
//  Only need aborted since we don't consider anything above minOpenWriteId 
//  At this point we should add any relevant jars that would be needed for the UDf. 
//  the index of c 
// list of terminal operation states.  We measure only completed counts for operations in these states. 
//  increment the counters only when there are no violations 
//  Varchar or char length 
//  set so we don't repeat this initialization 
//  Should be able to execute without failure in the session whose transport has been closed. 
//  Operation fails as invalid input 
//  handle SQLLine command in beeline which starts with ! and does not end with ; 
// right now only one parent 
//  check here for each dir we're copying out, to see if it   already exists, error out if so.   Also, treat dyn-writes as writes to immutable tables.   dryRun = true, immutable = true 
//  Filter operator 
//  Allow for empty string, etc. 
//  get metastore/thrift privilege object using metastore api 
//  transitive 
//  The output of a partial aggregation is a struct containing   a long count and doubles sum and variance. 
//  then drop the database 
// The failure occurred before we even made an entry in COMPACTION_QUEUE 
//  2. Add alias to 1) aliasToOpInfo and 2) opToAlias 
//  The old to new output position mapping will be the same as that 
//  Date is an integer internally 
//  A queue to notify separateRowGenerator to generate the next batch of rows. 
/*    * If task execution time out is configured for submit operation then job may need to   * be killed on execution time out. These parameters controls the maximum number of   * retries and retry wait time in seconds for executing the time out task.    */
//  So the next line works. 
//  call-1: open to read data - split 1 => mock:/mocktable8/0_0   call-2: listLocatedFileStatus(mock:/mocktable8)   call-3: getFileStatus(mock:/mocktable8/delta_0000001_0000001_0000/_metadata_acid)   call-4: getFileStatus(mock:/mocktable8/delta_0000001_0000001_0000/_metadata_acid) 
//  Check if an appropriate codec is available 
//  VALUE of SimpleEntry: rowcount 
/*  The random values must be between 0 and 1, distributed uniformly.     * So the average value of a large set should be about 0.5. Verify it is     * close to this value.      */
//  We will only do interrupt checking in the lowest-level operator for multiple joins. 
/*  Test decimal scalar divided column. This tests the primary logic   * for template ScalarDivideColumnDecimal.txt.    */
//  Verify 
//  populate the operator 
//  no metadata should get created. 
//  if the operation on metastore fails, we don't do anything in change management, but fail   the metastore transaction, as having a copy of the jar in change management is not going 
//  Function to setup locks 
//  first argument is hiveVersion, it is compatible if 2nd argument - dbVersion is   greater than or equal to it   check the compatible case 
//     not, or this is the identity, the rule will do nothing 
//  get a synchronized wrapper if the meta store is remote. 
//  Try to "return" stuff that was killed from "under" us. Should be a no-op. 
//  or compile another query 
//  2. It is an OR operator with enough children 
//  Do the work that cannot be done via async calls. 
//  The column has been obtained from cache. 
/*  Let's write more bytes to the files to test that Estimator is actually working returning the file size not from the filesystem  */
// -------------------------------- last block: affect all 32 bits of (c)   all the case statements fall through 
//  Test getTables() with no table name pattern 
//  Now add enough failed compactions to ensure purgeCompactionHistory() will attempt delete;   HiveConf.ConfVars.COMPACTOR_HISTORY_RETENTION_ATTEMPTED is enough for this.   But we also want enough to tickle the code in TxnUtils.buildQueryWithINClauseStrings() 
//  let's wait on the async ops before continuing 
//  sequence file read 
//  Column names 
// we only recompute stats after major compact if they existed before 
//  no need to reload 
//  For Acid table, Insert Overwrite shouldn't replace the table content. We keep the old 
//  Toss in timestamp and date. 
//  call dropPartition on each of the table's partitions to follow the   procedure for cleanly dropping partitions. 
//  MetaStoreClient-based impl of NotificationFetcher 
//  extract columns missing in current RS key/value 
//  Create 2 tables, one partitioned and other not. Also, have both types of full ACID and MM tables. 
//  First compare the length and then compare the directory name 
/* Note that HCatRecordSerDe.serializePrimitiveField() will be called before this, thus some    * type promotion/conversion may occur: e.g. Short to Integer.  We should refactor this so    * that it's hapenning in one place per module/product that we are integrating with.    * All Pig conversion should be done here, etc. */
//  since it is guaranteed to produce at most one row 
//  without vectorization 
//  Signing is not required for Tez. 
//  In Hive AST, right child of join cannot be another join,   thus we need to introduce a project on top of it.   But we only need the additional project if the left child   is another join too; if it is not, ASTConverter will swap   the join inputs, leaving the join operator on the left.   we also do it if parent is HiveSemiJoin since ASTConverter won't   swap inputs then   This will help triggering multijoin recognition methods that   are embedded in SemanticAnalyzer. 
//  Ensures that the list doesn't have dups, and keeps track of directories we have created. 
// alter partition 
//  find how much compressed data was added for this column 
/*  * Test the vectorized UDF adaptor to verify that custom legacy and generic * UDFs can be run in vectorized mode.  */
//  2^56 
//  No row was processed 
//  Nothing to do here 
//  First, just allocate just the projection columns we will be using. 
//  TableType specified was null, we need to figure out what type it was. 
//  hive has no max limit for binary 
//  evaluate union object 
//  this turns on split-update U=D+I 
// split each row (duplicate) which will cause an update into 2 rows and augment with 'op' col which has 0 to insert, 1 to update 
//  1. Get Row Resolvers, Column map for original left and right input of 
//  Construct using org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto.newBuilder() 
//  CAS race, look again. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getTypeInfo(org.apache.hive.service.cli.SessionHandle)    */
//  Nothing so far (and shouldn't be called). 
//  Create bloom filter with same number of bits, but different # hash functions 
//  One input path would mean only one map task 
//  Copy of TimestampWritable.millisToSeconds 
//  Mark any scratch small table scratch columns that would normally receive a copy of the key   as null, too. 
// Single MapReduce job is launched 
/*    * Deserializes 64-bit decimals up to the maximum 64-bit precision (18 decimal digits).   *   * NOTE: Major assumption: the input decimal64 has already been bounds checked and a least   * has a precision <= DECIMAL64_DECIMAL_DIGITS.  We do not bounds check here for better   * performance.  You can bounds check beforehand with:   *     Math.abs(decimal64Long) <= getDecimal64AbsMax(precision)    */
//  Month granularity 
//  rowId >= 'q' 
//  no op 
//  MODIFIER LETTER TRIANGULAR COLON U+02D0 (2 bytes) 
//  If global contains excludes, individual modules can only contain additional excludes. 
//  Clean up the database 
//  values, colexpmap and rowschema 
//  become part of the record; otherwise, we will just write over it later. 
//  Don't compact 4 and 5; 3 is opened. 
//  For RS-SEL-RS case. reducer operator in reducer task cannot be null in task compiler 
//  once the feature is stable 
//  synchronized (lock) 
//  Some other things that could be added here to model cost:   Cost of computing/sending partial BloomFilter results? BloomFilterSize * # mappers   For reduce-side join, add the cost of the semijoin table scan/dependent tablescans? 
//  Register for notifications inside the lock. Should avoid races with unregisterForNotifications   happens in a different Submission thread. i.e. Avoid register running for this task 
//             send(lpde.getPartitionName(),lpde.getTable().getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME),HCatConstants.HCAT_PARTITION_DONE_EVENT); 
// want to avoid expiring locks for a txn w/o expiring the txn itself 
//  Add another value. 
//  Register the shard sub type to be used by the mapper 
//  2. Determine which stripes to read based on the split. 
// Create operation log root directory, if operation logging is enabled 
//  longer the wait time for an attempt wrt to its start time, higher the priority it gets 
//  First breaking up the filter conditions into equality   comparisons between rightJoinKeys(from the original   filterInputRel) and correlatedJoinKeys. correlatedJoinKeys   can only be RexFieldAccess, while rightJoinKeys can be 
//  save the original job tracker 
//  zero. no need to shift/scale 
/*    * Create test input file with specified number of rows    */
//  returning void because we ignore this production. 
//  Currently using fileuri#checksum#cmrooturi#subdirs as the format 
//  No range check needed. 
//  test non-vectorized, acid, combine 
//  determine bit width for bitpacking and encode it in header 
//  No boolean value match for 4 char field. 
//  to bump its internal version. 
/*       This is removed using a poll because there can be a case where there partitions iterator is empty      but because both the producer and consumer are started simultaneously the while loop will execute      because producer is not terminated but it wont produce anything so queue will be empty and then we      should only wait for a specific time before continuing, as the next loop cycle will fail.        */
//  Clear out isNull array. 
//  Adds the missing scheme/authority for the new table location 
//  First child is subquery, second child is alias   We set the node of interest and QB to the subquery 
//  return key from any of the readers 
//  the incoming split may not be a file split when we are re-grouping TezGroupedSplits in   the case of SMB join. So in this case, we can do an early exit by not doing the   calculation for bucketSizeMap. Each bucket will assume it can fill availableSlots * waves   (preset to 0.5) for SMB join. 
//  (2) getPosition() on 2 different columns should never give the same value. 
//  String group comparison. 
//  fetch task query 
//  Note that delete_delta_3_3 should not be read, when a minor compacted   [delete_]delta_2_5 is present. 
//  traverse data and masks array together, check for set bits 
//       "set " + SESSION_USER_NAME,        "dfs -ls -d ${hiveconf:hive.metastore.warehouse.dir}/" + queryTab 
//  Copy the files from different source file systems to one destination directory 
//  otherwise, we didn't understand it, so mark it maybe 
//  EXCLUSIVE locks occur before SHARED locks 
// now do Insert from Union here to create data files in sub dirs 
//  we found a map objectinspector. Grab the objectinspector for the value and initialize it   aptly 
//  Need unique IDs to refer to each min/max key value in the DynamicValueRegistry 
//  TOK_FROM subtree 
//  Preserve only partitioning 
//  Drop database, everything in all 4 meta tables should disappear 
//  run common join task 
//  replace the node in place 
//  Matches 2 times: one time the original node, one time the new node created by the rule 
//  non-MM case 
/*    * Group-by re-orders the keys emitted hence, the keyCols would change.    */
//  Shutdown hook to clean up resources at process end. 
/*  * Simple one long key map join benchmarks. * * Build with "mvn clean install -DskipTests -Pdist,itests" at main hive directory. * * From itests/hive-jmh directory, run: *     java -jar target/benchmarks.jar org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneStringKeyBench * *  {INNER, INNER_BIG_ONLY, LEFT_SEMI, OUTER} *    X *  {ROW_MODE_HASH_MAP, ROW_MODE_OPTIMIZED, VECTOR_PASS_THROUGH, NATIVE_VECTOR_OPTIMIZED, NATIVE_VECTOR_FAST} *  */
//  Filter timestamp against timestamp, or interval day time against interval day time. 
//   Disable SARGs for deleteEventReaders, as SARGs have no meaning. 
//  for rule: MapJoin%.*MapJoin   have a child mapjoin. if the the current mapjoin is on a local work,   will put the current mapjoin in the rejected list. 
//  HiveDecimal suppresses trailing zeroes. 
/*        * 3 different kinds of vectorized reading supported:       *       *   1) Read the Vectorized Input File Format which returns VectorizedRowBatch as the row.       *       *   2) Read using VectorDeserializeRow to deserialize each row into the VectorizedRowBatch.       *       *   3) And read using the regular partition deserializer to get the row object and assigning       *      the row object into the VectorizedRowBatch with VectorAssignRow.        */
//  Set server's idle timeout to a very low value 
/*    * Used to check recursive CTE invocations. Similar to viewsExpanded    */
//  Serialize to bytes 
//  unquoted space 
// so that we know the type of table we are creating: acid/MM to match what was exported 
// Don't add partition data if it already exists 
//  fix up the input column numbers and output column numbers 
//  setObject to the yet unknown type java.util.Date 
//  Truncate the excess chars to fit the character length.   Also make sure we take supplementary chars into account. 
//  If row limit does not match, we currently do not merge 
//  pass 
//  3) Build plan 
//  The total size of local tables after we merge localWorks   is larger than the limit set by   HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD. 
//  There should be 2 original bucket files in the location (000000_0 and 000001_0) 
//  Suppress empty column map. 
//  the data to shuffle 
//  In all other cases, throw an exception. Its a white-list of allowed operations. 
//  Create the Hadoop archive 
//  If it contains an aggregate and it is not a full acid table,   we do not rewrite it (we need MERGE support) 
//  2. A Window Spec with no Parition Spec, is Partitioned on a Constant(number 0) 
//  Minor optimization, avoiding creating new objects. 
//  alias is not fully qualified 
//  passing null matches everything 
//  check whether this input operator produces output   If it has residual, we do not skip this output,   we will add a Select on top of the join 
//  now propagate the constant from the parent to the child 
//  Target path's last component is also the column family name. 
//  Various restrictions. 
//  Handle to stop this process from the outside if needed. 
//  We have no estimator for this type... assume low overhead and hope for the best. 
/* |  Use | Boundary1.type | Boundary1. amt | Sort Key | Order | Behavior                          || Case |                |                |          |       |                                   ||------+----------------+----------------+----------+-------+-----------------------------------||   1. | PRECEDING      | UNB            | ANY      | ANY   | start = 0                         ||   2. | PRECEDING      | unsigned int   | NULL     | ASC   | start = 0                         ||   3. |                |                |          | DESC  | scan backwards to row R2          ||      |                |                |          |       | such that R2.sk is not null       ||      |                |                |          |       | start = R2.idx + 1                ||   4. | PRECEDING      | unsigned int   | not NULL | DESC  | scan backwards until row R2       ||      |                |                |          |       | such that R2.sk - R.sk > amt      ||      |                |                |          |       | start = R2.idx + 1                ||   5. | PRECEDING      | unsigned int   | not NULL | ASC   | scan backward until row R2        ||      |                |                |          |       | such that R.sk - R2.sk > bnd1.amt ||      |                |                |          |       | start = R2.idx + 1                ||   6. | CURRENT ROW    |                | NULL     | ANY   | scan backwards until row R2       ||      |                |                |          |       | such that R2.sk is not null       ||      |                |                |          |       | start = R2.idx + 1                ||   7. | CURRENT ROW    |                | not NULL | ANY   | scan backwards until row R2       ||      |                |                |          |       | such R2.sk != R.sk                ||      |                |                |          |       | start = R2.idx + 1                ||   8. | FOLLOWING      | UNB            | ANY      | ANY   | Error                             ||   9. | FOLLOWING      | unsigned int   | NULL     | DESC  | start = partition.size            ||  10. |                |                |          | ASC   | scan forward until R2             ||      |                |                |          |       | such that R2.sk is not null       ||      |                |                |          |       | start = R2.idx                    ||  11. | FOLLOWING      | unsigned int   | not NULL | DESC  | scan forward until row R2         ||      |                |                |          |       | such that R.sk - R2.sk > amt      ||      |                |                |          |       | start = R2.idx                    ||  12. |                |                |          | ASC   | scan forward until row R2         ||      |                |                |          |       | such that R2.sk - R.sk > amt      ||------+----------------+----------------+----------+-------+-----------------------------------|    */
//  1st task requested host2, got host2 
// Authorize the operation. 
//  that operator writes into to the bucket/sort columns for that data. 
//  projection that casts proj expr to a nullable type. 
/*  ascending  */
//  Partition keys 
//  Add new node to the cache 
//  Relying on the RPC threads to keep the service alive. 
/*  (non-Javadoc)   * @see org.apache.hadoop.hive.ql.optimizer.Transform#transform(org.apache.hadoop.hive.ql.parse.ParseContext)    */
//  Execute a malformed query 
//  PART_STATS 
// TODO: Even listener for check  AddcheckConstraintEvent addcheckConstraintEvent = new AddcheckConstraintEvent(checkConstraintCols, true, this); 
// 2,3,4,5 
//  We assume splits will never start in the middle of the stripe. 
//  Reset the buffer we're going to use 
//  Test that existing exclusive table with new shared_write coalesces to 
//  we minimize allocations 
//  Change a column 
// Increments one HMS connection (Hive.get()) 
//  call-1: open to read data - split 1 => mock:/mocktable2/0_0 
//  long scalar/column IF 
//  FileSystem.CACHE 
//  timestamps are not supported, both dates were changed to CE. 
//  [A: 0, B: 0, B.x: 0, B.y: 0, C: 1] 
//  is INSERT OVERWRITE TABLE 
//  get HS2 site.xml loaded 
//  containerEnd/taskEnd invocation. 
//  Link the RPC and the promise so that events from one are propagated to the other as   needed. 
//  Note: this will determine the order of columns in the result. For now, the columns for each         table will be together; the order of the tables, as well as the columns within each         table, is deterministic, but undefined - RR stores them in the order of addition. 
//  Reorder tags if need be 
//  Keep draining the queue in the same session. 
//  No room for optimization since we cannot convert to an empty   Project operator. 
//  I am the first thread to detect the error, cleanup old connection & reconnect 
//  Only BoneCP should return true 
//  Don't add the partition or table created during the execution as the input source 
//  Cancel the watchKey since the output dir has been found. 
//  collect key/values for this row. 
//  Update largest relation 
//  x = p' - p 
//  True if only one date is null 
//  Setup values registry 
//  Ignore, and break. 
// mapper can span partitions  combine into as few as one split, subject to the PathFilters set   using combine.createPool. 
//  Hint to disable mapjoin. 
//  Just compare the magnitudes (i.e. signums set to 1). 
//  Don't propagate the error - termination was done as part of closing the client. 
// default 80k (runs slightly over 1 day long) 
//  Include the original blank value Long.MIN_VALUE in the negatives to make sure we get 
//  previous record in the write buffers (see writeBuffers javadoc). 
//  Pool is exhausted, return a new object 
//  Pattern for key1=value1;key2=value2 
//  Check the output of FixAcidKeyIndex - it should indicate the index was invalid. 
//  now it's time to rewrite the Aggregate 
//  left repeats and is null 
//  Use BucketizedHiveInputFormat so that one mapper processes exactly one file 
//  Iterate through the opHandles and close their operations 
//  Create segment file at the destination location with LinearShardSpec(2) 
//  Return if metadata-only 
//  Unpartitioned table 
//  if subquery is in PROJECT 
//  required   required   required 
//  fetch the first group for all small table aliases 
// Test that partition key is not allowed in data 
//  Grab the tag and the field 
//   If the hadoop cluster is secure, do a kerberos login for the service from the keytab 
//  matched HS2 instance is not leader 
//  Try to reconnect to a child job if one is found 
//  GroupBy query results 
//  we lost statistics & opTraits through cloning, try to get them back 
//  This is an async method, so always launch threads, even for a single task. 
//  For now, just 2 Decimal64 inputs and a Decimal64 or boolean output. 
//  ExecutorService for sending heartbeat to metastore periodically. 
//  Given a list of partStats, this function will give you an aggr stats 
//  This means the column was not included in the projection from the underlying read 
//  1. Generate the token for query user (applies to all splits). 
//  For each source to write to, get the appropriate lock type.  If it's   an OVERWRITE, we need to get an exclusive lock.  If it's an insert (no   overwrite) than we need a shared.  If it's update or delete then we 
/*  there are filter operators in the pipeline  */
//  check filter input contains no correlation 
//  analyzeCreateView uses this.ast, but doPhase1 doesn't, so only reset it   here. 
//  Make sure session init gets stuck in init. 
// double wait time until 5min 
//  We want to use the ReturnObjectInspectorResolver because otherwise   ObjectInspectorUtils.compare() will return != for two objects that have   different object inspectors, e.g. 238 and "238". The ROIR will help convert   both values to a common type so that they can be compared reasonably. 
// update the nextlevel with newly discovered sub-directories from the above 
//  Create the list if needed 
//  We don't expect cache requests from the middle. 
//  there's some special handling for dummyOps required. Mapjoins won't be properly   initialized if their dummy parents aren't initialized. Since we cloned the plan 
//  Zero. 
//  No message is needed. 
//  http://web.archive.org/web/20071223173210/http://www.concentric.net/~Ttwang/tech/inthash.htm 
//  Enable BlobStore optimizations for the rest of tests 
//  Null HiveConf is passed in jdbc driver side code since driver side is supposed to be   independent of conf object. Create new HiveConf object here in this case. 
//  if the version doesn't exist, then create it 
/*  If the counters are missing there is no point trying to print progress  */
//  need to clean data directory to ensure that there is no interference from old runs   Cleaning is happening here to allow debugging in case of tests fail   we don;t have to clean logs since it is an append mode 
//  create a walker which walks the tree in a DFS manner while maintaining the   operator stack. The dispatcher generates the plan from the operator tree 
//  Replicate insert event and verify 
//  Do Decimal64 conversion instead. 
//  Both are non-empty, only copy now 
//  The JDOException may be wrapped further in a MetaException 
//  Trigger rewriting to remove UNION branch with MV 
//  As all txns below min_uncommitted_txnid are either committed or empty_aborted, we are allowed 
//  Should now have new lock on ACIDTBLPART 
//  non-acid 
//  Check whether the shuffle version is compatible 
//  Construct using org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.newBuilder() 
//  vectorTaskColumnInfo. 
//  Already setup in the create method 
/*      * If the skewedValues contains ((1,2,3),(4,5,6)), and the user is looking for     * positions (0,2), the result would be ((1,3),(4,6))     * Get the skewed key values that are part of the join key.     * @param skewedValuesList List of all the skewed values     * @param positionSkewedKeys the requested positions     * @return sub-list of skewed values with the positions present      */
//  Queries without a source table currently are not supported by CBO 
//  Start a third batch, but don't close it.  this delta will be ignored by compaction since 
//  If the cookie based authentication is already enabled, parse the 
//  Nope, so look to see if we can find a conf file by finding our jar, going up one   directory, and looking for a conf directory. 
//  Put the mapping from table scan operator to part-pruner map 
//  Diff against table on target. 
//  location is not shown in test mode 
//  Only database object is updated 
//  CSVReader will throw an exception if any of separator, quote, or escape is the same, but   the CSV format specifies that the escape character and quote char are the same... very weird 
//  we don't have many file formats that implement InputFormatChecker. We won't be holding 
// Move data from temp directory the actual table directory 
//  IS_ALL_PARTS 
//  required   required   required   required   required   required   required   required   optional 
//  remote metastore mode 
//  id 
/*  This class should be replaced with org.apache.hadoop.mapred.lib.CombineFileRecordReader class, once   * https://issues.apache.org/jira/browse/MAPREDUCE-955 is fixed. This code should be removed - it is a copy   * of org.apache.hadoop.mapred.lib.CombineFileRecordReader    */
//  column statistics from different sources are put together and 
//  Event 9, 10 
// should fail because the TransactionBatch timed out 
//  Whether the native vectorized map join operator has performed its common setup. 
// update stmt has p=blah, thus nothing is actually update and we generate empty dyn part list 
//  Prewarm CachedStore 
/*          * Check.5.h :: For In and Not In the SubQuery must implicitly or         * explicitly only contain one select item.          */
//  add uncovered ACID delta splits 
// make sure we assign correct Ids 
//  These names/types are the data columns plus partition columns. 
//  Sleep before we send checkLock again, but do it with a back off 
// try a valid alter table partition key comment 
//  Add small table result columns. 
//  Validate the update of new column c, even in old rows. 
//  If dataStr is not null and dataStr is not a KV pattern, 
//  expand the nested script   If the metaDbType is set, this is setting up the information   schema in Hive. That specifically means that the sql commands need   to be adjusted for the underlying RDBMS (correct quotation   strings, etc). 
// populate source 
//  all of the joins fit into half the memory. Let's be safe and scale them out. 
/*        * parse ResultExpr Str and setup OI.        */
//  creat default dir 
/*  * This class is the payload for custom vertex. It serializes and de-serializes * @numBuckets: the number of buckets of the "big table" * @vertexType: this is the type of vertex and differentiates between bucket map join and SMB joins * @numInputs: The number of inputs that are directly connected to the vertex (MRInput/MultiMRInput). *             In case of bucket map join, it is always 1. * @inputName: This is the name of the input. Used in case of SMB joins. Empty in case of BucketMapJoin  */
//  Repl imports are replace-imports, and thus, are idempotent.   Note that this assumes that this ImportCommand is running on an export dump   created using EXPORT ... FOR REPLICATION. If the scope of ImportCommand   were to eventually expand to importing dumps created by regular exports,   then this needs updating. 
//  Test if rpc_server_address is not configured but HS2 server host is configured 
//  Make the list of transactional tables list which are getting read or written by current txn 
//  Note: given that we return pool sessions to the pool in the finally block below, and that 
//  End of input. Confirm we got end of stream indicator from server,   as well as DONE status from fragment execution. 
/*     We are testing for both type of modes always so not passing that as a parameter for now   */
//  Serialize the result struct 
//  retain this digit 
//  Tracks tasks which are running. Useful for selecting a task to preempt based on when it started. 
//  maxCapacity should be calculated based on a percentage of memoryThreshold, which is to divide   row size using long size 
// delete clause 
//  unique key of the leftInputRel 
/*    * Validation:   * 1) Substitute class name for "ThisClass".   * 2) Only public fields and methods are versioned.   * 3) Methods compare on [non-]static, return type, name, parameter types, exceptions thrown.   * 4) Fields compare on [non-]static, type, name, value when static    */
//  decimal means decimal(10,0) 
//  Note: the normalize() call with rounding in HiveDecimal will currently reduce the         precision and scale of the value by throwing away trailing zeroes. This may or may         not be desirable for the literals; however, this used to be the default behavior         for explicit decimal literals (e.g. 1.0BD), so we keep this behavior for now. 
//  This field is not a null. 
//  regular single-partition write into a partitioned table. 
//  Safety check 
//  Save the conf variable values so that they can be restored later. 
/*      * Initialization here is adapted from MapOperator.MapOpCtx.initObjectInspector method.      */
//  Validate there is an added NULL for column c. 
//  Get Output Committer 
//  call-5: open - mock:/mocktbl2/0_1 
//  No data to read. 
//  Third row 
//  If fatal errors happen we should kill the job immediately rather than 
//  Test basic right trim to vector. 
//  add unique element to list per occurrence order in skewed value.   occurrence order in skewed value doesn't matter. 
//  We store CHAR in vector row batch with padding stripped. 
//  RPC already handles retries, so we will just try to kill the session here.   This will cause the current query to fail. We could instead keep retrying. 
//  Serialize numDistinctValue Estimator 
//  Run hive metastore server 
// mock operationManager for session 
/*         Case data is sorted by time and an extra hashing dimension see DRUID_SHARD_KEY_COL_NAME        Thus use DRUID_SHARD_KEY_COL_NAME as segment partition in addition to time dimension        Data with the same DRUID_SHARD_KEY_COL_NAME and Time interval will end in the same segment         */
//  Check that writeid#5 has been excluded.   Check that the data is in sorted order. 
// should never happen since we are reading bucket_x written by acid write 
//  do not do any blocking IO ops on this thread. 
//  However, it can be a constant too. In that case, we need to track   the column that it originated from in the input operator so we can   propagate the aliases. 
/*    * This batch is only used by vector/row deserializer readers.    */
// special handling for SQL "delete from <table> where..." 
//  show database level privileges 
//  Add in hive-site.xml.  We add this first so that it gets overridden by the new metastore 
//  HiveServer2 specific configs 
//  Create a new outgoing vectorization context because column name map will change. 
// 12 chars - try to keep cols aligned 
//  set current user in session conf 
//   file pattern that is set in PROPERTIES_FILE 
//  pRS-pGBY-cRS 
//  Ensure Pig can write correctly to smallint/tinyint columns. This means values within the 
//  Alter all partitions 
//  Create the row object 
//  Final result 
//  We deserialize the result 
//  Temp tables that do not go through SemanticAnalyzer may not have location set - do it here.   For example export of acid tables generates a query plan that creates a temp table. 
//  Find which columns we need to update for this partition, if any. 
//       and preserve rows only from left side. 
//  Create root scratchdir with write all, so that user impersonation has no issues. 
//  if unionWork is null, it means it is the first time. we need to   create a union work object and add this work to it. Subsequent   work should reference the union and not the actual work. 
//  aggregation result null? 
//  from TXN_COMPONENTS. 
//  process the first node to extract tablename 
// now we have a table with data files at multiple different levels. 
//  CHECK_EXPRESSION 
//  no need to add for the default supported local jar driver 
//  Set the thread local ip address 
//  Reimplemented to use PrimitiveCategory rather than TypeInfo, because   2 TypeInfos from the same qualified type (varchar, decimal) should still be   seen as equivalent. 
//  init output 
//  func may be null when GBY op is closing.   see mvn test -Dtest=TestMiniTezCliDriver -Dqfile=explainuser_3.q   original behavior is to create FMSketch 
//  We need to make sure that all the field associated with the union are settable. 
//  can't use the current table as the big table, but it's too   big for the map side. 
//  Cycle consists of atleast one dynamic partition pruning(DPP)   optimization and atleast one min/max optimization.   DPP is a better optimization unless it ends up scanning the   bigger table for keys instead of the smaller table. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setObject(java.lang.String,   * java.lang.Object, int)    */
/*  (non-Javadoc)   * @see org.apache.hadoop.mapreduce.InputSplit#getLocations()    */
//  If partitioning columns of the child RS are assigned,   assign these to the partitioning columns of the parent RS. 
//  LOG.info("Found list record at " + writeBuffers.getReadPoint());   Assumes we are here after key compare. 
// AddNotNullConstraintEvent addCheckConstraintEvent = new AddNotNullConstraintEvent(checkConstraintCols, true, this);  listener.onAddCheckConstraint(addCheckConstraintEvent); 
//  5. If the product of the topPermutation and bottomPermutation yields      the identity, then we can swap the join and remove the project on 
//  Synchronized by locking on itself. 
//  when we make a new connection we should get it from miniHS2_2 this time 
//  tablename and pattern 
//  After we set originalData to null, we incref the buffer and the cleanup would decref it.   Note that this assumes the failure during incref means incref didn't occur. 
//  data for HLL++ bias correction 
//  f is a directory 
// base_n cannot contain update/delete.  Original files are all 'insert' and we need to compact  only if there are update/delete events. 
//  Create an aggregate on top, with the new aggregate list. 
//  Should generate (-inf,+inf) 
//  move onto the next null byte 
//  is marked as being read.  Defaults to true as that is the most common case. 
//  extend any repeating values and noNulls indicator in the inputs 
// return genConvertCol(dest, qb, tab, table_desc, input, Arrays.asList(0), convert);   In the case of update and delete the bucketing column is always the first column,   and it isn't in the table info.  So rather than asking the table for it,   we'll construct it ourself and send it back.  This is based on the work done in   genConvertCol below. 
//  These are global since ORC reuses objects between stripes. 
//  Steps:   1. Create the archive in a temporary folder   2. Move the archive dir to an intermediate dir that is in at the same      dir as the original partition dir. Call the new dir      intermediate-archive.   3. Rename the original partition dir to an intermediate dir. Call the      renamed dir intermediate-original   4. Rename intermediate-archive to the original partition dir   5. Change the metadata   6. Delete the original partition files in intermediate-original 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getBlob(java.lang.String)    */
//  remove `` 
//  Parse the configuration parameters 
//  Number of reducers is set to default (-1) 
//  each bucket. 
//  If do not match, ignore the line, return a row with all nulls. 
//  In effect, the input is NULL because of out-of-range precision/scale. 
//  ID 1 has been committed, all others open 
//  whether it contains a sort merge join operator 
//  Always choose the function with least implicit conversions. 
// Join operators which may be converted by CommonJoinResolver; 
//  CONF_OVERLAY 
//  In order to facilitate partition pruning, or the where clauses together and put them at the 
//  2. rewrite the AST, replace TABREF with masking/filtering 
//  T | T | T 
//  1. Insert MapSide RS 
//  End the pools array. 
//  send failover request again to miniHS2_1 and get a failure 
//  We got the expr for one full partition spec. Determine the prefix length. 
//  So save old values... 
//  See https://blogs.msdn.microsoft.com/sqlprogrammability/2006/03/29/multiplication-and-division-with-numerics/ 
//  we need staging directories as long as a single partition needed addition 
//  paths = bucket files of small table for current bucket file of big table   initializes a FetchOperator for each file in paths, reuses FetchOperator if possible   currently, number of paths is always the same (bucket numbers are all the same over   all partitions in a table). 
//  initialize export path 
//  Validate that the multi-join is a valid star join before returning it. 
//  Unequal strings 
// verifyRun("SELECT a from " + replDbName + ".mat_view", ptn_data_1, driverMirror); 
//  caches objects before constructing forward cache 
//  Add the rest to the memory consumption 
//  Verify the fetched log (incrementally) 
//  What we are trying to get is the equivalent of new Date(ymd).getTime() in the local tz,   where ymd is whatever d represents. How it "works" is this. 
//  We can create Calcite IS_DISTINCT_FROM operator for this. But since our   join reordering algo cant handle this anyway there is no advantage of   this.So, bail out for now. 
//  test when two jars with shared dependencies are added, the classloader contains union of the dependencies 
//  intersperse getAt and next calls 
//  Make sure the referenced schema exists 
//  It will only throw JSONException when stats.put(BASIC_STATS, TRUE)   has duplicate key, which is not possible 
//  only mechanical data retrieval should remain here. 
//  only a column family 
//     conf.setVar(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK, CheckInputReadEntityDirect.class.getName()); 
//  This is an unsupported operator 
//  create a walker which walks the tree in a BFS manner while maintaining the 
//  The target column list has the format "TargetWork -> [colName:colType(expression), ...], ..." 
//  note we use col[1] -- the key is provided again as col[0] 
//  class to read (and re-read) the values. 
/*    * (non-Javadoc)   *    * @see   * org.apache.hadoop.hive.ql.exec.UDAFMethodResolver#getEvaluatorClass(java   * .util.List)    */
//  We need to make sure that the underlying fields are settable as well.   Hence, the recursive call for each field.   Note that equalsCheck is false while invoking getConvertedOI() because   we need to bypass the initial inputOI.equals(outputOI) check. 
//  reuse super renewal logic 
//  The application-level name   Component name   Component description   Name for each metric record 
//  serialized sizes after serialization and deserialization should be equal 
//  Variables to hold state from before flattening so it can be easily restored. 
//  required   required   required   optional   optional   optional   optional   optional 
//  First, check the local cache. 
//  Do nothing. 
//  we'd have thrown an exception 
//  Need to hive.server2.session.hook to SessionHookTest in hive-site 
//  duplicate of the value. Merging should remove duplicates 
//  sleep for expiry time, and then fetch again 
//  Upgrade schema from 0.7.0 to latest 
//  Apply comparison rules 
// for close the local work 
//  output is also big, so the output size is 1 (medium) 
//  for left outer joins, left alias is sorted but right alias might be not 
//  No errors 
//  Grow. 
//  Try to roll the key if none is found. 
//  Don't construct an illegal cache key 
//  Try to fold (key = 86) and (key is not null) to (key = 86) 
//  initializes them. 
//  Finishes the vectorization context after all the initial 
//  All the locks are created under this parent 
//  such as "a%bc" 
//  preserve precision. 
//  Create a single child representing the scratch column where we will 
//  Test strict locking mode, i.e. backward compatible locking mode for non-ACID resources.   With non-strict mode, INSERT got SHARED_READ lock, instead of EXCLUSIVE with ACID semantics 
//  since we are running the mapred task in the same jvm, we should update the job conf 
//  the big table can be divided by no of buckets in small tables. 
//  Update max if max is greater than the largest value seen so far 
//  Disable ansi sql arithmetic changes 
//  setting these 2 parameters here just in case that if the code got   changed in future, these 2 are not missing. 
//  Boolean is purposely excluded. 
// now run another compaction make sure empty dirs don't cause issues 
//  check if it is potential to trigger nullscan 
// compress key and write key out 
//  of gathering stats 
//  Set READ_ALL_COLUMNS to false 
//  Update file sink descriptor 
//  is usually called after close() to commit or rollback a query and end the driver life cycle. 
//  Display Error Message for tasks with the highest failure count 
//  We DO NOT set a bit in the NULL byte when we are writing a NULL. 
//  reasons. Roots are data sources, leaves are data sinks. I know. 
//  For each path, do getSplits(). 
//  If fop2 exists (i.e this is not the top level filter and fop2 is not 
//  Create the required temporary file in the HDFS location if the destination 
//  Set stats config for FileSinkOperators which are cloned from the fileSink 
//  all its parents operators are in state CLOSE and called close()   to children. Note: close() being called and its state being CLOSE is   difference since close() could be called but state is not CLOSE if   one of its parent is not in state CLOSE.. 
// start "delete from tab1" txn 
//  this is an invalid decimal value, getting HiveDecimal from it will return null 
//  Rows we looked up as one repeated key are a match.  But filtered out rows   need to be generated as non-matches, too. 
/*      * Connect via kerberos and get delegation token      */
// at lest for now, Load Data w/Overwrite is not allowed in a txn: HIVE-18154 
//  Adding postgres jdbc driver if exists 
//  Note that partitioning fields dont need to change, since it is either   partitioned randomly, or by all grouping keys + distinct keys 
//  remote metastore situation. 
//  authorization error is not really expected in a filter call   the impl should have just filtered out everything. A checkPrivileges call   would have already been made to authorize this action 
//  The split doesn't exclusively serve one alias 
//  No outer join involved 
//  only left input repeating and has no nulls 
//  search for match in the rhs table 
/*  10 files x 1000 size for 10 splits  */
// this should block behind the X lock on  T7.p=1 
/*  Sum of lengths of all values seen so far  */
//  The join operation in the child is not on the same keys 
//  Per ListObjectInpsector.getListLength(), -1 length means null list. 
//  Add an all-null record 
//  Handle leading/trailing whitespace 
//  Format the stored by statement 
//  TODO: if more writers are added, separate out an EncodingWriterFactory 
//  condition for merging is not met, see GenMRFileSink1. 
//  validate response 
//  SessionState is null, this is unlikely to happen, just in case 
//  keys from FetchSampler are collected here 
//  Note: we cache slices one by one since we need to lock them before sending to consumer.         We could lock here, then cache them together, then unlock here and in return, 
//  The state only changes from true->false   Once set to false, it may not change back to true 
//  uncaught exception handler that will be set for all threads before execution 
// expect 1 base or delta dir in this list 
//  By default, no children or inputs. 
//  Exchange partition is not allowed with transactional tables.   If only source is transactional table, then target will see deleted rows too as no snapshot   isolation applicable for non-acid tables.   If only target is transactional table, then data would be visible to all ongoing transactions   affecting the snapshot isolation.   If both source and targets are transactional tables, then target partition may have delta/base 
//  The dispatcher fires the processor corresponding to the closest matching   rule and passes the context along 
//  An optional group containing multiple elements 
//  We are done with the buffers; unlike data blocks, we are also the consumer. Release. 
//  small table. 
//  two parts of kerberos principal 
//  cte is actually a subquery. 
//  Execute SELECT and verify that aborted operation is not counted for MM table. 
//  This should eventually hang in the delay code.   From the background thread. 
//  We reuse the same hashmap to reduce new object allocation.   This means counts can be empty when there is no input data. 
//  while 
//  18 9's -- quite reliable! 
//  MetaException here really means ClassNotFound (see the utility method).   So, if any of these happen, that means we can never succeed. 
//  getHiveDefaultLocation(). 
//  Initialize fetch work such that operator tree will be constructed. 
//  get the values of repetition and definitionLevel 
//  instead of maintaining complex state for the fetch of the next group,   we know for sure that at the end of all the values for a given key,   we will definitely reach the next key group. 
//  write a base 
//  spread k-1 bits to adjacent longs, default is 8   spreading hash bits within blockSize * longs will make bloom filter L1 cache friendly 
//  in DB is set to bootstrap dump location used in C but for table/partition, it is missing. 
//  Collect column stats which need to be rewritten and remove old stats 
//  Setup Local Dirs 
//  End of entry reached? 
// make sure we know we saw an error that we don't recognize 
/*                * Multi-Key specific save key and lookup.                */
//  For WebUI.  Kept alive after queryPlan is freed. 
//  Note that enableBitVector does not apply here because ColumnStatisticsObj   itself will tell whether bitvector is null or not and aggr logic can automatically apply. 
// Pig script was successful 
//  Repeat the procedure for the new select. 
//  WRITE_ID 
//  reducer 
//  Try some time zone boundaries 
//  removes any union operator and clones the plan 
//  there's no key to return 
//  if init file contains incorrect row 
//  reset and add counters. This can happen during start of query or a session being moved to another pool with its   own set of triggers 
//  %% is folded in the .*?.*? regex usually into .*? 
//  3. Build new Table 
//  Make sure that the user doesn't happen to be in the super group 
//  The cache buffer comprises the tail of the requested range (and possibly overshoots it).   The same as above applies - may throw if cache buffer is larger than the requested range,   and there's another range after this that starts in the middle of this cache buffer.   Currently, we cache at exact offsets, so the latter should never happen. 
//  set data to empty explicitly 
//  We are trying to check ACLs on the "workers" directory, which noone except us should be   able to write to. Higher-level directories shouldn't matter - we don't read them. 
//  show create table is more sensitive information, includes table properties etc 
//  non-transient field, used at runtime to kill a task if it exceeded memory limits when running in LLAP 
/*    * Patterns of isRepeating columns   * For boolean: tri-state: null, 0, 1   * For others: null, some-value   * noNulls: sometimes false and there are no NULLs.   * Random selectedInUse, too.    */
//  use the table default storage specification 
/*  @bgen(jjtree) Typei16  */
//  make sure the arguments make sense 
//  Valid schemes 
//  this is backward compatible for non-ACID resources, w/o ACID semantics 
//  We have ensured that the keys are columns 
//  Use EntityDescriptorProto.newBuilder() to construct. 
//  a delete delta file with 25,000 delete events. 
//  This is one of the columns we're setting, record it's position so we can come back   later and patch it up.   Add one to the index because the select has the ROW__ID as the first column. 
//  Add stuff here as WM is implemented. 
//  Special case for root parent 
//  Make sure that each session has its own UDFClassloader. For details see {@link UDFClassLoader} 
//  Currently this method only sets    - Database    - FunctionName    - OwnerName    - OwnerType    - ClassName 
//  First try selecting methods based on the type affinity of the arguments passed   to the candidate method arguments. 
//  2. Validate that join condition is legal (i.e no function refering to   both sides of join, only equi join)   TODO: Join filter handling (only supported for OJ by runtime or is it   supported for IJ as well) 
// now save stats for partition we won't modify 
//  Check that the tables used do not resolve to temp tables. 
//  The collect method override for TopNHash.BinaryCollector 
//  create the merge file work 
//  put all virtual columns in RowResolver. 
//  PKCOLUMN_NAME 
//  Fix needed due to dependency for hbase-mapreduce module 
//  Return the mapping for table descriptor to the expected table OI 
//  This is a mapping of which keys will be copied from the big table (input and key expressions) 
//  Obtain filter for shared TS operator 
//  Verify that driver works fine with latest schema 
//  Shouldn't happen 
// no exception thrown, so looks good 
//  Expected error: should throw javax.net.ssl.SSLPeerUnverifiedException 
//  Code borrowed from VectorReduceSinkOperator.initializeOp 
//         LOG.info("Partition "+ spec.getKey()); 
//  Write directly into our BytesColumnVector value buffer. 
//  Case 2- find rows which have been deleted. 
//  Add sign byte since high bit is on. 
//     When using only HBase2, then we could change to this 
//  this is safe because o0 is positive 
//  Key is stored in text format. Get bytes representation of constant also of   text format. 
//  to avoid https://bugs.openjdk.java.net/browse/JDK-7122142 
//  Once we are done processing the line, restore the old handler 
//  We acquired all of the locks, so commit and return acquired. 
//  Make sure getting table in the wrong catalog does not work 
//  We don't expect conflicts from bad estimates. 
//  No need to acquire a lock twice on the same object   It is ensured that EXCLUSIVE locks occur before SHARED locks on the same object 
//  Try with 0 row file. 
//  batch size of 5 and decaying factor of 2 
//  generate the dummy driver by using txt file 
//  There should still be one request, as the locks still held. 
//  For all the existing partitions, check if the value can be type casted to a non-null object 
//  before we notify though, lock the list, so lock cannot remove it from the list. 
//  For type casts 
//  need not be traversed again 
//  this will insert FS and TS between the RS and its parent 
//  We're faking out Hive to work through a type system impedence mismatch.   Pull out the backing array and convert to a list. 
//  Get the reflection methods from ue 
//  if tablePropKey that was passed in lead to a valid URI resolution, update it if  parts of it match the old-NN-loc, else add to badRecords 
//  One session will be running, the other will be queued in "A" 
//  check out the statistics 
/*  Get the big table row container  */
//  Don't acquire locks for any of these, we have already asked for them in DDLSemanticAnalyzer. 
//  Verify that an attempt was made to schedule the task, but the decision was to skip scheduling 
//  in mapreduce case, we need to always clear up as mapreduce doesn't have object registry. 
//  vertex's children vertex. 
/*    * Set the buffer that will receive the serialized data.  The output buffer will be reset.    */
//  retry on any other exception 
//  Catalogs cannot be parsed as part of the query. Seems to be a bug. 
//        DagClient as such should have no bearing on jobClose. 
//  Get colstats for the original table column for selCol if possible, this would have 
//  clear all ThreadLocal cached MapWork/ReduceWork after plan generation   as this may executed in a pool thread. 
//  we want to signal an error if the function doesn't exist and we're   configured not to ignore this 
// add the columns in join filters 
//  used for create joinOutputObjectInspector 
//  ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS;   The plan consists of a simple SparkTask followed by a StatsTask.   The Spark task is just a simple TableScanOperator 
//  Compute the pseudo-random position from the above, then derive the actual header. 
//  partitions' locations which might need to be deleted 
//  The stack contains either ... TS, Filter or   ... TS, Filter, Filter with the head of the stack being the rightmost   symbol. So we just pop out the two elements from the top and if the   second one of them is not a table scan then the operator on the top of 
//  The default is such that there is no throttling. 
//  Report suspicious gaps in writeBuffers 
//  otherwise replace parent by sibling. 
//  The ObjectInspector for the current column 
// check other parts 
//  Get total size and individual alias's size 
//  Run partition pruner to get partitions 
//  get all cols 
//  FS_TRASH_CHECKPOINT_INTERVAL_KEY   FS_TRASH_INTERVAL_KEY (hadoop-2) 
//  Can't multiply NULL. 
//  Make the list of transactional tables list which are getting written by current txn 
//  to check the lastRecordOutput 
//  Open default connections which will be used throughout the tests 
// tbl.getPath() is null for views 
//  no table specified, check all tables and all partitions. 
// 2nd match is not supposed to be there 
//  this is a constant (or null) 
//  (the old instance has not been unregistered), and the new instances has not registered yet. 
//  If the table is in the pending prewarm list, move it to the top 
//  cross-product - no keys really 
//  update the FileSinkOperator to include partition columns 
//  values will override any values set in the underlying Hadoop configuration. 
//  It is possible for some request to be queued after a main thread has decided to kill this   session; on the next iteration, we'd be processing that request with an irrelevant session. 
// there is no point trying to validate further if we have no type info about target field 
/*          * setup OI for input to resultExpr select list          */
//  Is not an EXTERNAL table 
/*    * ============================== HOW TO RUN THIS TEST: ====================================   *   * You can run this test:   *   * a) Via the command line:   *    $ mvn clean install   *    $ java -jar target/benchmarks.jar VectorSelectOperatorBench -prof perf     -f 1 (Linux)   *    $ java -jar target/benchmarks.jar VectorSelectOperatorBench -prof perfnorm -f 3 (Linux)   *    $ java -jar target/benchmarks.jar VectorSelectOperatorBench -prof perfasm  -f 1 (Linux)   *    $ java -jar target/benchmarks.jar VectorSelectOperatorBench -prof gc  -f 1 (allocation counting via gc)    */
//  null. 
//  INFO_MESSAGES 
/*            * With a repeating value we can finish all remaining rows.            */
//  Views derive the column type from the base table definition.  So the view definition   can be altered to change the column types.  The column type compatibility checks should 
//  When people forget to quote a string, op1/op2 is null.   For example, select * from some_table where not ds > 2012-12-1 . 
// close() should just do nothing 
//  get the 'available privileges' from file system 
//  executor is single thread, so we can guarantee   domain created before any ATS entries 
/*    * - Called on functions that transform the raw input.   * - this method is invoked during translation and also when the Operator is initialized during runtime.   * - a subclass must use this call to setup the shape of the raw input, that is fed to the partitioning mechanics.   * - subsequent to this call, a call to getRawInputOI call on the {@link TableFunctionEvaluator} must return the OI   *   of the output of this function.    */
//  See if this node is a TOK_TABLE_OR_COL.  If so, find the value and put it in the list.  If   not, recurse on any children 
// Down the semaphore or block until available 
//  Update the count of the number of values seen so far 
//  read type of encoding 
//  These two structures track the list of known nodes, and the list of nodes which are sending in keep-alive heartbeats. 
//  If cred provider doesn't have entry, fall back to conf 
//  Add type params 
//  overwrite a value 
//  aggregateData already has the ndv of the max of all 
//  Thread-safe. 
//  Seed with the buddy of this block (so the first iteration would target this block). 
//  Null check because in some test cases we get a null from ms.getCatalog. 
//  matching rule and passes the context along 
//  Fraction digits continue into middle longword. 
//  to slow down the reducer so that SHUFFLE_BYTES publishing and validation can happen, adding sleep between   multiple reduce stages 
//  Thomas Wang's integer hash function 
//  create resolver 
//  2. Build Aggregations 
//  It's possible that a parition column may have NULL value, in which case the row belongs   to the special partition, __HIVE_DEFAULT_PARTITION__. 
//  Order on sourceColumn. 
//  the threshold size, convert the join into map-join and don't create a conditional task 
//  Add views to planner 
//  The id of the actual Spark job 
//  ClassNotFoundException, InstantiationException, IllegalAccessException   Class could not be init-ed, use our local copy 
//  Test string literal to string column comparison 
//  query info is created by SQLOperation which will have start time of the operation. When JDBC Statement is not   used queryInfo will be null, in which case we take creation of Driver instance as query start time (which is also   the time when query display object is created) 
//  Reset the interrupt status. 
//  inputSplitNum that contains the first row in this block. 
//  Run with cascade 
//  Restricted to text for now as this is a new feature; only text files can be sliced. 
//  We provide a faster way to write a date without a Date object. 
//  Whether any error occurred during query compilation. Used for query lifetime hook. 
//  No preemption with ducks reversed. 
//  This would be an attempt directory. Add a watch, and track it. 
//  Add in our conf file 
//  optimize for common case - just one row for a key, container acts as row 
//  When minor compacting, write delete events to a separate file when split-update is   turned on. 
//  convert RexNode to ExprNodeGenericFuncDesc 
//  These aren't real column refs; instead, they are special   internal expressions used in the representation of aggregation. 
//  Should not happen. 
//  help 
//  This has already been inspected and rejected 
// binary type should not be seen. 
//  If it is not created by HiveSortJoinReduceRule, we cannot remove it 
//  @@protoc_insertion_point(builder_scope:TerminateFragmentResponseProto) 
//  Cant overwrite existing keys 
//  deleted. The user will need to call unarchive again to clear those up. 
//  this is expected as these mock files are not valid orc file 
//  Restore interrupt, won't handle here. 
//  In a map-side join, exactly one table is not present in memory.   The client provides the list of tables which can be cached in memory 
//  collect all branching operators 
// create a fake directory to throw exception 
//  Known? 
//  map _col0 to KEY._col0, etc 
//  Whether the cycle is running 
//  event operators point to table scan operators. When cloning these we   need to restore the original scan. 
//  we found at least one children with mismatch 
//  If unable to find stats for a column, return null so we can build stats 
//  DECIMAL_STATS 
//  Data structures coming originally from QBJoinTree 
//  Cancel the heartbeat 
/* append */
//  10^-32 + 1 
//  The state has changed during the update. Let's undo what we just did. 
//  This should never happen - we only schedule one attempt once. 
//  we need to get state transition updates for the vertices that will send   events to us. once we have received all events and a vertex has succeeded,   we can move to do the pruning. 
//  10^-38 + 1 
//  Allow implicit Numeric to String conversion 
//  the registrator jar should already be in CP when not in test mode 
//  WRITEID 
//  Create one input split for each segment 
//  no special char 
// (23*60*60 + 59*60 + 59)*10e9 + 999999999 
//  Second data dir, contains 2 files. 
//  We've killed something and may want to wait for it to die. 
//  Size surpasses limit, we cannot convert 
//  By default 
//  Copy the first methodParameterTypes.length - 1 entries 
//  test month diff with fraction considering time components 
//  we replace existing view. 
//  On Tez only: The hash map might already be cached in the container we run 
//  UNDONE: Presumption of *append* 
/*    * Element for Key: Long x Hash Table: HashMultiSet    */
//  Data 
//  The destination table 
//  get close enough 
//  We look at all methods that generate values for explain 
//  check for fatal error again in case it occurred after   the last check before the job is completed 
//  set correct scheme and authority 
//  Reverse in place 
//  required   required   required   required   required   required   required   optional   optional 
//  Generate group-by operator 
//  Abstract function to add HttpAuth Header 
//  TOKEN_OWNER 
//  MY_BOOL 
//  mapreduce.tez.input.initializer.serialize.event.payload should be set to false when using 
//  Build the path from bottom up 
// start explicit txn so that txnMgr knows it 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createStruct(java.lang.String, java.lang.Object[])    */
//  Create joinTree structures to fill them up later 
//  Called once on the client. 
//  4. Decompress the data. 
//  see if expr is already present in reduceKeys.   get index of expr in reduceKeys 
//  If we are here, then we have established that firstRecordInBatch <= deleteRecord.   Now continue marking records which have been deleted until we reach the end of the batch   or we exhaust all the delete records. 
//  more queries can be added here in the future to test acid joins 
//  Table names with schema name, if necessary 
//  When Dynamic partitioning is used, the RecordWriter instance initialized here isn't used. Can use null.   (That's because records can't be written until the values of the dynamic partitions are deduced.   By that time, a new local instance of RecordWriter, with the correct output-path, will be constructed.) 
//  Generate dummy pre-upgrade script with errors 
//  user sets default queue now 
//  if same sign, just add up the absolute values 
//  Recreate to refresh jobConf of currTask context. 
//  class PartitionDropSwitches; 
//  which is the Correlator. 
//  Materialization is allowed if it is not a view definition 
//  STATUS_CODE 
//  Find the first non-zero digit or the last digits if all are zero. 
//  we get a text input format here, we can not determine a file is text   according to its content, so we can do is to test if other file   format can accept it. If one other file format can accept this file,   we treat this file as text file, although it maybe not. 
//  Invalid if table is not partitioned, but endPoint's partitionVals is not empty 
//  Bloom filter rest 
//  Need a separate table for ACID testing since it has to be bucketed and it has to be Acid 
//  Move to next valid index. 
//  EXPORT TABLE tablename [PARTITION (part_column="value"[, ...])]   TO 'export_target_path' 
//  First, look up the column from the source against which * is to be   resolved.   We'd later translated this into the column from proper input, if   it's valid.   TODO: excludeCols may be possible to remove using the same 
//  For Druid storage handler 
//  count characters forward and watch for final run of pads 
//  call-3: open - mock:/mocktbl2/0_1 
//  HAS_UNKNOWN_PARTITIONS 
//  Original method used deepCopy(), do the same here. 
//  constant or null, just return it 
//  buildV9Directly - use druid default, no need to be configured by user 
// --------------------------- PTF handling: PTFInvocationSpec to PTFDesc -------------------------- 
//  test for string type 
//  As hive conf is changed, need to get the Hive DB again with it. 
//  HIVE-3508 has been filed for this 
//  default partition key 
//  required   required   required   required   required   required   required   required 
/*  If all input columns are repeating, just evaluate function     * for row 0 in the batch and set output repeating.      */
//  Unsupported aggregation. 
//  UDF 
//  each side better have 0 or more RS. if either side is unbalanced, cannot convert.   This is a workaround for now. Right fix would be to refactor code in the   MapRecordProcessor and ReduceRecordProcessor with respect to the sources. 
//  Make sure matching name but wrong type doesn't return 
//  The delete_delta_110_110 should not be read because it is greater than the high watermark. 
//  task3 not allocated 
//  add hive-exec jar 
//  smallBuffer might still be out of space 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.newBuilder() 
//  SHOW LOCKS db2 
//  ReducerTraits.UNIFORM 
//  Deserialize and check... 
//  Track the dependencies for the view. Consider a query like: select * from V;   where V is a view of the form: select * from T 
//  Hostname:port 
/*    * This class is used to read one field at a time.  Simple fields like long, double, int are read   * into to primitive current* members; the non-simple field types like Date, Timestamp, etc, are   * read into a current object that this method will allocate.   *   * This method handles complex type fields by recursively calling this method.    */
//  Generate the map of the input->output column name for the keys   we are about 
//  how many records already buffered 
//  output privileges, and asks for select-no-grant on input. 
//  What we were reading from disk originally. 
/*    * Specify the columns to deserialize into as an array.    */
//  Try to allocate using base-buffer approach from each arena. 
// When file system cache is disabled, you get different FileSystem objects   for same file system, so '==' can't be used in such cases  FileSystem api doesn't have a .equals() function implemented, so using  the uri for comparison. FileSystem already uses uri+Configuration for  equality in its CACHE .  Once equality has been added in HDFS-9159, we should make use of it 
//  Verify that we can drain the pool, then cycle it, i.e. the state is not corrupted. 
//  Bring up the server only after all other components have started. 
//  Check if this is a MapJoin. If so, do not split. 
//  Add the new operator as child of each of the passed in operators 
//  Logical loop over the rows in the batch since the batch may have selected in use. 
/*          * For tez to route data from an up-stream vertex correctly to the following vertex, the         * output name in the reduce sink needs to be setup appropriately. In the case of reduce         * side merge work, we need to ensure that the parent work that provides data to this merge         * work is setup to point to the right vertex name - the main work name.         *         * In this case, if the big table work has already been created, we can hook up the merge         * work items for the small table correctly.          */
//  Http transport mode.   We set the thread local ip address, in ThriftHttpServlet. 
//  The sixth will not be combined because of delete delta files.  Is that desired? HIVE-18110 
//  Potentially wait on the cache entry if entry is in PENDING status   Blocking here can potentially be dangerous - for example if the global compile lock   is used this will block all subsequent queries that try to acquire the compile lock,   so it should not be done unless parallel compilation is enabled.   We might not want to block for explain queries as well. 
//  There's a fixed number of partition cols that we might have filters on. To avoid   joining multiple times for one column (if there are several filters on it), we will   keep numCols elements in the list, one for each column; we will fill it with nulls,   put each join at a corresponding index when necessary, and remove nulls in the end. 
//  No need to handle MM tables - unsupported path. 
//  If any of the partition requests are null, then I need to pull all   partition locks for this table. 
//  PURGE 
//  UNDONE: For now... 
//  disable trash   FS_TRASH_CHECKPOINT_INTERVAL_KEY (hadoop-2)   FS_TRASH_INTERVAL_KEY (hadoop-2) 
/*    * Table scan has the table object and pruned partitions that has information   * such as bucketing, sorting, etc. that is used later for optimization.    */
// don't overwrite user choice if transactional attribute is explicitly set 
//  output entry should not be null for null input for this particular generic UDF 
//  Then we determine the local TZ offset at that magical time. 
//  write the results in the file 
/*            This API changed from 2.x to 3.0.  so this won't even compile with 3.0           but it doesn't need to since we only run this preUpgrade           */
//  one single call to get all column stats 
//  Project only the correlated fields out of each inputRel   and join the projectRel together.   To make sure the plan does not change in terms of join order,   join these rels based on their occurrence in cor var list which 
//  Null first/last 
//  Normally, I'd worry about the blanket false being passed in here, and that   it'd need to be integrated into an abort call for an OutputCommitter, but the   underlying recordwriter ignores it and throws it away, so it's irrelevant. 
//  Important: Restore the batch's selected array. 
//  Pig's schema contain no type information about map's keys and   values. So, if its a new column assume <string,string> if its existing   return whatever is contained in the existing column. 
//  Remove col stats 
//  LOG.debug(CLASS_NAME + " logical " + logical + " batchIndex " + batchIndex + " New Key " + currentKey + " " + saveJoinResult.name()); 
//  Send done event, which LlapRecordReader is expecting upon end of input 
//  Check if owner has write permission, else it will have to copy 
// create more staging data and test Load Data Overwrite 
//  be very expensive sometimes 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#cancelOperation(org.apache.hive.service.cli.OperationHandle)    */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBlob(int, java.io.InputStream)    */
//  check if it is no scan. grammar prevents coexit noscan/columns 
//  Shift the remaining bins left one position 
//  Stop the CachedStore cache update service. We'll start it explicitly to control the test 
//  see discussion in YARN-5551 for the memory accounting discussion 
//  does any result need to be emitted 
//  enabling this will cause test failures in Mac OS X 
//  file is required parameter 
//  Check for PARTITION BY key change when we have ORDER BY keys. 
//  write value to object that can be inspected 
//  We need to drop the table. 
//  staging area for results, to avoid new() calls 
// should this close updaters[]? 
//  6/ test serialization and deserialization with different schemas 
//  A candidate. 
//  1.1. If it is not a RexCall, we continue 
//  Size of stdout buffer in bytes 
/*     todo: parse    target/tmp/org.apache.hadoop.hive.upgrade.acid.TestUpgradeTool-1527286256834/compacts_1527286277624.sql    make sure it's the only 'compacts' file and contains    ALTER TABLE default.tacid COMPACT 'major';ALTER TABLE default.tacidpart PARTITION(p=10Y) COMPACT 'major';    *  */
//  Username must be present 
// should not happen as we have accounted for all types 
//  Long 
//  Given a key, find the corresponding column name. 
//  If we are rounding, we may introduce one more integer digit. 
//  No static partitions specified and hence all are dynamic partition keys and need to be part   of temp table (input data file). 
//  The op may not be a TableScan for mapjoins   Consider the query: select /*+MAPJOIN(a)*/ count(*) FROM T1 a JOIN T2 b ON a.key = b.key; 
//  Add view-based rewriting rules to planner 
/*    * Inner big table only join (hash multi-set).    */
//  Case 4 - NOT IN list 
//  earlier version of Hive. 
//  remove incomplete outputs.   Some incomplete outputs may be added at the beginning, for eg: for dynamic partitions. 
//  Get the registry 
//  Write buffer is full   Read buffer isn't used, switch buffer 
/*      * Since we componentize Windowing, no need to translate     * the Partition & Order specs of individual WFns.      */
// decimal(a,b) type 
//  getColumns(String catalog, String schemaPattern, String 
//  evaluate filter expression and update statistics 
//  LLAP IO is off, don't output. 
//  LOG.debug("VectorMapJoinFastKeyStore equalKey match on bytes"); 
//  Note: for now, LLAP is only supported in Tez tasks. Will never come to MR; others may   be added here, although this is only necessary to have extra debug information. 
/*  Check if hashmap is on disk or in memory  */
// by default if user hasn't provided any optional constraint properties 
// 2627 is unique constaint violation incl PK, 2601 - unique key 
//  and accepts the first one clazz.getMethods() returns 
//  To handle the case of - select * from (select * from V1) A;   the currentInput != null check above is needed.   the alias list that case would be A:V1:T. Lookup on A would return null,   we need to go further to find the view inside it. 
//  Need to close the dummyOps as well. The operator pipeline   is not considered "closed/done" unless all operators are 
//  Open a base txn which allocates write ID and then committed. 
//  The 'it' data source will produce data w/o ever ending   We want to see that memory pressure kicks in and some 
//  The queue should be ignored. 
//  child 2 is the optional comment of the column 
//  do this only if there is a pre event listener registered (avoid unnecessary   metastore api call) 
//  Have to reset the conf when we change it so that the change takes affect 
//  hive compiler is going to remove inner order by. disable that optimization until then. 
//  We could also allow cutting off versions and other stuff provided that SHA matches... 
//  requested host is still alive but cannot accept task, pick the next available host in consistent order 
//  The query materialization validation check only occurs in CBO. Thus only cache results if CBO was used. 
//  Verify we handle the key column types for an optimized table.  This is the effectively the   same check used in HashTableLoader. 
//  Remove the DDL_TIME so it gets refreshed 
//  Testing with repeating and no nulls 
//  set the bit to 1 if a value is not null 
//  read keys from token store 
//  Now register as permanent function 
// make sure we are checking the right (latest) compaction entry 
//  returns first one matches all of the params 
//  Select algorithm with min cost 
//  Check if it is possible to drop default database 
/*  write byte size of the string which is a vint  */
//  Get the row structure 
//  +1 for 
//  Separate the base files into acid schema and non-acid(original) schema files. 
//  we already added this column in select list. 
//  equals() 
//  Should not happen? 
//  bags always contain tuples 
//  IN(STRUCT(..)..) ExprNodeDesc list for the current table alias. 
//  Look at getting rid of fractional digits that will now be below HiveDecimal.MAX_SCALE. 
//  each evaluator has constant java object overhead 
//  Test and/or more... 
//  masks for quicker extraction of p, pPrime, qPrime values 
//  We don't need the buffer anymore. 
//  looks like a subq plan.   todo we can collapse this part of tree into single TS 
// compactions are not happening. 
//  JDO 
// this method also initializes the consoleReader which is 
//  Preserved at initialization time to have a session to use during resize. 
//  No more data. 
//  append colnum to make it unique 
// so that it can be cancelled later from CompleteDelegator 
//  Not sequential with next. 
//  The same thing that WriterImpl does when writing the footer, but w/o the footer. 
//  start the mr input and wait for ready event. number of MRInput is expected to be 1 
// create reader, look at footer  no need to check side file since it can only be in a streaming ingest delta 
//  Initialize table properties from the table parameters. This is required because the table   may define certain table parameters that may be required while writing. The table parameter   'transactional_properties' is one such example. 
//  Empty, maybe because CBO did not run; we fall back to   full Select query 
//  bounds of the column type are written, and values outside throw an exception. 
//    2. Constructing a conditional task consisting of a move task and a map reduce task 
//  has dynamic as well as static partitions 
//  Check if it's '\r' or '\n' 
//  initialize() has not been called   initialize() has been called and close() has not been called,   or close() has been called but one of its parent is not closed. 
//  since it is noscan, it is true table name in command 
//  Retry with different dump should fail. 
//  Get a deterministic count of number of tasks for the vertex. 
//                join cost 
//  test basic operation 
/*  * Specialized class for doing a vectorized map join that is an outer join on Multi-Key * using a hash map.  */
//  If we fell through to here this is not a valid type conversion 
//  Assumptions:   precision >= scale   scale >= 0 
//  If statsObjOld is found, we can merge. 
//  test add_partitions 
//  -1 special case.  Unsigned magnitude 1 - two's compliment adjustment 1 = 0. 
//  remove all non alphanumeric letters, replace whitespace spans with underscore 
// Pre-analyze hook is fired in the middle of these calls 
//  Fail with a good message 
//  Check if size of data to shuffle (larger table) is less than given max size 
// here we need X lock on p=1 partition to write and S lock on 'table' to read which should 
//  If the cq prefix is non-empty, add it to the CQ before we set the mutation 
//  Note: isRawFormat is invalid for non-ORC tables. It will always return true, so we're good. 
/*            * Single-Column Long outer null detection.            */
//  Set up conf 
/*  This is a test function that takes three different kinds * of arguments, for use to verify vectorized UDF invocation.  */
//  Default values 
//  exact type conversion or get out 
//  create and load the input data into the hbase table 
//  output), minus the distinct aggCall's input. 
//  If authorizer is not set, check for metastore authorizer (eg. StorageBasedAuthorizationProvider) 
//  keep the small table alias to avoid concurrent modification exception 
/*    * Tests with queries which can be pushed down and executed with directSQL, but the number of   * partitions which should be fetched is bigger than the maximum set by the   * hive.metastore.limit.partition.request parameter.    */
//  need a new run of the constant folding because we might have created lots   of "and true and true" conditions.   Rather than run the full constant folding just need to shortcut AND/OR expressions 
//  FetchTask should not depend on the plan. 
// https://db.apache.org/derby/docs/10.1/ref/rrefsqlj31783.html  sadly in Derby, FOR UPDATE doesn't meant what it should 
/*  Keeps track of all events that need to be processed - irrespective of the source  */
//  not zero 
//  a0, a2 should be empty 
//  non default session nothing changes. The user can continue to use the existing   session in the SessionState 
//  Now the left join 
// with feature on, multiple tasks may get into conflict creating/using TMP_LOCATION and if we were  to generate the target dir in the Map task, there is no easy way to pass it to OutputCommitter 
//  c13:array<array<string>> 
//  Create a input stream of given name.ext  and write sql statements to to it 
//  hashCode() 
//  ingest size bytes gets resetted on flush() whereas connection stats is not 
//  Account for maximum cache buffer size. 
//  Validate inputs and outputs have right protectmode to execute the query 
//  we have to close in the processor's run method, because tez closes inputs   before calling close (TEZ-955) and we might need to read inputs   when we flush the pipeline. 
//  Only consider range operators if we haven't already seen one 
//  all divides are by 0.50 so the result column is 2 times col 0. 
//  Map Type 
//  Merge the target works of the second DPP sink into the first DPP sink. 
//  table   exists 
//  flag to indicate if it's the first time to read parquet data page with this instance 
//  The fractional digits are gone; clear remaining round digits. 
//  alter partitioned table's partition set partition property 
//  If already a file with same checksum exists in cmPath, just ignore the copy/move   Also, mark the operation is unsuccessful to notify that file with same name already   exist which will ensure the timestamp of cmPath is updated to avoid clean-up by   CM cleaner. 
//  Check mapreduce path 
//  Not included in the input collations, but can be propagated as this Aggregate   will enforce it 
//  3rd task requested host3, got host1 since host3 is dead and host4 is full 
//  verify that the whitlelist params can be set 
//  Find the positions of the bucketed columns in the table corresponding   to the select list.   Consider the following scenario:   T1(key, value1, value2) bucketed/sorted by key into 2 buckets   T2(dummy, key, value1, value2) bucketed/sorted by key into 2 buckets   A query like: insert overwrite table T2 select 1, key, value1, value2 from T1   should be optimized. 
//  -database database 
// TODO Duplicated code for init method since vectorization reader path doesn't support Nested   column pruning so far. See HIVE-15156 
//  VectorPTFOperator is native vectorized. 
//  Get all files from the src directory 
//  NEW_CAT 
//  set the log stream 
//  This will happen for count(*), in such cases we arbitarily pick   first element from srcRel 
/* ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree  ~/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505502329802/warehouse/t/.hive-staging_hive_2017-09-15_12-07-33_224_7717909516029836949-1//Users/ekoifman/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505502329802/warehouse/t/.hive-staging_hive_2017-09-15_12-07-33_224_7717909516029836949-1/ -ext-10000     HIVE_UNION_SUBDIR_1      000000_0     HIVE_UNION_SUBDIR_2      000000_0     HIVE_UNION_SUBDIR_3         000000_04 directories, 3 files      */
//  Case 2: NO column stats, NO hash aggregation, grouping sets 
//  count characters 
//  for outer joins, it should not exceed 16 aliases (short type) 
//  Using volatile instead of locking updates to this variable, 
//  -f <query-file> 
//  Remove the reduce sink operator 
//  VALUE_TYPE_PTR 
//  v[1] where (product % MULTIPLER_INTWORD_DECIMAL) is the carry from v[0].  
// get a partition 
//  Test that exclusive blocks read and exclusive 
//  set vector[2] to null to verify correct null handling 
//  Delayed tasks will not kick in right now. That will happen in the scheduling loop. 
//  total size of the composite object 
//  UNDONE: Method is still under development. 
//    Rewrite logic:     1. Pass along any correlated variables coming from the input.   
//  By definition, this session is not in use and can no longer be in use, so it only   affects the session pool. We can handle this inline. 
//  required for jackson 
// Create test dbs and tables 
//  transactionalListener.onAddNotNullConstraint(addDefaultConstraintEvent);  } 
//  If statsObjOld is not found, we just use statsObjNew as it is accurate. 
//  Make sure location string is in proper format 
//  This should be really close to zero. 
//  Normal case. 
// do not need the lock for partitions since they are covered by the table lock 
//  Do some logging and force log rollover 
//  The logical indices for reading with readField. 
//  We must have the duck still; it should just go to the other task. 
//  Note: we may add an async option in future. For now, let the task fail for the user. 
//  ABORTED_BITS 
//  all children are done or no need to walk the children 
//  The row consists of string columns, double columns, some union<int, double> columns only. 
// Repeating null 
//  the event else noop. 
//  give a sequence number for all the partitions 
//  arbitrary 
//  object overhead + 8 bytes for long (fastTime) + 16 bytes for cdate 
//  Create the filter Operator and update the parents and children appropriately 
//  Test ALTER DATABASE SET LOCATION. 
// because row 8 was updated and thus has a different RecordIdentifier now 
//  time   != 0 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setClientInfo(java.lang.String, java.lang.String)    */
//  recreate 
//  show all privileges 
// @formatter:off 
//  Caching is disabled for MapInput due to HIVE-8920 
//  derivedSchema ArrayList.) 
//  Password may no longer be in the conf, use getPassword() 
//  use the serialization option switch to write primitive values as either a variable 
//  Make sure the node got deleted. 
//  see javadoc of HBaseCompositeKey 
//  remainder 
//  Non-static methods to wrap the static AccumuloOutputFormat methods to enable testing 
//  No match was found, so create new entries 
//  Insert a globally unique 16-byte value every few entries, so that one   can seek into the middle of a file and then synchronize with record   starts and ends by scanning for this value. 
//  History file name 
//  This shows the relevant bits of the original hash value   and how the conversion is moving bits from the index value   over to the leading zero computation 
//  A new connection should be able to call describe/use function without issue 
//  A partition must have at least sdId and serdeId set, or nothing set if it's a view. 
//  Whether a series key is NULL. 
//  update number of columns from sel(*) 
//  Get the union value. 
//  returns the location on disc of the jar of this class. 
//  No more. 
//  skip trailing blank characters 
//  Get binary service port # 
//  No truncation needed 
/* since ROW_IDs are not needed we didn't create the ColumnVectors to hold them but we        * still have to check if the data being read is committed as far as current        * reader (transactions) is concerned.  Since here we are reading 'original' schema file,        * all rows in it have been created by the same txn, namely 'syntheticProps.syntheticWriteId'         */
//  Not found in the cache. Must be parameterized types. Create it. 
//  > 50 
// Open matches Metastore state 
//  The original record was lost in the deserialization, so just go the   correct way, through objectinspectors 
//  isEmptyPartition = false 
//  store table descriptor in map-work 
//  prepare empty routing table 
//  Add partitions for the partitioned table 
/*  PbB0  */
//  check if we are already at current schema level 
//  The database name is not changed during alter 
//  Expected number of dropPartitions call 
//  We get the cost of the operator 
// also ensures that heartbeat() is no-op since client is likely doing it async 
//  Set an upper bound how much we're willing to push before it should flush   we've set the memory treshold at 100kb, each key is distinct   It should not go beyond 100k/16 (key+data) 
//  Check streaming side 
//  source filesystem 
//  Note that this depends on the fact that no-one in this class calls anything but   getConnection.  If you want to use any of the Logger or wrap calls you'll have to   implement them. 
//  remove pwd from conf file so that job tracker doesn't show this logs 
//  Random test string 
//  Check if this is a subquery / lateral view 
//  This is for a special case to ensure unit tests pass 
// If its a FileSink to bucketed files, also use MR-style shuffle to 
//  Don't do constant folding here.  Wait until the optimizer is changed to do it.   Family of related JIRAs: HIVE-7421, HIVE-7422, and HIVE-7424. 
//  create dumpfile prefix needed to create descriptor 
//  already deleted 
//  MY_ENUMLIST 
// create temp dir 
// Get first level array 
//  and does not call the real method, so explicitly unset the queue name here 
//  Remember the input file formats we validated and why. 
//  Add scale. 
//  If the kill failed and the user also thinks the session is invalid, restart it. 
//  epoch, days since epoch 
//  Working directory. 
//  Build the filter and add parameters linearly; we are traversing leaf nodes LTR. 
//  Identifier 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createSQLXML()    */
//  In Tez, TEZ_AM_VIEW_ACLS/TEZ_AM_MODIFY_ACLS is used as the base for Tez ATS ACLS,   so if exists, honor it. So we get the same ACLS for Tez ATS entries and   Hive entries 
//  For group type, we need to build the projected group type with required leaves 
//  Verify that the property has been properly set while creating the 
/*  Class private variables  */
//  stream buffers are arranged in enum order of stream kind 
//  LastAnalyzed is stored per column but thrift has it per several;   get the lowest for now as nobody actually uses this field. 
//  add in what we found to our type and table tables. 
//  column name can be anything since it will be named by UDTF as clause 
//  noop for now 
//  append the first group within pattern: "${" 
// create two connections 
//  Multiplication with overflow check. Overflow produces NULL output. 
//  also include the still-in-memory sidefile, before it has been truely spilled 
//  Meets all requirements 
//  Test invalid case 
//  try the repeating null case 
//  operator list 
//  TODO: this assumes indexes in getRowIndexes would match column IDs 
//  clean prep 
//  Check if there segments to load 
//  We already have a struct node for the current index. Insert the constant value   into the corresponding struct node. 
//  call-1: listLocatedStatus - mock:/mocktbl2   call-2: check side file for  mock:/mocktbl2/0_1 
/*         * Job request got timed out. Job kill should have started. Return to client with        * QueueException.         */
//  estimated count 
//  adjust the data bytes according to any possible offset that was provided 
//  Use UpdateQueryRequestProto.newBuilder() to construct. 
//  rules on how to recurse the ObjectInspector based on its type 
//  (Currently none)   leftSemiPerBatchSetup(batch); 
//  can be null for void type 
//  Poll on the operation status till the query is completed 
//  Locks not associated with a txn 
/*      * The highWaterMark should be min(currentTxn,txns.getTxn_high_water_mark()) assuming currentTxn>0     * otherwise if currentTxn=7 and 8 commits before 7, then 7 will see result of 8 which     * doesn't make sense for Snapshot Isolation. Of course for Read Committed, the list should     * include the latest committed set.      */
//  un/compressed sizes of file and no. of rows 
//  we have found a merge work corresponding to this closing operator. Hook up this work. 
//  addPartitionDesc already has the right partition location 
//  check parts of the error, not the whole string so as not to tightly   couple the error message with test 
//  Task state is unkown 
//  call-1: listLocatedStatus - mock:/mocktbl2   call-2: check side file for  mock:/mocktbl2/0_0 
/*  * Specialized class for doing a vectorized map join that is an outer join on a Single-Column String * using a hash map.  */
//  For FINAL and COMPLETE 
//  Alters and replacements are not undoable if they've taken effect already. They are retriable though.   Creates are undoable, but we cannot differentiate between creates, alters and replacements from a Command level. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setCursorName(java.lang.String)    */
//  Mode 
//  align on multiples from origin 
//  Allocate/Map one txn per aborted writeId and abort the txn to mark writeid as aborted. 
//  init udf 
//  Check if forcing the location is required. 
//  No pattern or DB 
//  2. Partially contained   topOffset + topLimit > bottomLimit && topOffset < bottomLimit 
//  scope opened/closed 10 times 
//  partitionname ==>  (key=value/)*(key=value) 
// create 1 row in a file 000001_0 (and an empty 000000_0) 
//  three array   two int array 
// are variables from IN(...) constant 
//  All protocols 
//  TODO: expose all WMContext's via /jmx to use in UI 
//  Time doesn't matter. 
//  NO COLUMN STATS 
//  No data for the split, or it fits in the middle of one or two slices. 
//  Try to split bigger blocks. 
//  TODO: can we pass custom things thru the progress? 
//  This is impossible to read from this chunk. 
//  2) to unpartitioned table 
//  Used for legacy HiveDecimalV1 setScale compatibility for binary / display serialization of 
//  Read the template into a string, expand it, and write it. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#clearParameters()    */
/*    * Job callable task for job status operation. Overrides behavior of execute() to get   * status of a job. No need to override behavior of cleanup() as there is nothing to be   * done if job sttaus operation is timed out or interrupted.    */
/*        * on firstRow invoke underlying evaluator to initialize skipNulls flag.        */
//  Enum class? 
//  creates connection to HMS and thus *must* occur after kerberos login above 
//  start 1st server again 
//  Create map for tracking gauges 
//  If the time is in seconds, converts it to milliseconds first. 
//  either schema literal or serialization class must be provided 
//  Get the relevant information for this column 
//  Pass the ValidTxnList and ValidTxnWriteIdList snapshot configurations corresponding to the input query 
// we didn't see this lock when running DELETE stmt above but now it showed up  so should "should never happen" happened... 
//  we should get back null 
//  Operation log configuration 
//  Reserved as much as we needed. 
//  read prompt configuration and substitute variables. 
/*    * Returns true if the readField method is supported;    */
//  Integer parsing move to next lower longword. 
//   private final Helper helper; 
//  Grimacing Face U+1F62C (4 bytes) 
//  Return our known table name 
//  remove this backup zk server 
//  The planner will not include unneeded columns for reading but other parts of execution   may ask for them.. 
//  FOREIGN_TBL_NAME 
//  after constant folding of child expression the return type of UDFCase might have changed,   so recreate the expression 
//  Value can be anything, use the obj inspector and respect binary 
//  divided by max split size 
//  drop table. ignore error. 
//  generate the data 
// The actual check should be the compare of the connection string of the external tables 
//  columns from SEL(*) branch only and append all columns from UDTF branch to it 
// ~ Inner Classes ---------------------------------------------------------- 
//  Trim additional bytes 
//  4. Generate Parse Context for Optimizer & Physical compiler 
//  batch size of 20 and decaying factor of 2 
//  Do not update metrics, we'd immediately add the session back if we are able to remove. 
// return all the dependency urls 
//  OUTER AND INNER JOINS 
//  Noop: orcDataReaderRef is owned by the parent object 
/*        * setup OI for input to resultExpr select list        */
/*    * Test routines to exercise VectorizedRowBatch   * by filling column vectors with data and null values.    */
//  Check whether the mapjoin is a bucketed mapjoin.   The above can be ascertained by checking the big table bucket -> small table buckets   mapping in the mapjoin descriptor.   First check if this map-join operator is already a BucketMapJoin or not. If not give up 
//  RUN_ASYNC 
// http://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html#_using_generic_and_specific_arguments 
//  56 bit mask to generate positive 56 bit longs   from random signed longs 
//  We have 3 cases here:   1) All the data is in the cache. Always a single slice, no disk read, no cache puts.   2) Some data is in the cache. Always a single slice, disk read and a single cache put.   3) No data is in the cache. Multiple slices, disk read and multiple cache puts. 
//  Can this happen? Delta cannot exceed 0.5. 
// the partition did not change,   so the new partition should be similar to the original partition 
//  Streaming evaluators fill in their results during the evaluate call. 
//  TODO Track stats of rejections etc per host 
//  Right now we only support one special character '/'.   More special characters can be added accordingly in the future.   NOTE:   If the following array is updated, please also be sure to update the   configuration parameter documentation 
//  1. Build GB Keys, grouping set starting position   1.1 First Add original GB Keys 
//  Inspect the test data 
//  if this ast has only one child, then no column name specified. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getNClob(java.lang.String)    */
//  End of synchronized (ti) 
//  6 : Drop table T1 => 1 event 
//  Wrapper extends ql.metadata.Table for easy construction syntax 
//  drop the table 
//  Accumulo token already in Configuration, but the Token isn't in the Job credentials like the 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createStatement(int, int)    */
/*    * Specify the columns to deserialize into as a list.    */
//  tag the original file name so we know where the file comes from   Note we currently only track the last known trace as   xattr has limited capacity. We shall revisit and store all original 
//  @@protoc_insertion_point(builder_scope:SubmitWorkResponseProto) 
//  create some data 
//  right repeats 
//  Use current for now. 
//  processing will decref once, and the last one will unlock the buffers. 
//  Don't support changing name or type 
//  Connection.getMetaData().getTableTypes() when type config is set to "CLASSIC" 
//  does the conversion to String by itself. 
//  End JoinExtractFilterRule.java 
//  This one can be used to deny permission for performing the operation 
// ---------------------------------------------------------------------------   Process Multi-Key Inner Join on a vectorized row batch.   
/*  dataColumnNums  */
//  Compare input 
/*    * Setup a QBJoinTree between a SubQuery and its Parent Query. The Parent Query   * is the lhs of the Join.   *   * The Parent Query is represented by the last Operator needed to process its From Clause.   * In case of a single table Query this will be a TableScan, but it can be a Join Operator   * if the Parent Query contains Join clauses, or in case of a single source from clause,   * the source could be a SubQuery or a PTF invocation.   *   * We setup the QBJoinTree with the above constrains in place. So:   * - the lhs of the QBJoinTree can be a another QBJoinTree if the Parent Query operator   *   is a JoinOperator. In this case we get its QBJoinTree from the 'joinContext'   * - the rhs is always a reference to the SubQuery. Its alias is obtained from the   *   QBSubQuery object.   *   * The QBSubQuery also provides the Joining Condition AST. The Joining Condition has been   * transformed in QBSubQuery setup, before this call. The Joining condition has any correlated   * predicates and a predicate for joining the Parent Query expression with the SubQuery.   *   * The QBSubQuery also specifies what kind of Join to construct.   *   * Given this information, once we initialize the QBJoinTree, we call the 'parseJoinCondition'   * method to validate and parse Join conditions.    */
//  and grand child 
//  This is SQL standard - return state.MaxNArray, or null if the size is zero. 
//  Test that existing shared_write partition with new shared_write coalesces to 
//  This two functions are for use only in the planner. It will fail in a task. 
//  Simple pattern: Y-M 
//  we cannot convert to bucket map join, we cannot convert to   map join either based on the size. Check if we can convert to SMB join. 
//  create the map-join operator 
//  TEST - repeating non-NULL & selection 
//  cardinality estimate from normalized bias corrected harmonic mean on 
//  Get the counters for the input vertex. 
//  both classes access by subclasses 
//  Timeseries query results as records (types defined by metastore) 
//  private ReadStringResults readStringResults; 
//  Even if the user isn't doing schema evolution, cut the schema   to the desired size. 
//  Insert two rows into the table. 
// see HIVE-6194 
//  Set base to the location so that the input format reads the original files. 
// locks from different transactions detected (or from transaction and read-only query in autocommit) 
//  If a operator wants to do some work at the beginning of a group   the first group 
//  Convert all NaN values in vector v to NULL. Should only be used if n > 0. 
//  in filter expression since it will be taken care by partitio pruner 
//  Don't store too many items; if the queue is full we'll block the checker thread.   Since the worker count determines how many queries can be running in parallel, it makes   no sense to produce more work if the backlog is getting too long. 
//  checking. 
//  String query = SessionState.get().getCmd(); 
//  Session handle should not be null 
//  Nothing will be added to the expression 
//  2.1.1. If semijoin... 
/*      * Acending.      */
//  print foreign key containing parents 
//  iterate through children and push down not for each one 
//  realm is ignored 
//  Walk the operator tree to the TableScan and build the mapping 
//  Make one's complement, masked only for the bytes read 
//  nothing to be done for filters - the output schema does not change. 
/* make initialDelay a random number in [0, 0.75*heartbeatInterval] so that if a lot      of queries land on the server at the same time and all get blocked on lack of      resources, that they all don't start heartbeating at the same time */
//  Arithmetic operations (re)set the results. 
//  Rewrite projects to replace column references by constants when possible 
//  RESOURCE_PLANS 
//  +-------------|-------------+   |xxxx100000000|1000000000000|  (lr=9 + idx=1024)   +-------------|-------------+                  \   +---------------|-----------+   |xxxx10000000010|00000000000|  (lr=2 + idx=0)   +---------------|-----------+ 
//  Is this a primitive type? 
//  add it to the log processor: 
//  If the alias is already there then we have a conflict 
//  Create MD provider 
/*    * This operator only process small tables Read the key/value pairs Load them into hashtable    */
//  reset the BufferReader, if fetching from the beginning of the file 
//  extract any drop privileges out of required privileges 
//  Without this, 2020-20-20 becomes 2021-08-20. 
//  No change for multiply by 10^0 or value 0. 
//  Query should have a fetch task. 
//  cause we cannot prune columns from UDTF branch currently, extract 
//  substituted 
// count distinct with more that one argument is not supported 
/*      * Custom build arguments.      */
//  get node type 
//  has to use LinkedHashMap to enforce order 
//  ??? 
//  Will be superceded by credential provider   Will not be superceded 
//  The bucketing and sorting positions should exactly match 
//  distinct is at lost position. 
//  Subquery: no rewriting needed 
//  VectorMapJoinOperator 
//  call function 
//  Create StructObjectInspector 
//  String type is never stored as anything other than an escaped string 
//  Test sd params - we check that all the parameters in an empty table   are retained as-is. We may add beyond it, but not change values for   any parameters that hive defines for an empty table. 
//  1st task requested host1, got host1 
//  clear work from ThreadLocal after splits generated in case of thread is reused in pool. 
//  we can not use "split" function directly as ";" may be quoted 
//  disabling vectorization as this test yields incorrect results with vectorization 
//  Do the per-batch setup for an left semi join. 
//  Get a timestamp from a string constant or cast 
// completion of "delete from tab1" txn 
//  construct a map join and set it as the child operator of tblScan_op 
//  finished writing array contents 
// for dynamic uris, re-lookup if there are new metastore locations 
//  T | unknown | unknown 
//  Cause Timestamp object to be replaced (in buggy code) with ZERO_TIMESTAMP. 
//  GBy for DISTINCT after windowing 
//  this = this.mag / 10**this.scale   right = right.mag / 10**right.scale   this * right = this.mag * right.mag / 10**(this.scale + right.scale)   so, we need to scale down (this.scale + right.scale - newScale) 
//  Both children in the expression should not be literal 
//  empty c'tor to make jackson happy 
//  verify the directories in table location 
//  Try ISO-8601 format 
/*  This method converts from the String representation of Druid type   * to the corresponding Hive type  */
//  Indicates the maximum capacity of the cache. Minimum value should be the number of threads. 
//  Note: doesn't check for overflow. Could AND with max refcount mask but the caller checks. 
//  One time update issue.  When the new 'hive' catalog is created in an upgrade the   script does not know the location of the warehouse.  So we need to update it. 
//  Build the schema for this table, which is slightly different than the   schema for the input table 
/*      * Propagate null values for a two-input operator and set isRepeating and noNulls appropriately.      */
/*          * Positioned to first.          */
//  append a leading 0 if needed 
//  Long.MIN_VALUE 
/*        * Currently in tez, the flow of events is thus:       * "Generate Splits -> Initialize Vertex" (with parallelism info obtained       * from the generate splits phase). The generate splits phase groups       * splits using the TezGroupedSplitsInputFormat. However, for bucket map       * joins the grouping done by this input format results in incorrect       * results as the grouper has no knowledge of buckets. So, we initially       * set the input format to be HiveInputFormat (in DagUtils) for the case       * of bucket map joins so as to obtain un-grouped splits. We then group       * the splits corresponding to buckets using the tez grouper which returns       * TezGroupedSplits.        */
//  stores each cell's length of a column in one DataOutputBuffer element 
//  Pairwise: InitOuputColHasNulls, InitOuputColIsRepeating, Column1HasNulls,   Column1IsRepeating, Column2HasNulls, Column2IsRepeating 
//  7. If GroupingSets, Cube, Rollup were used, we account grouping__id 
//  only need flip the MSB? 
//  service a fresh conf for every testMethod 
//  Note that the calls below will throw an exception if a Java SecurityManager   is installed and configured to forbid invoking setAccessible(). In practice   this is not a problem in Hive. 
//  reset the execContext for each new row 
//  The read field of the Union gives us its tag. 
//  disable expensive operations on the metastore 
//  Something else holds the lock at the moment. Don't bother cleaning up. 
//  test null input 
//  Read length 
//  adjacency list 
//  CLUSTER / WORKER END     Job submitted to the cluster    
//  Shutdown the current active one 
/*             get the operation logs once and print it, then wait till progress bar update is complete            before printing the remaining logs.           */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getRef(int)    */
/*    * Get the CPU & GC   *   * counters org.apache.tez.common.counters.TaskCounter   *  GC_TIME_MILLIS=37712   *  CPU_MILLISECONDS=2774230    */
//  Shutdown the timeout thread if any, while closing this operation 
//  evaluate on row 
//  out   of   range   due   to   time 
//  sort before comparing with expected results 
//  Clearing this before sending a kill is OK, since canFinish will change to false.   Ideally this should be a state machine where kills are issued to the executor,   and the structures are cleaned up once all tasks complete. New requests, however, 
//  UNDONE: We need to get the table schema inspector from self-describing Input File           Formats like ORC.  Modify the ORC serde instead?  For now, this works. 
//  38 9's digits. 
//  Fall back to regular API and create statuses without ID. 
//  If the table does not define any transactional properties, we return a default type. 
//  Trigger query hook before compilation 
//  update columnar lineage for each partition 
//  This node was in previous union inputs, but it is not in this one 
//  e.g broadcast data 
//  stores Explain output 
/*  100 files x 1000 size for 99 splits  */
/*    * A different batch for vectorized Input File Format readers so they can do their work   * overlapped with work of the row collection that vector/row deserialization does.  This allows   * the partitions to mix modes (e.g. for us to flush the previously batched rows on file change).    */
//  Not always there is a SessionState. Sometimes ExeDriver is directly invoked 
//  table level event that matches us 
//  SW.SW: Lock we are examining is shared write 
//  avoid a copy. 
//  Last row of last batch determines isGroupResultNull and decimal lastValue. 
//  Now that we have found real data, emit sign byte if necessary. 
//  Create executor 
//  the existing entry already has grant, or new priv does not have   grant   no update needs to be done. 
//  skip some piece of data: 
//  Useful for class generation via templates. 
//  normally statsKeyPref will be the same as dirName, but the latter 
//  TODO Make sure to cleanup created dirs. 
// ------------------------------------------------- 
//  CAST(expr AS COLTYPE) AS COLNAME 
//  Full ACID export goes thru UpdateDelete analyzer. 
//  tests the missing element layer, detected by a multi-field group 
//  Only done when it is a bucket map join only no SMB. 
//  Friday 30th August 1985 12:00:00 AM 
//  Generate split strategy for acid schema files, if any. 
//  replace-mode creates are really alters using CreateTableDesc. 
//  Create an export task and add it as a root task 
//  core pool size   max pool size   direct hand-off 
// New record reader ID 
//  If we've already satisfied the number of events we were supposed to deliver, we end it. 
//  Create environment for AM. 
//  If the first argument is null, return null. (It's okay for other arguments to be null, in   which case, "null" will be printed.) 
//  create _SUCCESS FILE if so requested. 
//  Get the top operator and it's child, all operators have a single parent 
/*    * TODO: Handle 1) cast 2), Windowing Agg Call    */
//  if archiving was done at this or at upper level its level   would be lesser or equal to specification size   it is not, which means no archiving at this or upper level 
//  Need to iterate twice since BytesWritable doesn't support append. 
//  processing the message that the successful init has queued for us. 
//  get too crazy 
// the lazy struct object inspector 
//  call-3: check existence of side file for  mock:/mocktbl/0_1 
//  String including '\000' style literal characters. 
//  always mark as llap 
// test that new columns gets added to table schema 
//  Verify proxy user privilege of the realUser for the proxyUser 
//  If both sides are constants, there is nothing to propagate 
//  Add this node to the parent node. 
//  todo use final fields 
//  Check if the score for this method is any better relative to others 
//  The operator is not of RexCall type   So we fail. Fall through. 
//  check out the types 
/*  * Specialized class for doing a vectorized map join that is an inner join on a Single-Column String * and only big table columns appear in the join result so a hash multi-set is used.  */
//  When truncated included is used, its length must be at least the number of source type infos. 
//  we cannot proceed and need to tell the hive client that retries won't succeed either 
//  Add new information from source to target 
/*    * Remove the SubQuery from the Where Clause Tree.   * return the remaining WhereClause.    */
//  Now, deserialize it. 
// here we assume that upstream code may have parametrized the msg from ErrorMsg  so we want to keep it 
//  last value should be present 
// For each matching partition, call getSplits on the underlying InputFormat 
//  from 64-bit linear congruential generator 
//  constant propagation, constant folding 
//  Verify moveOnlyTask is NOT optimized 
//  Create or add node for current pool. 
//  c1, c2 and c3 are sorted in the same order. 
//  get splits from Accumulo 
/*  Determine if two strings are equal from two byte arrays each   * with their own start position and length.   * Use lexicographic unsigned byte value order.   * This is what's used for UTF-8 sort order.    */
// if a table is using some custom I/O format and it's not in the classpath, we won't mark  the table for Acid, but today (Hive 3.1 and earlier) OrcInput/OutputFormat is the only  Acid format 
/*  neededVirtualColumns  */
//  Launch it in the parallel mode, as a separate thread only for MR tasks 
//  7. Build Rel for Limit Clause 
//  L_STRING 
//  Convert from Java to Writable 
//  save it to the current script, if any 
//  MESSAGE_FORMAT 
//  A column family or qualifier we don't want to include in the map 
//  in case the sizes match, preference is   given to the table with fewer partitions 
//  The HiveServer2 instance running this service 
//  iterate through the index, so that if we add more children,   they don't get re-visited 
//  Write json to the temp file. 
//  Should produce JSON 
//  skip, not the one we are looking for. 
//  If script preserves alias and value for columns related to keys, user can set this true 
//  Deactivate currently active resource plan. 
// extra heartbeat is logically harmless, but ... 
//  otherwise write to the file system implied by the directory   no copy is required. we may want to revisit this policy in future 
// new MockFile("mock:/tmp/base_000123/bucket_00001", 500, new byte[0]), 
//  make the MoveTask as the child of the MR Task 
//  MESSAGE 
//  Test getter for map object. 
//  Plug verifying metastore in for testing DirectSQL. 
//  Special-case for ORC. 
//  The node should always be known by this point. Log occasionally if it is not known. 
//  Currently sum(distinct) not supported in PartitionEvaluator 
/*    * Assign a row from an array of objects.    */
//  Base path for REPL LOAD 
//  This vectorized code pattern says:       If the input batch has no nulls at all (noNulls is true) OR      the input row is NOT NULL, copy the value.        Otherwise, we have a NULL input value.  The standard way to mark a NULL in the      output batch is: turn off noNulls indicating there is at least one NULL in the batch      and mark that row as NULL.        When a vectorized row batch is reset, noNulls is set to true and the isNull array      is zeroed.     We grab the key at index 0.  We don't care about selected or repeating since all keys   in the input batch are suppose to be the same.   
//  DAYS_SINCE_EPOCH 
//  test left input repeating 
//  If this map task has a FileSinkOperator, and bucketing/sorting metadata can be   inferred about the data being written by that operator, these are mappings from the directory 
//  single table alias reference, ignore it and move to the next expression node. 
//  Currently, we only support these no-precision-loss or promotion data type conversions:      TinyInt --> SmallInt    TinyInt --> Int    TinyInt --> BigInt      SmallInt -> Int    SmallInt -> BigInt      Int --> BigInt      Float -> Double      Since we stare Char without padding, it can become a String implicitly.    (Char | VarChar) -> String   
//  3rd task requested host2, got host1 as host2 and host3 are full 
//  Test that only the fixed property (...ForQueue) is used in order determination, not the dynamic call. 
//  Add support for configurable threads, however 1 should always be enough. 
//  ConfVars.SCRATCHDIR - {test.tmp.dir}/scratchdir 
//  Map that contains the rows for each alias 
//  We might still be able to push the limit 
//  A mapper can span multiple files/partitions.   The VectorPartitionContext need to be changed if the input file changed 
//  if all files are needed to meet the size limit, we disable   optimization. It usually happens for empty table/partition or   table/partition with only one file. By disabling this   optimization, we can avoid retrying the query if there is   not sufficient rows. 
//  Find the parsed delta files. 
//  add fake entries 
//  trigger clean errors for anyone who mixes up identity with hosts 
//  Same algorithm as TimestampWritable (not currently import-able here). 
//  write a non-null element 
//  5. Run Cleaner. It should remove the 2 delta dirs. 
//  overflow checks 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getTableTypes(org.apache.hive.service.cli.SessionHandle)    */
//  3. Obtain Stats for Partition Cols 
//  to a list. 
//  fallback to default 
//  Are we using the fact the input is sorted 
// writing both acid and non-acid resources in the same txn  tab1 write is a dynamic partition insert 
//  If it is an external table, we are done 
// create an empty (invalid) side file - make sure getLogicalLength() throws 
//  Specify that the results of this query can be cached. 
// test if null dbName works ("default" is used) 
//  we are aborting only the current transaction, so move the min range for heartbeat or disable heartbeat   if the current txn is last in the batch. 
//  Return mocked SerDeInfo 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getDate(int, java.util.Calendar)    */
//  Copy over the mandatory configs for the package. 
//  Generate row values. 
//  6.2.2 Update Output Row Schema 
//  Map from type name (such as int or varchar(40) to the corresponding PrimitiveTypeInfo 
//  everything qualifies (4 rows, all with value -1) 
//  if there's no RexInputRef in the projected expressions   return empty set. 
//  Check object. 
// Expecting to change the size of internal structures 
//  Proxy class within the tez.api package to access package private methods. 
/*          * sz Estimate = sz needed by underlying AggBuffer + sz for results + sz         * for maxChain + 3 * JavaDataModel.PRIMITIVES1 sz of results = sz of         * underlying * wdwSz sz of maxChain = sz of underlying * wdwSz          */
//  we need to create the merge join work 
//  Sort the splits, so that subsequent grouping is consistent. 
//  max length for varchar and char cases 
//  All are selected; 
//        "insert overwrite directory" command if there were no bucketing or list bucketing. 
// statementId = 1 
//  Save char/varchar as string 
//  For NULL fields, make up a valid max length. 
//  trigger failover on miniHS2_1 without authorization header 
// normilize table name for mapping 
//  We don't currently allow imposition of a type 
//  Array should have ListTypeInfo   Within the list, we extract types 
//  No locks, no txn, we outta here. 
//  Does the logger config look correct? 
// verifyRun("SELECT * from " + replDbName + ".mat_view2", unptn_data, driverMirror); 
//  delete remaining jars 
//  lock a table, as in dynamic partitions 
//  Correlating variables are a means for other relational expressions to use   fields. 
//  serialize the row as a struct 
//  is under the limit. 
//  build partition strings 
//  Not in VRB mode - the new cache data is (partially) ready, we should use it.   Force the rest of the data thru. 
//  Column is not a partition column for the table,   as we do not allow partitions based on complex   list or struct fields. 
//  parse out n-grams, update frequency counts 
//  remove the tag from key coming out of reducer   and store it in separate variable.   make a copy for multi-insert with join case as Spark re-uses input key from same parent 
//  this will take care of mapping between input column names and output column names. The   returned column stats will have the output column names. 
/*        * Check.12.h :: SubQuery predicates cannot only refer to Outer Query columns.        */
//  Show we cannot create a child of a file 
//  Iterate over the map and remove semijoin optimizations if needed. 
//  1 more than capacity. 
//  Check if this UDF has been provided with type params for the output varchar type 
//  execute final aggregation stage for simple fetch query on fetch task 
//  1. We try to merge this join with the left child 
//  create the adaptor for this function call to work in vector mode 
//  get the vlong that represents the map size 
//  switch to hive-site.xml with remote metastore 
//  Our one time process method initialization. 
//  start_date is Sun, 2 letters day name 
//  Delay with exponential backoff 
//  done with the row 
//  Sleep for 3 seconds 
/*  A UDF like one a user would create, implementing the UDF interface. * This is to be used to test the vectorized UDF adaptor for legacy-style UDFs.  */
//  f is a file 
//  For table level load, need not update replication state for the database 
// I don't think this can happen... but just in case 
//  This a 2nd batch with the same "column schema" as the big table batch that can be used to   build join output results in.  If we can create some join output results in the big table   batch, we will for better efficiency (i.e. avoiding copying).  Otherwise, we will use the 
//  Perform some operations 
//  load the partition 
//  Try with null args 
//  The FSOP configuration for the FSOP that is going to write initial data during ctas. 
//  Add all columns to make a vectorization context for   the TableScan operator. 
//  Do rounding of fractional digits. 
//  During map/reduce tasks, there may not be a valid HiveConf from the SessionState.   So lookup and save any needed conf information during query compilation in the Hive conf   (where there should be valid HiveConf from SessionState).  Plan serialization will ensure   we have access to these values in the map/reduce tasks. 
//  Leave the client to time out. 
// http://db.apache.org/derby/docs/10.7/ref/rrefsqljoffsetfetch.html 
//  Generate the right hand side of the IN clause 
/*    * the WindowingSpec used for windowing clauses in this QB.    */
//  We trigger the transform 
//   - processing is completed. 
//  Multiple stripes 
//  insert the dummy store operator here 
/*      * load entire archive     * There is some parallelism going on if you load more than 1 partition     * The file name changes from run to run between 000000_0 and 000001_0 and 000002_0     * The data is correct but this causes ROW__ID.bucketId/file names to change      */
// if here checked all parts and they are Acid compatible - make it acid 
//  Test behavior with non-chunked streams 
//  If there are aborted txns, then the minimum aborted txnid could be the min_uncommitted_txnid 
//  END_TIME 
//  The children type should be converted to return type 
//  unit tests can overwrite this to affect default dump behaviour 
//  lower case roleName 
//  Set it on our inputformat 
//  Column types 
//  TODO: if a side of the union has 2 columns with the same name, noone on the higher         level can refer to them. We could change the alias in the original node. 
//  then we make sure the file sink operators are set up right 
//  There is a kryo which after serialize/deserialize,   Timestamp becomes Date. We get around this issue in   SearchArgumentImpl.getLiteral. Once kryo fixed the issue   We can simplify SearchArgumentImpl.getLiteral 
// no insert schema was specified 
//  Describe how to deserialize data back from user script 
/*    * Sets the job state and result. Returns true if status and result are set.   * Otherwise, it returns false.    */
//  Table existed, and is okay to replicate into, not dropping and re-creating. 
//  LOCATION_URI 
//  at most one alias is unknown. we can safely regard it as a big alias 
//  this GenericUDF can't be pushed down 
//  only first stripe will satisfy condition and hence single split 
//  Used for value registry 
//  view is referring to old database, so no data 
//  For each table reference get the table name   and the alias (if alias is not present, the table name 
//  hive, so this is a no-op 
//  Generate the service ticket for sending to the server.   Locking ensures the tokens are unique in case of concurrent requests 
//  Make data consistent with encodings, don't store useless information. 
//  merges sampling data from previous MR and make partition keys for total sort 
//  Resend existing value if necessary. 
//  only the admin is allowed to list privileges for any user 
/*          * Nothing to do. Just retry.          */
//  create a Map to capture object privileges corresponding to privilege 
//  The getPrimitiveWritableObject method returns a new writable. 
// include same hl_last_heartbeat condition in case someone heartbeated since the select 
//  there can be multiple instances per node 
//       int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp); 
// check to make sure there are no duplicate ROW__IDs - HIVE-16832 
//  If oldReplState is less-than newReplState, allow. 
//  Fixup numbers to limit the range to 0 ... N-1. 
// the first group. 
//  check to see if this an input job or an outputjob 
//  serialize the configuration once .. 
//  Change correlator rel into a join.   Join all the correlated variables produced by this correlator rel 
//  If this is a secured cookie and the current connection is non-secured,   then, skip this cookie. We need to skip this cookie because, the cookie   replay will not be transmitted to the server. 
//  Avro requires NULLable types to be defined as unions of some type T   and NULL.  This is annoying and we're going to hide it from the user. 
//  Drop partition after dump 
//  the max is good, the min is too low 
//  list the loggers and their levels 
//  Then, create splits with the Druid queries. 
//  sort will try to open the output file in write mode on windows. We need to   close it first. 
//  detect correlations 
//  Now, we check if the partition already exists. If not, we go ahead.   If so, we error out if immutable, and if mutable, check that the partition's IF   matches our current job's IF (table's IF) to check for compatibility. If compatible, we   ignore and do not add. If incompatible, we error out again. 
//  Otherwise try default timestamp parsing 
//  ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint> 
//  Try to reduce 
/*  @bgen(jjtree) TypeMap  */
//  The structure of the AST for the rewritten insert statement is:   TOK_QUERY -> TOK_FROM            \-> TOK_INSERT -> TOK_INSERT_INTO                          \-> TOK_SELECT                          \-> TOK_SORTBY   The following adds the TOK_WHERE and its subtree from the original query as a child of   TOK_INSERT, which is where it would have landed if it had been there originally in the   string.  We do it this way because it's easy then turning the original AST back into a   string and reparsing it.  We have to move the SORT_BY over one,   so grab it and then push it to the second slot, and put the where in the first slot 
//  Determine if the user would need to sign fragments. 
//  Write a couple of batches 
/* this constant is a bit of a misnomer since we now always have a txn context.  It                   just means the operation is such that we don't care what tables/partitions it                   affected as it doesn't trigger a compaction or conflict detection.  A better name                   would be NON_TRANSACTIONAL. */
//  DATA 
//  an integer. 
// GenericUDFToUnixTimeStamp 
/*  not supported  */
// "unknown"; // No information to judge. 
//  5.2. Finally, hand off to the stripe reader to produce the data. 
//  4. We check if it is a permutation project. If it is 
//  heartbeat started.. 
//  there could be case where join operators input are not RS e.g.   map join with Spark. Since following estimation of statistics relies on join operators having it inputs as   reduced sink it will not work for such cases. So we should not try to estimate stats 
//  Pattern does not contain all date fields 
//  (partition to bucket file names) and (partition to bucket number) for 
//  Verify if Drop table on a non-existing table is idempotent 
//  Explaining this would really require a picture. Basically if the level is lower than   our level, that means (imagine a tree) we are on the leftmost leaf node of the sub-tree   under our sibling in the tree. So we'd need to look at the buddies of that leftmost leaf   block on all the intermediate levels (aka all intermediate levels of the tree between   this guy and our sibling). Including its own buddy on its own level. And so on for every   sub-tree where our buddy is not on the same level as us (i.e. does not cover the entire 
//  incorrect use of this class 
//  Write schema since we need it to pull the data out. (see point #1 above) 
/*     even for regular copy we have to use the same user permissions that distCp will use since    hive-server user might be different that the super user required to copy relevant files.    */
//  Only release cache chunks; do not release ProcCacheChunks - they may not yet have data. 
//  the indexes of the delimiters 
//  After load, shall see the overwritten data. 
// DataConstraintViolationError is expected 
// statementId = 0 
//  process join values 
// template, <ClassName>, <ValueType>, <OutputType>, <OutputTypeInspector> 
//  Create ReduceSinkOp operator 
//  populate local work if needed 
//  create fetchwork for partitioned table 
//  Turn on client-side authorization 
// this is the IOW case 
//  The synchronization here is not necessary, but tests depend on it. 
//  Since there's no close() here, maintain the initial read position between writes. 
//  fields in these aggregation classes. 
//  No actual result directory, no need to move anything. 
//  No conversion is possible for the reduce keys 
//  bugbug, need to deal with a named type here - i.e., look it up and proxy   to it   should raise an exception if this is a typedef since won't be any   children   and thus we can quickly find this comment and limitation. 
//  DO_GET_FOOTERS 
//  if the same day of the month then time part should be ignored 
//  With nulls and selected 
//  we need to clone some operator plans and remove union operators still 
//  != 
// If coming from big-table side, do some book-keeping, and continue traversal 
//  about filtering. 
//  must have at most one child 
//  Calculate result TypeInfo 
//  Print since otherwise exception is lost. 
/*  Update summary bitVector :     * Generate hash value of the long value and mod it by 2^bitVectorSize-1.     * In this implementation bitVectorSize is 31.      */
// (hl_txnid <> 0 AND hl_lock_state = '" + LOCK_WAITING + "') is for multi-statement txns where 
// use get() to make sure variable substitution works 
//  Meanwhile, the init fails. 
//  We do not need to do a column reset since we are carefully changing the output. 
//  verify that ptned table property set worked 
// this is a tmp table and thus Session scoped and acid requires SQL statement to be serial in a 
/*      * When there is no Order specified, we add the Partition expressions as     * Order expressions. This is an implementation artifact. For UDAFS that     * imply order (like rank, dense_rank) depend on the Order Expressions to     * work. Internally we pass the Order Expressions as Args to these functions.     * We could change the translation so that the Functions are setup with     * Partition expressions when the OrderSpec is null; but for now we are setting up     * an OrderSpec that copies the Partition expressions.      */
//  implicit type conversion hierarchy 
// The data type primitive category of the column being deserialized. 
//  We need to include isInsideView inside digest to differentiate direct 
//  we will clone here as RS will update bucket column key with its 
//  actually create the permanent function 
/*  Default list bucketing directory key. internal use only not for client.  */
//  For queue size estimation purposes, we assume all columns have weight one, and the following   types are counted as multiple columns. This is very primitive; if we wanted to make it better, 
//  As we're calling processOp again to process the leftover "tuples", we know the "row" is   coming from the spilled matchfile. We need to recreate hashMapRowGetter against new hashtables 
//  Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll   read formatted thrift objects from the output SequenceFile written by Tasks. 
//  Introduce a select after the union 
//  From java.sql.Timestamp used by vectorization to serializable org.apache.hadoop.hive.common.type.Timestamp 
/*  maxComplexDepth  */
//  local aliases need not to hand over context further 
//  Add the filter to the queryId appender 
//  mapjoin table descriptor contains a key descriptor which needs the field schema   (column type + column name). The column name is not really used anywhere, but it   needs to be passed. Use the string defined below for that. 
// set up props for read 
// read friendly string: [STX]ak[EXT]av[STX]bk[ETX]bv[STX]ck[ETX]cv[STX]dk[ETX]dv 
//  repeat 20 times 
//  One each for min, max and bloom filter 
//  Make sure the UGI is current. 
//  Reached the end of the tag 
//  create view 
/*        * if there is a remainder from numRows/numBuckets; then distribute increase the size of the first 'rem' buckets by 1.        */
//  we need to convert the thrift type to the SQL type 
//  used to handle skew join 
//  push down projections 
//  Custom parameters 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.newBuilder() 
//  check if 730517 Julian Days between Jan 1, 0005 and Jan 31, 2005.   This method used to test Julian Days between Jan 1, 2005 BCE and Jan 1, 2005 CE. Since BCE 
//  Start monitoring the Spark job, returns when the Spark job has completed / failed, or if 
//  COUNT, DENSE_RANK, and RANK do not care about column types.  The rest do. 
//  failure from not having permissions to create table 
//  Optimized for sequential key lookup. 
//  No security or the path is below the user path - full access. 
//  index into a map 
//  MAX_ROWS 
//  Detect queries of the form SELECT udtf(col) AS ...   by looking for a function as the first child, and then checking to see   if the function is a Generic UDTF. It's not as clean as TRANSFORM due to 
//  Convert from mapjoin to bucket map join if enabled. 
//  Hive values we have copied and use as is 
//  Test with multi-level scratch dir path 
//  clear the columnBuffers 
//  Make a cost based decision to pick cheaper plan 
//  The dispatcher generates the plan from the operator tree 
// keep track for error reporting 
// Ignored for some reason the bean was not found so don't output it 
//  Save the current record as the new extraValue for next time so that 
//  Now add to cache 
// todo: https://issues.apache.org/jira/browse/HIVE-15048 
//  For each node 
//  Make sure originalDate is at midnight in the local time zone,   since DateWritableV2 will generate dates at that time. 
//  Concurrency 
//  generated the log message). 
//  Index of the next free block to split. 
//  Do any hive related operations like moving tables and files 
//  A specification of binary storage should not affect ser/de. 
//  In the automation, the data warehouse is the local file system based. 
//  create a mapred task for this work 
// case 1: 01:01:01.0000000001 
//  Current replication state must be set on the Table object only for bootstrap dump.   Event replication State will be null in case of bootstrap dump. 
//  on tez we're avoiding to duplicate the file info in FileInputFormat. 
//  SessionManager is initialized 
//  The parameter keys for the table statistics. Those keys are excluded from 'show create table' command output. 
//  the first position of partition column 
//  Sort the methods before omitting them. 
//  Inherit table properties into partition properties. 
//  Target isNull was copied at beginning of method. 
//  lastFrom point to the same Text object which would make from.equals(lastFrom) always true 
//  Prev/next are already checked in the calls. 
//  But soon became very fast decimal specific. 
//  Last partition key - anything between /key= and end 
//  inclusive   exclusive 
//  Make a random array of byte arrays 
//  This makes it so that we can go back up the tree later 
//  tables. 
//  SortBy DESC 
//  Test replicated drop, should not drop, because evid < repl.state.id 
//  We create the join predicate info object. The object contains the join condition,   split accordingly. If the join condition is not part of the equi-join predicate,   the returned object will be typed as SQLKind.OTHER. 
//  Downloaded resources dir 
//  updatable map that holds instances of the class 
//  First, update buffer priority - we have just been using it. 
// ************************************************************************************************   Emulate BigInteger serialization used by LazyBinary, Avro, Parquet, and possibly others. 
//  Reload tables from the MetaStore 
// set arguments 
//  don't want cache hits from llap io for testing filesystem bytes read counters 
//  source: LlapDaemonProtocol.proto 
//  Since TxnUtils.getTxnStore calls TxnHandler.setConf -> checkQFileTestHack -> TxnDbUtil.setConfValues,   which may change the values of below two entries, we need to avoid polluting the original values 
//  Empty constructor for writable etc. 
//  Throw away lower digits. 
//  mask UDFs 
//  10. If there was a cluster state change, make sure we redistribute all the pools. 
//  prepare 
//  MY_STRINGLIST 
//  create default users 
//  Notify if we have successfully copied the file. 
//  Set handling for low resource conditions. 
//  caller should not try to allocate another arena before waiting for the previous one. 
//  If this operator has a materialized view below,   we make its cost tiny and adjust the cost of its 
//  our stats for NDV is approx, not accurate. 
//  required   optional   required 
//  RUN_AS 
//  This method takes care of bit-flipping for descending order 
/*    * skip mode should not throw exception when a invalid partition directory   * is found. It should just ignore it    */
//  non-partitioned table 
//  We don't add this to the resources because we don't want to read config values from it.   But we do find it because we want to remember where it is for later in case anyone calls 
//  Ignore the value we got from OpTraits.   The logic below will fall back to the estimate from numReducers 
//  the directory does not exist 
//  --listHAPeers 
//  hive jar 
//  Cleanup session log directory. 
//  PRE - all the fields are required and serialized in order - is   !isRealThrift 
//  Note: the stats for ACID tables do not have any coordination with either Hive ACID logic         like txn commits, time outs, etc.; nor the lower level sync in metastore pertaining         to ACID updates. So the are not themselves ACID. 
//  First partition key - anything between key= and first / 
//  optional   optional 
//  Step 4 : Reanalyze 
//  do not understand why it is needed and wonder if it could be combined with close. 
//  found a file at depth which is less than number of partition keys 
//  Preemption will finally be registered as a deallocateTask as a result of preemptContainer   That resets preemption info and allows additional tasks to be pre-empted if required. 
//  ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec, PARTITION partition_spec,...; 
//  existing n-gram, just increment count 
// the idea is that this will use LockHandle.dbConn 
//  Find out if we need to throw away the tuple or not. 
//  Copy current value; do not change current scale. 
//  optional .FragmentRuntimeInfo fragment_runtime_info = 9; 
//  we have a storage specification for a primitive column type 
//     } 
//  No-Op - we don't really write anything here .. 
//  There's a race between removing the current task from the preemption queue and the actual scheduler   attempting to take an element from the preemption queue to make space for another task.   If the current element is removed to make space - that is OK, since the current task is completing and   will end up making space for execution. Any kill message sent out by the scheduler to the task will   be ignored, since the task knows it has completed (otherwise it would not be in this callback).     If the task is removed from the queue as a result of this callback, and the scheduler happens to   be in the section where it's looking for a preemptible task - the scheuler may end up pulling the   next pre-emptible task and killing it (an extra preemption).   TODO: This potential extra preemption can be avoided by synchronizing the entire tryScheduling block.\   This would essentially synchronize all operations - it would be better to see if there's an   approach where multiple locks could be used to avoid single threaded operation.   - It checks available and preempts (which could be this task)   - Or this task completes making space, and removing the need for preemption 
//  if we find it. 
//  copy set of deduped locks back to original list 
//  None are null, so all are selected 
//  The number of stripes should match the key index count 
//  Special handling for timestamp column   field name   field type 
//  after the first child is evaluated. 
//  This is a non-native table.   We need to set stats as inaccurate. 
//  Thrift configs 
//  Add 5 partitions to all tables 
// here means no open transaction, but different queries 
/*  @bgen(jjtree) Typei64  */
//  SERDE 
//  Create a dummy TableScanOperator for the file generated through fileSinkOp 
// re-check locks which were in Waiting state - should now be Acquired 
//  Create views registry 
//  The plan needs to be broken only if one of the sub-queries involve a 
//  assertEquals(1 << 18, map.getCapacity()); 
// If there is no group of a file, no need to call chgrp 
//  not used for mock, but 
// Buffers to hold filter pushdown information 
//  either colstats is null or is estimated 
//  Check if a bigint is implicitely cast to a double as part of a comparison   Perform the check here instead of in GenericUDFBaseCompare to guarantee it is only run once per operator 
//  db1.testtable3.p should also be in COLUMNS, will fix in separate ticket 
//                    1.23456789012345678901234567890123456789012345 
//  exact match 
// so now we have written some new data to bkt=0 and it shows up 
// so generate empty Dyn Part call 
//  short-circuit quickly - forward all rows 
// tableHandle can be null if table doesn't exist 
/* this captures mapping of Hive type names to HCat type names; in the long run    * we should just use Hive types directly but that is a larger refactoring effort    * For HCat->Pig mapping see PigHCatUtil.getPigType(Type)    * For Pig->HCat mapping see HCatBaseStorer#validateSchema(...) */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setRef(int, java.sql.Ref)    */
//  nanosecond interval in 2 primitives) produces a type timestamp (TimestampColumnVector). 
/*  every line below this is identical for evaluateLong & evaluateString  */
//  Calite bug CALCITE-987 
/*  Let's write more bytes to the files to test that ContentSummaryInputFormat is actually working returning the file size not from the filesystem  */
//  Round to the specified number of decimal places using half-even round function. 
//  add backup task to runnable 
//  more than 1 thread should call this close() function. 
/*    * Use this constructor when only ascending sort order is used.   * By default for ascending order, NULL first.    */
//  I don't think event notifications in case of failures are necessary, but other HMS operations   make this call whether the event failed or succeeded. To make this behavior consistent,   this call is made for failed events also. 
//  Add a new column 
//  jline will detect if <tab> is regular character 
/*      * Restriction.14.h :: Correlated Sub Queries cannot contain Windowing clauses.      */
//  check if static partition appear after dynamic partitions 
//  Same as hash64's default seed. 
//  the jsonObject for this vertex 
//  2.1 The ndv is the minimum of the PK and the FK. 
/*  * Specialized class for doing a vectorized map join that is an left semi join on a Single-Column Long * using a hash set.  */
//  LOG.debug("VectorMapJoinFastLongHashTable findReadSlot key " + key + " slot " + slot + " pairIndex " + pairIndex + " found key (i = " + i + ")"); 
//  3. Virtual columns 
//  Fake two live session 
//  For each range we have, intersect them. If they don't overlap   the range can be discarded 
//  and produce the correlated variables in the new output. 
//  If a task contains an operator which instructs bucketizedhiveinputformat 
//  4. Perform another major compaction. Nothing should change. Both deltas and  both base dirs   should have the same name. 
// remove trailing , 
//  TODO: we should call this more often. In theory, for DATE type, time should never matter, but 
//       This is a sync call that will feed data to the consumer. 
//  verify that no rows were selected 
//  if this transaction isn't ok, skip over it 
//  Just check to make sure base_5 below is not new. 
/*  1 files x 100 size for 111 splits  */
//  TIME 
//  MySQL returns 0 if the string is not a well-formed double value.   But we decided to return NULL instead, which is more conservative. 
// matches 2 rows  matches 2 rows 
//  Timeout is chosen to make sure that even if one iteration takes more than   half of the script.timeout but less than script.timeout, we will still 
//  No update necessary. 
//  We suspect that LIKE pushdown into JDO is invalid; see HIVE-5134. Check for like here. 
//  empty set - cannot convert 
//  If we're using TS's stats for mapjoin optimization, check each branch and see if there's any   upstream operator (e.g., JOIN, LATERAL_VIEW) that can increase output data size. 
//  Each child should has its own outputObjInspector 
//  Backtrack value columns of cRS to pRS 
//  The instance 
//  We need to get the ColumnAccessInfo and viewToTableSchema for views. 
// remove the path, with which no alias associates 
//     sb.append((char) i);    } 
//  recently evicted index (used for next key/value)   count of excluded rows from previous flush 
//  Only columns present in the batch and non-complex types. 
//  in the same line. 
//  One byte is always available for writing. 
//  internal name for expressions and estimate column statistics for expression. 
//  only leader publishes instance uri as endpoint which will be used by clients to make connections to HS2 via   service discovery. 
//  Transfer columnVector objects from base batch to outgoing batch. 
//  Update the output position for the cor vars: only pass on the cor 
//  make sure the vector was flattened 
//  test second IF argument with nulls 
//  Create metrics directory if it is not present 
//  Stored as directories. We don't care about the skew otherwise. 
//  Out of range for whole batch. 
//  simple distribute-by goes here 
//  Note: we may later have special logic to pick up old AMs, if any. 
// Get the job info from the configuration, 
//  Add the relevant database 'namespace' as a WriteEntity 
//  Nothing so far. 
//  context for current input file 
//  STRUCT_ENTRY 
//  No table information yet; looks like it could be valid. 
//  e.getKey() (alias) can be null in case of constant expressions. see   input8.q 
//  Check if input can be pruned 
//  Operation not recognized, set to null and let upper level handle this case 
//  100,000. 
// then invalidate column stats 
//  No need to get footers 
/*  Test decimal scalar to decimal column addition. This is used to cover all the   * cases used in the source code template ScalarArithmeticColumnDecimal.txt.    */
//  Regardless of our matching result, we keep that information to make multiple use 
//  if there are aggregate functions or grouping sets we will need   value generator 
//  assumption is that environment has already been cleaned once globally   hence each thread does not call cleanUp() and createSources() again 
//  Load the incremental dump and ensure it does nothing and lastReplID remains same 
//  5. Perform another major compaction 
/*  This list may be modified by specific cli drivers to mask strings that change on every test  */
//  3. Generate input event. 
//  There is some information about the windowing functions that needs to be initialized   during query compilation time, and made available to during the map/reduce tasks via   plan serialization. 
//  Copied here until a utility version of this released in ORC. 
//  the +1 is for the main map operator itself 
//  Continue to handle changes to a specific plan. 
//  Boolean 
//  Various unsupported methods. 
//  When the reducer is encountered for the first time 
//  done. For broadcast joins that includes the dummy parents. 
//  Continue range.. 
//  Storage Descriptor data 
//  for JOIN-RS case, it's not possible generally to merge if child has   less key/partition columns than parents 
//  NoSuchObjectException gets swallowed by a TApplicationException in remote mode. 
//  Need to update the queryPlan's output as well so that post-exec hook get executed.   This is only needed for dynamic partitioning since for SP the the WriteEntity is   constructed at compile time and the queryPlan already contains that.   For DP, WriteEntity creation is deferred at this stage so we need to update 
//  although, its likely to be a valid exception, we will retry   with cbo off anyway.   for tests we would like to avoid retrying to catch cbo failures 
//  timestamp scalar/scalar 
//  not forward compatible 
//  Local mode outputcommitter hook is not invoked in Hadoop 1.x 
//  setup mapJoinTables and serdes 
//  The way this works is, a session in WM pool will move back to tez AM pool on a kill and will get   reassigned back to WM pool on GetRequest based on user pool mapping. Only if we remove the session from active   sessions list of its WM pool will the queue'd GetRequest be processed 
//  Get object cache 
//  Make sure flow and double equality compare works 
//  negative test 
//  exprInfo is the key 
//  It's ok to send a cancel to an already completed future. Is a no-op 
//  Bail out: empty list 
//  Most of the above will be failed offers and takes (due to speed of the thing). 
//  there should be 1 call to create partitions with batch sizes of 10 
//  Ignore changes in the amount of white space 
/*  (non-Javadoc)      * @see org.apache.hadoop.mapreduce.RecordWriter#write(java.lang.Object, java.lang.Object)       */
//  NEW TAI LUE LETTER LOW FA U+199D (3 bytes) 
//  Loop to rewrite rest of INSERT references 
//  The following steps seem roundabout, but they are meant to aid in   recovery if a failure occurs and to keep a consistent state in the FS 
//  if the value is repeating, use row 0 
//  match this configuration before merging else will not be merged 
//  class ExpressionBuilder; 
//  state == CLOSE doesn't mean all children are also in state CLOSE 
/*  * * The vectorized MapOperator. * * There are 3 modes of reading for vectorization: * *   1) One for the Vectorized Input File Format which returns VectorizedRowBatch as the row. * *   2) One for using VectorDeserializeRow to deserialize each row into the VectorizedRowBatch. *      Currently, these Input File Formats: *        TEXTFILE *        SEQUENCEFILE * *   3) And one using the regular partition deserializer to get the row object and assigning *      the row object into the VectorizedRowBatch with VectorAssignRow. *      This picks up Input File Format not supported by the other two.  */
//  Probably not a local filesystem; no need to check. 
//  enable trash so it can be tested   FS_TRASH_CHECKPOINT_INTERVAL_KEY (hadoop-2)   FS_TRASH_INTERVAL_KEY (hadoop-2) 
//  Case 1. Fail the reader, sending back the error we received from the reader event. 
//  Create the Jetty server. If jetty conf file exists, use that to create server 
//  A stripped down version of fastSetFromBytes. 
//  This hook verifies that the location of every output table is empty 
//  1/ test LazyBinarySerDe 
//  to the select list. 
// just rename the directory 
/*  * If a join has been automatically converted into a sort-merge join, create a conditional * task to try map-side join with each table as the big table. It is similar to * hive.auto.convert.join, but is only applicable to joins which have been automatically * converted to sort-merge joins. For hive.auto.convert.join, the backup task is the * map-reduce join, whereas here, the backup task is the sort-merge join. * * Depending on the inputs, a sort-merge join may be faster or slower than the map-side join. * The other advantage of sort-merge join is that the output is also bucketed and sorted. * Consider a very big table, say 1TB with 10 buckets being joined with a very small table, say * 10MB with 10 buckets, the sort-merge join may perform slower since it will be restricted to * 10 mappers.  */
/*    * for now a top level QB can have 1 where clause SQ predicate.    */
//  use this buffer to hold column's cells value length for usages in 
//  End HiveProjectOverIntersectRemoveRule.java 
//  we ensure that we don't try to read any data in case of skip read. 
//  so it doesn't matter if we wait for all inputs or any input to be ready. 
// this is the root of the partition in which the 'file' is located 
//  Make sure the signature works. 
/*  * Root abstract class for a hash table result.  */
//  Verify dropTable recycle table files 
//  We are going to use LBSerDe to serialize values; create OI for retrieval. 
//  Couldn't find the from that contains subquery; replace with ALLCOLREF. 
//  ETL strategy requested through config 
//  hmm..this looks a bit wierd...setup boots qtestutil...this part used to be in beforeclass 
// create RexNode for LHS 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getWarnings()    */
// this part of reduceKeys is later used to create column names strictly for non-distinct aggregates   with parameters same as distinct keys which expects _col0 at the end. So we always append   _col0 at the end instead of _col<i> 
//  LOG.debug("VectorMapJoinFastTableContainer load newThreshold " + newThreshold); 
//  Allocator uses memory manager to request memory, so create the manager next. 
//  Reset ckpt and last repl ID keys to empty set for allowing bootstrap load 
// baseReader.getRowNumber() seems to point at the start of the batch todo: validate 
//  check reset operation: 
//  set vector[0] to null object reference to verify correct null handling 
//  Join types should be all the same for merging (or returns null) 
//  08S01 (Communication error) is the default sql state.  Override the sqlstate 
//  format the upgrade script name eg upgrade-x-y-dbType.sql 
//  but, if the value is rounded by more scaling, 
//  We format it so we are sure we are getting the right value 
/*    * Return all the known job ids for this user based on the optional filter conditions.   * <p>   * Example usages:   * 1. curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=hsubramaniyan'   * Return all the Job IDs submitted by hsubramaniyan   * 2. curl -s   * 'http://localhost:50111/templeton/v1/jobs?user.name=hsubramaniyan%26showall=true'   * Return all the Job IDs that are visible to hsubramaniyan   * 3. curl -s   * 'http://localhost:50111/templeton/v1/jobs?user.name=hsubramaniyan%26jobid=job_201312091733_0003'   * Return all the Job IDs for hsubramaniyan after job_201312091733_0003.   * 4. curl -s 'http://localhost:50111/templeton/v1/jobs?   * user.name=hsubramaniyan%26jobid=job_201312091733_0003%26numrecords=5'   * Return the first 5(atmost) Job IDs submitted by hsubramaniyan after job_201312091733_0003.   * 5.  curl -s   * 'http://localhost:50111/templeton/v1/jobs?user.name=hsubramaniyan%26numrecords=5'   * Return the first 5(atmost) Job IDs submitted by hsubramaniyan after sorting the Job ID list   * lexicographically.   * </p>   * <p>   * Supporting pagination using "jobid" and "numrecords" parameters:   * Step 1: Get the start "jobid" = job_xxx_000, "numrecords" = n   * Step 2: Issue a curl command by specifying the user-defined "numrecords" and "jobid"   * Step 3: If list obtained from Step 2 has size equal to "numrecords", retrieve the list's   * last record and get the Job Id of the last record as job_yyy_k, else quit.   * Step 4: set "jobid"=job_yyy_k and go to step 2.   * </p>   * @param fields If "fields" set to "*", the request will return full details of the job.   * If "fields" is missing, will only return the job ID. Currently the value can only   * be "*", other values are not allowed and will throw exception.   * @param showall If "showall" is set to "true", the request will return all jobs the user   * has permission to view, not only the jobs belonging to the user.   * @param jobid If "jobid" is present, the records whose Job Id is lexicographically greater   * than "jobid" are only returned. For example, if "jobid" = "job_201312091733_0001",   * the jobs whose Job ID is greater than "job_201312091733_0001" are returned. The number of   * records returned depends on the value of "numrecords".   * @param numrecords If the "jobid" and "numrecords" parameters are present, the top #numrecords   * records appearing after "jobid" will be returned after sorting the Job Id list   * lexicographically.   * If "jobid" parameter is missing and "numrecords" is present, the top #numrecords will   * be returned after lexicographically sorting the Job Id list. If "jobid" parameter is present   * and "numrecords" is missing, all the records whose Job Id is greater than "jobid" are returned.   * @return list of job items based on the filter conditions specified by the user.    */
//  get delegation tokens from hcat server and store them into the "job"   These will be used in to publish partitions to   hcat normally in OutputCommitter.commitJob()   when the JobTracker in Hadoop MapReduce starts supporting renewal of 
//  Use the hints later in top level QB. 
//  Combine the column field schemas and the partition keys to create the   whole schema 
//  If this set is not empty, it means we need to generate a separate task for collecting 
// need merge isDirect flag to input even if the newInput does not have a parent 
//  by supplying using "o" this enforces identity/equls matching   which will most probably make the signature very unique 
//  The id of the JobHandle used to track the actual Spark job 
//  the lack of a special token. 
/*  Number of digits in mantissa.  */
//  Max time when waiting for read locks on node list 
//  All fractional digits become integer digits. 
//  most likely the user specified an invalid partition 
//  e.g., a dummy vertex for a mergejoin branch 
//  best attempt, shouldn't really kill DAG for this 
//  number of nodes on stack   current mark 
//  Whether there is to be a tag added to the end of each key and the tag value. 
// copy the hive conf into the job conf and restore it  in the backend context 
//  add some data and nulls 
//  get the Table objects for this batch of table names and get iterator 
//  adjust arrays 
//  nodes, one of them is column and the other is numeric const 
//  can happen with virtual columns. RS would add the column to its output columns   but it would not exist in the grandparent output columns or exprMap. 
//  if number of elements in map cannot be determined, this value will be used 
//  child should be a join for this to happen. 
//  The only ABA problem we care about. Ok to have another buffer in there; 
//  Adjustable. 
//  Assuming FileSystem.getAllStatistics() returns all schemes that are accessed on task side   as well. If not, we need a way to get all the schemes that are accessed by the tez task/llap. 
//  figure out subquery expression column's type 
//  Individual columns are going to be pivoted to HBase cells,   and for each row, they need to be written out in order   of column name, so sort the column names now, creating a   mapping to their column position.  However, the first 
//  setup to run concurrent operations 
//  (a scalar) and trivial to evaluate. 
//  test that read columns are initially an empty list 
// Found some columns in user specified schema which are neither regular not dynamic partition columns 
/*   * The job argument is passed so that configuration overrides can be used to initialize  * the metastore configuration in the special case of an embedded metastore  * (hive.metastore.uris = "").   */
/*        * create SelectListOI        */
//  PRINCIPAL_GRANTS 
//  Expected error 
//  We cannot merge (1.2) 
//  GenericUDF is stateful - we have to make a copy here 
// If the record reader (from which the record is originated) is already seen and valid,  no need to re-encode the record. 
//  Parameters for exporting metadata on table drop (requires the use of the) 
//  print parent op, i.e., where data comes from 
//  set up a db and table 
/*      * According to the javadoc, getMax() can return -1. In this case     * default to 200MB. This will probably never actually happen.      */
//  This hash function returns the same result as String.hashCode() when   all characters are ASCII, while Text.hashCode() always returns a   different result. 
//  end of struct 
//  Load data 
//  no custom composite key class provided. return null 
//  For HBase storage handler 
//  likewise 
//  Sign in2 with a different key. 
/*          * Gen Join between outer Operator and SQ op          */
//  See class comment about refcounts. 
//  6.1 Determine type of UDAF   This is the GenericUDAF name 
//  left is larger 
//  http://dev.mysql.com/doc/refman/5.0/en/connector-j-reference-error-sqlstates.html 
//  Verify that the names match the PARTITIONED ON clause. 
//  set the link between mapjoin and parent vertex 
// newer versions (12c and later) support OFFSET/FETCH 
/*          * Repeating.          */
//  Did we read all the data? 
//  Read the altered db via CachedStore (altered user from "user2" to "user1") 
//  SRC_TXN_TO_WRITE_ID_LIST 
//  The join columns which are also skewed 
// this succeeds as abortTxn is idempotent 
//  Copy the BigTable values into the overflow batch. Since the overflow batch may   not get flushed here, we must copy by value. 
//  Use HiveInputFormat if any of the paths is not splittable 
//  We check whether the join can be combined with any of its children 
//  Case 1: is repeating, no nulls 
//  [optional] alias of the column (external name 
//  Walk over all the sources (which are guaranteed to be reduce sink   operators). 
//  When split-update is enabled, we can choose not to write   any delta files when there are no inserts. In such cases only the delete_deltas   would be written & they are closed separately below. 
//  Sort all the inputs, outputs.   If a lock needs to be acquired on any partition, a read lock needs to be acquired on all 
//  If the input FileSinkOperator is a dynamic partition enabled, the tsMerge input schema   needs to include the partition column, and the fsOutput should have 
//  NOTE: in Hive AST Rows->Range(Physical) & Range -> Values (logical) 
//  -n <username> 
// OR 
//  1. We check if it is a permutation project. If it is 
//  Update maxLength if length is greater than the largest value seen so far 
//  instantiated. 
//  array of null column values   input ObjectInspectors 
//  Value becomes zero for rounding beyond. 
//  Format the storage format statements 
//  dynamic partition usecase - partition values were null, or not all were specified   need to figure out which keys are not specified. 
//  no longer a best match if more than one. 
//  Just bitwise-OR the bits together - size/# functions should be the same, 
//  This restricts macro creation to privileged users. 
/*           for now for simplicity we are doing just one directory ( one database ), come back to use          of multiple databases once we have the basic flow to chain creating of tasks in place for          a database ( directory )       */
//  to disk 
//  we reach a runlength here, use the previous length and reset   runlength 
//  If we're moving files around for an ACID write then the rules and paths are all different. 
//  We only consider the materialized view to be outdated if forceOutdated = true, i.e.,   if it is a rebuild. Otherwise, it passed the test and we use it as it is. 
//  3. Create GB 
//  for leaf, we don't do anything 
//  verify cm.recycle(Path) api moves file to cmroot dir 
//  Default location of HiveServer2 
//  If there are multiple aliases in source, we do not know   how to merge. 
/*  alternate1 = useColumnSortOrderIsDesc  */
// Flush if memory limits were reached 
//  Now we only try the first partition, if the first partition doesn't   contain enough size, we change to normal mode. 
//  Await future result with a timeout to check the abort field occasionally.   It's possible that the interrupt which comes in along with an abort, is suppressed   by some other operator. 
//  this event can never occur. If it does, fail. 
//  When either name or value is null, the set method below will fail,   and throw IllegalArgumentException 
//  There will not be any MR or Tez job above this task 
//  The following data is used to compute a partitioned table's NDV based   on partitions' NDV when useDensityFunctionForNDVEstimation = true. Global NDVs cannot be   accurately derived from partition NDVs, because the domain of column value two partitions   can overlap. If there is no overlap then global NDV is just the sum   of partition NDVs (UpperBound). But if there is some overlay then   global NDV can be anywhere between sum of partition NDVs (no overlap)   and same as one of the partition NDV (domain of column value in all other   partitions is subset of the domain value in one of the partition)   (LowerBound).But under uniform distribution, we can roughly estimate the global   NDV by leveraging the min/max values.   And, we also guarantee that the estimation makes sense by comparing it to the   UpperBound (calculated by "sum(\"NUM_DISTINCTS\")")   and LowerBound (calculated by "max(\"NUM_DISTINCTS\")") 
//  Dump and load only truncate (0 records) 
//  same string 
//  None. 
//  we've got what we need; mark the queue 
/*          * if the keyHash is missing in the bloom filter, then the value cannot         * exist in any of the spilled partition - return NOMATCH          */
//  If it is a windowing spec, we include it in the list   Further, we will examine its children AST nodes to check whether   there are aggregation functions within 
//  Original bucket files should stay until Cleaner kicks in. 
/*    * Right trim and truncate a slice of a byte array to a maximum number of characters and   * place the result into element i of a vector.    */
//  CHILDREN 
//  If the child SelectOperator does not have the ColumnExprMap,   we do not need to update the ColumnExprMap in the parent SelectOperator. 
//  so far same as java.math.BigDecimal, but the scaling below is   specific to ANSI SQL Numeric. 
//  @@protoc_insertion_point(builder_scope:SourceStateUpdatedRequestProto) 
//  This is needed for tracking the dependencies for inputs, along with their parents. 
//  We repartition: new number of splits 
/*  * Directly deserialize with the caller reading field-by-field the LazySimple (text) * serialization format. * * The caller is responsible for calling the read method for the right type of each field * (after calling readNextField). * * Reading some fields require a results object to receive value information.  A separate * results object is created by the caller at initialization per different field even for the same * type. * * Some type values are by reference to either bytes in the deserialization buffer or to * other type specific buffers.  So, those references are only valid until the next time set is * called.  */
//  no match 
//  make sure MAP task environment points to HIVE_JOB_CREDSTORE_PASSWORD 
//  First we parse the view query and create the materialization object 
/*  * represents a collection of rows that is acted upon by a TableFunction or a WindowFunction.  */
//  Testing multiByte string with reference starting mid array 
//  Group column found 
//  If one of the predicates is =, then any other predicate with it is illegal.   Add to residual 
//  Go over all the destination tables 
// //////////  Second Incremental //////////// 
//  SCHEMA_VERSION 
//  2c. This is a compressed buffer. We need to uncompress it; the buffer can comprise   several disk ranges, so we might need to combine them. 
//  A simple ROWNUM > offset and ROWNUM <= (offset + limit) won't work, it will return nothing 
//  Verify that there is no notifications available yet 
//  Check each ExprNodeDesc. 
//  move up files 
//  update the connection 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setURL(int, java.net.URL)    */
//  Two cases:   1. srcs has only a src directory, if rename src directory to destf, we also need to   Copy/move each file under the source directory to avoid to delete the destination   directory if it is the root of an HDFS encryption zone.   2. srcs must be a list of files -- ensured by LoadSemanticAnalyzer 
// it is OK, curReader is closed, for we only need footer buffer info from preReader. 
//  Wrap the inner parts of the loop in a catch throwable so that any errors in the loop   don't doom the entire thread. 
//  Take the empty buffer out of the free list. 
//  start heartbeat thread 
//  Get the "transactional" tblproperties value 
//  No duplicate names.  This should be ok 
//  create a task for this local work; right now, this local work is shared 
//  Only distinct nodes that are NOT part of the key should be added to distExprNodes 
//  We have just locked a buffer that wasn't previously locked. 
//   We can use alter table partition rename to convert/normalize the legacy partition    column values. In so, we should not enable the validation to the old partition spec    passed in this command. 
//  time, which needs to be thread protected. 
//  Boolean to long is done with an IdentityExpression   Boolean to double is done with standard Long to Double cast   See org.apache.hadoop.hive.ql.exec.vector.expressions for remaining cast VectorExpression   classes 
/*            * One element.            */
//  For now, if a bigint is going to be cast to a double throw an error or warning 
/*  Distinct value estimator  */
//  since we are using thrift, 'part' will not have the create time and   last DDL time set since it does not get updated in the add_partition()   call - likewise part2 and part3 - set it correctly so that equals check   doesn't fail 
//  Removes tasks from the runningList and sends out a preempt request to the system.   Subsequent tasks will be scheduled again once the de-allocate request for the preempted 
//  3. Update the existing row in newly-converted ACID table 
/*  * An single long value map optimized for vector map join.  */
/*  operators for which there is chance the optimization can be applied  */
//  We could not find a common category, return null 
//  Temporary, till the external interface makes use of a single connection per   instance. 
// Make sure that the table alias and column alias are stored  in the column info 
//  These tests inherently cause exceptions to be written to the test output   logs. This is undesirable, since you it might appear to someone looking   at the test output logs as if something is failing when it isn't. 
//  Note: this relies on the fact that we always evict the entire column, so if         we have the column data, we assume we have all the streams we need. 
//  1/ reserve spaces for the byte size of the map   which is a integer and takes four bytes 
//  If we are overwriting, we disable existing sources 
//  skip day/time part if both dates are end of the month 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#clearWarnings()    */
// count at the size for 'cost' estimation later 
// we just created top level node for this jobId 
//  Unique ID is registered based on Submit response. Theoretically, we could get a ping   when the task is valid but we haven't stored the unique ID yet, so taskNodeId is null.   However, the next heartbeat(s) should get the value eventually and mark task as alive. 
//  Hmm... why don't many other operations here need locks? 
//  we can bail out 
//  SASL/Kerberos properties 
//  L<->R for inner/semi join, L->R for left outer join, R->L for right outer join 
//  We optimize by assuming that a repeating list/map will run from   from 0 .. lengths[0] in the child vector.   Sanity check the assumption that we can start at 0. 
/*    * callback method used by subclasses to set the RawInputOI on the Evaluator.    */
//  optional string dag_name = 5; 
//  Re-attempts are left upto the RPC layer. If there's a failure reported after this,   mark all attempts running on this node as KILLED. The node itself cannot be killed from   here, that's only possible via the scheduler.   The assumption is that if there's a failure to communicate with the node - it will   eventually timeout - and no more tasks will be allocated on it. 
//  Now that we have found real data, emit sign byte if necessary and do negative fixup. 
/*        * Verify that new job requests have no issues.        */
//  Since warehouse path is non-qualified the database should be located on second filesystem 
//  the rowID column is a string 
//  found suitable join keys   add them to key list, ensuring that if there is a   non-equi join predicate, it appears at the end of the   key list; also mark the null filtering property 
//  To be used with primitive types 
//  metastore stats is unavailable, fallback to old way 
//  age <= '50' 
//  Elt is a special case because it can take variable number of arguments. 
//  2 VARCHAR test 
// reconfigure log4j after settings via hiveconf are write into System Properties 
//  try using both permanent functions 
//  we need to add up all the estimates from the siblings of this reduce sink 
//  Does a breadth first traversal of the tasks 
// Assert.assertFalse(itr.hasNext()); 
//  EmptyBuckets = true 
//  generate single split typically happens when reading data out of order by queries.   if order by query returns no rows, no files will exists in input path 
//  TODO Ideally, this should only need to send in the TaskAttemptId. Everything else should be   inferred from this.   Passing in parameters until there's some dag information stored and tracked in the daemon. 
//  The null in union type should be removed 
//  This is a percentage value between 0 and 1 
//  required   required   optional   optional   optional   optional 
//  If no keyValueSeparator is seen, all bytes belong to key, and 
//  TODO: These can eventually be used to replace generateTimestampScalarCompareTimestampColumn() 
//  We want message to be sent when session commits, thus we run in   transacted mode. 
//  Note: this sets LoadFileType incorrectly for ACID; is that relevant for load? 
//  has been rewritten; apply rules post-decorrelation 
//  since removeParent/removeChild updates the childOperators and parentOperators list in place   we need to make a copy of list to iterator over them 
//  Values should unique (given how we do the checking and "addOrMerge") 
//  Enums are one of two Avro types that Hive doesn't have any native support for. 
// MSSQL specific parser 
//  LATERAL VIEW OUTER not supported in CBO 
//  TODO: Ideally we want tez to use CallableWithMdc that retains the MDC for threads created in   thread pool. For now, we will push both dagId and queryId into NDC and the custom thread   pool that we use for task execution and llap io (StatsRecordingThreadPool) will pop them 
//  Define constants and local variables. 
//  in case of broadcast-join read the broadcast edge inputs   (possibly asynchronously) 
//  1.1) Extract name as we will need it afterwards: 
//  Now, check for overflow. 
//  Existence 
//  config parameter that suggests to hcat that metastore clients not be cached - default is false   this parameter allows highly-parallel hcat usescases to not gobble up too many connections that 
//  Remove the entry, if there's nothing left at the specific priority level 
//  Should still be able to get the 2nd session. 
//  Set values needed for numeric arithmetic UDFs 
// hasNext implies there is some column in the batch 
//  can not queue more requests as queue is full 
/*  Convert an integer value in milliseconds since the epoch to a timestamp value   * for use in a long column vector, which is represented in nanoseconds since the epoch.    */
/*  Miscellaneous errors, range 9000 - 9998  */
//  Found ugi, perform doAs(). 
//  check if any right pair exists for left objects 
//  1. Gen Calcite Plan 
//  Test for valid values for both. 
// so now, 'working' should be sorted like delta_5_20 delta_5_10 delta_11_20 delta_51_60 for example  and we want to end up with the best set containing all relevant data: delta_5_20 delta_51_60, 
/*    * Use this copy method when the source batch may get reused before the target batch is finished.   * Any bytes column vector values will be copied to the target by value into the column's   * data buffer.    */
//  Get an array of UTF-8 byte arrays from an array of strings 
//  and then compare the two tables 
//  Special case - Date requires its own specific BetweenDynamicValue class, but derives from FilterLongColumnBetween 
//  Others (a1) should be kept 
//  after that, inputOp is the parent of selOp. 
/*  not a real field  */
// Initialize metrics first, as some metrics are for initialization stuff. 
//   The maximum column length = MFieldSchema.FNAME in metastore/src/model/package.jdo 
/*    * (non-Javadoc)   *   * @see javax.sql.DataSource#getConnection()    */
//  try widening conversion, otherwise fail union 
//  look through the file with no rows selected 
//  Select none in 1st child, none as 2nd child, and none as 3rd. 
//  Row-mode is the expected value. 
//  Move all data from dest4_sequencefile to dest4 
//  Split join condition 
//  0. Register that we have visited this operator in this rule 
//  The following is a complex type for special handling 
//  a mixture of input columns and new scratch columns (for the aggregation output). 
// verify that we are indeed doing an Acid write (import) 
//  DB_PATTERNS 
//  2. Go thru the blocks; add stuff to results and prepare the decompression work (see below). 
//  The lowest digit is power = 0. 
//  Arrange so result* has a longer digit tail and it lines up; we will shift the shift* digits   as we do our addition and them into the result. 
// this has state, so can't be cached 
//  we do not want the start group to clear the storage 
//  Next we locate all the iterate methods for each of these classes. 
//  Set the config value to empty string, which should result in all catalogs being cached. 
//  create and put .hiverc sample file to default directory 
//  This is purely for testing convenience; has no bearing on FS operations such as list. 
//  SSL settings 
//  Get the distinct values of the GROUP BY fields and the arguments   to the agg functions. 
//  catch-all due to some exec time dependencies on session state   that would cause ClassNoFoundException otherwise 
//  We are revoking another duck; don't wait. We could also give the duck 
//  TODO: lossy conversion, distance is considered in seconds 
//  Add new operator to cache work group of existing operator (if group exists) 
//  Store the given token in the UGI 
//  Calculate the parameters 
//  Get the parent TS of victimRS. 
//  COLUMN_COUNT 
/*  For cast on constant operator in all members of the input list and return new list   * containing results.    */
//  4. Insert RS on reduce side with Reduce side GB as input 
//  If only one distinct aggregate and one or more non-distinct aggregates, 
//  Select all with a is not null child, none as 2nd child, and is null with 3rd, and then   expect the 3rd child to not be invoked. 
//  positions of variable arguments (columns or non-constant expressions) 
// ---------------------------------------   Just so we can get the output type... 
/*    * Reads through an undesired field.   *   * No data values are valid after this call.   * Designed for skipping columns that are not included.    */
/*    * These members have information for deserializing a row into the VectorizedRowBatch   * columns.   *   * We say "source" because when there is conversion we are converting th deserialized source into   * a target data type.    */
//  Partition columns are appended at end, we only care about stats column 
//  CREATED_AT 
//  a list of doubles 
//  we do not want the end group to cause a checkAndGenObject 
//  When there are PARTITION and ORDER BY clauses, will have different partition expressions. 
//  have to remember it. 
//  If the character set for encoding is constant, we can optimize that 
//  this time even more inaccurate 
//  replace the distinct with the count aggregation 
//  for hive script operator 
//  technique. 
//  Since we won't be able to update this as we add, for now, estimate 10x usage.   This shouldn't be much and this cache should be remove later anyway. 
//  for non-LLAP mode most of these are not relevant. Only noConditionalTaskSize is used by shared scan optimizer. 
//  Since row mode takes everyone. 
//  does it need an additional MR job 
//  TODO: set correct vendorCode field 
//  If the current expression node does not have a virtual/partition column or 
// create JavaBinaryObjectInspector 
//  the current char will be written out later. 
//  Examine the last child. It could be an alias. 
//  returnAfterUse will take care of this 
//  Decimal longwords. 
//  We do not need to do anything, it is in the OR expression 
//  it is an extraction fn need to be parsed 
//  If the type cast UDF is for a parameterized type, then it should implement   the SettableUDF interface so that we can pass in the params.   Not sure if this is the cleanest solution, but there does need to be a way 
// list  struct 
//  valid merge -- register set size gets bigger & dense automatically 
//  looks like a subq plan. 
//  BucketizedHiveInputFormat should be used for either sort merge join or bucket map join 
//  well. 
//  Primitive. 
//  CHAR NOT BETWEEN 
//  generate a map join task for the big table 
//  Output is type interval_day_time. 
//  property speficied file found in local file system   use the specified file 
//  Same as commitDropTable where we always delete the data (accumulo table) 
//  Callers duplicate the buffer, they have to for BufferChunk; so we don't have to. 
//  All must be selected otherwise size would be zero. Repeating property will not change. 
// org.apache.commons.exec.DefaultExecutor requires   that current directory exists 
//  The isNull check and work has already been performed. 
//  Copy the current object contents into the output. Only copy selected entries, 
//  Check for delegation token, if present add it in the header 
//  Preserving the old logic. Hmm... 
//  We have already explored the stack deep enough, but   we do not have a matching 
//  For n-way join, only spill big table rows once 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareStatement(java.lang.String,   * java.lang.String[])    */
//  Update aggregate partition column stats for a table in cache 
//  2. Add UDAF 
//  datetime type isn't currently supported 
//  v[4] 
// Helper classes for ConnParam comparison logics. 
/*        * Handle case with nulls. Don't do function if the value is null, to save time,       * because calling the function can be expensive.        */
//  should be a constant or column 
//  delete remaining directories for external tables (can affect stats for following tests) 
//  Note: we are creating a brand new the partition, so this is going to be valid for ACID. 
//  than the configured the header size 
//  eat it 
//  No node exists, throw exception 
//  part1 ... part20 
// for locks associated with a txn, we always heartbeat txn and timeout based on that 
//  Clean time out locks from the database not associated with a transactions, i.e. locks   for read-only autoCommit=true statements.  This does a commit,   and thus should be done before any calls to heartbeat that will leave 
//  Only creates the expiration tracker if expiration is configured. 
//  look for a row like "INFO  : Query ID = asherman_20170718154720_17c7d18b-36e6-4b35-a8e2-f50847db58ae" 
//  Find operators in work 
//  4.1) Adding ROW__ID field 
//  Check if owid is outside the range of all owids present. 
//  C also has one outer join filter associated with A(c.k>20), which is making 2=0:1 
//  Add a dummy node to cache   Partnames: [tab1part1...tab1part9] 
//  test.performanceTest(); 
//  if it's wrapped by top-level select star query, skip ambiguity check (for backward compatibility) 
//  Initialize any entries that could be used in an output vector to have false for null value. 
//  should be OK since the lock is ephemeral and will eventually be deleted   when the query finishes and zookeeper session is closed. 
//  use CombineHiveInputFormat for map-only merging 
//  There is a Project on top (due to nullability) 
//  Running example 
//        in any sensible way. So, for now the lock is going to be epic. 
/*      * does the row match the pattern represented by this SymbolFunction      */
//  MatchStats for each candidate 
/*  Temporarily holds location of exponent				 * in string.  */
//  Bytes remaining in the current chunk of data 
//  does not add back up task here, because back up task should be the same   type of the original task. 
//  For equal-priority rules, user rules come first because they are more specific; then apps, 
//  make sure the dpp sink has output for all the corresponding part columns 
//  Now get a non-existant entry 
//  tests a multimap structure 
// rollback is done for the benefit of Postgres which throws (SQLState=25P02, ErrorCode=0) if  you attempt any stmt in a txn which had an error. 
//  the Configuration 
//  If not ReduceSink Op, skip 
//  Optimize: whole decimal fits in one binary word. 
//  Need to subtract 1 as nwi_next would be the next write id to be allocated but we need highest   allocated write id. 
// no more files for current bucket 
//  the time and number counters become available only after the 1st 
//  Add min/max and bloom filter aggregations 
//  Some cf:cq 
//  limit is reached or batchSize reduces to 0, whichever comes earlier. 
/*  Move files one by one because source is a subdirectory of destination  */
/* as of hadoop 2.3.0, PseudoAuthenticationHandler only expects user.name as a query param      * (not as a form param in a POST request.  For backwards compatibility, we this logic      * to get user.name when it's sent as a form parameter.      * This is added in Hive 0.13 and should be de-supported in 0.15 */
//  scan 
//  The number of partitions aggregated per cache node   If the number of partitions requested is > this value, we'll fetch directly from Metastore 
//  FKCOLUMN_NAME 
//  #2 
// CTAS with non-ACID target table 
//  Cannot be a static because default basePersistDirectory is unique per-instance 
//  Fractional part powered up too high. 
//  If there is a PTF between cRS and pRS we cannot ignore the order direction 
//  -1 buckets means to turn off bucketing 
//  Same thing -- might be deleted by other nodes, so just go on. 
//  Launch upto maxthreads tasks 
/*  allowNull  */
//  #1 
//  read the first row in parquet data page, this will be only happened once for this instance 
//  Return true for exprNodeColumnDesc 
//  not a nullable union 
//  call-1: open to read data - split 1 => mock:/mocktable6/0_0   call-2: AcidUtils.getAcidState - split 1 => ls mock:/mocktable6   call-3: open to read data - split 2 => mock:/mocktable6/0_1   call-4: AcidUtils.getAcidState - split 2 => ls mock:/mocktable6   call-5: read footer - split 2 => mock:/mocktable6/0_0 (to get offset since it's original file) 
//  check second most significant part 
//  We always need to call reset on the codec. 
//  Find the list of scripts to execute for this upgrade 
//  for now allow only create-view with 'select with grant' 
//  add additional overhead of each map entries 
//  And now we wander straight into the swamp, when instead of adding, we subtract it from UTC   midnight to supposedly get local midnight (in the above case, 4:00 UTC). Of course, given 
//  #4 
//  we are done reading a batch, send it to consumer for processing 
//  3.4 Try GenericUDF translation 
//  correct version stored by Metastore during startup 
//  there should be 2 calls to create partitions with each batch size of 5 
//  Should not happen 
//  If dynamic allocation is enabled, numbers for memory and cores are meaningless. So, we don't   try to get it. 
//  When longer, we assume the caller will default with nulls, etc. 
//  for small tables only; so get the big table position first 
//  Note: the list is expected to be a few items; if it's longer we may want an IHM. 
//  Inject a behaviour where it throws exception if an INSERT event is found   As we dynamically add a partition through INSERT INTO cmd, it should just add ADD_PARTITION   event not an INSERT event 
//  Column aliases defined by query for lateral view output are duplicated 
//  then serialize again using hrsd, and compare results 
//  show table level privileges 
//  Assert values retrieved 
//  Inject a behaviour where some events missing from notification_log table. 
//  simply need to remember that we've seen an event operator. 
/*      * add Window Functions      */
//  Path must be reused. 
//  Bytes necessary to store extra bits of the second timestamp if storing a timestamp 
//  update table level column stats 
//  If current is JoinOperaotr, we will stop to traverse the tree   when any of parent ReduceSinkOperaotr of this JoinOperator is   not considered as a correlated ReduceSinkOperator. 
//  #3 
/*      * Sum input is DECIMAL_64 and output is DECIMAL.     *     * Any mode (PARTIAL1, PARTIAL2, FINAL, COMPLETE).      */
//  ADD_PARTITION EVENT to partitioned table 
//  Task Failed 
//  Called one or more times on the client and AM. 
//  Must be deterministic order map for comparison across Java versions 
//  Patch the optimized query back into original CTAS AST, replacing the   original query. 
// this works because logically we need S lock on NONACIDORCTBL to read and X lock to write, but 
//  Check if the bucketing and/or sorting columns were inferred 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setRowId(int, java.sql.RowId)    */
//  No mutator created 
//  try either yyyy-mm-dd, or integer representing days since epoch 
//  #6 
//  Move the intermediate archived directory to the original parent directory 
//  Process the last byte. 
//  either initTxnMgr or from the SessionState, in that order. 
//  TODO: Look at repeating optimizations... 
//  The partition spec is not present 
/* non acid txn managers don't support txns but fwd lock requests to lock managers        acid txn manager requires all locks to be associated with a txn so if we        end up here w/o an open txn it's because we are processing something like "use <database>        which by definition needs no locks */
//  #5 
//  Column-reference node, e.g. a column in the input row 
//  new threads. 
//    Operations involving/returning year-month intervals   
//  estimate number of reducers 
//  This type information specifies the data types the partition needs to read. 
//  read totalSeconds, nanos from DataInput 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setDate(int, java.sql.Date)    */
//  no filter will be executed for constant 
//  Handle *repeated* join key, if found. 
//  Stop the Composite Service 
//  holds the root of the operator tree we're currently processing   this could be a table scan, but also a join, ptf, etc (i.e.: 
//  load the HS2 connection url properties from hive-site.xml if it is present in the classpath 
//  extract drop privileges 
//  Check if the partitions exist in the sourceTable 
//  Not necessary here cause noone will be looking at these after us; set them for clarity. 
//  IF_EXISTS 
//  We need to initialize those MuxOperators first because if we first   initialize other operators, the states of all parents of those MuxOperators   are INIT (including this DemuxOperator),   but the inputInspector of those MuxOperators has not been set. 
//  Date part 
//  100 >= x   neg-infinity to end inclusive 
//  STATEMENT 
//  Prevent instantiation 
//  Set the table where we're writing this data 
//  #8 
//  Stop 
//  We now have to probe the global hash and find-or-allocate 
//  populate reduce task 
//  E.g. For scale 2 the minimum is "0.01" 
//  if newCols are not specified, use default ones. 
//  find out the null-bytes 
//  if its a partial line, continue collecting the pieces 
//  source: LlapPluginProtocol.proto 
//  Tez can handle unpopulated buckets 
//  #7 
//  Assumption: At this point Parse Tree gen & resolution will always   be true (since we started out that way). 
//  No instantiation. 
//  populate stage 
//  This is a dummy assigner 
//  The trailing zeroes extend into the integer part -- we only want to eliminate the   fractional zero digits. 
/* for CTAS, TransactionalValidationListener.makeAcid() runs to late to make table Acid         so the initial write ends up running as non-acid... */
// ---------------------------------------------------------------------------   Semi join specific members.   
//  join conditions 
//  r-------- 
/*  (non-Javadoc)   * @see org.apache.hadoop.mapreduce.RecordReader#getCurrentValue()    */
/*  Return true if this is one of a small set of functions for which   * it is significantly easier to use the old code path in vectorized   * mode instead of implementing a new, optimized VectorExpression.   *   * Depending on performance requirements and frequency of use, these   * may be implemented in the future with an optimized VectorExpression.    */
//  log an exception - this produces enough text to force a new logfile   (as appender.sliding.policies.size.size=1KB) 
//  returncode 
//  Get the 10^N power to turn digits into the desired decimal with a possible   fractional part. 
/*  Because we use Hive's 'string' type when Avro calls for enum, we have to expressly check for enum-ness  */
//  If the difference is larger than 2^128 (d4 != 0), then D is   definitely larger than power, so increment. 
//  Once we drop support for old Hadoop versions, change these   to getBytes() and getLength() to fix the deprecation warnings.   Not worth a shim. 
//  Assert that a and b are not the same, within epsilon tolerance. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setFetchDirection(int)    */
//  Check immediately after reducer is assigned, in cae the abort came in during 
//  Use the RowResolver from the input operator to generate a input   ObjectInspector that can be used to initialize the UDTF. Then, the   resulting output object inspector can be used to make the RowResolver 
//  As Exchange operator is introduced later on, we make a   sort operator create a new stage for the moment 
//  update this information in sparkWorkMap 
//  3. Build operator 
//        older-node tasks proactively. For now let the heartbeats fail them. 
//  multi-table inserts not supported 
//  then just look at the other locks. 
//  Nothing to do 
//  zone1, zone2 should already have been checked for nulls. 
//  Sanity check 
//  use varchar's text field directly 
//  Modify table schema. Add columns. 
// This method does not depend on MetastoreConf.LIMIT_PARTITION_REQUEST setting: 
//  TYPE_DESC 
// if partition spec node is present, set partition spec 
//  UNDONE... 
// Test for publish with missing partition key values 
//  more accurate information about the original NDV of the column before any filtering. 
//  if we have no replication state on record for the obj, allow replacement. 
//  read the type 
//  column stats for a group by column 
//  read the second flush and make sure we see all 5 rows 
//  Not used by the direct access client -- native vector map join. 
//  this can happen only on top most limit, not while visiting Limit Operator   since that can be within subquery. 
//  in ExecDriver as well to have proper local properties. 
//  No need to skip seek here, index won't be used anymore. 
//  If we had the entire pool, other list couldn't exist.   We exhausted the entire-pool-sized list. 
//  plans. 
//  In memory hashMap   Stores small table key/value pairs   Stores big table rows 
//  ROW is null for delete events. 
//  Missing database name in the query 
//  For the generation of the values expression just get the inputs 
/*    * ResultExpression is a Select List with the following variation:   * - the select keyword is optional. The parser checks if the expression doesn't start with   * select; if not it prefixes it.   * - Window Fn clauses are not permitted.   * - expressions can operate on the input columns plus the psuedo column 'path'   * which is array of   * structs. The shape of the struct is   * the same as the input.    */
//  check if we can convert to map join no bucket scaling. 
//  This implementation of vectorized JOIN is delegating all the work   to the row-mode implementation by hijacking the big table node evaluators   and calling the row-mode join processOp for each row in the input batch.   Since the JOIN operator is not fully vectorized anyway atm (due to the use   of row-mode small-tables) this is a reasonable trade-off. 
//  followed by a call to finishedAddingInitialColumns. 
//  remove local copy of HDFS location from resource map. 
//  all together so there is only one security check 
//  Get the application id of the Spark app 
//    testtable1.*:     S 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setClob(java.lang.String, java.sql.Clob)    */
//  If we're in replication scope, it's possible that we're running the export long after   the table was dropped, so the table not existing currently or being a different kind of   table is not an error - it simply means we should no-op, and let a future export   capture the appropriate state 
//  If involving local file system 
/*      * flip column references if join condition specified in reverse order to     * join sources.      */
//  For simpler access, we make these members protected instead of   providing get methods. 
/*  (non-Javadoc)      * @see org.apache.hadoop.mapreduce.RecordWriter#close(org.apache.hadoop.mapreduce.TaskAttemptContext)       */
//  Only support a single expression when it's a UDTF 
//             // re-home location, now that we know the rest of the partvals              Table table = jobInfo.getTableInfo().getTable();                List<String> partitionCols = new ArrayList<String>(); 
//  create a TaskInputOutputContext 
//  Even if table location is specified table creation should fail 
/*    * Verify table for Key: Long x Hash Table: HashMultiSet    */
//  Use SubmitWorkRequestProto.newBuilder() to construct. 
//  We only allow one single PK. 
//  3. Walk through UDAF & Collect Distinct Info 
//  And, round up may cause us to exceed our precision/scale... 
//  Start with capacity 1; make sure we expand on every put. 
//  Assert.assertEquals("expected uri", api.getAddedResource("jar")); 
//  set value for the union type 
//  Create the parent znodes recursively; ignore if the parent already exists. 
//  MYINT 
//  uninitialized bucket 
//  For dynamic partitioned hash join, assuming table is split evenly among the reduce tasks. 
//  NOTE: specify Dynamic partitions in dest_tab for WriteEntity 
//  see rowLength javadoc 
/*    * Called at the beginning of the compile phase to have another chance to optimize the operator plan    */
//  DATA_PATH 
/*  * An multi-key hash map based on the BytesBytesMultiHashMap.  */
// last partial batch 
//    outputTypeInfos[outputIndex++] = smallTableTypeInfos[smallTableRetainKeyColumnNums[i]];   } 
//  All columns are dynamic, nothing to do. 
//  Multiple children 
//  Set up the locations starting from startIndex, and wrapping around the sorted array. 
/*      * Determine the big table retained mapping first so we can optimize out (with     * projection) copying inner join big table keys in the subsequent small table results section.      */
//  Internal vars 
//  Codahale. We just include the pool name in the counter name. 
//  probably a cross product 
// should be no-op since p=3 exists 
/*    * Integer value was interpreted to timestamp inconsistently in milliseconds comparing   * to float/double in seconds. Since the issue exists for a long time and some users may   * use in such inconsistent way, use the following flag to keep backward compatible.   * If the flag is set to false, integer value is interpreted as timestamp in milliseconds;   * otherwise, it's interpreted as timestamp in seconds.    */
//  ... 
/*        * a mouthful, but safe: - a QB is guaranteed to have atleast 1       * destination - we don't support multi insert, so picking the first dest.        */
//  Adding oracle jdbc driver if exists 
// c14Value = (Map<?,?>) rowValues[13];  assertEquals(2, c14Value.size());  Map<?,?> mapVal = (Map<?,?>) c14Value.get(Integer.valueOf(1));  assertEquals(2, mapVal.size());  assertEquals(Integer.valueOf(12), mapVal.get(Integer.valueOf(11)));  assertEquals(Integer.valueOf(14), mapVal.get(Integer.valueOf(13)));  mapVal = (Map<?,?>) c14Value.get(Integer.valueOf(2));  assertEquals(1, mapVal.size());  assertEquals(Integer.valueOf(22), mapVal.get(Integer.valueOf(21))); 
//  Ensure only one final event is ever sent. 
//  Notify any queries waiting on this cacheEntry to become valid. 
//  looks like owner is an unsupported type 
//  Verify if drop partition on a non-existing partition is idempotent and just a noop. 
//  If we get to here, we know that we've archived the partition files, but   they may be in the original partition location, or in the intermediate   original dir. 
//  Add NOT NULL constraints 
//  Has to use full name to make sure it does not conflict with   org.apache.commons.lang.StringUtils 
// Use of ToolRunner "-files" option could be considered here 
/*    * build   *     ^(TOK_INSERT   *         ^(TOK_DESTINATION...)   *      )    */
//  Continue with next table 
/*    * Return the maximum absolute decimal64 value for a precision.    */
//  Generate the columns according to the column mapping provided   Note: The generated column names are same as the   family_name.qualifier_name. If the qualifier   name is null, each column is familyname_col[i] where i is the index of   the column ranging   from 0 to n-1 where n is the size of the column mapping. The filter   function removes any   special characters other than alphabets and numbers from the column   family and qualifier name   as the only special character allowed in a column name is "_" which is   used as a separator   between the column family and qualifier name. 
//  set first argument to IF -- boolean flag 
//  CLI 
//  5 workers to run getReflectionObjectInspector concurrently 
//  Now, compact 
//  check if the pruner only contains partition columns 
//  No need for overflow checks, assume selectivity is always <= 1.0 
//  if the union is the first time seen, set current task to GenMRUnionCtx 
//  Requesting less partitions than allowed should work 
//  position the cursor to line 0 
//     props.put(Constants.SERIALIZATION_NULL_FORMAT, "\\N");      props.put(Constants.SERIALIZATION_FORMAT, "1"); 
//  No need to execute at this stage 
//  It can happen that although there're some partitions in memory, but their sizes are all 0. 
//  The join key is a table column. Create the ExprNodeDesc based on this column. 
//  4. Result 
//  The dispatcher fires the processor corresponding to the closest matching 
//  We create a new sort operator on the corresponding input 
//  If they set ifNotExists check for existence first, and bail if it exists.  This is 
//  return table name for column name if no column has been specified. 
//  HIVE-5336. Re-number the position after remove such that:   (1) getPosition on a column always returns a value between ..schema.size()-1 
// we will create the folder if it does not exist. 
/* (can't push predicate to 'delete' delta)    * if we were to push to 'delete' delta, we'd filter out all rows since the 'row' is always NULL for    * delete events and we'd produce data as if the delete never happened */
//  set the timezone of the object mapper 
//  append the third group within pattern: "}" 
//  We can just restart the session if we have received one. 
/*  @bgen(jjtree) CommaOrSemicolon  */
//  handle file format check for table level 
//  Do implicit conversion accepting the source type and putting it in the same   target type ColumnVector type. 
//  Safety limit for potential list bugs. 
//  Look for tables but do not find any 
//  operator, we can use the same OI. 
/*       End of additional steps     */
//  Overwhelmingly executes once, or maybe twice (replacing stale value). 
//  We store CHAR type stripped of pads. 
// skip this and rest cmds in the line 
/*    *  The algorithm looks at all the mapjoins in the operator pipeline until   *  it hits RS Op and for each mapjoin examines if it has paralllel semijoin   *  edge or dynamic partition pruning.    */
//  run the optimizations that use stats for optimization 
//  Consolidation for outer joins 
//  date 
//  Max time when waiting for write locks on node list 
//  Random batchSize unique ordered integers of 1024 (VectorizedRowBatch.DEFAULT_SIZE) indices.   This could be smarter... 
//  Give it a pass. Optionally, have LiteralDelegate provide a getLiteralClass() to check. 
//  This is not the datanucleus id, but the id assigned by the sequence 
//  Preserve interrupt status 
//  Note: columnIds below makes additional changes for ACID. Don't use this var directly. 
//  Basic case 
//  we skip default directory only if all value is false 
//  Check if the metastore key is set first 
//  Inputs are not the same, bail out 
//  time 
//  If we are only processing a PARTITION BY, reset our evaluators. 
//  The value is before the value record offset.  Make byte segment reference absolute. 
/*    * For now, exclude CHAR until we determine why there is a difference (blank padding)   * serializing with LazyBinarySerializeWrite and the regular SerDe...    */
//  SKEWED_COL_NAMES 
//  10. Run rule to fix windowing issue when it is done over 
//  Bail out if it is not enabled for rewriting 
//  merging from 'target'(inner) to 'node'(outer) 
//  LinkedHashMap to provide the same iteration order when selecting a random host. 
//  found the class, so this would be hadoop version 2.4 or newer (See   HADOOP-10221, HADOOP-10451) 
//  The object for storing row data 
//  the time has come 
//  required   required   required   required   required   required   required 
/*  Backward-compatibility interface for the case where there is no explicit   * name for the function.    */
//  The object is reused during evaluating, make a copy here 
// this exception indicates that a {@code record} could not be parsed and the  caller can decide whether to drop it or send it to dead letter queue.  rolling back the txn and retrying won't help since the tuple will be exactly the same  when it's replayed. 
//  nulls on left, no nulls on right 
//  Drop first and then create 
//  set result to x / p (the quotient) 
/*    * Multiple file sink descriptors are linked.   * Use the task created by the first linked file descriptor    */
//  The row consists of some string columns, some Array<Array<int> > columns. 
//  The type info of each column being assigned. 
//  remove values in key exprs   schema for value is already fixed in MapJoinProcessor#convertJoinOpMapJoinOp 
/*      * When we have DECIMAL_64 as the input parameter then we have to see if there is a special     * vector UDAF for it.  If not we will need to convert the input parameter.      */
//        Filter.g stuff. That way this method and ...ByFilter would just be merged. 
//  there is only one FK 
//  set() will only allocate memory if the buffer of result is smaller than 
//  skip c19, since not selected by query 
//  verification passed - encode the reply 
//  Nothing to find for this type. 
//  Even if no reduction, let's still test the original   predicate to see if it was already a constant,   in which case we don't need any runtime decision 
//  Zookeeper related configs 
//  2. Get Hive Aggregate Info 
//  For each entry in dynamic-multi-dimension collection.   Retrieve skewed column.   Retrieve skewed   map. 
//  compose file text: 
//  Determine the default encoding type (specified on the table, or the global default   if none was provided) 
//  Add constraints.   We need not do a deep retrieval of the Table Column Descriptor while persisting the   constraints since this transaction involving create table is not yet committed. 
//  Status has not changed, continue waiting. 
//  Since we have done an exact match on TS-SEL-GBY-RS-GBY-(SEL)-FS 
//  Table operations. 
//  We don't want that. 
//  field to look for the record identifier in   field inside recId to look for row id in   field inside recId to look for original write id in   field inside recId to look for bucket in   OI for the original row   OI for the record identifier struct   OI for the long row id inside the recordIdentifier   OI for the original write id inside the record   identifer 
//  push down current ppd context to newly added filter 
//  data 
//  granularity 
//  string length should work after readFields() 
//  set to min possible value 
//  If a operator wants to do some work at the end of a group 
//  Move clock forward, and request a task at p=1 
//  Get outputFieldOIs 
//  For column to column only, we toss in date and interval_year_month. 
//  Verify if create table is not called on table t1 but called for t2 and t3. 
//  is not known, estimate that based on the number of entries 
//  Convert the stub from the configuration back into a normal Token 
// add props from params set in table schema 
//  This has to be called before initializing the instance of RawStore 
//  We should not use this optimization if sorted dynamic partition optimizer is used, 
//  A Statement#execute after ResultSet#close should be fine too 
//  first get the columns in named columns 
//  2. Then, update pool allocations. 
//  Format the row format statement 
//  There should be 2 original bucket files (000000_0 and 000001_0), plus one delta directory   and one delete_delta directory. When split-update is enabled, an update event is split into   a combination of delete and insert, that generates the delete_delta directory.   The delta directory should also have 2 bucket files (bucket_00000 and bucket_00001) 
//  10000 bytes per stripe   1024 bytes per split 
/*    * if the given filterCondn refers to only 1 table alias in the QBJoinTree,   * we return that alias's position. Otherwise we return -1    */
//  Check there are no compactions requests left. 
//  I64_VAL 
//  pre-calculated offset values for each alias 
//  is used 
// skip skipSize rows of batch 
//  sparse map. 
//  Erase both headers of the blocks to merge. 
//  Given the previous range and the current range, calculate the new sum   from the previous sum and the difference to save the computation. 
//  Literal bigint. 
//  A list of alrady loaded containers   Number of partitions each table should have   The partition to be spilled next 
//  Configure the AuthFilter with the Kerberos params iff security 
//  Converts amt days to milliseconds 
//  E.E: Lock we are examining is exclusive 
//  Remove the proxy privilege and the auth should fail (in reality the proxy setting should not be changed on the fly) 
//  current transaction 
//  Create schema with a serde, then remap it 
//  scalar/scalar 
//  1. Recompose filter possibly by pulling out common elements from DNF 
//  Parse out words in the sentence 
//  constructing the default MapredWork 
// teseted on Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production 
//  No escaping. 
/*    * TODO: expose this as an operation to client.  Useful for streaming API to abort all remaining   * trasnactions in a batch on IOExceptions.   * Caller must rollback the transaction if not all transactions were aborted since this will not   * attempt to delete associated locks in this case.   *   * @param dbConn An active connection   * @param txnids list of transactions to abort   * @param max_heartbeat value used by {@link #performTimeOuts()} to ensure this doesn't Abort txn which were   *                      hearbetated after #performTimeOuts() select and this operation.   * @param isStrict true for strict mode, false for best-effort mode.   *                 In strict mode, if all txns are not successfully aborted, then the count of   *                 updated ones will be returned and the caller will roll back.   *                 In best-effort mode, we will ignore that fact and continue deleting the locks.   * @return Number of aborted transactions   * @throws SQLException    */
//  retrieve enabled NOT NULL constraint from metastore 
//  18 digits   20 digits 
//  describe/explain/show commands 
//  Repeating null value 
// add another column 
//  We need to update the status of the creation signature 
//  6. Apply Partition Pruning 
//  operations that require insert/delete privileges 
//  Evaluate the value 
//  get StructFields for bucketed cols 
//  See SessionInitContext javadoc. 
//  only user belonging to admin role can create new roles. 
/*  For the case when the output can have null values, follow     * the convention that the data values must be 1 for long and     * NaN for double. This is to prevent possible later zero-divide errors     * in complex arithmetic expressions like col2 / (col1 - 1)     * in the case when some col1 entries are null.      */
//  insert the new task between current task and its child 
//  Tracks running and queued (allocated) tasks. Cleared after a task completes. 
//  Matches only ForwardOperators which are reducers and are followed by GroupByOperators 
//  Process the row batch that has less than DEFAULT_SIZE rows 
//  add self to the end of the queue 
//  For each task completion event, get the associated task id, job id 
/*  vContextEnvironment  */
//  just last one. 
//  Filtering for outer join just removes rows available for hash table matching. 
//  -hiveconf x=y 
//  Test basic assign to vector. 
//  in all the other cases, we can not merge 
//  This will be true if a node was examined by the Vectorizer class. 
//  The only allowed non-overlapping option is extra bytes at the end. 
//  normal. Check stopTimer() works. 
//  unsupported type 
//  Pick random avail port 
//  The JDOException or the Nucleus Exception may be wrapped further in a MetaException 
//  Processor creation. 
//  GBY,RS,GBY,RS,GBY... (top to bottom) 
//  if not already a part of the group-by 
//  Tests for the Partition add_partition(Partition partition) method 
//  update the subcache 
//  This get should fail because its variance ((10-0)/10) is way past MAX_VARIANCE (0.5) 
//  strict admin check -   allows ONLY if hadoop.security.instrumentation.requires.admin is set to true   when hadoop.security.instrumentation.requires.admin is set to true, checks if hadoop.security.authorization   is true and if the logged in user (via PAM or SPNEGO + kerberos) is in hive.users.in.admin.role list 
// look for the " nonEscapedSemiColon " in the query text not the table name which comes  in the result 
//  process method call. 
//  optional string user = 1; 
//  Drop db "testDatabaseOps1" via ObjectStore 
//  Right side 
//  Create the column expr map 
//  First $ separated substring will be txnId and the rest are ValidReaderWriteIdList 
//  Create the routes group 
//   @Ignore("not needed but useful for testing") 
//  PK_NAME 
//  Scale down no rounding to clear fraction. 
//  2^62*2^63 
//  exclusive 
//  Set that as the row id in the mutation 
//  The output* arrays start at index 0 for output evaluator aggregations. 
//  We need a pointer to the hash map since this class must be static to support having 
//  base object inspector   start column number   number of columns 
//  Complete first request. Second pending request should go through. 
//  Alter database set DB property 
//  add selectOp to match the schema 
//  get names of all tables under this dbName 
//  Populate semijoin select if needed 
//  hack off the last word and try again 
//  move to the next root node 
//  use LinkedHashMap<String, Operator<? extends OperatorDesc>>   getAliasToWork()   should not apply this for non-native table 
//  Check to see if the directory already exists before calling   mkdirs() because if the file system is read-only, mkdirs will   throw an exception even if the directory already exists. 
//  Copy the source files to cmroot. As the client will move the source files to another   location, we should make a copy of the files to cmroot instead of moving it. 
//  add/override properties found from hive-site with user-specific properties 
//  Should only be used if n > 0. 
//  There's usually nothing to escape so we will be optimistic. 
//  In order to fix HIVE-16948 
//  Test that timestamp arithmetic is done in UTC and then converted back to local timezone, 
//  When we have split-update and there are two kinds of delta directories-   the delta_x_y/ directory one which has only insert events and   the delete_delta_x_y/ directory which has only the delete events.   The clever thing about this kind of splitting is that everything in the delta_x_y/   directory can be processed as base files. However, this is left out currently   as an improvement for the future. 
//       LOG.info("discover ptns called"); 
//  case the HCAT_KEY_TOKEN_SIGNATURE property in the conf will not be set 
//  so exit the loop and check next lock 
//  Let YARN pick the queue name, if it isn't provided in hive-site, or via the command-line 
//  If there's no authentication, then directly substitute the user 
//  The joins have been automatically converted to map-joins.   However, if the joins were converted to sort-merge joins automatically,   they should also be tried as map-joins. 
//  Create a single insert delta with 150,000 rows, with 15000 rowIds per original transaction id. 
//  because inverse is scaled 2^128, 
//  would still be empty, because no stats are actually populated. 
//  LazySimple seems to throw away everything but \n and \r. 
//  Queries rejected from being cached because they exceeded the max cache entry size. 
//  Just retrieve value from conf 
//  HYBRID strategy 
//  don't re-display warnings we have already seen 
//  line 1:14 Table not found table_name 
//  Save to usedCacheEntry to ensure reader is released after query. 
//  All the data comes from disk. The reader may have split it into multiple slices.   It is also possible there's no data in the file. 
//  ensure that the table properties were copied 
//  We should initialize the SerDe with the TypeInfo when available. 
//  There can be up to 5e-16 error 
//  we are in unsecure mode. 
//  AM sent a shouldDie=true 
//  -Dtest.hms.client.configs=/tmp/conf/core-site.xml,/tmp/conf/hive-site.xml 
//  All children of RS are descendants 
//  check bucket/sort cols 
//  the first time we see a big key. If this key is not in the last   table (the last table can always be streamed), we define that we get 
//  Assumes value is written after key. 
//  Partition-keys added in order. 
// result 
//  For Map, check for virtual columns. 
//  trailing zeroes (or rounding). 
//  ConcurrentHashMap does not allow null - use a substitute value. 
//  Exchange multiple partitions using partial partition-spec (only one partition column) 
//  get tag value from object (last of list) 
//  If we reached here, either :   1. patternWithWildCardChar and patternWithoutWildCardChar are both nulls.   2. patternWithWildCardChar and patternWithoutWildCardChar are both not nulls.   This is an internal error and we should not let this happen, so throw an exception. 
// create 1 row in a file 000001_0_copy2 (and empty 000000_0_copy2?) 
//  case the statement is an INSERT 
//  3) Keep track of colname-to-posmap && RR for new op 
//  column node 
/*  (non-Javadoc)   * @see org.apache.hadoop.mapreduce.InputSplit#getLength()    */
/*    * This method looks in locations specified above and returns the first location where the file   * exists. If the file does not exist in any one of the locations it returns null    */
//  lists are compatible if and only-if the elements are compatible 
//  All inputs of this UnionOperator are in the same Reducer.   We do not need to break the operator tree. 
//  we've seen this terminal before and have created a union work object.   just need to add this work to it. There will be no children of this one   since we've passed this operator before. 
//  noop 
//  See if we need to kill some sessions because the pool was resized down while   a bunch of sessions were outstanding. See also deltaRemaining javadoc. 
//  internal variable 
//  test first IF argument repeating 
//  Generate the signer with secret. 
//  check keys are copied from token store when token is loaded 
//  Null for default partition. 
//  in subquery case, tmp may be from outside. 
//  this position in parent is a constant   now propagate the constant from the parent to the child 
//  Multi-byte truncation. 
//  get a record reader for the idx-th chunk 
// make it looks like 31-32 has been compacted, but not cleaned 
// hive conf option --hiveconf 
//  GLOBAL - all pools inherit 
//  This file descriptor is linked to other file descriptors.   One use case is that, a union->select (star)->file sink, is broken down.   For eg: consider a query like:   select * from (subq1 union all subq2)x;   where subq1 or subq2 involves a map-reduce job.   It is broken into two independent queries involving subq1 and subq2 directly, and   the sub-queries write to sub-directories of a common directory. So, the file sink 
// call the appropriate hive authorizer function 
// test content summary 
//  Task metrics. 
//  When sync is called it will return 100, the value signaling the end of the file, this should   result in a call to sync to the beginning of the block it was searching [50, 100], and it   should continue normally 
//  Validate the update. 
//  closing of open scope should be ok: 
// Hive Stuff 
//  When deal with big data, the VectorizedRowBatch will be used for the different file split   to cache the data. Here is the situation: the first split only have 100 rows,   and VectorizedRowBatch cache them, meanwhile, the size of VectorizedRowBatch will be   updated to 100. The following code is to simulate the size change, but there will be no   ArrayIndexOutOfBoundsException when process the next split which has more than 100 rows. 
//  Ordering of constant and column in expression is important in correct range generation 
//  ALTER VIEW AS SELECT requires the view must exist 
//  Instantiate a SerializationContext which this will use to lookup the   Serialization class and the   actual class being deserialized 
/*  grant priv required  */
/*    * These members have information for data type conversion.   * Not defined if there is no conversion.    */
//  Format the serde statement 
//  4. We create the Filter/Join with the new condition 
//  ok 
//  Lets split up 64-bit hashcode into two 32-bit hash codes and employ the technique mentioned   in the above paper 
//  Replacing it directly in the pool should unblock get. 
//  If there is a hint and no operator is removed then throw error 
/*      * for Select Distinct Queries we don't move any aggregations.      */
//  Trim blanks because OldHiveDecimal did... 
//  create table will start and coomit the transaction 
//  optional .SignableVertexSpec vertex = 1; 
//  BALLOT BOX WITH X U+2612 (3 bytes) 
//  uber > llap > container 
//  Create a new FetchWork to reference the new cache location. 
//  required   required   required   optional   optional   optional   optional 
//  check if the old values are still there in HIVE_CONF_RESTRICTED_LIST 
//  HIVE_SERVICE refers to a logical service name. For now hiveserver2 hostname will be   used to give service actions a name. This is used by kill query command so it can   be authorized specifically to a service if necessary. 
//  Throw away extra if more than 9 decimal places 
/*   * (non-Javadoc)  *  * @see  * org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider  * #authorize(org.apache.hadoop.hive.metastore.api.Database,  * org.apache.hadoop.hive.ql.security.authorization.Privilege[],  * org.apache.hadoop.hive.ql.security.authorization.Privilege[])   */
//  Simulate SHOW LOCKS with different filter options 
//  Answer must be 0 or 1 depending on relative magnitude   of dividend and divisor. 
//  Inserted in sort order. Hence no explict sort. 
//  re-set the static variables in HiveConf to default values 
/*    * LONG.    */
// for queries with a windowing expressions, the selexpr may have a third child 
//  more bits should be considered for finding q (longest zero runs)   set MSB to 1 
//  Use Input class to read length. 
//  1 sec is 0.000000373 months (1/2678400). 1 month is 31 days. 
//  Carefully check the children to make sure they are Decimal64. 
//  For unbucketed tables we have exactly 1 RecordUpdater (until HIVE-19208) for each AbstractRecordWriter which   ends up writing to a file bucket_000000.   See also {@link #getBucket(Object)} 
//  Special handling of grouping function 
//  Fraction digits from lower longword. 
//  add element to ListColumnVector one by one 
//  Use a final variable to properly parameterize the processVectorInspector closure. 
//  null means don't return metadata; we'd need the array anyway for now. 
//  if the colType is not the known type, long, double, etc, then get 
//  The storage handler does not provide predicate decomposition   support, so we'll implement the entire filter in Hive.  However,   we still provide the full predicate to the storage handler in   case it wants to do any of its own prefiltering. 
//  clone thread local file system statistics 
//  out.writeInt(numberRows); 
//  repeated .IOSpecProto output_specs = 11; 
//  For ORC & Parquet, all the following statements are the same   ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS   ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan;   There will not be any Spark job above this task 
//  the byte comparison is potentially expensive so is better to branch on null 
//  We index the get requests to make sure there are no ordering artifacts when we requeue. 
//  load the options first, so we can override on the command line 
//  Make a batch to test the trim functions. 
//  kv7.txt KEYS 
/*    * When no mapping is defined, it is assumed that the hive column names are equivalent to the column names in the   * underlying table    */
//  VALID_WRITE_IDLIST 
//  if this is an update we need to skip the first col since it is row id 
//  Marked as skipped previously. Don't bother processing the rest of the payload. 
// getPos() should always return 0 
//  Deserialize value into vector row columns. 
//  Get our own connection to the database so we can get table and partition information. 
//  Store arrayByteEnd+1 in startPosition[arrayLength]   so that we can use the same formula to compute the length of 
//  Various helper methods. 
//  Override the default delegation token lifetime for LLAP.   Also set all the necessary ZK settings to defaults and LLAP configs, if not set. 
//  Column will be removed from filter   Column will be removed from filter 
//  if the running class was loaded directly (through eclipse) rather than through a 
//  init 
//  By setting the comparison to less, the search should use the block [50, 100] 
//  HIVE-13704 states that we should use run() instead of execute() due to a hadoop known issue   added by HADOOP-10459 
//  Test the idempotent behavior of DROP FUNCTION 
//  note that we may have two or more duplicate partition names. 
//        Not clear of ReplCopyWork should inherit from CopyWork. 
//  not supported 
//  [-v|--verbose] 
//  Find the constant origin of a certain column if it is originated from a constant 
//  SELECT count(*) FROM t). 
//  NOTE: here we should use the new partition predicate pushdown API to   get a list of pruned list, 
//  Required to check the original types manually as PrimitiveType.equals does not care about it 
//  Ignore - this is expected. 
//  that a new one gets created for the next query on the same AM. 
/*    * Element for Key: row and byte[] x Hash Table: HashMap    */
//  converted table 
//  Skip trailing blank characters. 
//  Merge numDistinctValue Estimators 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getRowId(java.lang.String)    */
// it's set in parseRewrittenQuery() 
//  Assume one separator (depth) needed. 
//  Set OpTraits for dynamically partitioned hash join:   bucketColNames: Re-use previous joinOp's bucketColNames. Parent operators should be     reduce sink, which should have bucket columns based on the join keys.   numBuckets: set to number of reducers   sortCols: This is an unsorted join - no sort cols 
//  Finalize paths. 
//  any file claim remain in trash would be granted 
//  Need to preserve currentUser 
//  The create time is set 
//  Create a database and a table in that database.  Because the AlternateFailurePreListener is   being used each attempt to create something should require two calls by the RetryingHMSHandler 
//  UNDONE: Fall through for these... they don't appear to be supported yet. 
//  Whether there's any error occurred during query execution. Used for query lifetime hook. 
//  Deserialize group key into vector row columns. 
//  None are null, so none are selected 
//  Walk through all our inputs and set them to note that this read is part of an update or a 
//  No check needed for single byte read. 
//  Even if there are no results to move, at least check that we have permission   to check the existence of zeroRowsPath, or the read using the cache will fail. 
//  Try to narrow type of constant 
/*    * Responsible for the flow of rows through the PTF Chain.   * An Invocation wraps a TableFunction.   * The PTFOp hands the chain each row through the processRow call.   * It also notifies the chain of when a Partition starts/finishes.   *   * There are several combinations depending   * whether the TableFunction and its successor support Streaming or Batch mode.   *   * Combination 1: Streaming + Streaming   * - Start Partition: invoke startPartition on tabFn.   * - Process Row: invoke process Row on tabFn.   *   Any output rows hand to next tabFn in chain or forward to next Operator.   * - Finish Partition: invoke finishPartition on tabFn.   *   Any output rows hand to next tabFn in chain or forward to next Operator.   *   * Combination 2: Streaming + Batch   * same as Combination 1   *   * Combination 3: Batch + Batch   * - Start Partition: create or reset the Input Partition for the tabFn   *   caveat is: if prev is also batch and it is not providing an Output Iterator   *   then we can just use its Output Partition.   * - Process Row: collect row in Input Partition   * - Finish Partition : invoke evaluate on tabFn on Input Partition   *   If function gives an Output Partition: set it on next Invocation's Input Partition   *   If function gives an Output Iterator: iterate and call processRow on next Invocation.   *   For last Invocation in chain: forward rows to next Operator.   *   * Combination 3: Batch + Stream   * Similar to Combination 3, except Finish Partition behavior slightly different   * - Finish Partition : invoke evaluate on tabFn on Input Partition   *   iterate output rows: hand to next tabFn in chain or forward to next Operator.   *    */
//  to allow for future ctor mutabulity in design 
//  Column 0 
//  Inside of a MR job, we can pull out the actual properties 
//  If we're here, we want more events, and either batchIter is null, or batchIter 
//  ignore.. provides invalid url sometimes intentionally 
// add new column with cascade option 
//  Intermittent failure 
// if impersonation is turned on this called using the HiveSessionImplWithUGI  using sessionProxy. so the currentUser will be the impersonated user here eg. oozie  we cannot create a proxy user which represents Oozie's client user here since  we cannot authenticate it using Kerberos/Digest. We trust the user which opened  session using Kerberos in this case.  if impersonation is turned off, the current user is Hive which can open 
//  Ideally, we'd want 1 worker to try for every slot; e.g. if there are 4 workers we want 3   to re-read, i.e. probability of falling back = 0.75, or 1/4 < random([0,1)). However, we   make it slightly more probable (2.0x) to avoid too much re-reading. This is hand-wavy. 
//  Queue for disabled nodes. Nodes make it out of this queue when their expiration timeout is hit. 
//  Look for functions but do not find any 
//  return null if failed to find 
//  The token is already removed. 
//  Chooses a representative alias, index, and order to use as the String, the first is used   because it is set in the constructor 
/*  * Specialized class for doing a vectorized map join that is an inner join on a Single-Column Long * using a hash map.  */
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createNClob()    */
//  field within complex type 
//  We never lose pool session, so we should still be able to get. 
//  need to compute the input size at runtime, and select the biggest as   the big table. 
/*    * Create a new type for handling precision conversions from Decimal -> Double/Float   *    * The type is only relevant to boxLiteral and all other functions treat it identically.    */
//  We are looking based on the original FSOP, so use the original path as is. 
// now we have delta_0004_0004_0000 with delete events 
//  this SEL is SEL(*)   for LV 
/*  vectorization only works with struct object inspectors  */
//  close them all 
//  base file 
//  for thrift connects 
//  Init the list object inspector for handling partial aggregations 
//  NUM_TRUES 
//  real struct 
/*  Pretty print the values  */
//  Use modified portions of doFastScaleDown code here since we do not want to allocate a   temporary FastHiveDecimal object. 
//  Exponential back-off for NDVs.   1) Descending order sort of NDVs   2) denominator = NDV1 * (NDV2 ^ (1/2)) * (NDV3 ^ (1/4))) * .... 
//  Retrieve partitions. 
//  This is only used by ORC to derive the structure. Most fields are unused. 
//  Passing 0 for currentTxn means, this validTxnList is not wrt to any txn 
// if importing into existing transactional table or will create a new transactional table  (because Export was done from transactional table), need a writeId   Explain plan doesn't open a txn and hence no need to allocate write id. 
//  snapshot of subset of wm tez session info for printing in events summary 
//  As of now Hive Meta Store uses the same configuration as Hadoop SASL configuration 
//  test rounding 
//  Stats were available, try to reduce 
//  for casting floating point types to boolean 
//  for set role NONE, clear all roles for current session. 
//  read the data if it isn't null 
//  optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
//  D5. Test remainder. Carry indicates result<0, therefore QH 1 too 
//  all participating instances uses the same latch path, and curator randomly chooses one instance to be leader   which can be verified via leaderLatch.hasLeadership() 
//  generate struct for each of the given prefixes 
//  Using the second table, since a table called "test_table" exists in both databases 
//  Note: Thrift returns an SSL socket that is already bound to the specified host:port   Therefore an open called on this would be a no-op later   Hence, any TTransportException related to connecting with the peer are thrown here.   Bubbling them up the call hierarchy so that a retry can happen in openTransport,   if dynamic service discovery is configured. 
//  Registry uses ephemeral sequential znodes that are never updated as of now. 
//  HIVE-5839 
//        move the setupPool code to ctor. For now, at least hasInitialSessions will be false. 
//  GroupBy not distinct like, disabling 
//  first authorize the call 
//  the required outputLength. 
//  There can be atmost one element eligible to be converted to   metadata only 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getClob(java.lang.String)    */
//  Otherwise, return the row. 
//  the columns in nonPartColIndxsThatRqrStats/nonPartColNamesThatRqrStats/hiveColStats   are in same order 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getInt(java.lang.String)    */
//  ^(TOK_ALTERTABLE_ADDPARTS identifier ifNotExists? alterStatementSuffixAddPartitionsElement+) 
//  Sanity check the config. 
//  The interval to wake up and check the queue 
//  It's a column. 
//  USER_PRIVILEGES 
//  to provide the type params to the type cast. 
//  After Load from this dump, all target tables/partitions will have initial set of data but source will have latest data. 
// pretend this partition exists 
//  Create a writable object inspector for primitive type and return it. 
//  CLIENT/TOOL END     Singleton instance in the job client 
//  Column 1 
//  A call to increaseBufferSpace() or ensureValPreallocated() will ensure that buffer[] points to   a byte[] with sufficient space for the specified size.   This will either point to smallBuffer, or to a newly allocated byte array for larger values. 
//  set up the fetch operator for the new input file. 
//  Subtract 1. 
//  No reason to poll untill the job is initialized 
// cause them to be localized for the Sqoop MR job tasks 
/*    * The type information for all fields.    */
//  Incoming events can be ignored until the point when shuffle needs to be handled, instead of just scans. 
//  rs1's schema is key 0, max 1, min 2 
//  If there is a single discardable operator, it is a TableScanOperator   and it means that we have merged filter expressions for it. Thus, we   might need to remove DPP predicates from the retainable TableScanOperator 
//  Column 2 
//  already committed or aborted. 
//  select columns that actually do not exist in the file. 
//  1. Translate the UDAF 
//  check if right is valid 
//  c/p from OrcInputFormat 
//  Order by is not necessary, but MS SQL require it to use FETCH 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getArray(int)    */
//  drop role. ignore error. 
// In DbTxnManager.acquireLocks() we have   2 ReadEntity: [default@acidtblpart@p=p1, default@acidtblpart]   1 WriteEntity: default@acidtblpart Type=TABLE WriteType=INSERT isDP=false 
//  If it is a extract operator, we need to rewrite it 
//  Trigger query hooks before query execution. 
//  Subscriber can get notification about drop of a database in HCAT   by listening on a topic named "HCAT" and message selector string   as "HCAT_EVENT = HCAT_DROP_DATABASE" 
//  Table creation should succeed even if location is specified 
/*  * This servlet is based off of the JMXProxyServlet from Tomcat 7.0.14. It has * been rewritten to be read only and to output in a JSON format so it is not * really that close to the original.  */
//  We need to get the partition's column names from the partition serde.   (e.g. Avro provides the table schema and ignores the partition schema..). 
//  apply join filters on the row. 
/*    * Translate column names by walking the AST    */
//  no dynamic partitions 
//  Extract tables used by the query which will in turn be used to generate   the corresponding txn write ids 
//  remove this cor var from output position mapping 
//  Assumes the result set is set to a valid row 
//  Check if this txn state is already replicated for this given table. If yes, then it is 
//  Shouldn't really happen. 
//  ETL strategies will have start=3 (start of first stripe) 
/* analyzeExport() creates TableSpec which in turn tries to build     "public List<Partition> partitions" by looking in the metastore to find Partitions matching     the partition spec in the Export command.  These of course don't exist yet since we've not     ran the insert stmt yet!!!!!!!       */
//  Tracks completed requests pre node 
//  Successful task ID's 
//  Assumes the cache lock has already been taken. 
//  Session TxnManager that is already in use. 
//  the histogram object 
/*  (non-Javadoc)  * @see org.apache.hadoop.mapreduce.RecordReader#close()   */
//  Generate applicationId for the LLAP splits 
//  We are going to build the name 
//  Spark task we're currently processing 
//  2. If we did not generate anything for the new predicate, we bail out 
//  Hadoop queue information 
//  If a table import statement specified a location and the table(unpartitioned)   already exists, ensure that the locations are the same.   Partitioned tables not checked here, since the location provided would need 
//  Event 2 (alter: marker stats event), 3 (insert), 4 (alter: stats update event) 
//  Sorry, too many ifs but this form looks optimal 
//  In general, when can have unlimited # of branches,   we currently only handle either 1 or 2 branch. 
//  needed 
//  Forward the overflow batch over and over:        Reference a new one big table row's values each time        cross product      Current "batch" of small table values.     TODO: This could be further optimized to copy big table (equal) keys once         and only copy big table values each time...         And, not set repeating every time...   
//  Adding the S3 credentials from Hadoop config to be hidden 
//  Trim trailing zeroes and re-adjust scale. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setAutoCommit(boolean)    */
//  estimated number of rows will be product of NDVs 
//  lazily create instances 
//  Record current valid txn list that will be used throughout the query   compilation and processing. We only do this if 1) a transaction   was already opened and 2) the list has not been recorded yet,   e.g., by an explicit open transaction command. 
//  1) First branch is query, second branch is MV 
//  Desc will be null if its CREATE TABLE LIKE. Desc will be   contained in CreateTableLikeDesc. Currently, HCat disallows CTLT in   pre-hook. So, desc can never be null. 
//  We'll over allocate and then shrink the array for each type 
// this includes the mandatory alias 
//  third group 
//  Call out to the real configure method 
//  second, overflow IS an error in ANSI SQL Numeric.   CAST(10000 AS DECIMAL(38,38)) throws overflow error. 
// some commands like "show databases" don't start implicit transactions 
//  The heartbeat has timeout, double check whether we can remove it 
//  index for value(+ from keys, - from values) 
//  The UDTF expects arguments in an object[] 
//  Construct skewed-value to location map except default directory.   why? query logic knows default-dir structure and don't need to get from map 
// look at queryPlan.outputs(WriteEntity.t - that's the table)  for example, drop table in an explicit txn is not allowed  in some cases this requires looking at more than just the operation  for example HiveOperation.LOAD - OK if target is MM table but not OK if non-acid table 
//  this is the original parent of the currentRootOperator as we scan   through the graph. A root operator might have multiple parents and   we just use this one to remember where we came from in the current 
/*      * Don't catch any execution exceptions here and let the caller catch it.      */
//  Sets permissions and group name on partition dirs and files. 
//  we should have stats for all columns (estimated or actual) 
//  If new SerDe needs to store fields in metastore, but the old serde doesn't, save   the fields so that new SerDe could operate. Note that this may fail if some fields   from old SerDe are too long to be stored in metastore, but there's nothing we can do. 
//  log warning if row count is missing 
//  Use getPerfLogger to get an instance of PerfLogger 
//  partition was found, but was empty. 
//  generate basic tez config 
//  Set the HiveDecimalWritable from bytes without converting to String first for   better performance. 
/*  10 files x 100 size for 1 splits  */
//  Add a new partition via ObjectStore 
//  -e <query> 
//  stored stats 
/*        * Now new job requests should succeed as all cancel threads would have completed.        */
//  Nothing currently available for hash sets. 
//  leaving some table from the list of tables to be cached 
// MERGE statements could have inserted a cardinality violation branch, we need to avoid that 
//  normalize prop name 
//  this can be a max of 65, never > 127 
//  Add default dir at the end of each list 
//  included will not be null, row options will fill the array with trues if null 
//  setDays resets the isNull[i] to false if there is a parse exception 
//  Fail silently 
//  Test that existing exclusive partition with new shared_read coalesces to 
//  could be null for default partition 
//  3. Construct Join Rel Node and RowResolver for the new Join Node 
// Use i 
//  of its parent SparkWorks for the small tables 
//  If the modelerType attribute was not found, the class name is used   instead. 
//  Make vectorized operator 
//  "java version" system property is formatted   major_version.minor_version.patch_level.   Find second dot, instead of last dot, to be safe 
// set to empty "Text" if hive.metastore.token.signature property is set to null 
//  Group by operators select the key cols, so no need to find them in the values 
//  Only mark output NULL when input is NULL. 
//  create the directories FileSinkOperators need 
//  Need a way to know what thread to interrupt, since this is a blocking thread. 
//  This may be possible when srcType is string but destType is integer 
// since T3 overlaps with Long Running (still open) GC does nothing 
/*    * Creates the configuration object necessary to run a specific vertex from   * map work. This includes input formats, input processor, etc.    */
//  the file may not exist, and we just ignore this 
//  Show we cannot create a child of a null directory 
//  Bloom filter for the new node that we will eventually add to the cache 
// -----------------------------------------------------------------------------------------------   Convert to string/UTF-8 ASCII bytes methods.  ----------------------------------------------------------------------------------------------- 
//  some bookkeeping 
//  ForwardOperator so that we can add multiple filter/group by operators as children 
//  Event dump, each sub-dir is an individual event dump.   We need to guarantee that the directory listing we got is in order of evid. 
//  Try another slot. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNClob(int, java.io.Reader, long)    */
//  the schema for intersect all is like this   R3 + count(c) as cnt + min(c) as m   we create a input project for udtf whose schema is like this   min(c) as m + R3 
//  The simple validity check is to see if the file is of size 0 or not.   Other checks maybe added in the future. 
//  Don't mess with the cached object. 
//     Assert.assertFalse(metaStore.isPathExists(new Path(tableLocation + "/year=2017"))); 
//  can clean 
/*          * Get our Single-Column Long hash multi-set information for this specialized class.          */
// clean up if above throws 
//  if this rel references corVar   and now it needs to be rewritten   it must have been pulled above the Correlator   replace the input ref to account for the LHS of the   Correlator 
//  For explain plan, txn won't be opened and doesn't make sense to allocate write id 
//  Register function 
//  Power of ten way beyond our precision/scale... 
//  move all the children to the front of queue 
// Test outputformat with complex data type, and with reduce 
//  parsing error, invalid url string 
//  T is settable recursively i.e all the nested fields are also settable. 
//  with constant folding then the result will be IfExprLongColumnLongScalar. 
//  Use SignableVertexSpec.newBuilder() to construct. 
//  Test select root.col2 from root:struct<col1:struct<a:boolean,b:double>,col2:double> 
//  Initialize transient fields. To be called after deserialization of other fields. 
//  This is the threshold that the user has specified to fit in mapjoin 
/*    * Tests if LOCATION_1 is returned when file is present in the first directory in lookup order    */
//  BINARY_VALUE 
//  Simplify vector by brute-force flattening noNulls if isRepeating   This can be used to reduce combinatorial explosion of code paths in VectorExpressions 
//  skip entire the data: 
//  set nothing for prepared sql 
//  all files were copied successfully in last try. So can break from here. 
//  Timestamps with a second VInt storing additional bits of the seconds field. 
//  remember the count() positions 
/*  initialize rowNums to have 1 row  */
//    read back!   
// heartbeater should be running in the background every 1/2 second 
//  4. Create and return IN clause 
//  For pattern 'd', find digits. 
//  in theory, the below call isn't needed in non thrift_mode, but let's not   get too crazy 
//  Class loading is thread safe. 
//  If started from main(), and noLog is on, we should not output   any logs. To turn the log on, please set -Dtest.silent=false 
//  Explicitly mis-set the catalog name 
//  Second incremental load 
// now make sure we get the stats we expect for partition we are going to add data to later 
//  If the client did not specify qop then just negotiate the one supported by server 
//  Round using the "half-up" method used in Hive. 
//  will strain out the record id for the underlying writer. 
//  Load using same dump to a DB with view. It should fail as DB is not empty. 
//  1. Collect GB Keys 
//  Assume we don't need to fetch the rest of the skewed column data if we have no columns. 
//  Add the new task as child of each of the passed in tasks 
//    throw new HiveException("batch.selected is not in sort order and unique");   } 
// checkoutputspecs might've set some properties we need to have context reflect that 
//  TABLE_STATS 
//  now look in any jars we've packaged using JarFinder. Returns null   when 
//  The super method will reload a hash table partition of one of the small tables.   Currently, for native vector map join it will only be one small table. 
//  Swap the first element of the metastoreUris[] with a random element from the rest   of the array. Rationale being that this method will generally be called when the default   connection has died and the default connection is likely to be the first array element. 
/* , new Exception() */
//  Start with dummy vector operator as the parent of the parallel vector operator tree we are   creating 
//  //////////////////   String utilities   ////////////////// 
//  Aggregate this batch. 
//  used for testing 
//  There should be a reader event available, or coming soon, so okay to be blocking call. 
//  Decode the value if necessary 
//  runInternal, which defers the close to the called in that method. 
/*    * getConvertedOI without any caching.    */
/*  Locate the op where the branch starts.  This function works only for the following pattern.   *     TS1       TS2   *      |         |   *     FIL       FIL   *      |         |   *      |     ---------   *      RS    |   |   |   *      |    RS  SEL SEL   *      |    /    |   |   *      |   /    GBY GBY   *      JOIN       |  |   *                 |  SPARKPRUNINGSINK   *                 |   *              SPARKPRUNINGSINK    */
//  call-1: listLocatedStatus - mock:/mocktbl2   call-2: check side file for mock:/mocktbl2/0_0   call-3: open - mock:/mocktbl2/0_0   call-4: check side file for  mock:/mocktbl2/0_1 
//  1 NULL 
//  make sure only map-joins can be performed. 
//  safe to assume else is ORC as semantic analyzer will check for RC/ORC 
//  A1 removed from A.   Make sure that we can still get a session from A. 
//  5 minutes. 
//  For the failures, the users have been notified, we just need to clean up. There's no   session here (or it's unused), so no conflicts are possible. We just remove it.   For successes, the user has also been notified, so various requests are also possible;   however, to start, we'd just put the session into the sessions list and go from there. 
//  for Xlint, code will never reach here 
/*    * Sets the job state to COMPLETED and also sets the results value. Returns true   * if COMPLETED status is set. Otherwise, it returns false.    */
/*        * Trigger kill threads and verify that we get InterruptedException and expected       * Message. This should raise 3 kill operations and ensure that retries keep the time out       * occupied for 4 sec.        */
/*        we have reached the point where we are transferring files across fileSystems.     */
//  Fill high long from middle long, and middle long from lower long. 
/*    * Capture the CTE definitions in a Query.    */
/*              * Equal key series checking.              */
/*            * Optionally, read current value's big length.  {Big Value Len} {Big Value Bytes}            */
//  long range bias for 32-bit hashcodes 
//  https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#Running_as_the_superuser 
//  Try to find the class 
//  write the sync bytes   flush header 
// We store the position to the constant value for later use. 
/* do fast path first (in 1 statement) if doesn't work, rollback and do the long version */
//  optional int32 within_dag_priority = 3; 
//  Using a member variable in the closure will not do the right thing... 
//  Have to scan the directory to find min date greater than currentDir. 
//  sets up temp and task temp path 
//  Join clause: rewriting is needed 
//  optional int32 am_port = 7; 
//  This is a semijoin branch. Find if this is creating a potential   cycle with childJoin. 
//  Parameters for exporting metadata on table drop (requires the use of the)   org.apache.hadoop.hive.ql.parse.MetaDataExportListener preevent listener 
//  No temp tables, just call underlying client 
//  by now, 'prunedCols' are columns used by child operators, and 'columns' 
//  First we quickly check if the two table scan operators can actually be merged 
//  MY_16BIT_INT 
//  avro guarantees that the key will be of type string. So we just need to worry about   deserializing the value here 
//  The entire seconds field is stored in the first 4 bytes. 
//  I'm assuming that there is no transaction ID for a read lock. 
//  AUTHORIZER 
// default outputTypeInfo is long 
//  Now create session specific dirs 
/*  Comma separated list of classes in a batch  */
//  REPL_POLICY 
//  let writers release the memory for garbage collection 
//  1/ reserve spaces for the byte size of the struct   which is a integer and takes four bytes 
//  of the outputs of intermediate map reduce jobs. 
//  Don't break if this allocation failure was a result of a LOCALITY_DELAY. Others could still be allocated. 
//  Newer tasks first. 
//  This is just the directory.  We need to recurse and find the actual files.  But don't   do this until we have determined there is no base.  This saves time.  Plus,   it is possible that the cleaner is running and removing these original files,   in which case recursing through them could cause us to get an error. 
//  IS_REPLACE 
//  UTF-8 byte constants used by string/UTF-8 bytes to decimal and decimal to String/UTF-8 byte   conversion. 
/*      * For better performance on LONG/DOUBLE we don't want the conditional     * statements inside the for loop.      */
//  Directory where path resides 
//  There's no more data. 
//  higerbound when useDensityFunctionForNDVEstimation is true. 
//  hconf is null in unit testing 
//  Either this arena is being allocated, or it is already allocated, or it is next. The 
/*      * these are the functions that have a Window.     * Fns w/o a Window have already been processed.      */
//  push down filters 
//  keeps track of the right-hand-side table name of the left-semi-join, and 
/*  1 files x 1000 size for 10 splits  */
//  Node1 marked as failed, node2 has capacity. 
//  Literal smallint. 
//  KerberosName.getShorName can only be used for kerberos user, but not for the user   logged in via other authentications such as LDAP 
//  rowId <= '2014-07-01' 
//  Calculate which writer to use from the remaining values - this needs to   be done before we delete cols. 
//  This Project will be what the old input maps to,   replacing any previous mapping from old input). 
//  Note: this is called under the epic lock. 
//  Reset for reading. 
//  Check if this is the best match so far 
/*  fall through  */
//  always long 
//  No such table in the given database 
//  pre-compute normalization so we don't have to deal   with SQLExceptions later 
//  6. Generate Join operator 
//  Besides the HiveShims jar which is Hadoop version dependent we also   always need to include hive shims common jars. 
//  No locality delay 
//  no need to generate is not null predicate for partitioning or   virtual column, since those columns can never be null. 
//  5. Take care of view creation 
/*  Compare two strings from two byte arrays each   * with their own start position and length.   * Use lexicographic unsigned byte value order.   * This is what's used for UTF-8 sort order.   * Return negative value if arg1 < arg2, 0 if arg1 = arg2,   * positive if arg1 > arg2.    */
// FileSystem fs = FileSystem.get(getConf()); 
//  Decimal columns use HiveDecimalWritable. 
//  delim 
//  If it is not a column we need for the keys, move on 
//  We walk through the AST.   We replace all the TOK_TABREF by adding additional masking and filter if   the table needs to be masked or filtered.   For the replacement, we leverage the methods that are used for 
//     ve = vc.getVectorExpression(exprDesc);      assertTrue(ve instanceof IfExprVarCharScalarVarCharScalar); 
//  In HiveMetaStore, the deleteData flag indicates whether DFS data should be   removed on a drop. 
//  values we put in above. 
//  Adding a select operator to top of semijoin to ensure projection of only correct columns 
//  populate partition info 
//  in the projection. 
//  transform the table reference to an absolute reference (i.e., "db.table") 
//  Cache extractObject 
//  Case 9: column stats, NO grouping sets 
//  existing avro data 
//  Key is column name and the value is the col stat object 
//  SettableMapObjectInspector 
//  Check that HIVE_EXEC_INPUT_LISTING_MAX_THREADS has priority overr DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX 
//  optional string token_identifier = 8; 
/*  If this boolean is true, we bypass the cache.  */
//  Go over the schema and convert type to thrift type 
//  user is providing config, so it could be null. 
//  exact limit can be taken care of by the fetch operator 
// we do not care about the transformation or rewriting of AST   which following statement does   we only care about the restriction checks they perform.   We plan to get rid of these restrictions later 
//  This DemuxOperator can appear multiple times in MuxOperator's   parentOperators 
//  only one reducer if this configuration does not prevents 
//  GRANT_OPTION 
//  No room. 
//  required   required   required   required   required   required   required   required   required 
//  Reparse text, passing null for context to avoid clobbering   the top-level token stream. 
//  The list of bytes used for the separators in the column (a nested struct    such as Array<Array<int>> will use multiple separators).   The list of separators + escapeChar are the bytes required to be escaped. 
/*  * The root interface for a vector map join hash set.  */
//  Should return null when there is no column 
//  2) Insert data into both tables 
// The complete list of output columns. These should be added to the  Vectorized row batch for processing. The index in the row batch is  equal to the index in this array plus initialOutputCol. 
// The query needs ROW__ID: maybe explicitly asked, maybe it's part of   Update/Delete statement.  Either way, we need to decorate "original" rows with row__id 
//  Check if ARRAY_IDX argument is of category LIST 
//  no interpolation needed because position does not have a fraction 
//  Ignore leading zeroes. 
//  delete column statistics if present 
//  use left alias (~mysql, postgresql)   try widening conversion, otherwise fail union 
//  This means there is a second VInt present that specifies additional bits of the timestamp.   The reversed nanoseconds value is still encoded in this VInt. 
//  Check constraint fails only if it evaluates to false, NULL/UNKNOWN should evaluate to TRUE 
//  production is: byte 
//  default arg-less run simply runs, and does not care about failure 
//  starting Tez session pool in start here to let parent session state initialize on CliService state, to avoid   SessionState.get() return null during createTezDir 
//  some UDFs may emit strings of variable length. like pattern matching   UDFs. it's hard to find the length of such UDFs.   return the variable length from config 
//  check if stats need to be (re)calculated 
//  first promote the next group to be the current group if we reached a   new group in the previous fetch 
//  Test "alter table" with rename 
//  Any more input? 
//  Partitions added now should inherit table-schema, properties, etc. 
/*          * Initialize Single-Column Long members for this specialized class.          */
//  2.3 Check if GROUPING_ID needs to be projected out 
//    Rewrite logic:     1. rewrite join condition.   2. map output positions and produce cor vars if any.   
//  a root Task 
//     HCatUtil.logAllTokens(LOG,context); 
//  ~ Methods ---------------------------------------------------------------- 
//  Bootstrap constraint dump shouldn't fail if the table is dropped/renamed while dumping it.   Just log a debug message and skip it. 
/*  Testing for equality of doubles after a math operation is     * not always reliable so use this as a tolerance.      */
//  Lock database operation is to acquire the lock explicitly, the operation   itself doesn't need to be locked. Set the WriteEntity as WriteType:   DDL_NO_LOCK here, otherwise it will conflict with Hive's transaction. 
//  Following a suggestion from Gopal, quickly read in the bytes from the stream.   CONSIDER: Have ORC read the whole input stream into a big byte array with one call to   the read(byte[] b, int off, int len) method and then let this method read from the big   byte array. 
//  Keeps existing output column map, etc. 
//  same session 
//  Non-empty java opts with -Xmx specified twice 
//  Use table properties in case of unpartitioned tables,   and the union of table properties and partition properties, with partition   taking precedence, in the case of partitioned tables 
//  is not expected further down the pipeline. see jira for more details 
//  Reset port setting 
//  table as second read entity 
//  Each http request must have an Authorization header 
//  Note: for deallocateEvicted, we do not release the memory to memManager; it may   happen that the evictor tries to use the allowance before the move finishes.   Retrying/more defrag should take care of that. 
//  Note - important to retain the same key as the export 
//  operations require select priv 
//  The partition got dropped before we went looking for it. 
//  Hadoop DelegationTokenManager default is 1 week. 
/*    * When transforming a Not In SubQuery we need to check for nulls in the   * Joining expressions of the SubQuery. If there are nulls then the SubQuery always   * return false. For more details see   * https://issues.apache.org/jira/secure/attachment/12614003/SubQuerySpec.pdf   *   * Basically, SQL semantics say that:   * - R1.A not in (null, 1, 2, ...)   *   is always false.   *   A 'not in' operator is equivalent to a '<> all'. Since a not equal check with null   *   returns false, a not in predicate against aset with a 'null' value always returns false.   *   * So for not in SubQuery predicates:   * - we join in a null count predicate.   * - And the joining condition is that the 'Null Count' query has a count of 0.   *    */
//  Created on demand. 
//  the operator stack. 
//  we'll set up tez to combine spits for us iff the input format   is HiveInputFormat 
//  Ignore potentially incorrect values 
//  prevent view from referencing itself 
//  Generate binary sortable key for current row in vectorized row batch. 
//  Used by replication, copy files from source to destination. It is possible source file is   changed/removed during copy, so double check the checksum after copy, 
//  means serializing another instance. 
//  do not print duplicate status while still in middle of print interval. 
//  bail out 
//  Use UTC date to ensure reader date is same on all timezones. 
//  We always set the null flag to false when there is a value. 
// Read the template into a string; 
//  Bootstrap dump with empty db 
//  Case 2. 
//  sleep this many seconds between each retry. 
//  we first look for this alias from CTE, and then from catalog. 
//  If the partition columns can't all be found in the values then the data is not bucketed 
//  DEST_TABLE_NAME 
//  a binary operator (gt, lt, ge, le, eq, ne) 
//  portion of the join output. 
//  only 2 valid partitions should be added 
//  Create a client instance 
//  All the setup is done in GenMapRedUtils 
// old version of thrift client should have (lc.isSetOperationType() == false) but they do not  If you add a default value to a variable, isSet() for that variable is true regardless of the where the  message was created (for object variables.   It works correctly for boolean vars, e.g. LockComponent.isTransactional).  in test mode, upgrades are not tested, so client version and server version of thrift always matches so  we see UNSET here it means something didn't set the appropriate value. 
// H1 - should allocate 
//  runtime objects 
/*    * (non-Javadoc)   *    * @see   * org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator#finishPartition()   *    * for fns that are not ISupportStreamingModeForWindowing give them the   * remaining rows (rows whose span went beyond the end of the partition) for   * rest of the functions invoke terminate.   *    * while numOutputRows < numInputRows for each Fn that doesn't have enough o/p   * invoke getNextObj if there is no O/p then flag this as an error.    */
//  This helper object deserializes LazyBinary format small table values into columns of a row 
// disable datasource  Case it is an insert overwrite we have to disable the existing Druid DataSource 
//  Need a concurrent weakhashmap. WeakKeys() so that when underlying transport gets out of   scope, it still can be GC'ed. Since value of map has a ref to key, need weekValues as well. 
//  Raw socket connection (non-sasl) 
//  Allocate 1-1-... xN; free every other one, allocate N/2 (or N/4). 
//  the right token. 
//  Move from the tmp dir to an intermediate directory, in the same level as 
//  multiple instances of such classes 
//  unused 
//  shell_cmd = "/bin/bash -c \'" + shell_cmd + "\'"; 
//  Special module for tests in the rootDir. 
//  Or UDF, (rowId <= 'd' or rowId >= 'q') 
//  HCat will always prune columns based on what we ask of it - so the 
//  Okay, we're going to need these originals.  Recurse through them and figure out what we   really need. 
//  No existing lock for this partition. 
//  FUNC 
//  Now pick a server node randomly 
// 3)  test bad field names 
/*      * Check.5.h :: For In and Not In the SubQuery must implicitly or     * explicitly only contain one select item.      */
/*        * Do a round each as physical with no row selection and logical with row selection.        */
//  Build new join. 
//  some join alias could be changed: alias -> newAlias 
//  just return k1 is smaller than k2 
//  Infer schema 
//  set memory available for operators 
//  Does a pool exist for this path already 
//  unused / unknown reason 
//  end of union 
//  Invalid schemes 
// suppress multi-spaces 
//  Create the walker and  the rules dispatcher. 
//  then reopened session will use user specified queue name else default cluster queue names. 
//  Ok to be interrupted 
//  default block size is set to 8 as most cache line sizes are 64 bytes and also AVX512 friendly 
/*    * Wrapper class to write a HS2ConnectionConfig file    */
// Complete one task on host1. 
//  compare next part 
// --------------------------------------------------------------------------- 
//  all keys matched. 
//  Check query results cache.   If no masking/filtering required, then we can check the cache now, before   generating the operator tree and going through CBO. 
//  Should not have tried to print any thing. 
/*    * Returns the singleton instance of jobId->delegation-token-in-string-form cache    */
// check for QUERY_HINT expressions on ast 
//  Reset the location, db and table name and compare the partitions 
//  2^32 + .01 
/*  january is 0  */
//  when we reach here, we must have some data already (because size >0).   We need to see if there are any data flushed into file system. If not,   we can   directly read from the current write block. Otherwise, we need to read   from the beginning of the underlying file. 
//  Fetch existing Ingestion Spec from Druid, if any 
//  Set isNull before calls in case they change their mind. 
//  Use LlapOutputSocketInitMessage.newBuilder() to construct. 
//  Setting {min,max}SessionTimeout defaults to be the same as in Zookeeper 
//  ...for each of the tables that are part of the materialized view,   where the transaction had to be committed after the materialization was created... 
//  Create an instance of hive in order to create the tables 
//  Initiate a minor compaction request on the table. 
//  the function should support both short date and full timestamp format   time part of the timestamp should not be skipped 
//  Primitive column types ignore nulls and just copy all values. 
//  only input 2 side has nulls 
// updating bucket column should move row from one file to another - not supported 
//  LOG.info("Writing offset " + tailOffset + " at " + lrPtrOffset); 
// make sure to get all deltas within a single transaction;  multi-statement txn  generate multiple delta files with the same txnId range  of course, if maxWriteId has already been minor compacted, all per statement deltas are obsolete 
//  If the query contains more than one join 
//  LOG.debug("finishOuterRepeated batch #" + batchCounter + " " + joinResult.name() + " batch.size " + batch.size + " someRowsFilteredOut " + someRowsFilteredOut); 
/*  matches skewed values.  */
//  Use vector parent to get VectorizationContext. 
/*       here is a portion of the above "explain".  The "filterExpr:" in the TableScan is the pushed predicate      w/o PPD, the line is simply not there, otherwise the plan is the same       Map Operator Tree:,         TableScan,          alias: acidtbl,          filterExpr: (a = 3) (type: boolean),            Filter Operator,             predicate: (a = 3) (type: boolean),             Select Operator,             ...        */
//  Note - Kerberos user w/o appId doesn't have access. 
//  Mixing down into the lower bits - this produces a worse hashcode in purely   numeric terms, but leaving entropy in the higher bits is not useful for a   2^n bucketing scheme. See JSR166 ConcurrentHashMap r1.89 (released under Public Domain)   Note: ConcurrentHashMap has since reverted this to retain entropy bits higher   up, to support the 2-level hashing for segment which operates at a higher bitmask 
//  we need to check if the properties are valid, especially for stats.   they might be changed via alter table .. update statistics or   alter table .. set tblproperties. If the property is not row_count   or raw_data_size, it could not be changed through update statistics 
/*             for table replication if we reach the max number of tasks then for the next run we will            try to reload the same table again, this is mainly for ease of understanding the code            as then we can avoid handling == > loading partitions for the table given that            the creation of table lead to reaching max tasks vs,  loading next table since current            one does not have partitions.            */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setCharacterStream(int, java.io.Reader,   * long)    */
//  prints short events information that are safe for consistent testing 
//  LOG.debug("VectorMapJoinFastLongHashTable findReadSlot returning not found");   We know we never went that far when we were inserting.   LOG.debug("VectorMapJoinFastLongHashTable findReadSlot key " + key + " slot " + slot + " pairIndex " + pairIndex + " largestNumberOfSteps " + largestNumberOfSteps + " (i = " + i + ")"); 
//  Create the output OI array 
//  Expected for permanent UDFs at this point. 
//  Use the current return type when creating a new call, for   operators with return type built into the operator   definition, and with no type inference rules, such as   cast function with less than 2 operands. 
//  1.2 Create TableScanDesc 
//  if column value is provided, replace column name with value 
//  Hive UDTF only has a single input 
//  Connection.getMetaData().getTableTypes() 
//  Output will also be repeating and null 
//  All are selected 
//  SCHEMA_VERSIONS 
//  returns Path of the partition created (if any) else Path of table 
//  CredentialProvider/CredentialProviderFactory may not exist, depending on the version of   hadoop-2 being used to build Hive. Use reflection to do the following lines   to allow the test to compile regardless of what version of hadoop-2.   Update credName entry in the credential provider.  CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);  provider.createCredentialEntry(credName, credPassword.toCharArray());  provider.createCredentialEntry(credName3, credOnlyPassword.toCharArray());  provider.flush(); 
//  sessionState.getQueueName() comes from cluster wide configured queue names.   sessionState.getConf().get("tez.queue.name") is explicitly set by user in a session.   TezSessionPoolManager sets tez.queue.name if user has specified one or use the one from   cluster wide queue names.   There is no way to differentiate how this was set (user vs system).   Unset this after opening the session so that reopening of session uses the correct queue   names i.e, if client has not died and if the user has explicitly set a queue name 
//  number of digits in a (1..37)   number of digits in b (1..37) 
//  Table files don't have footer rows. 
//  Reload tables from the MetaStore, and create data files 
//  Transport specific confs 
//  For now only alter name, owner, class name, type 
/*  logicalColumnIndex  */
//  Try to get cluster information once, to avoid immediate cluster-update event in WM. 
//  If we found an exact bin match for value v, then just increment that bin's count.   Otherwise, we need to insert a new bin and trim the resulting histogram back to size.   A possible optimization here might be to set some threshold under which 'v' is just   assumed to be equal to the closest bin -- if fabs(v-bins[bin].x) < THRESHOLD, then   just increment 'bin'. This is not done now because we don't want to make any 
//  Counters for debugging, we cannot use existing counters (cntr and nextCntr)   in Operator since we want to individually track the number of rows from 
//  There may be multiple selects - chose the one closest to the table 
//  Look for single column optimization. 
//  Test alter table 
//  However, it must end after the split end, otherwise the next one would have been read. 
//  get partition metadata if partition specified 
/*                * Restriction 2.h Subquery isnot allowed in LHS                */
//  if aggr is distinct, the parameter is name is constructed as   KEY.lastKeyColName:<tag>._colx 
//  OR(=($0, 1), =($0, 2), AND(=($0, 0), =($1, 8)))   transformation creates 9 nodes AND(OR(=($0, 1), =($0, 2), =($0, 0)), OR(=($0, 1), =($0, 2), =($1, 8)))   thus, it is NOT triggered 
//  The user has not returned and the kill has failed.   We are going to brute force kill the AM; whatever user does is now irrelevant. 
//  create new local work and setup the dummy ops 
//  We cannot merge (1.1) 
// found a conflict 
//  Turn on speculative execution for reducers 
// We can read more than we need if the actualCount is not multiple   of the byteBuffer size and file is big enough. In that case we cannot  use flip method but we need to set buffer limit manually to trans. 
//  Remove unused table scan operators 
//  compare hive version and metastore version 
//  rowId <= 'm' 
/*    * Pass in configs and pre-create a parse context    */
//  if we got exception during doing the unlock, rethrow it here 
//  both the classname and the protocol name are Table properties   the only hardwired assumption is that records are fixed on a   per Table basis 
//  COL_NAME 
//  Get the parameter values 
// SubQueryUtils.rewriteParentQueryWhere(clonedSearchCond, subQueryAST); 
//  now this shouldn't find the path on the fs 
//  we're replacing the current big table with a new one. Need   to count the current one as a map table then. 
/*          * Get our Single-Column String hash map information for this specialized class.          */
//  For the current context for generating File Sink Operator, it is either INSERT INTO or INSERT OVERWRITE. 
//  default is ClassLoaderContextSelector which is created during automatic logging   initialization in a static initialization block.   Changing ContextSelector at runtime requires creating new context factory which will   internally create new context selector based on system property. 
//  Poll on the operation status, till the operation is complete 
// second column exists 
//  Start an INSERT statement transaction and roll back this transaction. 
//  Set the table as transactional for compaction to work 
// Update min counter if new value is less than min seen so far 
//  in the absence of uncompressed/raw data size, total file size will be used for statistics   annotation. But the file may be compressed, encoded and serialized which may be lesser in size   than the actual uncompressed/raw data size. This factor will be multiplied to file size to estimate 
//  Restore the interrupted status, since we do not want to catch it. 
//  set all properties specified via command line 
//  Operand one|Operand another | or result   unknown | T | T 
//  Now output this timestamp's millis value to the equivalent toTz. 
//  To avoid having a huge BloomFilter we need to scale up False Positive Probability 
//  Setup deserializer/obj inspectors for the incoming data source 
//  estimate row count 
/*  we just made an existing table full acid which wasn't acid before and it passed all checks      initialize the Write ID sequence so that we can handle assigning ROW_IDs to 'original'      files already present in the table.  */
// catching exceptions here makes sure that the thread doesn't die in case of unexpected  exceptions 
//  check only the partition that exists, all should be well 
//  constant varchar projection 
//  3. SelectOperator should use only simple cast/column access 
// load same data again (additive) 
//  The pool is empty; queue the request. 
/*  100 files x 1000 size for 111 splits  */
//  2. Build nonPart/Part/Virtual column info for new RowSchema 
//  histogram used for quantile approximation   the quantiles requested 
//  because only 10 fractional digits, it's not this much accurate 
/*    * This test collector operator is for MapJoin row-mode.    */
//  Should this also just be ignored? Throw for now, doAs unlike queue is often set by admin. 
//  NOT_NULL_CONSTRAINT_COLS 
// If it's a FileSink to bucketed files, use the bucket count as the reducer number 
//     Collections.sort(kvs, CellComparator.COMPARATOR); 
//  Determine input vector expression using the VectorizationContext. 
//  Initialize the resources from command line 
//  is source CMD 
//  Fall through and look for other options... 
//  The union task is empty. The files created for all the inputs are   assembled in the union context and later used to initialize the union   plan 
//  see DefaultRuleDispatcher#dispatch() 
//  first project all group-by keys plus the transformed agg input 
//  If we've filled the buffer, write it out 
//  Unknown category 
//  Before Cleaner, there should be 6 items: 
// abort proactively so that we don't wait for timeout  perhaps we should add a version of RecordWriter.closeBatch(boolean abort) which 
//  restore data 
//  branch 
//  Fail if mis-configured. 
//  if not every partition uses bitvector for ndv, we just fall back to   the traditional extrapolation methods. 
//  Flip inclusion. 
//  Monday 1st July 1985 12:00:00 AM 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBinaryStream(int, java.io.InputStream)    */
//  blank " " (1 byte)   Smiling Face with Open Mouth and Smiling Eyes U+1F604 (4 bytes) 
//  database. 
// Asserting other partition parameters can also be changed, but not the location 
//  Message Bus related properties. 
//  This method is used to check whether the subType is a sub type of the groupType.   For nested attribute, we need to check its existence by the root path in a recursive way. 
//  (numSlotsAvailable can go negative, if the callback after the thread completes is delayed) 
//  This is expected 
//  Check the limit 
//  this is for ConditionalTask 
//  We get the information from Calcite. 
//  With a timeout of 3000. 
//  Per JDBC spec, if the driver does not support catalogs,   it will silently ignore this request. 
//  check entries beyond 2nd one 
//  TODO: assert iKey is HBaseSerDe#HBASE_KEY_COL 
//  need different record handler for MergeFileWork 
//  Set constraint name if null before sending to listener 
//  The common case by far. 
/*          * if the keyHash is missing in the bloom filter, then the value cannot exist in any of the         * spilled partition - return NOMATCH          */
//  right now are a few cheap redundant update calls; let's just do the simple thing. 
//  Update messages 
//  HiveConf hive.stats.ndv.error default produces 16 
//  a column family becomes a MAP 
//  Make sure Hadoop credentials objects do not reuse the maps. 
//  Setup the necessary metadata if originating from analyze rewrite 
//  Ensure Pig schema is correct. 
//  either variables will never be null because a default value is returned in case of absence 
//  Allocate writeId from test txn and then verify ValidWriteIdList.   Write Ids of committed and self test txn should be valid but writeId of open txn should be invalid. 
// see bucket_num_reducers.q bucket_num_reducers2.q   todo: try using set VerifyNumReducersHook.num.reducers=10; 
//  Not supported for the test case 
/*    * If this task contains a sortmergejoin, it can be converted to a map-join task if this operator   * is present in the mapper. For eg. if a sort-merge join operator is present followed by a   * regular join, it cannot be converted to a auto map-join.    */
//  link the work with the work associated with the reduce sink that triggered this rule 
//  First, handle the condition where the first fetch was never done (big table is empty). 
//  First try to extract a long value from the strings, and compare them. 
//  UNDONE: For now, all... 
//  user did NOT specify partition 
// For Windows OS, we need to pass HIVE_HADOOP_CLASSPATH Java parameter while starting 
//  test the same day of month 
//  class PartitionListComposingSpecProxy; 
//  skewed value 
//  Build row type from field <type, name> 
//  Simple truncation. 
//  iterate through all RS and locate the one introduce by enforce bucketing 
//  Fail - "transactional" is set to true, but the table is not bucketed 
//  Verify that we have got correct set of delete_deltas. 
//  v[5] is not calculated since high integer is always 0 for our decimals. 
//  It is possible to have a file with same checksum in cmPath but the content is   partially copied or corrupted. In this case, just overwrite the existing file with   new one. 
//  skip when already at EOF: 
/*  Test parent references from PreparedStatement  */
//  pos of driver alias 
//  We use a mapping object here so we can build the projection in any order and   get the ordered by 0 to n-1 output columns at the end.     Also, to avoid copying a big table key into the small table result area for inner joins,   we reference it with the projection so there can be duplicate output columns 
//  requires exact types on both sides of SetOp) 
//  Physical optimizers which follow this need to be careful not to invalidate the inferences   made by this optimizer. Only optimizers which depend on the results of this one should 
//  We can use the whole batch for output of no matches. 
/*    * Return length in characters of UTF8 string in byte array   * beginning at start that is len bytes long.    */
/*  *	Source code for the "strtod" library procedure. * * Copyright 1988-1992 Regents of the University of California * Permission to use, copy, modify, and distribute this * software and its documentation for any purpose and without * fee is hereby granted, provided that the above copyright * notice appear in all copies.  The University of California * makes no representations about the suitability of this * software for any purpose.  It is provided "as is" without * express or implied warranty.  */
//  For caching aggregate column stats for all and all minus default partition   Key is column name and the value is a list of 2 col stat objects 
//  figure out which kind of bloom filter we want for each column   picks bloom_filter_utf8 if its available, otherwise bloom_filter 
/*  @bgen(jjtree) UnflagArgs  */
//  All selected, do nothing 
//  Hive 0.12 behavior where double * decimal -> decimal is gone. 
//  No VectorUDFAdaptor usage. 
//  default 
//  add all children 
//  required   required   required   required   optional   optional   optional 
//  always just write mono 
//  check if all parents are finished 
/*  * This is the base class for the output parser. * Output parser will parse the output of a Pig/ * Hive/Hadoop or other job and extract jobid. * Note Hadoop jobid extract is rely on the API * Hadoop application submitting the job. Different * api will result in different console output. The * jobid extraction logic is not always working in * this case  */
//  Use UserPayloadProto.newBuilder() to construct. 
// gbInfo already has ExprNode for gbkeys 
//  testing druid queries row filtering is present 
//  default no-op implementation 
//  Just remove delete_delta, if there have been no delete events. 
//  TABLE-ONLY, fetch partitions if regular export, don't if metadata-only 
/*    * A PTF Function must provide the 'external' names of the columns in the transformed Raw Input.   *    */
//  MUTATE DATA 
//  nullIndicator is now at a different location in the output of 
//  Make sure we only return valDecompressor once. 
// 0 means there is no transaction, i.e. it a select statement which is not part of  explicit transaction or a IUD statement that is not writing to ACID table 
//  use tez to combine splits 
//  New plan is absolutely better than old plan. 
//  IS_ENABLE_AND_ACTIVATE 
//  No Op, return to the caller since long polling timeout has expired 
//  See the above. Release the headers after unlocking. 
//  Note: just like the move path, we only do one level of recursion. 
//  test repeating nulls case 
/*    * Allocate the source deserialization related arrays.    */
//  Nothing to do. 
//  RexLiteral's equal only consider value and type which isn't sufficient   so providing custom comparator which distinguishes b/w objects irrespective   of value/type 
//  This should never happen, if it does, it's a bug with the potential to produce   incorrect results. 
//  Get estimation parameters 
//  The IEEE 754 floating point spec specifies that signed -0.0 and 0.0 should be treated as equal. 
//  temporary single-batch context used for vectorization 
//  Consider a query like: select * from V, where the view V is defined as:   select * from T   The inputs will contain V and T (parent: V)   T will be marked as an indirect entity using isDirect flag.   This will help in distinguishing from the case where T is a direct dependency 
//  violation in ETL queue 
//  Table created in hive catalog should have been automatically set to transactional 
//     operators 
//  1. Kill queries. 
//  Allocate output column and get column number; 
//  partSpecToFileMapping is null if big table is partitioned 
//  Clear completed instances in this case. Don't want to provide information from the previous run. 
//  Determine the transaction manager to use from the configuration. 
//  Drop the tables when we're done.  This makes the test work inside an IDE 
//  History File stream 
//  the version that was included with the original magic, which is mapped 
/*  Datatype of column  */
//  prove invariant to the compiler: len1 = len2   all array access between (start1, start1+len1)    and (start2, start2+len2) are valid   no more OOB exceptions are possible 
/*    * RowResolver helper methods    */
//  Execute generated plan. 
//  it may be a field name, return the identifier and let the caller decide   whether it is or not 
//  set of joins already processed 
//  Establish context 
// no such partition 
//  Check the stripped property is the empty string 
//  if all the ColumnStatisticsObjs contain bitvectors, we do not need to   use uniform distribution assumption because we can merge bitvectors   to get a good estimation. 
//  Create a data container 
//  should not happen 
//  Test that exclusive blocks exclusive and write 
//  when we run a task after the jar has been added. 
//  Whether the IF statement boolean expression was repeating. 
//  split the children into vertices that make up the union and vertices that are 
//  figure out correlation and presence of non-equi join predicate 
//  appends might exist after the root message, so strip tokens off until we 
//  array of strings type, or an array of arrays of strings. 
//  If this constant was created while doing constant folding, foldedFromCol holds the name of   original column from which it was folded. 
// Override Hive specific operators 
//  Not the right host. 
//  We only need a username for UGI to use for groups; getGroups will fetch the groups   based on Hadoop configuration, as documented at 
// make sure not to loose 
//  when bias correction is enabled 
//  Release the unreleased stripe-level buffers. See class comment about refcounts. 
//  1 is excluded via property, 3 already has stats, so we only expect two updates. 
//  This run of Initiator doesn't add any compaction_queue entry 
//  1) We extract the collations indices 
//  Test that existing shared_read table with new shared_read coalesces to 
//  run as a batch 
//  Woe us. 
//  Verify dropTable recycle partition files 
//  avoid instantiation 
//  Return just the single Range 
//  only green qualifies, and it's in entry 1 
//  Add cast expression if needed. Child expressions of a udf may return different data types   and that would require converting their data types to evaluate the udf.   For example decimal column added to an integer column would require integer column to be   cast to decimal. 
//  NOTE: partColsIsNull is only used for PTF, which isn't supported yet. 
//  Possible reasons to end up here:   - Unable to read version from manifest.mf   - Version string is not in the proper X.x.xxx format 
//  Complex type constants currently not supported by VectorUDFArgDesc.prepareConstant. 
//  Handle normal case 
//  Current match may be out of order w.r.t. the global name list, so add specific parts. 
//  COL_NAMESPACE 
//  no need to override assigns - all assign ops will fail due to 0 size 
//  -D 
//  for each row as it is produced 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createClob()    */
//  Insert junk in middle of file. Assumes file is on local disk. 
//  rename partitioned table 
//  All values are filtered out 
// Final check, find size of already-calculated Mapjoin Operators in same work (spark-stage). 
// Current state of each selected column - e.g. current run length, etc. 
//  Make sure the proper transaction manager that supports ACID is being used 
//  find the same key 
//  Invalid app name. Checked later. 
//  We cannot apply the reduction 
//  Accept int writables and convert them. 
//  row 2 - results should be null 
//  Allow date to string casts.  NOTE: I suspect this is the reverse of what we actually   want, but it matches the code in o.a.h.h.serde2.typeinfo.TypeInfoUtils.  I can't see how   users would be altering date columns into string columns.  The other I easily see since   Hive did not originally support datetime types.  Also, the comment in the Hive code   says string to date, even though the code does the opposite.  But for now I'm keeping 
//  number of aborted txns found in exceptions 
//  Launch Task 
//  Test a single, high-precision multiply of random inputs. 
//  each branch 
/*  We just checked the user specified schema columns among regular table column and found some which are not            'regular'.  Now check is they are dynamic partition columns              For dynamic partitioning,              Given "create table multipart(a int, b int) partitioned by (c int, d int);"              for "insert into multipart partition(c='1',d)(d,a) values(2,3);" we expect parse tree to look like this               (TOK_INSERT_INTO                (TOK_TAB                  (TOK_TABNAME multipart)                  (TOK_PARTSPEC                    (TOK_PARTVAL c '1')                    (TOK_PARTVAL d)                  )                )                (TOK_TABCOLNAME d a)               ) */
// We have found a valid cookie in the client request. 
//  -h 
//  Do any of the other fields need to be set? 
//  Open 5 txns 
//  If the inputOI is the same as the outputOI, just return an   IdentityConverter. 
//  "(TS%FIL%)|(TS%FIL%FIL%)" 
//  Pretend like it has fractional digits so we can get the trailing zero count. 
//  The output path used by the subclasses.   Used as a final destination; same as outPath for MM tables. 
//  if TopNHashes are active, proceed if not already excluded (i.e order by limit) 
//  No need to check again if it exists. 
//  Normally, on import, trying to create a table or a partition in a db that does not yet exist   is a error condition. However, in the case of a REPL LOAD, it is possible that we are trying   to create tasks to create a table inside a db that as-of-now does not exist, but there is   a precursor Task waiting that will create it before this is encountered. Thus, we instantiate   defaults and do not error out in that case. 
//  D2 already done - iulRindex initialized before normalization of R. 
//  Empty list 
//  this indicates original query was either correlated EXISTS or IN 
//  -g 
//  In future, this may examine config to return appropriate HCatReader 
//  specific files if they exist. 
/*   This class authenticates HS2 web UI via PAM. To authenticate use   * httpGet with header name "Authorization"   * and header value "Basic authB64Code"    where  authB64Code is Base64 string for "login:password"  */
//  Indices 
//  Convert the work in the SMB plan to a regular join   Note that the operator tree is not fixed, only the path/alias mappings in the   plan are fixed. The operator tree will still contain the SMBJoinOperator 
// create a file we'll import later 
//  local mode   command like dfs 
//  Have to rely on Hive implementation of filesystem permission checks. 
//  1. Generate RS operator   1.1 Prune the tableNames, only count the tableNames that are not empty strings   as empty string in table aliases is only allowed for virtual columns. 
//  Not using ByRef now since it's unsafe for text readers. Might be safe for others. 
//  For compare, we will convert requisite children   For BETWEEN skip the first child (the revert boolean) 
//  CREATE TABLE ... AS 
//  handle repeating null 
//  Indicate we used the last row's bytes for large buffer. 
//  MSCK called to drop stale paritions from metastore and there are   stale partitions. 
//  Add signature 
//  key = (key + (key << 3)) + (key << 11); 
//  Turn off passwords, enable sasl and set a keytab 
/*    * Outer join (hash map).    */
//  clear the previous values recorded. 
//  Classify partitions within the table directory into groups,   based on shared SD properties. 
//  Initialize the metrics system 
//  to use MiniMRCluster. MAPREDUCE-2350 
//  handle repeating case 
//  If (numReducers > 0 && newNumReducers > 0 && newNumReducers != numReducers),   we will not consider ReduceSinkOperator with this newNumReducer as a correlated   ReduceSinkOperator 
//  Perform a major compaction 
/*    * Verify table for Key: byte[] x Hash Table: HashMap    */
/*        * Now new job requests should succeed as list operation has no cancel threads.        */
//  hive added files and jars 
//  next node in consistent order died or does not have free slots, rollover to next 
//  NOTE: power can be positive or negative.   NOTE: e.g. power = 2 is effectively multiply by 10^2 
//  add the privs for roles in curRoles to new role-to-priv map 
//  DEFAULT_PARTITION_NAME 
//  Get our vector map join fast hash table variation from the   vector map join table container. 
//  LOG.info("VectorMapJoinFastTableContainer load keyCountAdj " + keyCountAdj);   LOG.info("VectorMapJoinFastTableContainer load threshold " + threshold);   LOG.info("VectorMapJoinFastTableContainer load loadFactor " + loadFactor);   LOG.info("VectorMapJoinFastTableContainer load wbSize " + wbSize); 
//  Insert 5 rows to both tables 
//  retrieve token and store in the cache 
//  The ANTLR grammar looks like :   1.  KW_CONSTRAINT idfr=identifier KW_FOREIGN KW_KEY fkCols=columnParenthesesList   KW_REFERENCES tabName=tableName parCols=columnParenthesesList   enableSpec=enableSpecification validateSpec=validateSpecification relySpec=relySpecification   -> ^(TOK_FOREIGN_KEY $idfr $fkCols $tabName $parCols $relySpec $enableSpec $validateSpec)   when the user specifies the constraint name (i.e. child.getChildCount() == 7)   2.  KW_FOREIGN KW_KEY fkCols=columnParenthesesList   KW_REFERENCES tabName=tableName parCols=columnParenthesesList   enableSpec=enableSpecification validateSpec=validateSpecification relySpec=relySpecification   -> ^(TOK_FOREIGN_KEY $fkCols  $tabName $parCols $relySpec $enableSpec $validateSpec)   when the user does not specify the constraint name (i.e. child.getChildCount() == 6) 
//  Order keys 
//  replace bucketing columns with hashcode % numBuckets 
//  Create IN clauses 
/*      * This test verifies that the ALTER TABLE ... SET OWNER command will change the     * owner metadata of the table in HMS.      */
//  5. Release the copies we made directly to the cleaner. 
/*      * Convert our source object we just read into the target object and store that in the     * VectorizedRowBatch.      */
// prefixing with '_mask_' to ensure no conflict with named  columns in the file schema 
//  LOG.info("returning "+t); 
//  simple join from 2 relations: denom = max(v1, v2) 
//  look for any valid versions.  This will also throw NoSuchObjectException if the schema   itself doesn't exist, which is what we want. 
//  Hive will re-use the Configuration object that it passes in to be   initialized.  Therefore we need to make sure we don't look for any 
//  in this TestFilter. 
//  Protected for testing and so we can pass in a conf for testing. 
//  grantor 
//  get the list of Dynamic partition paths 
//  100 is the start 
/*  * - starting from the given rowIdx scan in the given direction until a row's expr * evaluates to an amt that crosses the 'amt' threshold specified in the BoundaryDef.  */
//  Try to read the dropped partition via CachedStore 
//  string IN 
//  1)  to partitioned table 
//  nothing to add 
//  Task has completed 
//  src.position(start) can't accept negative numbers. 
//  need three params to differentiate between this and 2 param method auto   generated since   some calls in the autogenerated code use null param for 2nd param and thus 
//  Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways   to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough. 
//  Create Key/Value TableDesc. When the operator plan is split into MR tasks,   the reduce operator will initialize Extract operator with information 
//  If we've opened a transaction we need to commit or rollback rather than explicitly 
/*      * CHAR: strategic blanks, string length beyond max      */
//  c14:map<int,map<int,int>> 
//  fallback to regular logic 
//  extract all the columns 
//  We know they are not equal because the one with the larger scale has non-zero digits   below the other's scale (since the scale does not include trailing zeroes). 
//  add supported protocols 
//  TODO : verify if any escaping is needed for values 
//  Use method addDelegationTokens instead of getDelegationToken to get all the tokens including KMS. 
//  the biggest output from a parent 
//  NOTE: Hive by convention doesn't pushdown non deterministic expressions 
//  local plan is not null, we want to merge it into SMBMapJoinOperator's local work 
//  there are no elements in the list 
//  Get partial aggregation results and store in reduceValues 
//  2048 comes from tblproperties   Compact ttp1 
//  Create the lineage context 
//  If source path is a subdirectory of the destination path (or the other way around):     ex: INSERT OVERWRITE DIRECTORY 'target/warehouse/dest4.out' SELECT src.value WHERE src.key >= 300;     where the staging directory is a subdirectory of the destination directory   (1) Do not delete the dest dir before doing the move operation.   (2) It is assumed that subdir and dir are in same encryption zone.   (3) Move individual files from scr dir to dest dir. 
//  find the move task 
//  Serialize the hcatPartitionSpec. 
/*                * Multi-Key specific lookup key.                */
//  Make sure buffers are eligible for discard. 
//  Clear rounding portion in middle longword and add 1 at right scale (roundMultiplyFactor);   lower longword result is 0; 
//  The other overload should have been used. 
// Factory method for serDe 
/*    * fastSerializationUtilsRead middle word is 63 bits. So, we need a multiplier 2^63    *   *    2^63 =   *      9223372036854775808 (Long.MAX_VALUE) or   *      9,223,372,036,854,775,808 or   *      922,3372036854775808 (16 digit comma'd)    */
//  If the table alias to information map already contains the current table, 
//  If we reached here, we have match for HS2 generated cookie 
//  -deregister <versionNumber> 
//  are running using the same umbilical. 
//  Set up the security for plugin endpoint.   We will create the token and publish it in the AM registry.   Note: this application ID is bogus and is only needed for JobTokenSecretManager. 
// 2,3,4 
//  This is the start of container-annotated logging. 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.newBuilder() 
//  The ANTLR grammar looks like :   1. KW_CONSTRAINT idfr=identifier KW_PRIMARY KW_KEY pkCols=columnParenthesesList    constraintOptsCreate?   -> ^(TOK_PRIMARY_KEY $pkCols $idfr constraintOptsCreate?)   when the user specifies the constraint name.   2.  KW_PRIMARY KW_KEY columnParenthesesList   constraintOptsCreate?   -> ^(TOK_PRIMARY_KEY columnParenthesesList constraintOptsCreate?)   when the user does not specify the constraint name. 
//  Tests for Partition appendPartition(String tableName, String dbName, String name) method 
//  currently all Tez work is on the cluster 
//  Verify no preemption requests - since everything is at the same priority 
//  -ve hashcodes had conversion to positive done in different ways in the past   abs() is now obsolete and all inserts now use & Integer.MAX_VALUE    the compat mode assumes that old data could've been loaded using the other conversion 
//  init aggregationParameterFields 
//  -p 
//  While processing bucket columns, atleast one bucket column   missed. This results in a different bucketing scheme.   Add empty list 
//  Add sorting/bucketing if needed 
//  long_size / tableRowSize + long_size 
//  Skip class loading if the class name didn't change 
//  It can be fully qualified name or use default database 
//  the outputOps in this vertex. 
//  for bucketed tables, hive.optimize.sort.dynamic.partition optimization 
//  Do not display vectorization objects. 
//  8.090000000000000000000000000000000000000123456 
//  Abort all the allocated txns so that the mapped write ids are referred as aborted ones. 
//  Bogus warnings despite closeQuietly. 
//  Fall-back for an unexpected RelNode type 
//  this means that nothing was set for default stripe size previously, so we should unset it. 
//  Start the cache threads.   Cache also serves as buffer manager. 
//  The data must be of type String 
//  this means the process will exit without waiting for this thread 
//  this shouldn't throw an exception 
//  The serialized (non-NULL) series keys.  These 3 members represent the value. 
//  streaming ingest writer with single transaction batch size, in which case the transaction is   either committed or aborted. In either cases we don't need flush length file but we need to   flush intermediate footer to reduce memory pressure. Also with HIVE-19206, streaming writer does   automatic memory management which would require flush of open files without actually closing it. 
// so now if HIVEFETCHTASKCONVERSION were to use a stale value, it would use a 
// r = runStatementOnDriver("explain  " + query);  logResuts(r, "Explain logical1", ""); 
//  LOG.debug(CLASS_NAME + " repeated key " + key); 
//  For Non-MM tables, directory structure is   <table-dir>/<staging-dir>/<partition-dir> 
//  Test 256 or greater is n % 256 
//  HS2 init without the knowledge of LLAP usage (or lack thereof) in the cluster. 
//  handles cases where the query has a predicate "column-name=constant" 
//  if we have a singleton AND or OR, just return the child 
//  There is bitvector, but it is not adjacent to the previous ones. 
//  Log at warn, given how unfortunate this is. 
// load 1st row 
//  handle string types properly 
//  http://wiki.apache.org/pig/PigErrorHandlingFunctionalSpecification#Error_codes 
/*    * Gets list of job ids and calls getJobStatus to get status for each job id.    */
//  -r 
//  override MR properties from tblproperties if applicable 
// before reparsing, i.e. they are known to SemanticAnalyzer logic 
//  Both inputs to the join are unique. There is nothing to be gained by   this rule. In fact, this aggregate+join may be the result of a previous   invocation of this rule; if we continue we might loop forever. 
//  make a copy 
// //////////////////////////////////////////// 
//  number of elements   sum of elements   sum[x-avg^2] (this is actually n times the variance) 
//  returns true if the specified path matches the prefix stored 
//  set 1st entry to null 
/*    * Substitute for any carriage return or line feed characters in line with the escaped   * 2-character sequences \r or \n.   *   * @param line  the string for the CRLF substitution.   * @return If there were no replacements, then just return line.  Otherwise, a new String with   *         escaped CRLF.    */
//  This stream can be separated by RG using index. Let's do that. 
//  Since rowCount is used later to instantiate a BytesBytesMultiHashMap 
/*      * Connect via kerberos with trusted proxy user      */
//  Add test parameters from storage formats specified in ADDITIONAL_STORAGE_FORMATS table. 
//  src is scratch directory, need to trim the part key value pairs from path 
//  configure the broker 
//  Connection.getMetaData().getTableTypes() when type config is set to "HIVE" 
//  Testing multiByte string substring 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setObject(java.lang.String,   * java.lang.Object, int, int)    */
//  Drop the materialized view but not its data 
//  Should also not be able to add fields of different types with same name 
//  Especially since LLAP is prone to turn it off in the MapJoinDesc in later 
//  Async op in progress; the callback will take care of this. 
//  get next batch of table names in this list 
/*            * Failed to set job status as COMPLETED which mean the main thread would have           * exited and not waiting for the result. Kill the submitted job.            */
//  TaskWrapper is used in structures, as well as for ordering using Comparators 
/*    * It represents a WindowFrame applied to a Partitioning. A Window can   * refer to a <i>source</i> Window by name. The source Window provides the   * basis for this Window definition. This Window specification   * extends/overrides the <i>source</i> Window definition. In our e.g. the   * Select Expression $sum(p_retailprice) over (w1)$ is translated into a   * WindowFunction instance that has a Window specification that refers   * to the global Window Specification 'w1'. The Function's specification   * has no content, but inherits all its attributes from 'w1' during   * subsequent phases of translation.    */
//  When we are doing vector deserialization, these are the fast deserializer and   the vector row deserializer. 
//  ObjectStore also stores db name in lowercase 
//  loglevel & args are parsed by the python processor 
/*      * If the user asking the token is same as the 'owner' then don't do     * any proxy authorization checks. For cases like oozie, where it gets     * a delegation token for another user, we need to make sure oozie is     * authorized to get a delegation token.      */
//  Generate the list bucketing pruning predicate 
// Set transitive to true by default 
//  Just forward the row as is 
//  we haven't added anything so should return an all ok 
//  </Classification> 
//  Now, apply SARG if any; w/o sarg, this will just initialize stripeRgs. 
/*      * add columns from inpRR      */
//  go through all map joins and find out all which have enabled bucket map 
//  clone the original join operator, and replace it with the MJ 
//  restore  input and output streams 
//  Not updated yet. 
//  Connect Jersey 
//  Note that -1000000000 is a valid value corresponding to a nanosecond timestamp   of 999999999, because if the second VInt is present, we use the value   (-reversedNanoseconds - 1) as the second VInt. 
//  Operator is a file sink or reduce sink. Something that forces a new vertex. 
//  We want the driver to try to print the header... 
//  LO     LEFT\RIGHT   skip  filtered   valid   skip        --(1)     --(1)    --(1)   filtered    +-(1)     +-(1)    +-(1)   valid       +-(1)     +-(4*)   ++(2)     * If right alias has any pair for left alias, continue (3)   -1 for continue : has pair but not in this turn    0 for inner join (++) : join and continue LO 
//  It's all good: create a new entry in the map (or update existing one) 
//  In case the job context is not up yet, let's wait, since this is supposed to be a   "synchronous" RPC. 
/*    * Scratch objects.    */
//  We got using() clause in previous join. Need to generate select list as   per standard. For * we will have joining columns first non-repeated   followed by other columns. 
//  Has this filesink already been processed 
//  This is constant for whole series. 
//  first to make sure we go through a complete iteration of the loop before resetting it. 
/* when running in unit test mode, pass this property to HCat,      which will in turn pass it to Hive to make sure that Hive      tries to write to a directory that exists. */
//  A new table is always created with a new column descriptor 
//  year   month   day 
//  != 0 
//  set the bit to 1 if an element is not null 
//  Check if the file format of the file matches that of the table. 
//  we use this map to map the position of argList to the position of grouping set 
//  we're interested in specific partitions,   don't check for any others 
//  Number of rows not matching the regex 
//  Attempting to fix a valid - should not result in a new file. 
//  Loop through each of the lists of exprs, looking for a match. 
//  Check if we should turn into streaming mode 
//  Test negative integers result in "" 
// Attempt extended Acl operations only if its enabled, 8791but don't fail the operation regardless. 
//  Read each expression and save it to the value registry 
//  @@protoc_insertion_point(builder_scope:SourceStateUpdatedResponseProto) 
//  skewed value has the same length. 
//  The value read in last time 
//  stop once we see a dynamic partition 
// Test when the session is opened by a user. (HiveSessionImplwithUGI) 
//  then assume it is from its own vertex 
//  Add rounding. 
//  schema. 
//  currently testProccedures always returns an empty resultset for Hive 
//  Possible concurrent modification issues if we try to remove cache entries while   traversing the cache structures. Save the entries to remove in a separate list. 
//  otherwise fall back to return null, i.e. to use local tmp dir only 
//  Make sure nothing escapes this run method and kills the metastore at large,   so wrap it in a big catch Throwable statement. 
/*    * These members have information for extracting a row column objects from VectorizedRowBatch   * columns.    */
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createArrayOf(java.lang.String,   * java.lang.Object[])    */
//  compute statistics for columns viewtime 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setEscapeProcessing(boolean)    */
//  Generate test data 
//  Take away the only session, as if it was expiring. 
//  StandardStruct uses ArrayList to store the row. 
//  no-arg ctor required for JSON deserialization 
//  Create rules registry to not trigger a rule more than once 
//  it wasn't create db/table 
//  User provided a fully specified partition spec but it doesn't exist, fail. 
//  If a join occurs before the sort-merge join, it is not useful to convert the the sort-merge   join to a mapjoin. It might be simpler to perform the join and then a sort-merge join   join. By converting the sort-merge join to a map-join, the job will be executed in 2   mapjoins in the best case. The number of inputs for the join is more than 1 so it would   be difficult to figure out the big table for the mapjoin. 
//  construct output object inspector 
//  Left input positions are not changed. 
//  -------------------------------- First Pass ---------------------------------- //   Identify SparkPartitionPruningSinkOperators, and break OP tree if necessary 
//  HCat will allow these operations to be performed. 
//  1. Insert two rows to a partitioned MM table. 
//  Identical strings 
//  In this case, we are assuming that there is a single distinct function. 
//  5 : Create and drop partition P2 to T2 10 times => 20 events 
//  invalid character present, return null 
//  Directory 
//  Location will vary by system. 
//  It is of the form topSubQuery:innerSubQuery:....:innerMostSubQuery 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.newBuilder() 
//  no integer part 
// only have 1 file so done 
//  if writable constant is null then return size 0 
/*  its a map  */
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareStatement(java.lang.String, int)    */
//  Same priority for all tasks. 
//  Create table on Target. 
//  make left a child of right 
//  4.2) Modifying filter condition. The incremental rewriting rule generated an OR   clause where first disjunct contains the condition for the UPDATE branch.   TOK_WHERE     or        and <- DISJUNCT FOR <UPDATE>           =              .                 TOK_TABLE_OR_COL                    $hdt$_0                 a              .                 TOK_TABLE_OR_COL                    $hdt$_1                 a           =              .                 TOK_TABLE_OR_COL                    $hdt$_0                 c              .                 TOK_TABLE_OR_COL                    $hdt$_1                 c        and <- DISJUNCT FOR <INSERT>           TOK_FUNCTION              isnull              .                 TOK_TABLE_OR_COL                    $hdt$_0                 a           TOK_FUNCTION              isnull              .                 TOK_TABLE_OR_COL                    $hdt$_0 
//  exceptions in the range 
//  We assume the VARCHAR maximum length was enforced when the object was created. 
//  Stub out the serializer 
//  test repeating logic 
//  [SUMMARY|OPERATOR|EXPRESSION|DETAIL] 
//  Return immediately. No entries found for pruning. Verified via the timeout. 
// AND b < 1 
/*      * allow multiple mappings to the same ColumnInfo.     * When a ColumnInfo is mapped multiple times, only the     * first inverse mapping is captured.      */
//  Steps:   1. Extract the archive in a temporary folder   2. Move the archive dir to an intermediate dir that is in at the same      dir as originalLocation. Call the new dir intermediate-extracted.   3. Rename the original partitions dir to an intermediate dir. Call the      renamed dir intermediate-archive   4. Rename intermediate-extracted to the original partitions dir   5. Change the metadata   6. Delete the archived partitions files in intermediate-archive 
/*    * In skip mode msck should ignore invalid partitions instead of   * throwing exception    */
//     assertTrue(exc instanceof HCatException);      assertEquals(ErrorType.ERROR_PUBLISHING_PARTITION, ((HCatException) exc).getErrorType());   With Dynamic partitioning, this isn't an error that the keyValues specified didn't values 
//  class FSPaths 
//  Unnecessary: data.putInt(bOffset, -1); data.putInt(bOffset + 4, -1); 
// for other tasks, just return its children tasks 
//  LASTHEARTBEAT 
//  It's ok to update metrics for two tasks in parallel, but not for the same one. 
//  TODO CAT - for now always use the default catalog.  Eventually will want to see if   the user specified a catalog 
//  If the returned value is HiveDecimal, we assume maximum precision/scale. 
//  class ConnectionImpl 
//  Now do an in-place reversal in result.getBytes(). First, reverse every 
//  BINARY_STATS 
//  Set DDL time to now if not specified 
//  All tables or partitions are bucketed, and their bucket number is   stored in 'bucketNumbers', we need to check if the number of buckets in 
//  no scheme - use default file system uri 
//  These aren't column types, they are info for how things are stored in thrift. 
//  If any of the type isn't exact, double is chosen. 
//  HCat wants to intercept following tokens and special-handle them. 
//  Ok, use result with some or all fractional digits stripped. 
//  for auth 
//  Converting to/from external table 
// TezEdgeProperty.EdgeType 
//  Do nothing - this should generate proper index. 
//  we need to specify the reserved memory for each work that contains Map Join 
//  Fixup parent and child relations. 
//  look them up to make sure they are all there 
// statementId is from directory name (or 0 if there is none) 
//  Using biggest small table, calculate number of partitions to create for each small table 
//  need to use a new filesystem object here to have the correct ugi 
//  position of last sync   16 random bytes 
//  set up the staging directory to use 
//  GROUP_PRIVILEGES 
//  no cache 
// https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#Running_as_the_superuser 
//  2.4 Determine the Direction of order by 
//  Different columns means different commands have to be run. 
//  Verify that the partitions specified are continuous   If a subpartition value is specified without specifying a partition's value 
//  Merge them together. 
/*    * A conditional node is constructed if its condition is true. All the nodes   * that have been pushed since the node was opened are made children of the   * conditional node, which is then pushed on to the stack. If the condition is   * false the node is not constructed and they are left on the stack.    */
//  The following two keys should ideally be used to control RM connect timeouts. However, 
//  after constant folding of child expression the return type of UDFWhen might have changed,   so recreate the expression 
//  URIs are checked for string equivalence, even spaces make them different 
// check if orc and not sorted 
//  Only print the first line of the stack trace as it contains the error message, and other   lines may contain line numbers which are volatile   Also only take the string after the first two spaces, because the prefix is a date and   and time stamp 
//  Release the HMS connection for this service thread 
//  add more variables as required 
//  since count has a return type of BIG INT we need to make a literal of type big int   relbuilder's literal doesn't allow this 
//  This is a table or dynamic partition 
//  These functions only work on the STRING type. 
//  Schedule CMClearer thread. Will be invoked by metastore 
//  call-1: listLocatedStatus - mock:/mocktbl   call-2: check existence of side file for mock:/mocktbl/0_0   call-3: open - mock:/mocktbl/0_0   call-4: check existence of side file for mock:/mocktbl/0_1 
//  There are some txns in the list which has no write id allocated and hence go ahead and do it.   Get the next write id for the given table and update it with new next write id. 
//  if scheme is specified but not authority then use the default authority 
//  Stores result in cache 
//  Set output names in ReduceSink 
//  llap cache purge requires admin privilege as it mutates state (cache) on the cluster 
//  by default we put row into partition in memory 
//  There should be 3 delta directories. The new one is the aborted one. 
/*    * This method helps to re-use a session in case there has been no change in   * the configuration of a session. This will happen only in the case of non-hive-server2   * sessions for e.g. when a CLI session is started. The CLI session could re-use the   * same tez session eliminating the latencies of new AM and containers.    */
//  The caller has already stopped the session. 
//  We might want to setXAttr for the new location in the future 
//  first argument is the column to be transformed 
//  Using property defined in HiveConf.ConfVars to test System property   overriding 
//  Indicates the initial capacity of the cache. 
//  The separator for the hive row would be using \x02, so the separator for this struct would be   \x02 + 1 = \x03 
//  GRANTOR 
//  Based on the plan outputs, find out the target table name and column names. 
//  Step 1: Check if mapJoinTask has a single child. 
//  these are closure-bound for all the walkers in context 
//  Should only down-cast if within valid range. 
//  compile time. 
//  DELEGATION_TOKEN 
//  List was recreated while we were exhausting it. 
//  CLASS_NAME 
// kerberos connections to HMS if required. 
//  If we had a cache range already, we expect a single matching disk slice.   Given that there's cached data we expect there to be some disk data. 
/*        * Now, tasks would have passed. Verify that new job requests should succeed with no issues.        */
//  Close and release resources within a running query process. Since it runs under   driver state COMPILING, EXECUTING or INTERRUPT, it would not have race condition 
//  get the SEL(*) branch 
//  get the context info and set up the shared tmp URI 
//  LOG.debug(CLASS_NAME + " currentKey " +        VectorizedBatchUtil.displayBytes(currentKeyOutput.getData(), 0, currentKeyOutput.getLength())); 
//  Leave this one for the next round. 
//  writeId for data from non-acid table and so writeIdHwm=0 would ensure those data are readable by any txns. 
//  Case 3: column stats, hash aggregation, NO grouping sets 
//  To check whether we have enough memory to allocate for another hash partition,   we need to get the size of the first hash partition to get an idea. 
//  We just act as a pass-thru between the session and allocation manager. We don't change the   allocation target (only WM thread can do that); therefore we can do this directly and   actualState-based sync will take care of multiple potential message senders. 
//  From this point on, the update is in motion - if someone changes the state again, that 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#registerOutParameter(java.lang.String, int)    */
// cleaner is a static object, use static synchronized to make sure its thread-safe 
//  zero out the bits above bitsToWrite 
//  Setting key to same value, should not trigger configChange event during shutdown 
/*      * Bloom filter merge input and output are BYTES.     *     * Just modes (PARTIAL2, FINAL).      */
//  Don't worry about this, as it likely just means it's already been created. 
// Remove subquery 
//  CREATE_TABLE - INSERT - TRUNCATE - INSERT - The result is just one record. 
//  2.1 This is the new number of rows after PK is joining with FK 
//  switch the database 
//  Not based on ARP and cannot assume uniform distribution, bail. 
//  restore index 
//  The Hive type of this column 
// for non-prefix maps 
//  otherwise all null afterwards 
/*    * For primitive types, use LazyBinary's object.   * For complex types, make a standard (Java) object from LazyBinary's object.    */
/*  1 files x 1000 size for 9 splits  */
//  restart sensitive instance id 
//  Other nodes may be trying to delete this at the same time,   so just log errors and skip them. 
//  override this with a no-op if subclass doesn't need to treat NaN as null 
//  if any expression of child is referencing parent column which is result of function 
/*  * Abstract class for a hash map result.  For reading the values, one-by-one.  */
//  pRS-cRS 
//  to cleanup the entries less than min_uncommitted_txnid from the TXN_TO_WRITE_ID table. 
//  Create table associated with the import 
//  This way, the only way the recursive stack of fetchNextBatch returns is if:      a) We got a nonempty result, and we can consume      b) We reached the end of the queue, and there are no more events.   So, when we return from the fetchNextBatch() stack, if we have no more   results in batch, we're done. 
//  E.g. "hive-metastore/_HOST@EXAMPLE.COM". 
// this will cause next txn to be marked aborted but the data is still written to disk 
//  Got a match, return the value 
//  getDelay <=0 means the task will be evicted from the queue. 
//  Events can start coming in the moment the InputInitializer is created. The pruner   must be setup and initialized here so that it sets up it's structures to start accepting events.   Setting it up in initialize leads to a window where events may come in before the pruner is 
//  write sql statements to file 
//  Even if the service hasn't started up. It's OK to make this invocation since this will   only happen after the AtomicReference address has been populated. Not adding an additional check. 
// For now because subquery is only supported in filter 
//  put a empty list or null 
//  scope close: 
//  evaluate will return a Text object 
//  Drain the buffer 
//  row count using the classic formula. 
//  We can update all partitions with a single analyze query. 
//  End RelShuttleImpl.java 
//  Don't restrict child expressions for projection.   Always use loose FILTER mode. 
/*    * If the parent reduce sink of the big table side has the same emit key cols as its parent, we   * can create a bucket map join eliminating the reduce sink.    */
/*    * Job request execution time out in seconds. If it is 0 then request   * will not be timed out.    */
//  Alter the db via CachedStore (can only alter owner or parameters) 
// Test for merging of configs 
//  Start by getting the work part of the task and call the output plan for   the work 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getAutoCommit()    */
//  null values and values of zero length are not added to the cachedMap 
//  the rest ops are FKs. 
//  The values from HiveIntervalDayTime.getTotalSeconds(). 
//  along the way for the columns that the group by uses as keys 
//  Testing substring starting from index 1 
//  The MathExpr class contains helper functions for cases when existing library 
//  Initialize a result 
//  Copy of partitions that will be split into batches 
/*  Get the mode of the lock encoded in the path  */
//  Different result after clearing. 
//  Open a new connection with these conf & vars 
// LockRequestBuilder dedups locks on the same entity to only keep the highest level lock requested 
//  Compute the fixed size overhead for the keys 
//  Precondition: make sure this is done after the rest of the SerDe initialization is done. 
//  get arguments 
/*        * recreate cluster, so that it picks up the additional traitDef        */
//  5/ update the byte size of the map 
//  If any partition is updated, then update repl state in partition object 
//  Write the value 
//  and return false. 
//  bucketized keys (note that the order need not be the same). 
//  -f 
//     deserializedBigInteger.equals(deserializedBigIntegerExpected)); 
//  Decrease, then increase qp - sessions should not be killed on return. 
//  Here the global state is confined to just this process. 
//  -e 
//  2nd level GB: create a GB (all keys + sum(c) as a + sum(VCol*c) as b) for 
//  Preserve the original view definition as specified by the user. 
//  using binary search. 
//  End RelShuttle.java 
//  if property specified file not found in local file system   use default setting 
//  Reusable output for serialization 
//  Get all user jars from work (e.g. input format stuff). 
//  Failure handling of IMPORT command and REPL LOAD commands are different.   IMPORT will set the last repl ID before copying data files and hence need to allow   replacement if loaded from same dump twice after failing to copy in previous attempt.   But, REPL LOAD will set the last repl ID only after the successful copy of data files and 
/*  nothing  */
//  all other tables are small, and are cached in the hash table 
//  get all the tasks nodes from root task 
//  TXNID 
//  Handle repeating case 
//  production is: set<FieldType()> 
//  Try to eat a dot now since it could be the end.  We remember if we saw a dot so we can 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getDate(java.lang.String)    */
//  If the login context name is not set, we are in the client and don't need auth. 
//  When stopping the process we are redirecting from,   the streams might be closed during reading.   We should not log the related exceptions in a visible level   as they might mislead the user. 
/*  columnar splits of unknown size - estimate worst-case  */
//  We remove it from the TS too if it was pushed 
//  Try to infer the type of the constant only if there are two 
//  Disable memory estimation for this test class 
//  @@protoc_insertion_point(builder_scope:UpdateFragmentRequestProto) 
//  Create split for the previous unfinished stripe. 
//  Test that existing shared_read table with new exclusive coalesces to 
//  use LinkedHashMap to make sure the iteration order is 
// ************************************************************************************************   Decimal Division / Remainder. 
/*    * ============================== HOW TO RUN THIS TEST: ====================================   *   * You can run this test:   *   * a) Via the command line:   *    $ mvn clean install   *    $ java -jar target/benchmarks.jar VectorGroupByOperatorCountBench -prof perf     -f 1 (Linux)   *    $ java -jar target/benchmarks.jar VectorGroupByOperatorCountBench -prof perfnorm -f 3 (Linux)   *    $ java -jar target/benchmarks.jar VectorGroupByOperatorCountBench -prof perfasm  -f 1 (Linux)   *    $ java -jar target/benchmarks.jar VectorGroupByOperatorCountBench -prof gc  -f 1 (allocation counting via gc)   *    $ java -jar target/benchmarks.jar VectorGroupByOperatorBench -p hasNulls=true -p isRepeating=false -p aggregation=bloom_filter  -p processMode=HASH -p evalMode=PARTIAL1   *    $ java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:6006,suspend=y,server=y -jar target/benchmarks.jar VectorGroupByOperatorBench    */
//  Addition with overflow check. Overflow produces NULL output. 
//  Create the DemuxOperaotr 
//  @@protoc_insertion_point(builder_scope:QueryCompleteResponseProto) 
//  how to get around that. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#getParameterMetaData()    */
//  set to one of the roles user belongs to. 
//  Compute the reducers run time statistics for the job 
//  modify conf by using 'set' commands 
/*  Base Case. It's leaf.  */
//  map key separator 
//  An error heuristic could have generated different ErrorAndSolution   for each task attempt, but most likely they are the same. Plus,   one of those is probably good enough for debugging 
//  Update null counter if a null value is seen 
//  Batch of rows to emit per processNextRecord() call. 
//  This is to mimic previous behavior where NoSuchObjectException was thrown through this   method. 
//  No version annotation? 
//  Fill the all the vector entries with provided value 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareStatement(java.lang.String, int[])    */
//  In case the job is empty, there won't be JobStart/JobEnd events. The only way 
//  regenerate the valueTableDesc 
//  test UDF considers the difference in time components date1 and date2 
//  Create the ObjectInspectors for the fields 
//  Operands 
//  Sign stays the same. 
//  The caller of this method should guarantee this 
//  Add sign byte since high bit is off. 
//  results 
// oterwise it may later release permit acquired by someone else 
//  Either the token should be passed in here, or in ctor. 
//  If the table has property EXTERNAL set, update table type   accordingly 
// not allowed in a tx 
//  Write another large value. This should use a different byte buffer 
// get detailed "tableInfo" from query "desc extended tablename;" 
//  Check ownership for all partitions 
//  PKTABLE_NAME 
//  This is always replaced atomically, so we don't care about concurrency here. 
//  We don't write a first empty value.   Get an offset to reduce the relative offset later if there are more than 1 value. 
//  pull apart the kids of the OR expression 
//  Add value to NumDistinctValue Estimator 
//  get the path expression for the 1st row only 
//  First, scale up with 2. Check overflow 
//  we are not running this mapred task via child jvm 
//  Boring scenario #1 - two concurrent increases. 
//  Cache administration 
//  Add the test. 
//  Add another partition to the source. 
//  We are the last to initialize. 
//  do a deep copy, in case downstream changes it. 
// ((ISetLongArg) expr).setArg(3); 
//  a row with no columns 
//  purpose 
//  Alter table for perform schema evolution. 
//  Copy critical columns. 
//  Only the KILLED case requires a message to be sent out to the AM. 
//  We don't throw a new exception for this -- just keep going with the   next one. 
//  HiveDecimal -> Float -> Number 
//  Helper method. 
//  In Hadoop 1.X and Hadoop 2.X HADOOP_HOME is gone and replaced with HADOOP_PREFIX 
//  add another partitioning key based on floor(1/rand) % targetShardsPerGranularity 
//  try ignoring the 200 transaction and make sure it works still 
//  Serde for FetchTask 
//  REQUEST_TYPE 
//  project the relevant key column 
//  attempt to locate an existing jar for the class. 
// when compacting each split needs to process the whole logical bucket 
// everything is now in base/ 
//  Return less than because of right's digits below left's scale. 
//  build the locations in a predictable order to simplify testing 
//  check that hook to disable transforms has not been added 
//  Columns 0..N-1 are keys. Column N is the aggregate value input 
//  false - not in cache yet 
//  Replace virtual columns with nulls. See javadoc for details. 
//  all is well 
//  All are selected, do nothing 
//  write it out from hive to an rcfile table, and to an orc table 
//  currently we support only raw data size stat 
/*    * This helper method copies the group keys from one vectorized row batch to another,   * but does not increment the outputBatch.size (i.e. the next output position).   *    * It was designed for VectorGroupByOperator's sorted reduce group batch processing mode   * to copy the group keys at startGroup.    */
/*  * This is a pluggable policy to choose the candidate map-join table for converting a join to a * sort merge join. The largest table is chosen based on the size of the tables.  */
//  Instantiating the HMSHandler with hive.metastore.checkForDefaultDb will cause it to   initialize an instance of the DummyRawStoreForJdoConnection 
//  Duplicate. 
//  Set partition and order columns in overflowBatch.   We can set by ref since our last batch is held by us. 
//  The partitons are also the same so check the fieldschema 
//  Test StandardObjectInspector both with field comments and without 
//  statuses can be null if it is DDL, etc 
//  repeated .org.apache.hadoop.hive.ql.hooks.proto.MapFieldEntry otherInfo = 50; 
//  remove from list of live operations 
//  Insert the current table alias entry into the map if not already present in tableAliasToInfo. 
//  Evaluation of the decimal Constant Vector Expression after the vector is 
//  Make sure we've built the lock manager 
//  Continue with next database 
//  if it was null, the new (V2) authorization plugin must be specified in   config 
//  see http://en.wikipedia.org/wiki/Approximations_of_%CF%80   Below is the simple Newton's equation 
//  Handle aborted deltas. Currently this can only happen for MM tables. 
//  The set object containing the IN list.   We use a HashSet of HiveDecimalWritable objects instead of HiveDecimal objects so   we can lookup DecimalColumnVector HiveDecimalWritable quickly without creating   a HiveDecimal lookup object. 
//  the process of choosing a new blank works. 
//  Make sure we don't collide with the source. 
//  Already an existing semijoin branch, reuse it 
//  if correlation optimizer will not try to optimize this query 
//  Dynamic Partition Insert case 
//  A Statement#close after ResultSet#close should close the statement 
//  Time based log retrieval may not fetch the above log line so logging to stderr for debugging purpose. 
//  Following two config keys are required by FileOutputFormat to work   correctly.   In usual case of Hadoop, JobTracker will set these before launching   tasks.   Since there is no jobtracker here, we set it ourself. 
//  Normalize to positive. 
//  TopN query 
//  BRound with digits 
//  Add BIGINT values 
//  Since no NULLs, we can provide values for all rows. 
//  We need to go lookup the table and get the select statement and then parse it. 
//  does not make sense to have any of the metastore config variables to be 
//  this makes sure MJ has the same downstream operator plan as the original join 
//  compare the results fetched last time 
//  There may not be a base dir if the partition was empty before inserts or if this   partition is just now being converted to ACID. 
//  ever be created for a taskAttempt 
//  Listener parameters aren't expected to have many values. So far only   DbNotificationListener will add a parameter; let's set a low initial capacity for now.   If we find out many parameters are added, then we can adjust or remove this initial capacity. 
/*  * Base class for mocking job operations with concurrent requests.  */
//  is the 3 letter sequence "foo". 
//  deferClose indicates if the close/destroy should be deferred when the process has been   interrupted, it should be set to true if the compile is called within another method like 
//  could also check WRITE_SET but that seems overkill 
//  sign mark 
//  We have no data from this point on (could be unneeded), skip. 
//  Lets first test for default permissions, this is the case when user specified nothing. 
//  onCreateTable alters the table to add the topic name.  Since this class is generating   that alter, we don't want to notify on that alter.  So take a quick look and see if 
//  indicate that we've replaced the value 
//  ensure the table is online 
//  write a null element (element field is omitted) 
//  Get partition-list from source. 
//  DEFAULT_POOL_PATH 
//  Make sure we don't compact if we don't need to compact. 
//  If CBO did not optimize the query, we might need to replace grouping function   Special handling of grouping function 
// redact the sensitive information from the configuration values 
//  This is a noop, return successfully 
//  0. We check the conditions to apply this transformation,      if we do not meet them we bail out 
//  TODO: De-link from SessionState. A TezSession can be linked to different Hive Sessions via the pool. 
//  the 2 element list 
//  LOG.debug("VectorMapJoinFastLongHashTable expandAndRehash new logicalHashBucketCount " + logicalHashBucketCount + " resizeThreshold " + resizeThreshold + " metricExpands " + metricExpands); 
//  converts partNames into "partName1 string, partName2 string" 
/*  useExternalBuffer  */
//  Restart even if there's an internal error. 
//  Initialize the function localizer. 
//  Add another session. 
//  check if it has sq_count_check 
//  Thread executing the query 
//  Convert the field to Java class String, because objects of String type 
//  if its retrying, first regenerate the path list. 
// ------------------------------------------------------------------------------------------------ 
//  0/0 for entry 0 should be set as NULL 
//  delete jar and its dependencies added using query1 
//  if the vertex name is longer than column 1 width, trim it down 
/*                * Common outer join result processing.                */
//  Coordinator is running as Overlord as well. 
//  Finally, remove the partition columns from the end of derivedSchema.   (Clearing the subList writes through to the underlying 
//  1959 
//  We need to update the exprNode, as currently   they refer to columns in the output of the join;   they should refer to the columns output by the RS 
//  v[7] -- since left integer #5 is always 0, some products here are not included. 
//  also add the last VCol 
//  info from RR) 
//  The number of duplicates for each series key (NULL or non-NULL). 
//  We don't check compatibility of two object inspectors, but directly   pass them into ObjectInspectorUtils.compare(), users of this class   should make sure ObjectInspectorUtils.compare() doesn't throw exceptions   and returns correct results. 
//  We will try pushdown first, so make the filter. This will also validate the expression, 
/*      * used to initialize Streaming Evaluator.      */
//  create a table 
//  for the optimization that reduce number of input file, we limit number   of files allowed. If more than specific number of files have to be   selected, we skip this optimization. Since having too many files as   inputs can cause unpredictable latency. It's not necessarily to be   cheaper. 
//  Only permanent functions need to be authorized.   Built-in function access is allowed to all users.   If user can create a temp function, they should be able to use it   without additional authorization. 
// complete 2nd txn 
//     The result of the swapping operation is either      i)  a Project or, 
//  Preempt only if there's no pending preemptions to avoid preempting twice for a task. 
//  send these potentially large objects at longer intervals to avoid overloading the AM 
//  The DB foo is non-existent. 
// ---------------------------------------------------------------------------   Process Single-Column String Inner Big-Only Join on a vectorized row batch.   
//  Decimal types can be specified with different precision and scales e.g. decimal(10,5),   as opposed to other data types which can be represented by constant strings.   The regex matches only the "decimal" prefix of the type. 
//  All the catalogs should be cached 
//  Finally, get all the stuff for serdes - just the params. 
//  Set config so that TxnUtils.buildQueryWithINClauseStrings() will   produce multiple queries 
/*  Key is the database name. Value a map from the qualified name to the view object.  */
//  Create a fake fs root for local fs 
//  if left's signum wins, we don't need to do anything 
//  Check to see if this is a table level request on a partitioned table.  If so, 
//  for one element the variance is always 0 
//  A mapping from a hadoop job ID to the stack traces collected from the map reduce task logs 
//  create colinfo and then row resolver 
//  Generic Function node, e.g. CASE, an operator or a UDF node 
//  single split 
//  in-memory HDFS 
//  Turn escape on. 
//  NODE_TYPE 
//  We assume cache chunks would always match the way we read, so check and skip it. 
// this can never be null or empty; 
/*        * in case of TopN for windowing, we need to distinguish between rows with       * null partition keys and rows with value 0 for partition keys.        */
/*  Count of all values seen so far  */
//  aggregation classes 
//  go over all the input paths, and calculate a known total size, known 
//  Get the Key 
/*    * Extract column from the given ExprNodeDesc    */
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.newBuilder() 
//  Testing negative substring index 
//  Then partition number if any. 
//  There are non-numeric arguments that don't match from one UDF to   another. We give up at this point. 
//  Tracks pending preemptions per host, using the hostname || Always to be accessed inside a lock 
//  set ColumnAccessInfo for view column authorization 
//  Map of Integer to String 
//  Remove entry for operator 
//  merge task could be after dynamic partition insert 
//  Use type promotion 
//  IS_DYNAMIC_PARTITION_WRITE 
//  equal to sum of small tables size. 
//  If there are aggregations in order by, we need to remember them in qb. 
//  need to run this; to get consistent filterop conditions(for operator tree matching) 
// replace view 
//  For numeric, we'll do minimum necessary cast - if we cast to the type   of expression, bad things will happen. 
//  Do nothing but count. 
//  Get the transaction 
//  boolean that says whether to slow start or not 
//  We need to consolidate 2 or more buffers into one to decompress. 
//  initialize stats publisher if necessary 
//  Create a client to manage our transaction 
/*      * index of Rank function.      */
//  preempt on any host. 
//  Determin which task has been preempted. Normally task2 would be preempted based on it starting   later. However - both may have the same start time, so either could be picked. 
/*  * Directly serialize with the caller writing field-by-field a serialization format. * * The caller is responsible for calling the write method for the right type of each field * (or calling writeNull if the field is a NULL). *  */
//  first table in union query with view as parent 
//  Test the validation of incorrect NULL values in the tables 
/* trace error if not exists */
//  To track cleaner metrics 
//  We have exhausted our current batch, read the next batch. 
// jump out the loop if we need input from the big table 
//  set fake input and output streams 
//  synchronous event processing loop. Won't return until all events have 
//  Throw exception 
// swallow the exception since it won't affect the final result 
//  If the session has a delegation token obtained from the metastore, then cancel it 
//  append as-is 
//  Try authenticating with the http/_HOST principal 
//  column/column 
//  get the db names out 
//  since it is not used further up the tree 
//  Last row of last batch determines isGroupResultNull and long lastValue. 
//  Empty parameters are sent, no COLUMN_MAPPING 
//  External client currently cannot use guaranteed. 
/*    * Executes the callable task with help of execute() call and gets the result   * of the task. It also sets job status as COMPLETED if state is not already   * set to FAILED and returns result to future.    */
//  have to do initialization here, because the super's constructor   calls next and thus we need to initialize before our constructor 
//  Install the Configuration in the runtime. 
//  2. Extract columns and values 
//  Index of entries by table usage. 
//  If the field that is passed in is NOT a primitive, and either the   field is not declared (no schema was given at initialization), or   the field is declared as a primitive in initialization, serialize   the data to JSON string.  Otherwise serialize the data in the   delimited way. 
//  The parameters are checked manually, so do not check them 
//  batch size of 13 and decaying factor of 2 
// weren't able to check 
//  There should be 2 delta directories 
//  if the aggregation type is sum, we do a scale-up 
//  Should generate [2014-01-01, 2014-07-01) 
//  This is called from HCat, so always allow embedded metastore (as was the default). 
//  Core logic to load hash table using HashTableLoader 
//  disabled for acid path 
//  If the IMetaStoreClient#close was called, HMSHandler#shutdown would have already   cleaned up thread local RawStore. Otherwise, do it now. 
//  Add a shutdown hook for cleanup, if there are elements remaining in the cache which were not cleaned up.   This is the best effort approach. Ignore any error while doing so. Notice that most of the clients   would get cleaned up via either the removalListener or the close() call, only the active clients   that are in the cache or expired but being used in other threads wont get cleaned. The following code will only   clean the active cache ones. The ones expired from cache but being hold by other threads are in the mercy   of finalize() being called. 
//  check the easy cases first 
//  Make sure nothing really moved 
//  3. Simulate emitting records in processNextRecord() with large memory usage limit. 
// previously, when path is empty or null and no default path is specified,   __HIVE_DEFAULT_PARTITION__ was the return value for escapePathName 
// in practice we don't really care about the data in any of these tables (except as far as  it creates partitions, the SQL being test is not actually executed and results of the  wrt ACID metadata is supplied manually via addDynamicPartitions().  But having data makes  it easier to follow the intent 
//  Export case 
//  use multiple lines for statements not terminated by the delimiter 
//  /////////////////////////////////////   ResultSet output formatting classes   ///////////////////////////////////// 
//  PRIMITIVE_ENTRY 
//  For serialization. 
//  It's a table alias. 
//  Our reading is positioned to the value. 
//  1. If the table has a Sample specified, bail from Calcite path. 
//  try alternate config param 
//  serializeScale < fastScale. 
// record partitions that were written to 
//  Optimize the scenario when there are no grouping keys - only 1 reducer is 
//  No other columns provided non-NULL values.  We can return repeated output. 
//  PartitionView does not have SD and we do not need to update its column stats 
//  this will be used by the outputcommitter to pass on to the metastore client   which in turn will pass on to the TokenSelector so that it can select 
//  Map 1 .......... container  SUCCEEDED      7          7        0        0       0       0 
//  Don't fail; this is best-effort. 
//  Skip the MM directory that we have found. 
/*      * arguments      */
//  If simple map join, the whole relation goes in memory 
//  The assign method will be overridden for CHAR and VARCHAR. 
//  We have found a map. Systematically deserialize the values of the map and return back the   map 
/*    * Thread pool to execute job requests.    */
//  Do the join. It does fetching of next row groups itself. 
//  Authorization is done. Just call super. 
//  push the feed to its subscribers 
/*        Start of cleanup     */
//  At this point, we've set up all the tables and ptns we're going to test drops across   Replicate it first, and then we'll drop it on the source. 
//  ROW__ID is always in the first field 
//  Note: we assume reuse is only possible for the same user and config. 
//  Use ranges and duplicate multipliers to reduce the size of the display. 
//  col2:double>); 
/*      * (non-Javadoc)     * @see org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver#getOutputNames()     * Set to null only because carryForwardNames is true.      */
//  get aliasToPath and pass it to the heuristic 
//  Try to insert up to n times. Rehash if that fails. 
//  initialize FetchTask right here 
/*    * IMPLEMENTATION NOTE:   *    We implement HiveDecimal with the mutable FastHiveDecimal class.  That class uses   *    protected on all its methods so they will not be visible in the HiveDecimal class.   *   *    So even if one casts to FastHiveDecimal, you shouldn't be able to violate the immutability   *    of a HiveDecimal class.    */
//  Do not create an identity project if it does not rename any fields 
//  change file modification time and look for cache misses 
//  Keep an open txn which refers to the aborted txn. 
//  update the attrs 
//  table is deleted 
//  The hash code for each non-NULL key. 
//  POOL_TRIGGERS 
//  A MuxOperator should only have a single child 
//  Determine the name of our map or reduce task for debug tracing. 
//  do a group by to aggregate min,max and bloom filter. 
//  Password file contents are trimmed of trailing whitespaces and newlines 
//  don't do zero-divide if one comes up at random 
//  only one result column   verify the column name   verify the column name 
//  extract stages 
//  Do not allow temp table rename if the new name already exists as a temp table 
//  These 4 types are not supported yet.   We should define a complex type date in thrift that contains a single int   member, and DynamicSerDe   should convert it to date type at runtime. 
//  number of digits to retain from the end 
//  tableScan is only available during compile 
//  Check if two arguments were passed 
//  and again, one last time. 
//  Note: we may make size/etc. configurable later. 
//  Note: we don't pass the config to reopen. If the session was already open, it would         have kept running with its current config - preserve that behavior. 
// Calcite creates null literal with Null type here but 
//  If it is a RIGHT / FULL OUTER JOIN, we need to iterate through the row container   that contains all the right records that did not produce results. Then, for each   of those records, we replace the left side with NULL values, and produce the   records.   Observe that we only enter this block when we have finished iterating through   all the left and right records (aliasNum == numAliases - 2), and thus, we have   tried to evaluate the post-filter condition on every possible combination. 
/*  first_name is null or       first_name <> 'sue' or       id >= 12 or       id <= 4;  */
// push down projections to columnar store works for RCFile and ORCFile 
// Uses level parallel implementation of a bfs. Recursive DFS implementations  have a issue where the number of threads can run out if the number of  nested sub-directories is more than the pool size.  Using a two queue implementation is simpler than one queue since then we will  have to add the complex mechanisms to let the free worker threads know when new levels are  discovered using notify()/wait() mechanisms which can potentially lead to bugs if  not done right 
//  Stop requested, and handled inside. 
//  load the properties from config file 
//  The fields that HS2 uses to give AM information about plugin endpoint.   Some of these will be removed when AM registry is implemented, as AM will generate and publish them. 
//  First try non-random values 
// note: inserts go into 'new part'... so this won't fail 
// throw exception to simulate an issue with cleaner thread 
//  If the file is missing or getting modified, then refer CM path 
//  Get the other one by examining Join Op 
//  make sure the columns does not already exist 
//  FINAL REDUCTION:   Case 7: NO column stats  numRows / 2   Case 8: column stats, grouping sets  Min(numRows, ndvProduct * sizeOfGroupingSet)   Case 9: column stats, NO grouping sets - Min(numRows, ndvProduct) 
//  Enable TransactionalValidationListener + create.as.acid 
// if HADOOP_PROXY_USER is set, create DelegationToken using real user 
//  Check specificFilterSet for QTest specific ones. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setSavepoint()    */
//  When all inputs are accounted for, the output is forwarded appropriately. 
//  directory creation is otherwise within the writers 
// ************************************************************************************************   Binary to Decimal Conversion. 
//  originally copied from org.apache.hadoop.mapred.lib.InputSampler but seemed to have a bug 
//  List of compactions to clean. 
// Get the output location in the order partition keys are defined for the table. 
// Start Metrics 
//  Storage information. 
//  and 0 if condition needs to be computed. 
//  do nothing, handled below - types will mismatch 
//  bucketed or sorted table/partition they cannot be merged. 
//  blocks if no more llap queries can be submitted. 
//  Cached successfully, add to policy. 
//  Note: we assume here the session, before we resolve killQuery result here, is still         "in use". That is because all the user ops above like return, reopen, etc.         don't actually return/reopen/... when kill query is in progress. 
//  apply overlay query specific settings, if any 
//  get the collection separator and map key separator 
//  Evaluate the aggregation input argument expression. 
//  as the initialCapacity which cannot be 0, we provide a reasonable   positive number here 
//  Col2 
//  Add the jar file 
// @Ignore("see bucket_num_reducers_acid.q") 
//  Test different format types 
/*    * Generate the GroupByOperator for the Query Block (parseInfo.getXXX(dest)).   * The new GroupByOperator will be a child of the reduceSinkOperatorInfo.   *   * @param mode   *          The mode of the aggregation (PARTIAL1 or COMPLETE)   * @param genericUDAFEvaluators   *          If not null, this function will store the mapping from Aggregation   *          StringTree to the genericUDAFEvaluator in this parameter, so it   *          can be used in the next-stage GroupBy aggregations.   * @return the new GroupByOperator    */
//  BOOL_VAL 
//  All partitions with blurb="isLocatedOutsideTablePath" should have 2 columns, 
//  Expected if the operation takes time. Continue the loop, and wait for op completion. 
//  null args: 
//  Note: the sinks and DDL cannot coexist at this time; but if they could we would 
//  MAP requires 2 levels: key separator and key-pair separator. 
//  close off the buffer with a normal tag 
//  now check the table folder and see if we find anything   that isn't in the metastore 
//  Logger jobs 
//  Get ready for a another round of small table values. 
/*      * Get the list of table scan operators for this join. A interface supportSkewJoinOptimization     * has been provided. Currently, it is only enabled for simple filters and selects.      */
//  Col1 
//  need at least 1 ZK server for testing 
// if here we are now doing an Acid read so must have OrcSplit.  CombineHiveInputFormat is 
//  This helper object serializes LazyBinary format reducer values from columns of a row 
//  no check for the line? How to check?   if the line is invalid for any reason, the job will fail. 
//  closeHBaseConnections 
//  Create cost metadata provider 
//  Parse until pair separator (currentLevel). 
//  need a state store eventually for current state & measure backoffs 
// construct the -setfacl command 
/*  10 files x 1000 size for 99 splits  */
//  COLS 
//  found the plan is already connected which means this is derived from the cache. 
/*    * Element for Key: byte[] x Hash Table: HashMultiSet    */
/*  Hcat requires ALTER_DATA privileges for ALTER TABLE LOCATION statements      * for the old table/partition location and the new location.       */
//  Add 1 for tag 
//  we're continuing an existing command 
//  wrapping for exception handling 
//  leadership state changes and sending out notifications to listener happens inside synchronous method in curator.   Do only lightweight actions in main-event handler thread. Time consuming operations are handled via separate   executor service registered via registerLeaderLatchListener(). 
//  This call fills in the column names, types, and partition column count in 
//  no leading 0 for month and day should work 
//  grant option 
// handle the isNull array first in tight loops 
//  extract stats keys from StatsTask 
//  Should not be used. 
//  both branches are null. 
//  Col3 
//  generation. 
//  Use VertexOrBinary.newBuilder() to construct. 
//  1 WriteEntity: default@acidtblpart Type=TABLE WriteType=INSERT isDP=false 
//  This collector is just a row counter. 
//  targetPath path is /x/y/z/1/2/3 here /x/y/z is present in the file system   create the structure till /x/y/z/1/2 to work rename for multilevel directory   and if rename fails delete the path /x/y/z/1   If targetPath have multilevel directories like /x/y/z/1/2/3 , /x/y/z/1/2/4   the renaming of the directories are not atomic the execution will happen one   by one 
//  Assume the full ACID table. 
//  can't happen 
//  split exclusively serves alias, which needs to be sampled   add it to the split list of the alias. 
//  2nd task requested host2, got host4 since host3 is dead and host2 is full 
//  create a syntax tree for a function call 'myisnull(col0, "UNKNOWN")' 
//  Map each aborted write id with each allocated txn. 
//  There should still be four directories in the location. 
//  Bucket MapJoin in LLAP, make sure the caches are populated.   Get the subcache. 
//  If enablejobreconnect param was not passed by a user, use a cluster   wide default 
//  optional   optional   optional   optional   optional 
//  This also provides completion information, and a possible notification when task actually starts running (first heartbeat) 
//  Rows to be emitted with a separate thread per processNextRecord() call. 
//  1.2 Add GrpSet Col 
//  SerDe property for how the Hive column maps to Accumulo 
//  Only allow constant field name for now 
//  This function checks whether all bucketing columns are also in join keys and are in same order 
//  Test getTables() 
//  Test that 2 separate partitions don't coalesce. 
//  header 
//  Copy bytes into scratch buffer. 
//  block to make sure move happened successfully 
//  Make sure the cleanup doesn't leave the pool without a session. 
// release S on T6  attempt to X on T6 again - succeed 
//  key columns 
//  LOG.info("extractThriftToken("+tokenStrForm+","+tokenSignature+")"); 
//  placeholder; minimum pf value is enforced in NGramEstimator 
//  there should be only one ColumnStatistics 
//  Sanity check; restrictedConfig is always set in setup. 
//  NEW TAI LUE LETTER THA U+1992 (3 bytes) 
//  Round towards negative infinity. 
//  PARTITION_VALUES 
//  Verify that COLUMN_STATS_ACCURATE is removed from params 
/*  resultSignum  */
//  Get the field objectInspector and the field object. 
//  It is a constant, we can ignore it 
//  Note: other parent readers init everything in ctor, but union does it in startStripe. 
//  No move pending and no intervening discard, the allocator can release. 
//  Hadoop is missing a public API to check for snapshotable directories. Check with the directory name   until a more appropriate API is provided by HDFS-12257. 
//  If it is the first child, we set the mode variable value   Otherwise, if the mode we are working on is different, we   bail out 
//  this is to keep track of null literal which already has been visited 
//  Reset the selection vector. 
//  Strip the leading ";" if provided   (this is the assumption with which we're going to start configuring sessionConfExt) 
//  IMPORTANT NOTE: For Multi-OR, the VectorizationContext class will catch cases with 3 or                   more parameters... 
// create the serialized string for type 
//  Decimals are stored as BigInteger, so convert and compare 
//  only write to the record file if we are writing a line ...   otherwise we might get garbage from backspaces and such. 
//  Map filter to the new filter over join 
/* since COMPACTOR_MAX_NUM_DELTA=2,    we expect files 1,2 to be minor compacted by 1 job to produce delta_21_23    * 3,5 to be minor compacted by 2nd job (file 4 is obsolete) to make delta_25_33 (4th is skipped)    *    * and then the 'requested'    * minor compaction to combine delta_21_23, delta_25_33 and delta_35_35 to make delta_21_35    * or major compaction to create base_35 */
// this uses VectorizedOrcAcidRowBatchReader 
//  If we are going to use cache, change the path to depend on file ID for extra consistency. 
//  accumulate the counts 
//  If available, copy state to registry for optimization rules 
//  Task is just created 
//  write header 
//  At least one task would have been added to update the repl state 
/* Assert.assertEquals(1,        TxnDbUtil.countLockComponents(((DbLockManager.DbHiveLock) locks.get(0)).lockId));     */
//  standard object 
//  if false, prune it 
/*  neither side is repeating  */
//  no truncate, the table is missing either due to drop/rename which follows the truncate.   or the existing table is newer than our update. 
//  see javadoc of AccumuloCompositeRowId 
//  2^32-1 + .01 
/*  (non-Javadoc)   * @see org.apache.hadoop.mapreduce.RecordReader#initialize(   * org.apache.hadoop.mapreduce.InputSplit,   * org.apache.hadoop.mapreduce.TaskAttemptContext)    */
//  Next check if this table has partitions and if so   get the list of partition names as well as allocate 
//  leading spaces are significant 
//  These are null when evaluate() is called for the first time 
// since the loop left it == txnToWriteIds.size() 
//  Acquire different locks at different levels 
//  firstFetchHappened == true. In reality it almost always calls joinOneGroup. Fix it? 
//  No more matches expected. 
//  follow it. 
//  we also need to delete partdate=2008-01-01 to make it consistent. 
//  We now have a vector of aggregation buffer sets to use for each row   We can start computing the aggregates.   If the number of distinct keys in the batch is 1 we can   use the optimized code path of aggregateInput 
/*            * if we could not get status for some reason, log it, and send empty status back with           * just the ID so that caller knows to even look in the log file            */
//  Sample data 
//  Assuming that a valid UGI with kerberos cred is created by HS2 or LLAP 
/*      * translate args      */
//  It is possible the table is deleted during fetching tables of the database,   in that case, continue with the next table 
//  This happens when the code inside the JMX bean threw an exception, so   log it and don't output the bean. 
//  It's hard to distinguish a union with null from a null union. 
//  Use nullIndicator to decide whether to project null.   Do nothing if the literal is null. 
//  Rip apart the object inspector, making sure we got what we expect. 
//  4. Gather GB Memory threshold 
//  the udtf op 
//  no HS2 instances are running 
//  make this client wait if job tracker is not behaving well. 
//  If we're here, then socket1.getLocalPort was the port to exclude   Since both sockets were open together at a point in time, we're   guaranteed that socket2.getLocalPort() is not the same. 
//  so we don't lose this. A bit of a weird dance here. 
//  at the end 
/*    * Phase1: hold onto any CTE definitions in aliasToCTE.   * CTE definitions are global to the Query.    */
//  Create cmroot with permission 700 if not exist 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getResultSet()    */
//  Test a query where timeout does not kick in. Set it to 5s; 
/*  (non-Javadoc)   * This provides a LazyFloat like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  Wildcard bind 
//  Fetch the TableScan Operator. 
//  call-1: listLocatedStatus - mock:/mocktbl 
//  bail on mux-operator because mux operator masks the emit keys of the   constituent reduce sinks 
//  key expiration - create an already expired key 
// @VisibleForTesting 
//  The following union operation returns a union, which traverses over the   first set once and then  then over each element of second set, in order,   that is not contained in first. This means it doesn't replace anything   in first set, and would preserve the WriteType in WriteEntity in first   set in case of outputs list. 
//  we've not seen this terminal before. we need to check   rootUnionWorkMap which contains the information of mapping the root   operator of a union work to a union work 
//  Someone is allocating an arena. 
//  Re-enable timeouts 
// This the set of entities that the statement represented by extLockId wants to update 
//  Session should not be lost; however the fraction should be discarded. 
//  VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + " NOMATCH" + " currentKey " + currentKey); 
/*    * check if a Select Expr is a constant.   * - current logic used is to look for HiveParser.TOK_TABLE_OR_COL   * - if there is none then the expression is a constant.    */
//  not fetching from a table directly but from a temp location 
/*    * Constructors of various flavors follow.    */
//  old schemas within it. 
//  serialize json based on field annotations only 
//  check implementation class: 
//  List of softreferences 
//  If negotiation is complete, remove this handler from the pipeline, and register it with   the Kryo instance to handle encryption if needed. 
//  this is really just -1 
//  Case 5: column stats, NO hash aggregation, NO grouping sets 
/*    * Lookup an long in the hash set.   *   * @param key   *         The long key.   * @param hashSetResult   *         The object to receive small table value(s) information on a MATCH.   *         Or, for SPILL, it has information on where to spill the big table row.   *   * @return   *         Whether the lookup was a match, no match, or spilled (the partition with the key   *         is currently spilled).    */
//  hex input is also supported 
//  base. 
/*    * DECIMAL_64.    */
//  MIDDLE pattern 
//  join operator 
//  initialize map local work 
//  Add all non-virtual columns from the TableScan operator. 
// ---------------------------------------------------------------------------   Process Multi-Key Inner Big-Only Join on a vectorized row batch.   
//  lazily create PathChildrenCache 
//  group by requires "ArrayList", don't ask. 
//  priority = 30 / (4000 - 100) = 0.0076 
//  Type-specific handling 
//  Its not clear, if this rewrite is always performant on MR, since extra map phase   introduced for 2nd MR job may offset gains of this multi-stage aggregation.   We need a cost model for MR to enable this on MR. 
//  get table metadata 
//    { "key": { "reducesinkkey0":int, "reducesinkkey1":int }, "value": { "_col6":int } } 
//  sparse-sparse merge 
// table ownership for create/drop/alter index 
//  get the tables for the desired patten - populate the output stream 
//  Keep separate from the creating events in case the send blocks. 
/*      * The following tests will verify the deprecation variable is still usable.      */
//  Assume the worst. 
//  Tell the operator the status of the next key-grouped VectorizedRowBatch that will be delivered   to the process method.  E.g. by reduce-shuffle.  These semantics are needed by PTF so it can 
//  Add the actual source input 
//  Move the pointer to the next byte, since we have written 
//  We will generate results for all matching and non-matching rows. 
//  get the group by keys to ColumnInfo 
// failed because object doesn't exist 
/*  Thread simulating a user session in HiveServer2.  */
/*      * Keeps track of regular timed heartbeats. Is primarily used as a timing mechanism to send /     * log counters.      */
/*  verify the connection fails after canceling the token  */
//  Decode UTF-8 
//  Same object. 
//  be aware that result could be the same object as z. 
// next() should always return false 
//  optional string executionMode = 4; 
//  NULL plan means WM is disabled via a command; it could still be reenabled. 
//  set that parent initialization is done and call initialize on children 
//  Note: this thing should know nothing about ACID or schema. It reads physical columns by index;         schema evolution/ACID schema considerations should be on higher level. 
//  3. Get Aggregation FN from Calcite given name, ret type and input arg 
//  we are currently walking the big table side of the merge join. we need to create or hook up 
//  partition values are specified on non-partitioned table 
//  sort and pick partition keys 
//  remove requested quantiles from the head of the list 
//  Lock a few blocks without telling the policy. 
//  We need to add a cast to DATETIME Family 
//  Use Case 8. 
//  if row count is 0 and where there are no nulls it means index is disabled and we don't have stats 
//  Some cases were converted before calling getVectorExpressionForUdf.   So, emulate those cases first. 
//  Return aggregate collations 
//  If this is the last byte, leave the high bit off 
//  Add partitions located in the table-directory (i.e. default). 
//  fileHeader, resultType, arg2Type, arg3Type 
// testing MATCHED AND with CASE statement  using target.a breaks this 
//  If old table is in the cache but the new table *cannot* be cached 
//  A null cq implies an empty column qualifier 
//  2^56 * 2^56 
//  Only log on the first wait, and check after wait on the last iteration. 
//  return "0sec if no difference 
/*        * use a run-length encoding. We only record run length if a same       * 'prevValueLen' occurs more than one time. And we negative the run       * length to distinguish a runLength and a normal value length. For       * example, if the values' lengths are 1,1,1,2, we record 1, ~2,2. And for       * value lengths 1,2,3 we record 1,2,3.        */
//  The key of the next lowest reader. 
/*     * Expected result 0th entry i the RecordIdentifier + data.  1st entry file before compact */
//  to tasks started after the addFile() call completes. 
//  the key is found in MapColumnVector, set the value 
//  This means there are tables of something in the database 
//  Save the evaluator so that it can be used by the next-stage 
//  to this plan:     Project-A' (all gby keys + rewritten nullable ProjExpr)     Aggregate (groupby(all left input refs)                   agg0(rewritten expression),                   agg1()...)       Project-B' (rewriten original projected exprs)         Join(replace corvar w/ input ref from LeftInputRel)           LeftInputRel           RightInputRel   
//  Update the aggregations. 
//  This is arbitrary. Note that metadata may come from a big scan and nuke all the data   from some small frequently accessed tables, because it gets such a large priority boost   to start with. Think of the multiplier as the number of accesses after which the data   becomes more important than some random read-once metadata, in a pure-LFU scheme. 
//  1st Cancel 
//  Negative numbers indicate a column to be (deserialize) read from the small table's   LazyBinary value row. 
//  String or string equivalent is considered numeric when used in arithmetic operator. 
/*    * STRING.   *   * Can be used to write CHAR and VARCHAR when the caller takes responsibility for   * truncation/padding issues.    */
/*    * fastSerializationUtilsRead lower word is 62 bits (the lower bit is used as the sign and is   * removed).  So, we need a multiplier 2^62   *   *    2^62 =   *      4611686018427387904 or   *      4,611,686,018,427,387,904 or   *      461,1686018427387904 (16 digit comma'd)    */
//  The ObjectInspector for the row ID 
//  if numReducers < 0 and newNumReducers > 0 
//  continue.. null out the field? 
//  those tables directory. 
//  If the serializer is ThriftJDBCBinarySerDe, then it requires that NoOpFetchFormatter be used. But when it isn't,   then either the ThriftFormatter or the DefaultFetchFormatter should be used. 
// ************************************************************************************************   Decimal Rounding. 
//  Update the joinKeys appropriately. 
//  3. Perform a major compaction 
//  Add a column.   Change SerDe, File I/O formats. 
//  Store the changes 
//  set method. This requires calcite change 
//  the combinations below definitely result in overflow 
// remove currTask from parentTasks 
//  -ve dates are also valid dates - the dates are within 1959 to 2027 
//  We will store all the new /changed properties in the job in the   udf context, so the the HCatInputFormat.setInput method need not 
//  For now, olderClass has 1 version and newerClass 2 versions... 
//  Allow endFunctionListeners to add any counters they have collected 
//  From https://docs.microsoft.com/en-us/sql/t-sql/language-elements/reserved-keywords-transact-sql#odbc-reserved-keywords 
//  partition level column stats merging 
//  create new operator: HashTable DummyOperator, which share the table desc 
// create a few read locks, all on the same resource 
//  Check whether current operators are equal 
//  so let's create a new list and copy it if we don't have a linked list 
/*      * validate and setup SymbolInfo      */
//  Possible because a queryComplete message from the AM can come in first - KILL / SUCCESSFUL,   before the fragmentComplete is reported 
//  No heartbeat 
//  Given the keyIndex these arrays return:     The ColumnVector.Type,     The type specific index into longIndices, doubleIndices, etc... 
// must be just a column name 
//  varchar 
//  child can be EXPR AS ALIAS, or EXPR. 
//  deletes for the bucket being taken into consideration for this split processing. 
//  prevent instantiation 
//  For dynamic partitioned writes without all keyvalues specified,   we create a temp dir for the associated write job 
//  Since "ifexists" was not set to true, trying to create the same table   again   will result in an exception. 
//  Set of values to look for. Include the original blank value Long.MIN_VALUE to make sure 
//  Relative positions of the blocks don't change over time; priorities we expire can only   decrease; we only have one block that could have broken heap rule and we always move it   down; therefore, we can update priorities of other blocks as we go for part of the heap -   we correct any discrepancy w/the parent after expiring priority, and any block we expire   the priority for already has lower priority than that of its children. 
//  If table of current event has partition flag different from existing table, it means, some   of the previous events in same batch have drop and create table events with same same but   different partition flag. In this case, should go with current event's table type and so   create the dummy table object for adding repl tasks. 
//  verify udf reflect is allowed (no exception will be thrown) 
//  bigTableCandidates can never be null 
//  Find tables which name contains _to_find_ or _hidden_ in the default database 
//  Subscriber can get notification about addition of a table in HCAT   by listening on a topic named "HCAT" and message selector string   as "HCAT_EVENT = HCAT_ADD_TABLE" 
//  Independent of hashtable and can be modified, no need to copy. 
//  This is integer because in Derby DN converts boolean to char, breaking sysdb. 
//  Open the log file, and read the lines, parse out stack traces 
//  File checksum is not implemented for local filesystem (RawLocalFileSystem) 
//  0. Handle initialization results. 
//  Create another table for incremental repl verification 
//  Cancel the query 
/*            * We have a "regular" single rows from the Input File Format reader that we will need           * to deserialize.            */
//  Determine which rows are left. 
//  NEW_SCHEMA 
//  8. Incase this QB corresponds to subquery then modify its RR to point   to subquery alias 
//  remove valid columns 
//  find file instead of dir. dont change inputpath 
//  done processing the task 
//  the subqueries are map-only jobs 
//  If there is a correlation condition anywhere in the filter, don't   push this filter past project since in some cases it can prevent a   Correlate from being de-correlated. 
//  accessing order of join cols to bucket cols, should be same 
//  task to the new task. 
//  NOTE: this partitionSpec has to be ordered map 
//  If credential provider has entry for our credential, then it should be used 
//  First value is written without: next relative offset, next value length, is next value last   flag, is next value length small flag, etc. 
//  Need to drop the primary DB as metastore is shared by both primary/replica. So, constraints 
//  Make it a power of 2 by backing down (i.e. the -2). 
//  v[1] where (product % MULTIPLER_INTWORD_DECIMAL) is the carry from v[0]. 
//  did not find the column 
//  Store the bucket path to bucket number mapping in the table scan operator.   Although one mapper per file is used (BucketizedInputHiveInput), it is possible that   any mapper can pick up any file (depending on the size of the files). The bucket number 
//  no rename, the table is missing either due to drop/rename which follows the current rename.   or the existing table is newer than our update. 
//  Avro considers bytes primitive, Hive doesn't. Make them list of tinyint. 
//  Set HiveConf statics to default values 
//  updateCurrentKey needs to be called to initialize the master key   (there should be a null check added in the future in rollMasterKey)   updateCurrentKey(); 
//  TODO Setup SSL Shuffle 
//  vectors 
//  if collection usage threshold is not support, worst case set memory threshold on memory usage (before GC) 
/*    * Helper to determine what java options to use for the containers   * Falls back to Map-reduces map java opts if no tez specific options   * are set    */
//  In one work, only one map join operator can be bucketed 
//  check singleAggRel is single_value agg 
/*  (non-Javadoc)   * This provides a LazyBinary like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  table for which show locks is being executed 
//  set a pass a property to operation and check if its set the query config 
//  timestamp BETWEEN 
//  bypass only if outerRR is not null. Otherwise we need to look for expressions in outerRR for   subqueries e.g. select min(b.value) from table b group by b.key 
//  Ascending   Null first (default for ascending order) 
//  interface into the registry service 
//  This is a 1-stage map-reduce processing of the groupby. Tha map-side   aggregates was just used to   reduce output data. In case of distincts, partial results are not used,   and so iterate is again   invoked on the reducer. In case of non-distincts, partial results are   used, and merge is invoked   on the reducer. 
// see also https://issues.apache.org/jira/browse/HIVE-9938 
//  Try empty rows query 
// returns IP addr 
//  Wait for another iteration to make sure event gets processed for D2 to receive allocation. 
//  Check that we are cleaning up the empty aborted transactions 
// the row__ids are the same after compaction 
//  These constants are also imported by org.apache.hadoop.hive.ql.io.AcidUtils. 
//  For run length byte encoding, record the number of bits within current byte to consume to 
//  remove reducer 
//  This still could be DPP. 
//  Whole repeated key batch was filtered out. 
//  This spawns a separate thread to walk through the cache and removes expired nodes.   Only one cleaner thread should be running at any point. 
//  Step3: move to the file destination 
//  to handle the rest of the aggregation that the bottom aggregate hasn't handled 
//  returns rows from possibly multiple bucket files of small table in ascending order   by utilizing primary queue (borrowed from hadoop) 
//  Need to preserve loggedInUser 
//  We may have already connected `work` with `childWork`, in case, for example, lateral view:      TS       |      ...       |      LVF       | \      SEL SEL       |    |      LVJ-UDTF       |      SEL       |      RS   Here, RS can be reached from TS via two different paths. If there is any child work after RS,   we don't want to connect them with the work associated with TS more than once. 
//               degree of parallelism 
//  Make it a power of 2. 
//  Partition value can't end in this suffix 
//  The planner gives us a subset virtual columns available for this table scan.      AND   We only support some virtual columns in vectorization.     So, create the intersection.  Note these are available vectorizable virtual columns.   Later we remember which virtual columns were *actually used* in the query so   just those will be included in the Map VectorizedRowBatchCtx that has the   information for creating the Map VectorizedRowBatch.   
//  This instance will not be added back, since it's services are not up yet. 
//  match! 
//  Compare cell value with constant value in filter   1 if they match and cell value isn't other, return true   2 if they don't match but cell is other and value in filter is not skewed value,     return unknown. why not true? true is not enough. since not true is false,     but not unknown is unknown.     For example, skewed column C, skewed value 1, 2. clause: where not ( c =3)     cell is other, evaluate (not(c=3)).     other to (c=3), if ture. not(c=3) will be false. but it is wrong skip default dir     but, if unknown. not(c=3) will be unknown. we will choose default dir.   3 all others, return false 
//  default to false 
//  Setting hive.server2.enable.doAs to True ensures that HS2 performs the query operation as   the connected user instead of the user running HS2. 
//  If we found something before we ran out of components, use it. 
//  Verify the actual locations being correct.   os13 should be on a different location. Splits are supposed to be consistent across JVMs,   the test is setup to verify a different host (make sure not to hash to the same host as os11,os12).   If the test were to fail because the host is the same - the assumption about consistent across JVM 
/*    * Kills templeton job with multiple retries if job exists. Returns true if kill job   * attempt is success. Otherwise returns false.    */
// if needRequireLock is false, the release here will do nothing because there is no lock 
//  Since equiv f, should get back first container 
//  queryPlan here. 
//  For spark, in non-local mode, any added dependencies are stored at   SparkFiles::getRootDirectory, which is the executor's working directory.   In local mode, we need to manually point the process's working directory to it, 
//  Return empty result since only constant Desc exists 
//  before closing the operator check if statistics gathering is requested   and is provided by record writer. this is different from the statistics   gathering done in processOp(). In processOp(), for each row added   serde statistics about the row is gathered and accumulated in hashmap.   this adds more overhead to the actual processing of row. But if the   record writer already gathers the statistics, it can simply return the   accumulated statistics which will be aggregated in case of spray writers 
//  Calculate unique skewed elements for each skewed column. 
//  One serialized key for 1 or more rows for the duplicate keys.   LOG.info("reduceSkipTag " + reduceSkipTag + " tag " + tag + " reduceTagByte " + (int) reduceTagByte + " keyLength " + serializedKeySeries.getSerializedLength());   LOG.info("process offset " + serializedKeySeries.getSerializedStart() + " length " + serializedKeySeries.getSerializedLength()); 
//  Trailing space should ignored for char comparisons.   So write stripped values for this SerDe. 
//  setting these props to match LazySimpleSerde 
//  Check column types 
//  check if task is started 
//  Internal representation is an integer representing day offset from our epoch value 1970-01-01 
//  Not public since we must have the serialize write object. 
//  classloader invokes this static block when its first loaded (lazy initialization). 
//  constant or null expr, just return 
//  1. We apply the transformation 
//  we create a dummy vertex for a mergejoin branch for a self join if this 
//  FUTURE: Add INTERVAL_YEAR_MONTH, etc, as desired. 
//  Advance the primary reader to the next record 
//  4. Walk through UDAF & Collect UDAF Info 
//  UNIQUE_CONSTRAINT_COLS 
//  An array was passed as parameter 2, make sure it's an array of primitives 
//  Number of arguments to this UDF   External Name 
//  Valid paths 
// surrogate pair case 
//  Fetch rows from splits 
//  We do not need to do anything, we bail out 
//  The numbers of input columns and output columns should match for regular query 
/*        * Raise custom exception like IOException and verify expected Message.        */
//  old test is moved to msck_repair_2.q 
//  walk operator tree to create expression tree for filter buckets 
//  The columns in the group by expressions should not intersect with the columns in the   distinct expressions 
// we have WHEN NOT MATCHED AND <boolean expr> THEN INSERT 
// so the plan knows we are 'reading' this db - locks, security... 
// note that this KeyInterval may be adjusted later due to copy_N files 
// Check if the existing partition values can be type casted to the new column type 
//  table matcher. 
//  NEXT_TXN_ID.ntxn_next could be min_uncommitted_txnid. 
//  Right now we assume only FM and HLL are available. 
/*        * since there is a collision index will be used for the next value        * so have the map point back to original index.        */
//  Add the Auth filter 
//  Fill the buffer with key value pairs. 
//  Is this field a null? 
//  Edge case? 
//  In this case, A-B-D join will be executed first and ABD-C join will be executed in next 
//  generate absolute path relative to current directory or hdfs home   directory 
//  Reads the index file for each requested mapId, and figures out the overall   length of the response - which is populated into the response header. 
//  The actual deserialization may involve nested records, which require recursion. 
//  different queries in the session may be using the same lock manager. 
//  Lock operations themselves don't require the lock. 
//  partition columns   partition values 
//  Update the leaf in place 
//  create conditional work list and task list 
//  if custom root specified, update the parent path 
//  Nope, so look to see if our home dir has been explicitly set 
/*  Object Inspectors corresponding to the struct returned by TerminatePartial and the long     * field within the struct - "count"      */
/*                * In a single-threaded init case, with this the ordering of sessions in the queue will be               * (with 2 sessions 3 queues) s1q1, s1q2, s1q3, s2q1, s2q2, s2q3 there by ensuring uniform               * distribution of the sessions across queues at least to begin with. Then as sessions get               * freed up, the list may change this ordering.               * In a multi threaded init case it's a free for all.                */
//  because this file will be fetched by fetch operator 
//  2. Reduce filter with stats information 
//  Summary for column statistics 
/*            * Clear out any rows we may have processed in row-mode for the current partition..            */
//  added project if need to produce new keys than the original input   fields 
//  look for getFieldName() or isFieldName() 
//  Only used for dynamic partitioned hash joins (mapjoin operator in the reducer) 
//  Choose first up to a full batch with no selection. 
//  Implementation of row container 
//  As writeIdHwm is known, query all writeIds under the writeId HWM.   If any writeId under HWM is allocated by txn > txnId HWM or belongs to open/aborted txns,   then will be added to invalid list. The results should be sorted in ascending order based   on write id. The sorting is needed as exceptions list in ValidWriteIdList would be looked-up 
//  value will be NULL. 
//  test when first argument has nulls 
//  13. To record move events, we need to cluster fraction updates that happens at step 11. 
//  Using the hook on startup ensures that the hook always has priority   over settings in *.xml.  The thread local conf needs to be used because at this point   it has already been initialized using conf. 
//  Iterate over the rest of the children 
//  If cbucketcols or pbucketcols have constant node expressions avoid the merge. 
// insert clause 
//  Ket is partition values and the value is a wrapper around the partition object 
//  No need to collect statistics of index columns 
//  5. Put uncompressed data to cache. 
//  handle the single block case 
//  parent reduce sinks 
//  Clear all in-memory partitions first 
//  NOT_NULL_CONSTRAINTS 
// HCat doesn't support transactional tables 
//  Round to power of 2 here, as is required by WriteBuffers 
// delta/ with files in raw format are a result of Load Data (as opposed to compaction  or streaming ingest so must have interval length == 1. 
//  Linear interpolation to get the exact percentile 
//  Drop databases created by other test cases 
//  LOG.info("First tail offset " + writeBuffers.getWritePoint()); 
//  Use Case 9. 
//  Pre-install the database so all the tables are there. 
//  Check that dropping database from wrong catalog fails 
//  first create expression from defaultValueAST 
//  WriteId of recently committed txn which was open when get ValidTxnList snapshot should be invalid as well. 
//  all columns 
//  Since bigTableValueExpressions may do a calculation and produce a scratch column, we   need to map to the right batch column. 
//  accepting Object means accepting everything,   but there is a conversion cost. 
//  different level the drop command specified. 
//  LinkedHashMap have a repeatable iteration order. 
//  Cleanup the mapwork path 
//  2 jobs, 2 stages per job, 2 tasks per stage. 
//  reset the resolver 
//  This is the overwhelmingly common case. 
// first child should be rowid 
//  Link backtrack SelectOp to FileSinkOp 
//  initialize reduce operator tree 
//  for example, original it is max 0, dist 1, min 2 
//  constant = constant expressions. We shouldn't be getting this 
//  arbitrary column names used internally for serializing to spill table 
//  5. GroupingSets, Cube, Rollup 
//  If the whole column vector has no nulls, this is true, otherwise false. 
//  Use Case 10. 
//  Generate the intermediate aggregate B, the one on the bottom that converts   a distinct call to group by call.   Bottom aggregate is the same as the original aggregate, except that 
//  operations. So, READ COMMITTED is sufficient. 
//  else fallback to HLLOriginal algorithm 
//  FILES_ADDED 
//  Create the filter for the queryId appender 
//  with HIVE-7832, the dictionaries will be disabled after writing the first   stripe as there are too many distinct values. Hence only 3 stripes as   compared to 25 stripes in version 0.11 (above test case) 
//  Do the per-batch setup for an outer join. 
//  Get the table from metastore 
//  we already check if rowCnt is null and rowCnt==0 means table is   empty. 
//  Reset the conf variable values that we changed for this test. 
//  We should copy only those table parameters that are specified in the config. 
//  skip, no semijoin branch 
//  Copy to new slot table. 
//  counts are cached to avoid repeated complex computation. If register value 
//  Stores the Tablescan operators processed to avoid redoing them. 
//  number of registers - 2^p 
/*  * Test submission of concurrent job requests with the controlled number of concurrent * Requests. Verify that we get busy exception and appropriate message.  */
//  If the child has a different schema, we create a Project operator between them both,   as we cannot prune the columns in the GroupBy operator 
//  One serialized key for 1 or more rows for the duplicate keys. 
//  For backwards compatibility, since some threads used to be hard coded but only run if   frequency was > 0 
//  Sum all non-null double column values; maintain isGroupResultNull. 
//  Uses non-transactional table, cannot be considered 
// Inet4Address/Inet6Address 
//  Verify the ValidWriteIdList with one open txn on this table. Write ID of open txn should be invalid. 
//  no hash aggregations for group by 
//  it's set to to set our own conf value. 
//  equal to 3 
//  checking against the partition in question instead. 
/*   registrations and un-registrations will happen as and when tasks are submitted or are removed.  reference counting is likely required.  A connection needs to be established to each app master.  Ignore exceptions when communicating with the AM.  At a later point, report back saying the AM is dead so that tasks can be removed from the running queue.  Race: When a task completes - it sends out it's message via the regular TaskReporter. The AM after this may run another DAG,  or may die. This may need to be consolidated with the LlapTaskReporter. Try ensuring there's no race between the two.  Single thread which sends heartbeats to AppMasters as events drain off a queue.    */
//  End AggregateExpandDistinctAggregatesRule.java 
//  grouping(c1, c2, c3)   is equivalent to 
//  todo HIVE-3841   sampling on task running 
//  STAGE_COUNTERS 
//  Perform any bucket expressions.  Results will go into scratch columns. 
//  all returned type will be Text 
//  Index of the last seen delimiter in the given line 
//  1. We go over the expressions in the project operator      and we separate the windowing nodes that are result 
//  get nanos between [epoch at toZone] and [local time at toZone] 
//  update the pending state for now as we release this lock to take both. 
//  This means the DB name is null 
//  class HiveEndPoint 
//  the setChildren method initializes the object inspector needed by the operators   based on path and partition information. 
//  Knuth, "The Art of Computer Programming", 3rd edition, vol.II, Alg.D,   pg 272   D1. Normalize so high digit of D >= BASE/2 - that guarantee 
//  Probably an expression, cant handle that 
//  Same; with the termination after the failed update, we should maintain the correct count. 
//  Start the session in a fire-and-forget manner. When the asynchronously initialized parts of   the session are needed, the corresponding getters and other methods will wait as needed. 
//  Use the big table row as output. 
//  Also try creating a UGI object for the SPNego principal 
//  hcatalog specific configurations, that can be put in hive-site.xml 
/*    * a) Order predciates based on ndv in reverse order. b) ndvCrossProduct =   * ndv(pe0) * ndv(pe1) ^(1/2) * ndv(pe2) ^(1/4) * ndv(pe3) ^(1/8) ...    */
//  remove the current task from its original parent task's dependent task 
//  Walk over the input schema and copy in the output 
//  filter enabled, injection enabled, exception not expected 
//  The table is not bucketed, add a dummy filter :: rand() 
//  order does not matter below, so go wide 
//  Set the bucketing version 
//  Use LinkedHashMap to provide deterministic order 
//  This intentionally does not include interval types. 
//  Test an invalid case without version 
//  Re-escape any backtick (`) characters in the identifier. 
//  Merge the names from the imposed schema into the types   from the derived schema. 
//  Start after group keys. 
//  Defining a bunch of configs here instead of in HiveConf. These are experimental, and mainly   for use when retry handling is fixed in Yarn/Hadoop 
//  We couldn't do SQL filter pushdown. Get names via normal means. 
//  copy whole value for strings 
//  STATE 
//  Drop all partitions from "tbl1", "tbl2", "tbl3" and add 2 new partitions to "tbl4" and "tbl5" 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getDate(int)    */
//  Test boolean-valued (non-filter) IN expression for strings 
//  TimestampColumnArithmeticDateColumn.txt   TimestampScalarArithmeticDateColumn.txt   TimestampColumnArithmeticDateScalar.txt 
//  Default to driver's txn manager if no txn manager specified 
//  its a ddl query. 
//  ~ Static fields/initializers --------------------------------------------- 
// longer term we should always have a txn id and then we won't need to track locks here at all 
//  new entry in the hash table 
//  The Routing appender, which manages underlying appenders 
//  Give preference to TBLPROPERTIES over SERDEPROPERTIES   (really we should only use TBLPROPERTIES, so this is just   for backwards compatibility with the original specs). 
/*    * Flush a partially full deserializerBatch.   * @return Return true if the operator tree is not done yet.    */
//  Since the log length of the sql operation may vary during HIVE dev, calculate a proper maxRows. 
// use inspector to get a byte[] out of LazyBinary 
//  Add the request interceptor to the client builder 
//  Explode 
//  Add alias, table name, and partitions to hadoop conf so that their   children will inherit these 
//  overflow batch. 
/*  * Specialized class for doing a vectorized map join that is an left semi join on Multi-Key * using hash set.  */
/*      * from the prunedCols list filter out columns that refer to WindowFns or WindowExprs     * the returned list is set as the prunedList needed by the PTFOp.      */
//  If the expired nodes did not result in cache being cleanUntil% in size, 
//  We go through these hijinxes because java considers System.getEnv   to be read-only, and offers no way to set an env var from within   a process, only for processes that we sub-spawn. 
//  Monday 26th August 1985 12:00:00 AM 
//  If any aggregate functions do not support splitting, bail out 
//  3. We build the new predicate and return it 
//  Float family, timestamp are handled via descriptor based lookup, int family needs 
// directory is empty or doesn't have any that could have been produced by load data 
//  QL execution stuff 
// 1 partitions updated (and no other entries) 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#execute(java.lang.String, int)    */
//  TODO: relying everywhere on the magical constants and columns being together means ACID         columns are going to be super hard to change in a backward compat manner. I can         foresee someone cursing while refactoring all the magic for prefix schema changes.   Exclude the row column. 
//  We expect that colId will be the same for all (or many) SDs. 
//  Concurrent increase and revocation, increase fails - no revocation is needed. 
/*    * Represents a UDAF invocation in the context of a Window Frame. As   * explained above sometimes UDAFs will be handled as Window Functions   * even w/o an explicit Window specification. This is to support Queries   * that have no Group By clause. A Window Function invocation captures:   * - the ASTNode that represents this invocation   * - its name   * - whether it is star/distinct invocation.   * - its alias   * - and an optional Window specification    */
//  Make sure we capture the same metrics as Hadoop2 metrics system, via annotations. 
//  Relies on the fact that cache does not actually store these. 
// points at the last txn which we don't want to heartbeat 
//  Only trigger major compaction for ttp2 (delta.pct.threshold=0.5) because of the newly inserted row (actual pct: 0.66) 
//  Tamil Om U+0BD0 (3 bytes) 
//  Create temporary scratch dir 
//  All of this is completely bogus and mostly captures the following function:   f(output) = I-eyeballed-the(output) == they-look-ok.   It's pretty much a golden file... 
//  Next column has a similar name as previous, but different casing.   This is allowed in Druid, but it should fail in Hive. 
//  Need to be final to pass it to an inner class 
//  BlockingDeque methods 
//  flush if necessary 
//  No support. 
//  Read database, table via CachedStore 
//  Update the aggs 
//  flag to indicate if there is no data in parquet data page 
//  Create a standard copy of the object. 
/*    *  Pre-allocated members for storing information on single- and multi-valued-small-table matches.   *   *  ~ValueCounts   *                Number of (empty) small table values.   *  ~AllMatchIndices   *                (Logical) indices into allMatchs to the first row of a match of a   *                possible series of duplicate keys.   *  ~DuplicateCounts   *                The duplicate count for each matched key.   *    */
//  FUNCTION_NAME 
// next command should produce an error 
//     to merge it with its right child 
//  0 - Primary Key   1 - PK-FK relationship   2 - Unique Constraint   3 - Not Null Constraint 
/*        * Multiple values.        */
/*    * A PTF Input represents the input to a PTF Function. An Input can be a Hive SubQuery or Table   * or another PTF Function. An Input instance captures the ASTNode that this instance was created from.    */
//  This method can be replaced with Files.copy(source, target, REPLACE_EXISTING)   once Hive uses JAVA 7. 
//  Store text of the ORIGINAL QUERY 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getFunctions(org.apache.hive.service.cli.SessionHandle)    */
//  Otherwise, return null 
//  This is not called when building HashTable; we don't expect it to be called ever. 
//  ** Methods that need a data object ** 
//  At the end of this function, the stream should be pointing to the last token that   corresponds to the value being skipped. This way, the next call to nextToken   will advance it to the next field name. 
//  3, 8 
//  For LOAD, you only add it if it does exist as you might be loading an outdated MV 
// check behavior while change the order of columns 
//  ALTER MV ... REBUILD 
//  if it is a corVar, change it to be input ref. 
//  throw a HiveException for non-rcfile. 
//  The SIMD optimized form of "a != b" is "((a - b) ^ (b - a)) >>> 63" 
//  run sql operations 
//  Revalidated the new version after upgrade 
//  sparse-dense merge 
//  It's possible that the Job doesn't have the token in its credentials. In this case, unwrapAuthenticatinoToken 
//  Either we've found multiple big table branches, or the current branch cannot   be a big table branch. Disable mapjoin for these cases. 
//  cols 
//  We also need to update the expr so that the index query can be generated. 
// unique to the load func and input file name (table, in our case). 
//  The tasks from now on are more important than the candidate. 
//  Defining partition names in unsorted order 
//  Format a single cluster sort statement 
//  try to combine next level works recursively. 
//  Set the start and stop rows only if asked to 
//  We could just remove here and handle the missing tail during read, but that can be 
//  MAPPING 
//  LOG.info("Creating list record: copying " + lengthsLength + ", lrPtrOffset " + lrPtrOffset); 
/*    * Recursively extract fields from ExprNodeDesc. Deeply nested structs can have multiple levels of   * fields in them    */
//  TRUNCATE_PARTITION EVENT on partitioned table 
//  not setting ranges scans the entire table 
//  Capture stdout and stderr 
//  add to destination pool 
//  Get a list of indexes for which the columns in the schema are the same 
//  Multiple parents - find the right one based on the table alias in the parentExpr 
//  Check that an exception from getMetaData() is reported correctly 
//  class PartitionIterator; 
// r = runStatementOnDriver("explain formatted " + s); 
//  If there is only 1 table ALIAS, return it 
// if this column is coming from right input only then we update num nulls 
//  not qualify this optimization 
//  check for required fields 
//  Wrap the transport exception in an RTE, since UGI.doAs() then goes   and unwraps this for us out of the doAs block. We then unwrap one   more time in our catch clause to get back the TTE. (ugh) 
//  Thread-unsafe position used at write time. 
//  higher compute cost. 
//  No nulls case 
//  This query will give a runtime error 
//  Drop a primary key 
//  GET_PROGRESS_UPDATE 
//  get aggregation evaluators 
//  Remove a column 
//  set the local work, so all the operator can get this context 
//  SECRET 
//  create dispatcher and graph walker 
/*  mergeSum  */
//  Note fs.delete will fail on Windows. The reason is in OutputCommitter,   Hadoop is still writing to _logs/history. On Linux, OS don't care file is still   open and remove the directory anyway, but on Windows, OS refuse to remove a   directory containing open files. So on Windows, we will leave output directory   behind when job fail. User needs to remove the output directory manually 
//  I-(1/2)            D-(1/2)    I-(1/3)     U-(1/3)     D-(2/2)     I-(1/1) - new part 
//  Generate ReduceSinkOperator 
//  Finally check if it is serializable 
//  hadoop-1 gets 3 and hadoop-2 gets 0. *sigh* 
//  SCHEMA 
//  maximum table size 
//  Run initiator to execute CompactionTxnHandler.cleanEmptyAbortedTxns() 
//  Helper methods 
//  If the function is being added under a database 'namespace', then add an entity representing   the database (only applicable to permanent/metastore functions).   We also add a second entity representing the function name.   The authorization api implementation can decide which entities it wants to use to   authorize the create/drop function call. 
//  If rightInputRel is a filter and contains correlated   reference, make sure the correlated keys in the filter   condition forms a unique key of the RHS. 
//  Round up. 
// now recompute state since we've done minor compactions and have different 'best' set of deltas 
//  struct<map<k:v,k:v>_map<k:v,k:v>>> 
//  Given these 2 lines we don't need to double check later. 
/*      * load partition that doesn't exist in T     * There is some parallelism going on if you load more than 1 partition which I don't     * understand. In testImportPartitionedCreate2() that's reasonable since each partition is     * loaded in parallel.  Why it happens here is beyond me.     * The file name changes from run to run between 000000_0 and 000001_0 and 000002_0     * The data is correct but this causes ROW__ID.bucketId/file names to change      */
//  Copy the group key to output batch now.  We'll copy in the aggregates at the end of the group. 
//  START 
//  This does the testing using a remote metastore, as that finds more issues in thrift 
//  This string constant is used to indicate to AlterHandler that 
//  1.1 Build col details used by scan 
//  view column authorization again even if it is triggered again. 
//  Literal tinyint. 
//  Timer is shared across entire factory and must be released separately 
//  char columns should have correct display size/precision 
//  do rest of tests on db we just picked up above. 
// ensure there is partition dir 
//  in case of replication, idempotent is taken care by getTargetTxnId 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getClientInfo()    */
//  Test that existing exclusive partition with new shared_write coalesces to 
//  outer join cannot be performed on a table which is being cached 
//  If this Project has correlated reference, create value generator 
//  Return the type string of the first argument (argument 0). 
//  this is SEL(*) cols + UDTF cols 
//  Path being passed to us is a table dump location. We go ahead and load it in as needed.   If tblName is null, then we default to the table name specified in _metadata, which is good.   or are both specified, in which case, that's what we are intended to create the new table as. 
//  Test LazyHCatRecord init and read 
/*    * Map    */
//  we need to read bucket number which is the last column in value (after partition columns) 
// Codahale artifacts are lazily-created. 
//  Must be consistent with uncompressed stream seek in ORC. See call site comments. 
//  In the case of altering a table for its partitions we don't need to lock the table   itself, just the partitions.  But the table will have a ReadEntity.  So mark that   ReadEntity as no lock. 
//  get exception in resolving partition   it could be DESCRIBE table key   return null   continue processing for DESCRIBE table key 
//  We expect the DAGs to not be super large, so store full dependency set for each vertex to 
//  make 5 stripes with 2 rows each 
//  3. Add UDAF args deduped to reduce values 
//  not comparing hashCtx - irrelevant 
//  0 implies Netty default of 2 * number of available processors 
//  No boolean value match for extended 1 char field. 
//  Test when two jars are added with shared dependencies and one jar is deleted, the shared dependencies should not be deleted 
//  Caller must remember small value length. 
//  Must have one of those at this point. 
//  Write a large value. This should use a different byte buffer 
//  no rows will be filtered 
//  add parents for the newly created operator 
/*      * Called when the value in the partition has changed. Update the currentRank      */
//  If we get a UnionOperator, right now, we only handle it when   we can find correlated ReduceSinkOperators from all inputs. 
//  For eg: select count(1) from T where t.ds = .... 
//  2 original files, 2 delta directories, 1 delete_delta directory and 2 base directories 
//  Get the partition object if it already exists 
//  we errored 
//  This weirdness of setting it in our conf and then reading back does two things.   One, it handles the conversion of the TimeUnit.  Two, it keeps the value around for   later in case we need it again. 
//  Test that read can acquire after write 
//  initialize pathToAliases 
//  3. For uncompressed case, we need some special processing before read. 
//  id is appended since there could be multiple scalar subqueries and FILTER 
//  the positions in rsColInfoLst are as follows   --grpkey--,--distkey--,--values--   but distUDAF may be before/after some non-distUDAF,   i.e., their positions can be mixed.   so for all UDAF we first check to see if it is groupby key, if not is it distinct key 
//  main memory HashMap 
/*      * Basic algorithm:     *     * 1. Determine if rounding part is non-zero for rounding.     * 2. Scale away fractional digits if present.     * 3. If rounding, clear integer rounding portion and add 1.     *      */
//  VERSION 
//  scale/signum 
//  init aggregationClasses 
/*  Whether to skipPruning - depends on the payload from an event which may signal skip - if the event payload is too large  */
//  This won't have a decimal part because the HAS_DECIMAL_MASK bit is not set. 
//  COMBINING ACUTE ACENT U+0301 (2 bytes) 
//  may not be true with correlation operators (mux-demux) 
//  get the function documentation 
//  foo <= 'm' 
//  a bucket centered at 'v' already exists, so this must be checked in the next step. 
//  Verify that there is now only 1 new directory: base_xxxxxxx and the rest have have been cleaned. 
//  to use this via command line arg "decimal(7_2)"   to use this via command line arg "decimal(38_18)" 
//  if row count exists or stats aren't to be estimated return 
//  remember for additional processing later 
//  Timestamp values are PST (timezone for tests is set to PST by default) 
//  Delete the data in the database 
//  9. Handle select distinct as GBY if there exist windowing functions 
//  no means no 
//  Now we delete the rest of tables 
//  If table is missing, then partitions are also would've been dropped. Just no-op. 
/*    * Captures how the Input to a PTF Function should be partitioned and   * ordered. Refers to a /Partition/ and /Order/ instance.    */
//  need to check paths and partition desc for MapWorks 
// If we return false, it is just a noop 
/*  All must be selected otherwise size would be zero.         * Repeating property will not change.          */
//  We have some disk buffers... see if we have entire part, etc. 
//  will be invoked anyway in TezTask. Doing it early to initialize triggers for non-pool tez session. 
//  expecting this exception 
//  get the number of columns in the user's rows 
//  Verify that a task kill went out for all nodes running on the specified host. 
/*      * 7. Order Use in Order By from the block above. RelNode has no pointer to     * parent hence we need to go top down; but OB at each block really belong     * to its src/from. Hence the need to pass in sort for each block from     * its parent.     * 8. Limit      */
//  RawLocalFileSystem seems not able to get the right permissions for a local file, it   always returns hdfs default permission (00666). So we can not overwrite a directory   by deleting and recreating the directory and restoring its permissions. We should   delete all its files and subdirectories instead. 
//  This is an SMB join. 
//  List<Coord>   Coord holds two doubles 
//  optional string user = 7; 
//  NumberFormatException value out of range   other unknown cases 
//  Dummy create table command to mark proper last repl ID after dump 
/*  (non-Javadoc)   * In order to update a Decimal128 fast (w/o allocation) we need to expose access to the   * internal storage bytes and scale.   * @return    */
//  Add the UDTFOperator to the operator DAG 
/*        * Validate input formats of all the partitions can be vectorized.        */
//  this may happen if we were able to establish connection once, but its no longer valid 
//  If ckpt property not set or empty means, bootstrap is not run on this object. 
//  It also set up the connection between each parent work and child work. 
//  alter table add column: change the metadata 
//  Add another partition without stats. 
//  figure out if a table is Acid or not. 
//  close ResultSet, ignore exception if any 
//  set alias to work and put into smallTableAliasList 
// Validate that some progress is being made 
//  this means there is no existing partition 
//  if skip authorization, skip checking;   if it is inside a view, skip checking;   if authorization flag is not enabled, skip checking. 
//  Round with digits 
//  now set the output for the history 
//  wait until rj.isComplete 
//  of newAggRel 
//  Open the session if it is closed. 
//  These errors happen if the JNI lib is not available for your platform. 
//  Now add hivemetastore-site.xml.  Again we add this before our own config files so that the 
//  We need to carry the insideView information from calcite into the ast. 
//  Prefix for separate row keys 
//  The children after not, might need a cast. Get common types for the two comparisons.   Casting for 'between' is handled here as a special case, because the first child is for NOT and doesn't need 
//  Check for part of log message as well as part of progress information 
// The following code is for mapjoin 
//  null error message here means the user has access. 
/*    * INTERVAL_YEAR_MONTH.    */
//  disable resource monitoring, although it should be off by default 
// add the number of partitions given by the current batchsize 
//  rightInputRel has this shape:           Filter (references corvar)           FilterInputRel 
//  Configure web application contexts for the web server 
//  counters for task execution side 
//  lower case null is used within json objects 
//  nulls are considered not matching for equality comparison   add the position of the most recently inserted key 
/*      * All ROW__IDs are unique on read after conversion to acid     * ROW__IDs are exactly the same before and after compaction     * Also check the file name (only) after compaction for completeness      */
//  Base type name to PrimitiveTypeEntry map. 
//  this means the key didn't exist, so the insertion point is negative minus 1. 
//  if the aggregation buffer is not estimable then get all the   declared fields and compute the sizes from field types 
//  3. Allocate the buffers, prepare cache keys.   At this point, we have read all the CBs we need to read. cacheBuffers contains some cache   data and some unallocated membufs for decompression. toDecompress contains all the work we   need to do, and each item points to one of the membufs in cacheBuffers as target. The iter 
//  Header row + 2 transactions = 3 rows 
//  This key will be put in the conf file when planning an acid operation 
//  for fast check of possible existence of RS (will be checked again in SimpleFetchOptimizer) 
//  SettableStructObjectInspector 
//  Must support offsets to be able to split. 
//  Only applicable to n-way Hybrid Grace Hash Join 
//  Despite having a fixed schema from Hive, we have sparse columns in Accumulo 
//  Lookup type infos for our input types and expected return type 
//  Make one have a non-standard location 
//  In case of any other exception, retry. If this also fails, report original error and exit. 
//  Set the FS perms to read-only access, and create ACL entries allowing write access. 
//  We are skipping the CDS table here, as it seems to be totally useless. 
//  long NOT BETWEEN 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTime(int)    */
//  check that the columns referenced in rightJoinKeys form an 
/*      *  we analyze each side and let the     *    left and right exprs in the Conjunct object.     *     * @return Conjunct  contains details on the left and right side of the conjunct expression.      */
// Log with double base 
//  currently, this is sole field affecting mergee task 
/*  Unit test for GitHub Howl issue #3  */
//  Get all valid partition paths and existing partitions for them (if any) 
//  To the consumer of joinOutputProjRel, nullIndicator is located 
//  So far we've read the one's complement   add 1 to turn it into two's complement 
//  If the output has some extra fields, set them to NULL in convert(). 
//  If input contains header, skip header. 
//  selectObjs hold the row from the select op, until receiving a row from 
//  create task to aliases mapping and alias to input file mapping for resolver 
//  If not equal, convert all to double and compare 
//  Get the column names of the aggregations for reduce sink 
//  for auto reduce parallelism - minimum reducers requested 
// this will create delta_20_24 and delete_delta_20_24. See MockRawReader 
//  Provide a facility to set current timestamp during tests 
//  Try to get as consistent view as we can; make copy of the headers. 
//  recreate the partition if it existed before 
//     "char", 
//  The same applies to files added with "addFile". They're only guaranteed to be available 
//  even though the stats were estimated we need to warn user that   stats are not available 
//  No encoding => must have no data. 
//  With Data 
//  Add in fields used in the condition. 
//  describe the table - populate the output stream 
//  Recalculate the HDFS stats if auto gather stats is set 
//  the join key. 
//  skip rowid 
//  all of the integer types   float   double   string, char, varchar 
//  in order to make the dependencies accessible. 
//  gen would have prevented it) 
//  nothing to do here. 
//  Conversion of Avro primitive types to Hive primitive types   Avro             Hive   Null   boolean          boolean    check   int              int        check   long             bigint     check   float            double     check   double           double     check   bytes            binary     check   fixed            binary     check   string           string     check                    tinyint                    smallint 
//  constant means no filter, ignore it when it is null 
//  we might have to connect parent work with this work later. 
//  true   true   true   false   false   false   false   true 
//  Create a file with all the job properties to be read by spark-submit. Change the   file's permissions so that only the owner can read it. This avoid having the 
/*    * Get the list of tracking jobs.  These can be used to determine which jobs have   * expired.    */
//  implement RelOptRule   We override the rule in order to do union all branch elimination 
//  Create parameter converters 
//  we check if there is only one immediate child task and it is stats task 
//  UK_NAME 
//  use the multi-char delimiter to parse the lazy struct 
//  Here we recursively check:   1. whether there are exact one LIMIT in the query   2. whether there is no aggregation, group-by, distinct, sort by,      distributed by, or table sampling in any of the sub-query.   The query only qualifies if both conditions are satisfied.     Example qualified queries:      CREATE TABLE ... AS SELECT col1, col2 FROM tbl LIMIT ..      INSERT OVERWRITE TABLE ... SELECT col1, hash(col2), split(col1)                                 FROM ... LIMIT...      SELECT * FROM (SELECT col1 as col2 (SELECT * FROM ...) t1 LIMIT ...) t2);   
//  check semantic conditions 
//  tasks including all of dependencies. 
//  ptf node form is: ^(TOK_PTBLFUNCTION $name $alias?   partitionTableFunctionSource partitioningSpec? expression*)   ptf node guaranteed to have an alias here 
//  get bigKeysDirToTaskMap 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setTimestamp(int, java.sql.Timestamp)    */
//  it has already been initialized using hiveConf. 
/*    * Gets status of job form job id. If maximum concurrent job status requests are configured   * then status request will be executed on a thread from thread pool. If job status request   * time out is configured then request execution thread will be interrupted if thread   * times out and does no action.    */
// from Load Data into acid converted table 
//  We thought we had the entire part to cache, but we don't; convert start to   non-cached. Since we are at the first gap, the previous stuff must be contiguous. 
//  MY_ENUM_STRING_MAP 
// clean tables in default db 
//  always send secure cookies for SSL mode 
//  By default ArgumentCompletor is in "strict" mode meaning   a token is only auto-completed if all prior tokens   match. We don't want that since there are valid tokens 
//  clear CurrentFunctionsInUse set, to capture new set of functions 
//  Make CHAR and VARCHAR type info parsable. 
//  Set up context for readNextComplexField. 
//  Return false only occurred error when execution the sql and the sql should follow the rules 
//  For now. 
//  create a split for the given partition 
//  for each dynamically created DP directory, construct a full partition spec   and load the partition based on that 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#isClosed()    */
//  return value not checked due to concurrent access 
//  output debug info 
//  These tasks should have come from the same job. 
//  Create a view with name already exist. Just to verify if failure flow clears the added create_table event. 
/*    * List status jobs request. If maximum concurrent job list requests are configured then   * list request will be executed on a thread from thread pool. If job list request time out   * is configured then request execution thread will be interrupted if thread times out and   * does no action.    */
//  For all parents (other than the big table), insert a dummy store operator 
//  Clean up the databases 
//  Server args 
//  OBJECT_TYPE 
//  EVENTS 
//  create the project after GB. For those repeated values, e.g., select 
//  tracing down the operator tree from the table scan operator 
// note the enum names match field names in the struct 
//  the remaining parameters in definedRestrictedSet are starting parameter name 
//  getManifestAttribute ("Specification-Title"),   getManifestAttribute ("Implementation-Version"),   getManifestAttribute ("Implementation-ReleaseDate"),   getManifestAttribute ("Implementation-Vendor"),   getManifestAttribute ("Implementation-License"), 
//  How many levels of ancestors to keep in the stack during dispatching 
//  first, underflow is NOT an error in ANSI SQL Numeric.   CAST(0.000000000....0001 AS DECIMAL(38,1)) is "0.0" without an error. 
//  Need to run a Spark job to make sure the jar is added to the class loader. Monitoring   SparkContext#addJar() doesn't mean much, we can only be sure jars have been distributed 
//  test when third argument has nulls and repeats 
//  If this import is being done for replication, then this will be a managed table, and replacements   are allowed irrespective of what the table currently looks like. So no more checks are necessary. 
/*    * A flag to indicate whether to cancel the task when exception TimeoutException or   * InterruptedException or CancellationException raised. The default is cancel thread.    */
//  as the index. 
//  2. Insert MapSide RS 
//  The test table has 500 rows, so total query time should be ~ 2500ms 
//  Retrieve the MBean server 
//  query will try to add 64 more partitions to already existing 57 partitions but will get cancelled for violation 
//  running all the ZK servers 
//  Note: we could call HiveFileFormatUtils.getPartitionDescFromPathRecursively for MM tables.         The recursive call is usually needed for non-MM tables, because the path management         is not strict and the code does whatever. That should not happen for MM tables.         Keep it like this for now; may need replacement if we find a valid use case. 
// if here it means currently committing txn performed update/delete and we should check WW conflict 
//  Read-lock. Not updating any stats at the moment. 
// seconds 
//  read the whole file 
//  Publish configs for this instance as the data on the node 
/*       * Check.5.h :: For In and Not In the SubQuery must implicitly or      * explicitly only contain one select item.       */
//  2/31 to 2/29 
//  This should not happen unless we are evicting a lot at once, or buffers are large (so 
//  When we introduce a discrepancy to the state we give the task to an updater unless it   was already given to one.  If the updater is already doing stuff, it would handle the   changed state when it's done with whatever it's doing. The updater is not going to   give up until the discrepancies are eliminated. 
//  Find functions which name contains _to_find_ in the dummy database 
//  Events can start coming in the moment the InputInitializer is created. The pruner   must be setup and initialized here so that it sets up it's structures to start accepting events.   Setting it up in initialize leads to a window where events may come in before the pruner is   initialized, which may cause it to drop events. 
/*  If statistics for the column already exist use it.  */
//  Note that we need to call getResults for simple fetch optimization.   However, we need to skip all the results. 
//  compute distance and store it in sorted map 
//  @@protoc_insertion_point(class_scope:UpdateFragmentResponseProto) 
//  The sort order (ascending/descending) for each field. Set to true when descending (invert). 
//  Make sure we don't collide with the source files.   MM tables don't support concat so we don't expect the merge of merged files. 
//  create VertexGroup 
// case substring from index to the end 
//  sleep for expiry time, and then fetch again   sleep twice the TTL interval - things should have been cleaned by then. 
//  Walk through the AST. 
//  The process has not logged in using keytab   this should be a test mode, can't use keytab to authenticate   with zookeeper. 
// ========================== 20000 range starts here ========================// 
//  Check job config for overrides, otherwise use the default server value. 
//  FastBitSet, rather than using 31 integers. 
//  the user tries to actually use this session and fails, proceeding to return/destroy it. 
// insert into newTableName select * from ts <where partition spec> 
/*  these are things that goes through singleton initialization on most queries  */
//  One last check. 
//  IS_TBL_LEVEL 
//  Verify that getNextNotification(last) returns zero events if there are no more notifications available 
//  nulls 
//  then OR the current node with the previous one 
//  -5 years + 121 months = -5 years + 10 years + 1 month = 5 years 1 month 
//  Create map of source file system to destination path to list of files to copy 
//  remove ',' at begining 
//  Qualified column access for which table was not found 
//  write out the plan to a local file 
//  Change the resource plan, so that the session gets killed. 
//  Get the updated path list 
//  set really low batch size to ensure batching 
//  unpartitioned table 
//  Verify that partition_1 was added correctly, and properties were inherited from the HCatTable. 
//  if some application depends on the original value being set. 
/*    * Note that when serializing a row, the logical mapping using selected in use has already   * been performed.  batchIndex is the actual index of the row.    */
//  Get task detail link from the jobtask page 
//  Project the column corresponding to the distinct aggregate. Project   as-is all the non-distinct aggregates 
//  Since the start is 0, and the length is 100, the first call to sync should be with the value   50 so return that for getPos() 
/*    * Relative start position of the windowing. Can be negative.    */
//  initialize HCatInputFormat 
//  Infix operator 
//  Rounding numbers that increase int digits 
/*   * Impl note: Hive provides authorization with it's own model, and calls the defined  * HiveAuthorizationProvider from Driver.doAuthorization(). However, HCat has to  * do additional calls to the auth provider to implement expected behavior for  * StorageDelegationAuthorizationProvider. This means, that the defined auth provider  * is called by both Hive and HCat. The following are missing from Hive's implementation,  * and when they are fixed in Hive, we can remove the HCat-specific auth checks.  * 1. CREATE DATABASE/TABLE, ADD PARTITION statements does not call  * HiveAuthorizationProvider.authorize() with the candidate objects, which means that  * we cannot do checks against defined LOCATION.  * 2. HiveOperation does not define sufficient Privileges for most of the operations,  * especially database operations.  * 3. For some of the operations, Hive SemanticAnalyzer does not add the changed  * object as a WriteEntity or ReadEntity.  *  * @see https://issues.apache.org/jira/browse/HCATALOG-244  * @see https://issues.apache.org/jira/browse/HCATALOG-245   */
//  No need to kill anything. 
//  If currTask is rootTasks, remove it and add its children to the rootTasks which currTask is its only parent   task 
//  W/ size 0, query will fail, but at least we'd get to see the query in debug output. 
//  Do not merge if we do not know how to connect two operator trees. 
//  If all arguments are of known length then we can keep track of the max   length of the return type. However if the return length exceeds the 
//  create operator info list to return 
//  The source column names for ORC serde that will be used in the schema. 
//  If value is null, the type should also be VOID. 
//  setup the compression codec 
//  Do NOTHING if output is NULL. 
//  construct outer struct 
//  Create two tables one as user "foo" and other as user "bar" 
/* now both batches have committed (but not closed) so we for each primary file we expect a side    file to exist and indicate the true length of primary file */
//  sql std authorization is managing privileges at the table/view levels   only   ignore partitions 
//  Remove any semijoin branch associated with hashjoin's parent's operator 
//  assumptions about the range of numeric data being analyzed. 
//  preceeding work must be set to the newly generated map 
//     Assert.assertEquals("Comparing Parameters(totalSize)", "0",          createdTable.getParameters().get("totalSize"));      Assert.assertEquals("Comparing Parameters(numFiles)", "0", 
// skip first child since it is struct 
// second "split" is delta_200_200 
//  reloadable jars 
//  Cleanup the synthetic predicate in the tablescan operator by   replacing it with "true" 
/*    * The storage arrays for this column vector corresponds to the storage of a HiveIntervalDayTime:    */
/*      * DOUBLE: Min and max.      */
//  allocate a temporary output dir on the location of the table 
//  batches will be sized 12,4,1 
//  In checking if a mapjoin can be converted to bucket mapjoin, 
//  be used, because that is the correct move task for the "merge and move" use case. 
//  The input file has changed - every operator can invoke specific action 
//  Round to the specified number of decimal places using the standard Hive round function. 
//  Base64 encoded and stringified token for server 
/*            * {Rel Offset Word} [Big Value Len] [Next Value Small Len] {Value Bytes}           *           * 2nd and beyond records have a relative offset word at the beginning.            */
//  access the fields 
//  HBase input formats are not thread safe today. See HIVE-8808. 
//  It is guaranteed there is only 1 list within listBucketCols. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setUnicodeStream(int, java.io.InputStream,   * int)    */
//  Largest size allowed in smallBuffer 
//  Disable auth so the call should succeed 
//  Do nothing as this was fixed by MAPREDUCE-1447. 
//  1. Insert five rows to Non-ACID table. 
/*   * (non-Javadoc)  *  * @see  * org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider  * #authorize(org.apache.hadoop.hive.ql.metadata.Partition,  * org.apache.hadoop.hive.ql.security.authorization.Privilege[],  * org.apache.hadoop.hive.ql.security.authorization.Privilege[])   */
//  FULL_TABLE_NAMES 
//  Cap at fact-row-count, because numerical artifacts can cause it 
//  Send the bucket IDs associated with the tasks, must happen after parallelism is set. 
//  there are no elements in the union 
/*        * once the source node is reached; stop traversal for this QB        */
//  There should be one base dir in the location 
//  print the stack from all threads for debugging purposes 
/*    * Type of job request.    */
//  Network cost of map side join 
//  Spill the big table rows into appropriate partition:   When the JoinResult is SPILL, it means the corresponding small table row may have been   spilled to disk (at least the partition that holds this row is on disk). So we need to 
//  Build map to not convert multiple times, further remove already included predicates 
//  Set the location in the StorageDescriptor 
//  as HiveServer2 output is consumed by JDBC/ODBC clients. 
//  we need to know if it is COUNT since this is specialized for IN/NOT IN   corr subqueries. 
//  Remove the value of every key found matching 
//  convert each key-value-map to appropriate expression. 
// this should not schedule a new compaction due to prior failures, but will create Attempted entry 
//  Add tbl_id and empty bitvector 
//  Means user specified a table, not a partition 
//  convert to RequiredPrivileges 
//  Reserve 4 bytes for the hash (don't just reserve, there may be junk there) 
//  otherwise GBevaluator and expr nodes may get shared among multiple GB ops 
//  miniHS2_2 will continue to be leader 
//  LAST_ANALYZED 
// ************************************************************************************************   Emulate SerializationUtils Deserialization used by ORC. 
//  Translate the double into sign, exponent and significand, according 
// return true if both are null, or false if one is null and the other isn't 
//  Replicate the remaining INSERT OVERWRITE operations on the table. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceTest#tearDown()    */
//  add the select operator 
//  For each BaseWork with MJ operator, we build a SparkWork for its small table BaseWorks 
/*         each Partition may have different I/O Format so have to check them all before deciding to        make a full CRUD table.        Run in batches to prevent OOM        */
// ************************************************************************************************   Decimal Math. 
//  else do common code 
//  skip the driver and directly loadable tables 
/*    * Wait time in milliseconds before another cancel request is made.    */
// 1,2,3,4 
//  Find the parsed deltas- some of them containing only the insert delta events 
//  If the "strict" mode is on, we have to provide partition pruner for each table. 
//  Test gt/lt/lte/gte/like for strings. 
//  7. Perform Logical optimization 
//  data array and masks array are then traversed together and checked for corresponding set bits. 
//  5. Update the maps   NOTE: Output RR for SortRel is considered same as its input; we may   end up not using VC that is present in sort rel. Also note that   rowtype of sortrel is the type of it child; if child happens to be   synthetic project that we introduced then that projectrel would 
//  eliminate stripes that doesn't satisfy the predicate condition 
//  destination on disk 
//  MM tables need custom handling for union suffix; DP tables use parent too. 
//  The same umbilical is used by multiple tasks. Problematic in the case where multiple tasks 
//  4) Return new operator 
//  If a third parameter has been specified, it should be an integer that specifies the number 
/*    * parse a String as a Select List. This allows table functions to be passed expression Strings   * that are translated in   * the context they define at invocation time. Currently used by NPath to allow users to specify   * what output they want.   * NPath allows expressions n 'tpath' a column that represents the matched set of rows. This   * column doesn't exist in   * the input schema and hence the Result Expression cannot be analyzed by the regular Hive   * translation process.    */
// FileOutputFormat.getWorkOutputPath takes TaskInputOutputContext instead of  TaskAttemptContext, so can't use that here 
//  Reset everything for the next arena; assume everything has been cleaned. 
/*                * Multi-Key specific save key.                */
//  create a deeply nested table which has more partition keys than the pool size 
//  3) Create ROW_ID column in select clause from left input for the RIGHT OUTER JOIN.   This is needed for the UPDATE clause. Hence, we find the following node:   TOK_QUERY     TOK_FROM        TOK_RIGHTOUTERJOIN           TOK_SUBQUERY              TOK_QUERY                 ...                 TOK_INSERT                    ...                    TOK_SELECT   And then we create the following child node:   TOK_SELEXPR      .         TOK_TABLE_OR_COL            cmv_mat_view 
//  create an empty file (which is not a valid rcfile) 
//  Smile mapper is used to read query results that are serilized as binary instead of json 
//  we can still fold, since here null is equivalent to false. 
//  1 second 
//  nesting level limits 
//  interceptor for adding username, pwd 
//  Test VARCHAR literal to string column comparison 
//  if all left data in small tables are less than and equal to the left data   in big table, let's them catch up 
//  has nulls, not repeating 
//  Use Case 6. 
// ~ Static fields/initializers --------------------------------------------- 
//  Vectorization only supports PRIMITIVE data types. Assert the same 
/*        * Multi-Key check for repeating.        */
//  Read permission/no permissions, or the expected user. 
/*      * 2. build Map-side Op Graph. Graph template is either:     * Input -> PTF_map -> ReduceSink     * or     * Input -> ReduceSink     *     * Here the ExprNodeDescriptors in the QueryDef are based on the Input Operator's RR.      */
//  Hive assumes that user names the files as per the corresponding   bucket. For e.g, file names should follow the format 000000_0, 000000_1 etc.   Here the 1st file will belong to bucket 0 and 2nd to bucket 1 and so on. 
//  flatten AND 
//  Re-use an existing, available column of the same required type. 
//  Test capability for tests. 
//  This code, with branches and all, is not executed if there are no string keys 
//  modifiable 
//  test with nulls in input 
//  there were no grp and perms to begin with. 
//  We start: 
//  actualBatchSize is half of batchSize when 1 exception is expected 
//  set columnAccessInfo for ViewColumnAuthorization 
//  PART_ARCHIVE_LEVEL 
//  optional .VertexOrBinary work_spec = 1; 
//  Create a new open session request object 
//  Zero(es). 
//  Use try .. finally to cleanup temp file if something goes wrong 
//  All alters done, now we replicate them over. 
//  Delete the original node... 
// per acid write to test nonAcid2acid conversion mixed with load data 
//  Sort columns are not allowed for full ACID table. So, change it to insert-only table 
//  Current value. 
//  Vector SerDe can be disabled both on client and server side. 
//  More than TS operator 
//  if length of (prefix/ds=__HIVE_DEFAULT_PARTITION__/000000_0) is greater than max key prefix 
// create 1 row in a file 000000_0_copy_2 
//  First delete any MVs to avoid race conditions 
//  Determine the temp table path 
//  update for "{\"writeid\":1,\"bucketid\":536936448,\"rowid\":0}\t1\t1\t1\t" 
//  Use the cache rather than full query execution.   At this point the caller should return from semantic analysis. 
//  of the specified operator class 
//  Reuse the partition specs from dest partition since they should be the same 
//  optional string application_id_string = 1; 
//  Check to see if we have seen this request before.  If so, ignore it.  If not,   add it to our queue. 
// so that we can test "old" files 
//  LOG.debug("generateHashMultiSetResultSingleValue enter..."); 
//  In future, this may examine context to return appropriate HCatWriter 
//     Driver driver = new Driver(conf);      SessionState.start(new CliSessionState(conf)); 
//  In case of test, if we might not want to remove the log directory 
// "no inputs"; // Cannot use with input formats. 
// make 2 more inserts so that we have 000000_0_copy_1, 000000_0_copy_2 files in export 
//  We fill starting with highest digit in highest longword (HIGHWORD_DECIMAL_DIGITS) and   move down.  At end will will shift everything down if necessary. 
//  Use Case 12. 
//  Multi-column.  Also covers table in non-default database 
//  (\uD867\uDE3D is Okhotsk atka mackerel in Kanji). 
/*            * Multi-Key outer null detection.            */
/*    * Input array is used to fill the entire size of the vector row batch    */
//  Drop any constraints on the table 
//  Format a sorted by statement 
//  descriptor. 
//  Restore the context. 
//  fetch operator 
//  scriptOperator to echo the output of the select 
// ------------------------------------------------------------- 
//  Have that the NULL does not interfere with the current equal key series, if there   is one. We do not set saveJoinResult.        Let a current MATCH equal key series keep going, or      Let a current SPILL equal key series keep going, or      Let a current NOMATCH keep not matching. 
//  if a table was created in a user specified location using the DDL like   create table tbl ... location ...., it should be treated like an external table   in the table rename, its data location should not be changed. We can check   if the table directory was created directly under its database directory to tell   if it is such a table 
/*  Sets the field and tag in the union. Returns the union.  */
//  is sorted. 
/*    * Maximum number of times a cancel request is sent to job request execution   * thread. Future.cancel may not be able to interrupt the thread if it is   * blocked on network calls.    */
//  The ACID state is probably absent. Warning is logged in the get method. 
//  if we pushed the predicate into the table scan we need to remove the 
//  use that task 
//  Druid timestamp column 
//  if log4j configuration file not set, or could not found, use default setting 
//  HTTP today, but might not be 
//  Use defaults... Partitions are put in the table directory. 
/*        * Restriction.8.m :: We allow only 1 SubQuery expression per Query.        */
//  only needed when grouping sets are present 
//  Test that existing shared_read db with new shared_write coalesces to 
//  glorified cast from Iterable<TBase> to Iterable<Partition> 
//  All tables good on destination, drop on source. 
//  So, min(txn_id) would be a non-zero txnid. 
//  get all join columns from join keys 
//  Make sure we only return keyDecompressor once. 
//  Filters don't change the column names - so, no need to do anything for them 
/*    * Can this mapjoin be converted to a bucketed mapjoin ?   * The following checks are performed:   * a. The join columns contains all the bucket columns.   * b. The join keys are not transformed in the sub-query.   * c. All partitions contain the expected number of files (number of buckets).   * d. The number of buckets in the big table can be divided by no of buckets in small tables.    */
//  No location expected with AccumuloStorageHandler 
//  fall through 
//  Check Vectorized ORC reader against ORC row reader 
//  Value becomes null for rounding beyond. 
//  Open a txn to be tested for ValidWriteIdList. Get the ValidTxnList during open itself.   Verify the ValidWriteIdList with no open/aborted write txns on this table. 
//  Now we are handling exact types. Base implementation handles type promotion. 
//  required   required   required   optional   optional   required   optional   optional 
//  We don't support masking/filtering against ACID query at the moment 
//  update file sink descriptor 
//  move on to process to the next parsedDelta. 
//                    1234567890123456789012345678901234567.8                              1         2         3 
//  default threshold for using main memory based HashMap 
//  Prefix for primary row keys 
//  For MapWork, getAllRootOperators is not suitable, since it checks   getPathToAliases, and will return null if this is empty. Here we are 
//  And finally cache policy uses cache to notify it of eviction. The cycle is complete! 
//  Create new Project-Sort-Project sequence 
//  This request at a lower priority should not affect anything. 
/*  * Interface for a vector map join hash table (which could be a hash map, hash multi-set, or * hash set) for a single long.  */
//  add aux jars 
// delete something, but make sure txn is rolled back 
//  Set the config value to 2 catalogs other than hive 
/*          * Single-Column String specific repeated lookup.          */
//  Create the struct if needed 
//  context.addFilter(Utils.getXSRFFilterHolder(null, null), "/" ,   FilterMapping.REQUEST);   Filtering does not work here currently, doing filter in ThriftHttpServlet 
//  Initialization fails and so does the retry, no resource plan change. 
//  Not needed anymore. 
//  zero length key is not allowed by block compress writer, so we use a byte   writable 
//  The size to flush the string buffer at 
//   TODO: Fix LoadPartitionDoneEvent. Currently, LPDE can only carry a single partition-spec. And that defeats the purpose. 
//  outside the inner loop results in NPE/OutOfBounds errors 
// release locks from "select a from T7" - to unblock hte drop partition  retest the the "drop partiton" X lock 
//  We skipped over leading 0x00s and 0xFFs 
//  If the expr is column op const, will try to cast the const to string   according to the data type of the column 
//  No need to check finishable here; if it was set it would already be in the queue. 
//  Perform some checks on whether the node will become available or not. 
//  Since we're only creating a view (not executing it), we don't need to   optimize or translate the plan (and in fact, those procedures can 
//  This will throw an expected exception since client-server modes are incompatible 
//  we didn't find case or when udf 
//  delimited way. 
//  Set this if we encounter a condition we were not expecting. 
//  BINARY_COLUMNS 
//  mr 
//  Note that this will be invoked in 2 cases:      1) DirectSQL was disabled to start with;      2) DirectSQL threw and was disabled in handleDirectSqlError. 
//  Note: no location check here; the buffer is always locked for move. 
//  As of HIVE-8745, these decimal values should be trimmed of trailing zeros. 
//  Null last (default for descending order) 
//  Calcite always needs the else clause to be defined explicitly 
//  Change all the linked file sink descriptors 
//  Also, we prefer a missed heartbeat over a stuck query in case of discrepancy in ET. 
//  Create a directory with no permissions 
//  TableDesc#getDeserializer will ultimately instantiate the AccumuloSerDe with a null   Configuration   We have to accept this and just fail late if data is attempted to be pulled from the   Configuration 
//  new *hive-site.xml file 
//  Empty java opts 
// Remove any virtual cols 
//  2^62 
//  Scaling down may have opened up trailing zeroes... 
//  Vectorized implementation of ROUND(Col, N) function 
//  first field always starts from 0, even when missing 
//  ASCENDING 
//  New operators 
//  we have the dag now proceed to get the splits: 
//  need to capture the timing 
//  begin to walk through the task tree. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.operation.Operation#close()    */
//  Verify mergeOnlyTask is NOT optimized (a merge task writes directly to finalDirName, then a MoveTask is executed) 
//  Remove operator and combine 
//  Each headers[i] is a "virtual" byte at i * minAllocation. 
//  NOTE: here we should use the new partition predicate pushdown API to get a list of   pruned list, 
//  CMS   Parallel GC   G1GC   other vendors like IBM, Azul etc. use different names 
//  Hive QueryId is not always unique. 
//  Make sure the user has not requested an insane amount of txns. 
//  Execute one instruction; terminate on executing a script if there is an error   in silent mode, prevent the query and prompt being echoed back to terminal 
/* create delta_1_1_0/bucket0 with 1 row and close the file */
//  1. Gen Optimized AST 
//  web port cannot be obtained 
//  refs. These comparisons are AND'ed together. 
/*    * Uses generic JDBC escape functions to add a limit clause to a query string    */
//  Don't need to recordShuffleInfo since the out of sync unregister will not remove the   credentials 
//  DemuxOperator forwards a row to exactly one child in its children list   based on the tag and newTagToChildIndex in processOp() method.   So we need not to do anything in here. 
/*  @bgen(jjtree) Struct  */
//  compact ttp2, by running the Worker explicitly, in order to get the reference to the compactor MR job 
//  Can there be no ACLs? There's some access (to get ACLs), so assume it means free for all. 
//  block to make sure kill happened successfully 
// map join dump file name 
//  same instance of Driver, which can run multiple queries. 
//  Increase qp, check that the pool grows. 
//  We may need to do linear interpolation to get the exact percentile 
/*  10 files x 1000 size for 9 splits  */
//  Do this only on your own peril, and never in the production code 
//  DateColumnArithmeticTimestampColumn.txt   DateScalarArithmeticTimestampColumn.txt   DateColumnArithmeticTimestampScalar.txt 
//  Marker to track if the previous character was an escape character 
//  The default setting is "throw"; assume doValidate && !doSkip means throw. 
//  Not doing any check 
//  Execute extended optimization   For each RS, check whether other RS in same work could be merge into this one.   If they are merged, RS operators in the resulting work will be considered 
//  Signature for wrapped storer, see comments in LoadFuncBasedInputDriver.initialize 
//  Constant node 
/*    * - a subclass must provide this method.   * - this method is invoked during translation and also when the Operator is initialized during runtime.   * - a subclass must use this call to setup the shape of its output.   * - subsequent to this call, a call to getOutputOI call on the {@link TableFunctionEvaluator} must return the OI   * of the output of this function.    */
//  Remove if the path is not present 
//  In case of multi-table insert, the path to alias mapping is needed for   all the sources. Since there is no   reducer, treat it as a plan with null reducer 
//  It can be optimized later so that an operator operator (init/close) is performed   only after that operation has been performed on all the parents. This will require   initializing the whole tree in all the mappers (which might be required for mappers   spanning multiple files anyway, in future) 
/*  (non-Javadoc)   * This provides a LazyInteger like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  Calculate complete dynamic-multi-dimension collection. 
/*      * 3. convert filterNode      */
/*         We already handled all delete deltas above and there should not be any other deltas for        any table type.  (this was acid 1.0 code path).          */
//  This VectorExpression (IN) is a special case, so don't return a descriptor. 
//  Find the biggest reduce sink 
//  inputTs is the year/month/day/hour/minute/second in the local timezone.   For this UDF we want it in the timezone represented by fromTz 
//  For each component in this lock request,   add an entry to the txn_components table 
//  Setup exprNode 
//  getCanonicalHostName would either return FQDN, or an IP. 
//  HTTP Server 
//  Top level query 
//  used to group dependent tasks for multi table inserts 
// "select * from tab1" txn 
// //////////  Bootstrap   //////////// 
//  unfortunately making prunedPartitions immutable is not possible   here with SemiJoins not all tables are costed in CBO, so their 
//  Use milliseconds parser if pattern matches our special-case millis pattern string 
/*      * ============================ [PERF] ===================================     * This function is called for every row. Setting up the selected/projected     * columns at the first call, and don't do that for the following calls.     * Ideally this should be done in the constructor where we don't need to     * branch in the function for each row.     * =========================================================================      */
//  Don't send the parsedDbName, as this method will parse itself. 
/*    * Close.    */
//  If a operator wants to do some work at the beginning of a group 
//  MODIFIER LETTER SMALL X U+02E3 (2 bytes) 
//  3) If too many sessions are outstanding (e.g. due to expiration restarts - should      not happen with in-use sessions because WM already kills the extras), we will kill 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#executeUpdate(java.lang.String, int[])    */
//  Try to fold, otherwise return the expression itself 
//  The mapping from the index of a child operator to its corresponding 
//  For instance, this is the case when we are creating the table. 
//  Create currJobContext the latest so it gets all the config changes 
//  Only stored to update based on the original in fixTmpPath.   Only stored to update based on the original in fixTmpPath. 
//  execute() of Prepared statement 
//  a column family 
//  If it multiple level of folder are there fs.rename is failing so first   create the targetpath.getParent() if it not exist 
//  Test class to read a series of values to the designated input stream 
//  This implementation of vectorized JOIN is delegating all the work   to the row-mode implementation by hijacking the big table node evaluators   and calling the row-mode join processOp for each row in the input batch.   Since the JOIN operator is not fully vectorized anyway at the moment   (due to the use of row-mode small-tables) this is a reasonable trade-off. 
//  generates grouping set   position of grouping set, generally the last of keys   declared grouping set values   bitsets acquired from grouping set values 
/*        * We have a field and are positioned to it.  Read it.        */
//  Now we create the filter with the transactions information.   In particular, each table in the materialization will only have contents such that:   ROW_ID.writeid <= high_watermark and ROW_ID.writeid not in (open/invalid_ids)   Hence, we add that condition on top of the source table.   The rewriting will then have the possibility to create partial rewritings that read   the materialization and the source tables, and hence, produce an incremental 
//  Since we cannot merge operators into a single MR job from here,   we should remove ReduceSinkOperators added into walked in exploitJFC 
//  Where the log files wll be written 
//  Original bucket files and delta directory should have been cleaned up. 
//  Finally add the fixed acid key index. 
//  This method finds any columns on the right side of a set statement (thus rcols) and puts them 
//  Is there a select following   Clone the select also. It is useful for a follow-on optimization where the union 
// getDataSize tries to estimate stats if it doesn't exist using file size   we would like to avoid file system calls  if it too expensive 
//  Cache settings will need to be setup in llap-daemon-site.xml - since the daemons don't read hive-site.xml 
//  Floor on an integer argument is a noop, but it is less code to handle it this way. 
//  perform HA upgrade 
// if a lock is associated with a txn we can only "unlock" if if it's in WAITING state   which really means that the caller wants to give up waiting for the lock 
//  Sum all non-null decimal column values for avg; maintain isGroupResultNull; after last row of   last group batch compute the group avg when sum is non-null. 
//  if we get some non-literals, we need to punt 
//  try again with some different data values and divisor 
//  SLOW: Do remainder with BigInteger. 
//  We will load into MM directory, and hide previous directories if needed. 
//  currently only handles one input input 
//  call-5: open - mock:/mocktbl/0_1 
// -----------------------------------------------------------------------------------------------   Value conversion methods.  ----------------------------------------------------------------------------------------------- 
//  these are mostly copied from the root pom.xml 
//  We will be returning a Text object 
//  reset 
//  For grouping sets, add a dummy grouping key 
//  verify that some dummy param can be set 
// ---------------------------------------------------------------------------   Process Single-Column Long Outer Join on a vectorized row batch.   
//  If we do, we cannot merge, as we would end up with a cycle in the DAG. 
//  Exception from the RPC layer - communication failure, consider as KILLED / service down. 
//  End of string. 
//  maxPosition is the 1.0 percentile 
//  create tables as user1 
//        need to make sure we don't get two write IDs for the same table. 
//  Another small write. smallBuffer should be re-used for this write 
/*  Dynamic partition pruning is enabled only for map join   * hive.spark.dynamic.partition.pruning is false and   * hive.spark.dynamic.partition.pruning.map.join.only is true    */
//  if the offset is bigger than our current number of fields, grow it 
//  Step 2.1: Connect the operator trees of two MapRedTasks. 
//  if we already found a variable, this isn't a sarg 
//  This will replace the old value if there is one   Overwrite the existing file 
/*  vertex is waiting for input/slots or complete  */
//  TODO: pause fetching 
//  Verify the scenario when maxProbeSize is a very small value, it doesn't fail 
//  rename un-managed files to conform to Hive's naming standard   Example:   /warehouse/table/part-m-00000_1417075294718 will get renamed to /warehouse/table/.hive-staging/000000_0   If staging directory already contains the file, taskId_copy_N naming will be used. 
//  Register with the AMReporter when the callable is setup. Unregister once it starts running. 
//  If it exists, we want this to be an error condition. Repl Load is not intended to replace a   db. 
//  0/0 for entry 0 should work but generate NaN 
//  k1 not equals k3 
//  This method is used to traverse the DAG created in tasks list and add the dependent task to 
//  Given a candidate map-join, can this join be converted.   The candidate map-join was derived from the pluggable sort merge join big 
//  Set the updated fetch size from the server into the configuration map for the client 
//  First, see if we have sessions that we were planning to restart/kill; get rid of those. 
//  if we're generating the splits in the AM, we just need to set 
//  When the next value is small it was not recorded with the old (i.e. next) value and we 
//        synchronized. Best-effort to display the queue in order. 
// ************************************************************************************************   Emulate SerializationUtils Serialization used by ORC. 
//  cannot hold all map tables in memory. Cannot convert. 
//  RS then ReduceSinkDeDuplication optimization should merge them 
// Else, recurse to the children. 
//  Return the wrapper of the root node 
//  Deserialize data to column values, and populate the row record 
//  org.apache.thrift.protocol.TJSONProtocol.class.getName()); 
//  mark if agg produces count(*) which needs to reference the 
/*  256 files x 1000 size for 111 splits  */
//  Old containers which are likely shutting down, or new containers which   launched between YARN Service status/diagnostics. Skip for this iteration. 
//  Create the tez tmp dir and a directory for Hive resources. 
//  TBL_TYPES 
//  string NOT BETWEEN 
//  Given that we are trying to reuse, this session MUST be in some pool.sessions.   Kills that could have removed it must have cleared sessionToReuse. 
//  rcfile write 
//  Thread pool for taking entities off the wait queue. 
//  Test string + double 
//  Expected error: should throw java.security.cert.CertificateException 
//  ip address size check - check for something better than non zero 
//  return true when the child type and the conversion target type is the 
//  1) Get valid txn list. 
//  split, do not include ; itself 
//  Ignore exceptions from stop 
//  one-time setup to make query able to run with Tez 
//  Set high worker count so we get a longer queue. 
//  Should we get all partitions for a partitioned table? 
// if HiveConf has changed, new object should be returned 
//  By default, we need the results from dropPartitions(); 
//  A setVal with the same function signature as rightTrim, leftTrim, truncate, etc, below. 
//  this vertex has multiple reduce operators 
//  verify invalid column error 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setReadOnly(boolean)    */
//  Create client 
/*      * use an array instead of only one object in case in future hive does not do     * the byte copy.      */
//  Special case for unions. These items translate to VertexGroups 
//  TODO HIVE-14042. Cleanup may be required if exiting early. 
//  Note: this is a recursive struct 
//  This should only be true for copy tasks created from functions, otherwise there should never 
//  when we have partial partitions specification we must assume partitions   lie in standard place - if they were in custom locations putting   them into one archive would involve mass amount of copying   in full partition specification case we allow custom locations 
//  Someone is not done.   Both user and the kill have returned. 
//  Map from PrimitiveTypeInfo to AbstractPrimitiveWritableObjectInspector. 
//  Schedule a task - it should get the only duck; the 2nd one at the same pri doesn't get one.   When the first one finishes, the duck goes to the 2nd, and then becomes unused. 
// -----------------------------------------------------------------------------------------------   Mutate operations.  ----------------------------------------------------------------------------------------------- 
//  2. Get Table Metadata 
//  Adding them to restricted list. 
//  Set this to read because we can't overwrite any existing partitions 
//  Case 3.1 - Max in list members: 1000, Max query string length: 1KB, and exact 1000 members in a single IN clause 
//  Make the union operator 
//  no stats/indexes 
//  Field length is difference between positions hence one extra. 
//  source table scan 
//  NOTE: we use JavaPrimitiveObjectInspector instead of   StandardPrimitiveObjectInspector 
//  for consistency with tables. 
//  These values come from setValueResult when it finds a key.  These values allow this 
//  called at runtime to initialize the custom edge. 
//  want to isolate any potential issue it may introduce. 
//  Look for an unlikely database name and see if either MetaException or TException is thrown 
//  we've reached our limit, throw the last one. 
//  since this test runs on local file system which does not have an API to tell if files or   open or not, we are testing for negative case even though the bucket files are still open 
//  The same id as reported by TaskRunnerCallable.getRequestId 
//  months) produces a type date via a calendar calculation. 
// create delta_0002_0002_0000 (can't push predicate) 
/*  we don't want to cancel the delegation token if we think the callback is going to     to be retried, for example, because the job is not complete yet  */
/*    * Config name used to find the number of concurrent requests.    */
// do we need to add a connection status listener?  What will that do? 
/*    * (non-Javadoc)   *   * @see java.sql.Wrapper#isWrapperFor(java.lang.Class)    */
//  this should be the case only if this is a create partition.   The privilege needed on the table should be ALTER_DATA, and not CREATE 
//  Use TCompactProtocol to read serialized TColumns 
//  trivial case, nothing to read. 
//  Different table name 
//  NULL 1 
//  We gather the operators that will be used for next iteration of extended optimization 
//  Series of equal keys. 
//  register to session first for backward compatibility 
//  EUROPE_FRANCE 
//  most recent instance of the pmf 
//  Figure out the partition spec from the input.   This is only done once for the first row (when stat == null)   since all rows in the same mapper should be from the same partition. 
//  Regardless of the above, we should have the key we've signed with. 
//  fall-through to throw exception, its not expected for execution to reach here. 
//  all partition column type should be string   partition column is virtual column 
//  All non-primitive OIs are writable so we need only check this case. 
//  used only for explain. 
// can't have relative path if there is scheme/authority 
//  NULL 0 
//  By definition here we copy up to the limit of the buffer. 
//  temporary type-safe casting 
/*    * Truncate a slice of a byte array to a maximum number of characters and   * place the result into element i of a vector.    */
//  for split sampling. shrinkedLength is checked against IOContext.getCurrentBlockStart,   which is from RecordReader.getPos(). So some inputformats which does not support getPos() 
//  Previously this was handled by filterTableNames.  But it can't be anymore because we can no   longer depend on a 1-1 mapping between table name and entry in the list. 
//  No nanos. 
//  check if the new configs are added to HIVE_CONF_RESTRICTED_LIST 
//  try dropping table as user1 - should succeed 
//  the big table has reached a new key group. try to let the small tables 
//  enable dynamic partitioning 
//  integer and boolean types require no conversion, so use a no-op 
//  2. Use self alias 
//  If a double quote is seen and the index is not inside a single quoted string and the previous character   was not an escape, then update the hasUnterminatedDoubleQuote flag 
//  localizing files for AM, submitting DAG) 
//  Set later with setOutput* methods. 
//  If this is used in the future - make sure to disable grouping in the payload, if it isn't already disabled 
//  Get the string value and convert to a Interval value. 
//  Try single stripe 
//  ===== EVENT METHODS 
//  Add the transformation that computes the lineage information. 
//  Assumption:   1. This will be run last after PP, Col Pruning in the PreJoinOrder   optimizations.   2. If ProjectRel is not synthetic then PPD would have already pushed   relevant pieces down and hence no point in running PPD again.   3. For synthetic Projects we don't care about non deterministic UDFs 
//  Subscriber can get notification about addition of a database in HCAT   by listening on a topic named "HCAT" and message selector string   as "HCAT_EVENT = HCAT_ADD_DATABASE" 
/*    * Returns {@code true} if such base file can be used to materialize the snapshot represented by   * this {@code ValidWriteIdList}.   * @param writeId highest write ID in a given base_xxxx file   * @return true if the base file can be used    */
//  return 
/*    * Update the VectorPTFDesc with data that is used during validation and that doesn't rely on   * VectorizationContext to lookup column names, etc.    */
// verify that a new sessionstate has default db 
//  1) Add new vector child to the vector parent's children list. 
//  rare case 
//  Subclasses can override this step (for example, for temporary tables) 
//  sort is trivial 
//  drop table is idempotent 
// make sure to compare them as Entity, i.e. that it's the same table or partition, etc 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder() 
//  boolean that tells if the HiveMetaStore (remote) server is being used.   Can be used to determine if the calls to metastore api (HMSHandler) are being made with   embedded metastore or a remote one 
//  Verify that objectStore fetches the latest notification event ID 
//  if list bucketing then bail out 
//  Create a ColumnStatistics Obj 
//  test that there is no lead second adjustment 
//  store column name in map-work 
//  get partition metadata 
//  Should come back a null. 
//  Format the properties statement 
//  Test one random high-precision subtract. 
//        calling close on an unopened session is probably harmless. 
//  get the needed columns by id and name 
//  Finally, check the filter for non-built-in UDFs. If these are present, we cannot 
//  The output of the lateral view join will be the columns from the select   parent, followed by the column from the UDTF parent 
// compaction doesn't filter deltas but *may* have a reader for 'base' 
// Testing substring index starting with 1 and zero length 
// 0. SetOp rewrite 
//  Everything in the batch has already been filtered out. 
//  Inherent most properties from table level schema and overwrite some properties   in the following code.   This is mainly for saving CPU and memory to reuse the column names, types and 
//  source of replication not set 
//  do bounds check for OOB exception 
//  Count non-null column rows. 
//  Logged at INFO in multiple other places. 
//  Require ADMIN privilege 
//  pos of outer join alias=<pos of other alias:num of filters on outer join alias>xn   for example,   a left outer join b on a.k=b.k AND a.k>5 full outer join c on a.k=c.k AND a.k>10 AND c.k>20     That means on a(pos=0), there are overlapped filters associated with b(pos=1) and c(pos=2).   (a)b has one filter on a (a.k>5) and (a)c also has one filter on a (a.k>10),   making filter map for a as 0=1:1:2:1. 
//  change file length and look for cache misses 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getTableTypes(org.apache.hive.service.cli.SessionHandle)    */
//  Check that there is one datasource with the published segment 
//  Special case (rare) - the segment is on buffer boundary. 
//  Test translation of both IN filters and boolean-valued IN expressions (non-filters). 
//  Ok, this isn't one we track.  Just return whatever matches the string 
//  represents the total memory that this Join operator will use if it is a MapJoin operator 
//  One last test: if we are enabling the rewrite, we need to check that query 
// try an invalid alter table with partition key name 
//  UDTF is not handled yet, so the parent SelectOp of UDTF should just assume   all columns. 
//  check delegation token in job conf if any 
//  Test in upper case 
//  Rollup and Cubes are syntactic sugar on top of grouping sets 
//  8. We create the group_by operator 
//  CBO related 
//  we need plugins to handle llap and uber mode 
//  Number the test rows with collection order. 
//  To avoid long overflow, we will divide the max row count by denominator   and use that factor to multiply with other row counts 
//  For the CHAR and VARCHAR data types, the maximum character length of   the columns.  Otherwise, 0. 
//  If none of the columns need to be cast there's no need for an additional select operator 
//  limit * 64 : compensation of arrays for key/value/hashcodes 
//  Junk after exponent. 
//  super hack city notice the mod plus only happens after firstfield   hit, so == 0 is right. 
//  We will estimate map as an object (only if it's a field). 
//  if after merge the sparse switching threshold is exceeded then change   to dense encoding 
//  ObjectInspector for input data. 
//  Same for char 
//  Determine minimum of all non-null double column values; maintain isGroupResultNull. 
/*  'smith' = last_name   */
//  the reducer. 
//  UNDONE: Trim trailing zeroes... 
//  ERROR_MESSAGE 
//  tableNames = client.listTableNamesByFilter(dbName, filter, (short) 2);   assertEquals(2, tableNames.size()); 
//  all small aliases are staged.. no need full bucket context 
//  Set so we can verify they are reset by operation 
//  Should be no need for child vector expressions, which would imply casting/conversion. 
/*  * ELT(index, string, ....) returns the string column/expression value at the specified * index expression. * * The first argument expression indicates the index of the string to be retrieved from * remaining arguments.  We return NULL when the index number is less than 1 or * index number is greater than the number of the string arguments.  */
//  there should be expectedCallCount calls to drop partitions with each batch size of 
// First child should be operand 
//  Insert the additional http headers 
//  no jar is found. 
//  ROW 
//  log4j2 
// case 3: verify the difference. 
//  as we process the grouping sets. 
//  we do not need to apply the optimization 
//  optional   required   required 
//  A noop if we are in process of sending or if we have the correct value. 
//  last param no complete 
//  sparse registers are delta and variable length encoded 
//  when we are running current query 
//  ----------------------------------------------------------------------------------------------- 
// it seems that LoadTableDesc has Operation.INSERT only for CTAS... 
//  unexpected! 
//  This position is a constant. 
//  we use the default field delimiter('\1') to replace the multiple-char field delimiter   but we cannot use it to parse the row since column data can contain '\1' as well 
//  One call per root Input 
//  check the inspectors 
//  Use Case 11. 
//  Only columns can be selected for both sorted and bucketed positions 
//  entry point (aliasNum = 0) 
//  we need the expr that generated the key of the reduce sink 
//  Add to cache (same group as tsOp) 
//  Use Case 7. 
//  NodeId can be null if the task gets unregistered due to failure / being killed by the daemon itself 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getTransactionIsolation()    */
//  col2 
//  if both old and new params are not null, merge them 
//  Reserve space for the int length. 
//  executeQuery should always throw a SQLException, 
//  Intentionally overwrites anything the user may have put here 
//  false for continue : has pair but not in this turn 
//  replace original STDDEV_POP(x) with     SQRT(       (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x))       / COUNT(x)) 
// not to be confused with useVectorizedInputFileFormat at Vectorizer level  which represents the value of configuration hive.vectorized.use.vectorized.input.format 
//  Try the different getters 
//  convert as such. 
//  physical optimizer stages... 
// now that exceptions (aka abortedTxnList) is sorted 
//  BLOCKED_BY_INT_ID 
//  First, try to reuse from the same pool - should "just work". 
//  for those stmtHandle passed from HiveDatabaseMetaData instead of Statement 
//  Bail out if there is nothing to push 
//  test dropping tables and trash behavior 
//  insert some data in new schema 
//  ** Methods that does not need a data object ** 
//  row id 
//  col1 
//  Perform any key expressions.  Results will go into scratch columns. 
/*      * 2. initialize WFns.      */
//  Uses a no-op proxy 
//  orc creates 1000 batch size to make memory check align with 5000 instead of 5120 
/*    * Initialize using an StructObjectInspector.   * No projection -- the column range 0 .. fields.size()-1    */
//  private void writeObject(ObjectOutputStream out) throws IOException {   out.writeObject(types_by_column_name);   out.writeObject(ordered_types);   } 
//  create a file with 5 blocks spread around the cluster 
//  It is possible that the row got absorbed in the operator tree. 
// supports AcidInputFormat which do not use the KEY pass ROW__ID info 
//  check that right row(s) are selected 
//        cluster state changes, it will notify us, and we'd update the queries again. 
//  Request two messages 
// If coming from small-table side, do some book-keeping, and skip traversal. 
//  If serialization.format property has the default value, it will not to be included in   SERDE properties 
// first try full match 
//  Join key origin has been traced to a table column. Check if the table is external. 
//  Write the remaining part of the array 
//  for outer joins, contains the potential nulls for the concerned aliases 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#executeUpdate(java.lang.String, int)    */
//  Ok to not lock the list for this and use a volatile lastAccessTime instead 
//  created in case the mapjoin failed. 
//  table should not be null. 
//  Constant from Accumulo's AuthenticationTokenIdentifier 
//  one to produce these will be the same as using any other. 
//  drop the table first, in case some previous test created it 
//  PASS 
//  The base composite-service is already stopped, don't do anything again. 
//  Run ExecDriver in another JVM 
// now start concurrent "select * from tab1" txn 
//  Check there is no log file without the suffix 
//  2. Update Col Stats Map with col stats for columns from right side of 
//  rc will only be overwritten if close errors out 
//  If cred provider has entry and conf does not, cred provider is used. 
//  SR.E: Lock we are examining is exclusive 
//  this entry of output is not present in the output schema   so, we first check the table schema to see if it is a part col 
//  Fetch the column expression. There should be atleast one. 
// this should block behind the X lock on  T6 
//  Default to all of user's authorizations when no configuration is provided 
// Publish the new partition(s) 
//  IncrementalRows constructor should buffer the first "incrementalBufferRows" rows 
//  TODO constraintCache 
//  Job Hash Map 
//  The object was added later for the same class; see addToProcessing. 
//  Now do the add with Java BigDecimal 
//  parts of the partition 
//  LazySimple seems to work better with an row object array instead of a Java object... 
//  Timeseries query results as records 
//  Create remote dirs once. 
// To change body of overridden methods use File | Settings | File Templates. 
//  spot check null propagation 
//  if we have a base to work from 
//  add to list of saved historic operations 
//  Basic sanity check. Other cases are not skipped because it is similar to the case for Long. 
//  number of spilled partitions   only one (last one) partition is left in memory   how often (# of rows apart) to check if memory is full   configuration for n-way join   write buffer size for BytesBytesMultiHashMap 
//  Reader will check for the event queue upon the end of the input stream - no need to interrupt. 
/*      * Populate vectorMapJoininfo.      */
//  Constructor to with the individual addInitialColumn method 
//  Ignore Index tables, those will be dropped with parent tables 
//  A helper object that efficiently copies the big table key columns (input or key expressions) 
//  remember the event operators we've seen 
//  First, we find the SELECT closest to the top. 
//  If we join on different keys on different tables, we can no longer apply   multi-join conversion as this is no longer a valid star join.   Bail out if this is the case. 
//  For spark job with empty source data, it's not submitted actually, so we would never   receive JobStart/JobEnd event in JobStateListener, use JavaFutureAction to get current   job state. 
//  HIVE-4889 
//  Read the altered "tbl" via CachedStore 
//  now add the keywords from the current connection 
//  If owner information is unchanged, then DB properties would've changed 
//  Currently SMB is broken, so we cannot check if it's  compatible with IO elevator.   So, we don't use the below code that would get the correct MapWork. See HIVE-16985. 
// we are compacting and it's acid schema so create a reader for the 1st bucket file that is not empty 
//  If it is not a materialized view, we do not rewrite it 
//  Tailing zeroes difference ok... 
//  Check columns. 
//  Closing the operator can sometimes yield more rows (HIVE-11892) 
/*          * Extract information from reference word from slot table.          */
//  optional int64 delete_delay = 2 [default = 0]; 
//  Note that the pool is per EDC - within EDC, CVBs are expected to have the same schema. 
//  Extract type for the arguments 
//  This is expected to fail. 
//  restrict with any filters found from WHERE predicates. 
//  After this optimization, the tree should be like:    TS -> (FIL "skewed rows") * -> RS -                                       \                                         ->   JOIN                                       /           \    TS -> (FIL "skewed rows") * -> RS -             \                                                     \                                                       ->  UNION -> ..                                                     /    TS -> (FIL "no skewed rows") * -> RS -          /                                          \        /                                           -> JOIN                                          /    TS -> (FIL "no skewed rows") * -> RS -   
//  for windows paths 
// user running the test belongs to 
//  Set the collection fields; some code might not check presence before accessing them. 
//  replace each of the position alias in GROUPBY with the actual column name 
//  If old table is in the cache and the new table can also be cached 
//  Multiple threads could try to initialize at the same time. 
//  if number of elements in list cannot be determined, this value will be used 
/*          * Default any additional data columns to NULL once for the file (if they are present).          */
/*  @bgen(jjtree) ConstMap  */
/*      * Do common analysis of the IF statement boolean expression.     *     * The following protected members can be examined afterwards:     *     *   boolean isIfStatementResultRepeated     *   boolean isIfStatementResultThen     *     *   int thenSelectedCount     *   int[] thenSelected     *   int elseSelectedCount     *   int[] elseSelected      */
//  create db 
//                    12345678901234567890123456789012345678                              1         2         3 
//  add the following strings:   1. column name   2. table name   3. tablename.columnname 
//  pRS-cRS-cGBY 
//  First we remove the input operators of the expression that 
//  Row column information. 
//  For now, we always convert to double if we can't find a common type 
//  Create default route 
/*  Validate skewed information.  */
//  Insert the value corresponding to the current expression in currExprNodeInfo.exprNodeValues. 
//  We should technically update memory usage if updating the old object, but we don't do it   for now; there is no mechanism to properly notify the cache policy/etc. wrt parallel evicts. 
//  If the data is not escaped, reference the data directly. 
//  at some point, these should be inserted as a "db" 
//  replace the original selectOp in the parents with selectUnionOp 
//  Create a new conf object to bypass metastore authorization, as we need to   retrieve all materialized views from all databases 
//  asc   nulls first 
//  authorize this call on the schema objects 
//  Data structures 
//  First time registration, or new register comes in before the previous unregister. 
//  We rely on the caller to supply a reasonable total; we could log a warning   if this doesn't match the allocation of the last session beyond some threshold. 
//  Shouldn't hit 14 digits until year 2286 
//  The column has been read from disk. 
//  reset value in case any date fields are missing from the date pattern 
//  Handle dual nature. 
//  test nulls propagation 
//  Clone the table, 
// Check metrics during semantic analysis. 
//  print out last part of buffer 
//  find the extra table 
//  Offset relative to the beginning of the stream of where this RG ends. 
//  required   optional   optional   optional   optional   optional 
//  Based on update() above. 
//  This is the overwrite case, we do not care about the accuracy. 
//  The only allowed flag is new-alloc, and that only if we are not discarding. 
//  for left-semi join, generate an additional selection & group-by   operator before ReduceSink 
//  Attempt to match Oracle semantics for timestamp arithmetic,   where timestamp arithmetic is done in UTC, then converted back to local timezone 
//  If the table's location is currently unset, it is left unset, allowing the metastore to   fill in the table's location.   Note that the previous logic for some reason would make a special case if the DB was the   default database, and actually attempt to generate a  location.   This seems incorrect and uncessary, since the metastore is just as able to fill in the   default table location in the case of the default DB, as it is for non-default DBs. 
//  end ListIterator 
//  since metastore connections don't require the url, this is allowable. 
//  UGI information is not available at connection setup time, it will be set later   via set_ugi() rpc. 
/*        * Sleep until all threads with clean up tasks are completed. 2 seconds completing task       * and 1 sec grace period.        */
//  makes the message more informative - helps to find bugs in client code 
//  get ObjInspectors for entire record and bucketed cols 
//  Anything else is a FAIL. 
//  Note: CopyWork supports copying multiple files, but ReplCopyWork doesn't. 
// 1 split since mutateTransaction txn just does deletes 
/*  For the case when the output can have null values, follow     * the convention that the data values must be 1 for long and     * NaN for double. This is to prevent possible later zero-divide errors     * in complex arithmetic expressions like col2 % (col1 - 1)     * in the case when some col1 entries are null.      */
//  Set needed columns for this dummy TableScanOperator 
//  UNDONE: Provide isRepeated, selected, isNull 
//  Catch the exception caused by missing jpam.so which otherwise would   crashes the thread and causes the client hanging rather than notifying   the client nicely 
//  now localTask is the parent task of the current task 
//  Confirm grouping. 
//  One scheduler pass from the nodes that are added at startup 
//  normalize the columns sizes 
//  Validate the third parameter, which should be an integer to represent 'k' 
/*  move the last 16 bytes to the prefix area  */
//  waitOnPrecursor determines whether or not non-existence of   a dependent object is an error. For regular imports, it is.   for now, the only thing this affects is whether or not the   db exists. 
//  "AVG_DECIMAL" 
//  This is the min number of reducer(s) for the bottom layer ReduceSinkOperators to avoid query 
//  Run load on primary itself 
//  Verify we hit an error while connecting 
//  flag for bucket map join. One usage is to set BucketizedHiveInputFormat 
//  We are here when the left and right are non-zero and have the same sign. 
//  bit packing 
// replace ${hiveconf:hive.metastore.warehouse.dir} with actual dir if existed.  we only want the absolute path, so remove the header, such as hdfs://localhost:57145 
//  Forgive error 
//  A mapping, once established, is not dependent upon the file channel that was used to   create it. delete file and hold onto the map 
//  for each dir, get all files under the dir, do getSplits to each   individual file,   and then create a BucketizedHiveInputSplit on it 
// ValidWriteIdList with HWM=MAX_LONG, i.e. include the data for aborted txn 
//  3. Insert ReduceSide GB 
//  if this is a selStarNoCompute then this select operator   is treated like a default operator, so just call the super classes   process method. 
//  a array of bitvectors where each entry denotes whether the element is to   be used or not (whether it is null or not). The size of the bitvector is   same as the number of inputs(aliases) under consideration currently. 
//  Hadoop IPC wraps InterruptedException. GRRR. 
//  Intended predicate to be removed 
//  Adds tables only for create view (PPD filter can be appended by outer query) 
//  only vectorized orc input is cached. so there's a reason to 
// we use the same mechanism to copy "files"/"otherFiles" and "libdir", but we only want to put  contents of "libdir" in Sqoop/lib, thus we pass the list of names here 
//  this must be lead/lag UDAF 
//  Allocated 
//  Trim trailing zeroes -- but only below the decimal point. 
//  Unlikely to be thrown. 
//  We need to make sure that the list element type is settable. 
//  Create database without location clause 
//  c21-c23 
//  The query has enforced that a sort-merge join should be performed.   For more details, look at 'removedReduceSinkBucketSort' in FileSinkDesc.java 
//  That is always true now; but it wasn't some day, the below would throw in getColumnData. 
//  Allow debugging by disabling column reuse (input cols are never reused by design, only 
//  A field because we cannot multi-inherit. 
//  Kill previously launched child MR jobs started by this launcher to prevent having 
//  test delete column stats; if no col name is passed all column stats associated with the 
//  These need to be based on the target. 
//  First, kill any running MR jobs 
//  private BinarySortableDeserializeRead keyBinarySortableDeserializeRead; 
//  if the return type of the GenericUDF is boolean and all partitions agree on   a result, we update the state of the node to be TRUE of FALSE 
//  Synchronize on the cache entry so that no one else can invalidate this entry 
//  Fraction digit parsing move to next lower longword. 
//  If one of the children (left or right) is:   (i) a union, or   (ii) an identity projection followed by a union,   merge with it 
//  now that the primary reader has advanced, we need to see if we 
//  This cache range is a prefix of the requested one; the above also applies.   The cache may still contain the rest of the requested range, so don't set gotAllData. 
//  Setup stdout and stderr 
//  Note: scratchdir is reused implicitly because the sessionId is the same. 
//  the value has a schema and not a FieldSchema 
//  validate sort columns and bucket columns 
//  LLAP cache can be disabled via config or isPlanCache 
//  If does not contain a limit operation, we bail out 
//  outside of [0..20] range 
//  Correlate does not have an ON clause.   For a LEFT correlate, predicate must be evaluated first.   For INNER, we can defer. 
//  partCount not equally divided into batches.  last batch size will be less than batch size 
//  Suppress leading zeroes. 
//  Extra element to make sure we have the same formula to compute the   length of each element of the array. 
//  The other list doesn't exist, create it at the first index of our op. 
//  We have statistics for the table. Size appropriately. 
//  Add original direcotries to obsolete list if any 
//  The context for creating the VectorizedRowBatch for this Map node that   the Vectorizer class determined. 
//  if DP is enabled, get the final output writers and prepare the real output row 
// register JVM metrics 
//  Lock was outdated and it was removed (then maybe another transaction picked it up)   or changed its state 
//  Skip combine for all paths 
/*  Test decimal column to decimal scalar division. This is used to cover all the   * cases used in the source code template ColumnDivideScalarDecimal.txt.   * The template is used for division and modulo.    */
//  Normal case, no variable-length arguments 
//  Account for potential partial chunks. 
//  groupingId = PrimitiveObjectInspectorUtils.getInt(arguments[0].get(), groupingIdOI); 
//  Register the pending events to be sent for this spec. 
//  Get the id for the next entry in the queue 
//  This method takes Object[], so it accepts whatever types that are   passed in. 
/*  Just copy the payload.  {@link recordIdColumnVector} has already been populated  */
//  matching Oracle behavior. 
//  For ORC case, send the boundaries of the stripes so we don't have to send the footer. 
//  convert skewData to contain ExprNodeDesc in the keys 
//  The cost of the result 
//  When people forget to quote a string, op1/op2 is null.   For example, select * from some_table where ds > 2012-12-1 and ds < 2012-12-2 . 
//  fetch the table marked by the message and compare 
//  For now this only is used to determine the bucketing/sorting of outputs, in the future   this can be removed to optimize the query plan based on the bucketing/sorting properties 
//  Re-align the positionMap by -1 for the columns appearing after hcatFieldSchema. 
//  A very simple counter to keep track of number of rows processed by an   operator. It dumps   every 1 million times, and quickly before that 
//  for non partitioned table this will represent the last tableName replicated, else its the name of the 
//  logging inside 
//  Cache key 
//  first try known drivers... 
//  Cache size should be 0 now 
//  test Feb of leap year, 2/29 
/*  Submits the request and returns  */
//  crtTblDesc 
//  Mapping from constraint name to list of default constraints 
//  Some keys need to be left to null corresponding to that grouping set. 
//  Write cost 
// die! 
//  remove this branch 
//  A synonym in some places in the code... 
//  No padding needed. 
//  so the operation is atomic. 
//  We need to check if the total size of local tables is under the limit.   At here, we are using a strong condition, which is the total size of   local tables used by all input paths. Actually, we can relax this condition   to check the total size of local tables for every input path.   Example:                 UNION_ALL                /         \               /           \              /             \             /               \         MapJoin1          MapJoin2        /   |   \         /   |   \       /    |    \       /    |    \     Big1   S1   S2    Big2   S3   S4   In this case, we have two MapJoins, MapJoin1 and MapJoin2. Big1 and Big2 are two   big tables, and S1, S2, S3, and S4 are four small tables. Hash tables of S1 and S2   will only be used by Map tasks processing Big1. Hash tables of S3 and S4 will only   be used by Map tasks processing Big2. If Big1!=Big2, we should only check if the size   of S1 + S2 is under the limit, and if the size of S3 + S4 is under the limit.   But, right now, we are checking the size of S1 + S2 + S3 + S4 is under the limit.   If Big1=Big2, we will only scan a path once. So, MapJoin1 and MapJoin2 will be executed   in the same Map task. In this case, we need to make sure the size of S1 + S2 + S3 + S4 
//  initialize the keys and values 
//  Enabled to accept quoting of all character backslash qooting mechanism 
//  If user has fully specified partition, validate that partition exists 
//  Is the user trying to insert into a external tables 
//  However, the fastBigIntegerBytes can take on trailing zeroes -- so make it larger. 
//  create table 
//  Stub out the ZKI mock 
//  Check based on the Hive integer type we need to test with isByte, isShort, isInt, isLong   so we do not use corrupted (truncated) values for the Hive integer type. 
//  re-open the hms connection 
//  Whether this OI is for the column-level schema (as opposed to nested column fields). 
//  remember the event operators we've abandoned. 
//  base tables set up, let's replicate them over 
//  Convert from bucket map join to sort merge bucket map join if enabled. 
//  doesn't require additional MR Jobs 
//  If input produces correlated variables, move them to the front,   right after any existing GROUP BY fields. 
/*      * 1. initialize args      */
//  Must use raw local because the checksummer doesn't honor flushes. 
/*  a and b shouldn't be even; If a and b are even, then none of the values       * will set bit 0 thus introducing errors in the estimate. Both a and b can be even       * 25% of the times and as a result 25% of the bit vectors could be inaccurate. To avoid this       * always pick odd values for a and b.        */
//  validate the (materialized) view statement 
// Update JobConf using MRInput, info like filename comes via this 
//  best effort attempt to write all output from the script before marking the operator 
//  Assumes row schema => string,int,string 
//  Get max split size for HiveRelMdParallelism 
//  map work should start with our ts op 
//  because of scaling down, this could happen 
/*            * First of Multiple elements.            */
//  check the hints to see if the user has specified a map-side join. This   will be removed later on, once the cost-based 
// show tables <dbname> is in invalid show tables syntax. Hive does not return  any tables in this case 
//  We assume stream list is sorted by column and that non-data   streams do not interleave data streams for the same column. 
// HIVE-17138: this creates 1 delta_0000013_0000013_0000/bucket_00001 
//  Currently only used during re-optimization related parts. 
// add mapreduce job tag placeholder 
/*    * If there is no credential provider configured for hadoop, jobConf should not contain   * credstore password and provider path even if HIVE_JOB_CRESTORE_PASSWORD env is set    */
//  a bit before we restart the loop. 
//  (bytes[fieldByteEnd] == separator) 
//  LazySimpleSerDe doesn't support projection. 
/*    * for a given Work Descriptor, it extracts information about the ReduceSinkOps   * in the Work. For Tez, you can restrict it to ReduceSinks for a particular output   * vertex.    */
//  get the input and prepare the output 
//  We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless. 
//  EXPERIMENT 
//  1. Do few checks to determine eligibility of optimization   2. look at ExprNodeFuncGenericDesc in select list to see if its min, max, count etc.      If it is   3. Connect to metastore and get the stats   4. Compose rows and add it in FetchWork 
//  Don't try to log anything when appender is stopped 
//  and then see what happens based on the provided schema 
//  Test between 
//  Validate the third parameter, which should also be an integer 
//  of the Hive config values. 
// use smallint as outputTypeInfo 
//  InputFormat 
//  the vertex that should be inlined. <Operator, list of Vertex that is 
//  Start HiveServer2 with given config   Fail if server doesn't start 
//  We should not update the following 3 values if SerDeInfo contains these.   This is to keep backward compatible with getSchema(), where these 3 keys   are updated after SerDeInfo properties got copied. 
//  Retrieve skewed columns. 
//  Tests concurrent modification, and that results are the same per input across threads   but different between inputs. 
//  Rule requires that aggregate key to be the same as the join key.   By the way, neither a super-set nor a sub-set would work. 
//  Either we were interrupted by one of:   1. handleEvent(), in which case there is a reader (error) event waiting for us in the queue   2. Some other unrelated cause which interrupted us, in which case there may not be a reader event coming.   Either way we should not try to block trying to read the reader events queue. 
/*    * split pairs by delimiter.    */
//  arbitrary tokens, the renewer should be the principal of the JobTracker 
//  Add test parameters from official storage formats registered with Hive via   StorageFormatDescriptor. 
//  static partition and list bucketing 
//  ignore the char after escape_char 
//  data and we can just ignore them. 
//  We have already updated the metrics for the failure; change the state. 
/*  This needs to return a "live" connection to be used by operation that follows it.          Thus it only closes Connection on failure/retry.  */
//  otherwise, compare it with power and half. 
// return System.getProperty("test.build.data", "build/test/data") + "/dfs/"; 
//  acidOp flag has to be checked to use JAVA hash which works like   identity function for integers, necessary to read RecordIdentifier   incase of ACID updates/deletes. 
//  we try to fold the expression to remove e.g. cast on constant 
//  The other list will be exhausted when it commits. Create new one pending that commit. 
//  A_STRING 
//  This multiply produces more than 38 digits --> overflow.  --------------------------------------------------- 
//  Eventually, we want this to be a richer description of having   a DB, TABLE, ROLE, etc scope. For now, we have a trivial impl   of having only DB and TABLE scopes, as determined by whether   or not the tableName is null. 
//  Wrap thrift connection with SASL for secure connection. 
//  Don't pass the parsed db name, as get_partitions_by_filter will parse it itself 
//  this is not transactional 
//  Flip the sign bit (and unused bits of the high-order byte) of the seven-byte long back. 
//  Is this a truncate column command 
//  These are the vectorized batch expressions for filtering, key expressions, and value 
//  Number of tokens to drop, between sleep intervals; 
//  verify if table has been the target of replication, and if so, check HiveConf if we're allowed   to override. If not, fail. 
//  We are going to add splits for these directories with recursive = false, so we ignore   any subdirectories (deltas or original directories) and only read the original files.   The fact that there's a loop calling addSplitsForGroup already implies it's ok to   the real input format multiple times... however some split concurrency/etc configs   that are applied separately in each call will effectively be ignored for such splits. 
//  Removed from heap without evicting. 
//  ignored. 
//  No limit, nothing to propagate, we just bail out 
//  For now, we don't support joins on or using DECIMAL_64. 
//  not a conversion function 
// "123456789", "987654321", "1234", "4321", 
//  Set proxy user privilege and initialize the global state of ProxyUsers 
//  partial time. time part will be skipped 
//  We need to extrapolate this partition based on the other partitions 
//  dummy handle for ThriftCLIService 
//  there are some required privileges missing, create error message   sort the privileges so that error message is deterministic (for tests) 
//  walk the list and acquire the locks - if any lock cant be acquired, release all locks, sleep 
//  remove small table ailias from aliasToWork;Avoid concurrent modification 
//  initialize output vector buffer to receive data 
//  not sure we need this exec context; but all the operators in the work   will pass this context throught 
//  replace stderr and run command 
//  In case of dynamic queries, it is possible to have incomplete dummy partitions 
//  In the cases that have multi-stage insert, e.g. a "hive.skewjoin.key"-based skew join,   it can happen that we want multiple commits into the same directory from different   tasks (not just task instances). In non-MM case, Utilities.renameOrMoveFiles ensures   unique names. We could do the same here, but this will still cause the old file to be   deleted because it has not been committed in /this/ FSOP. We are going to fail to be   safe. Potentially, we could implement some partial commit between stages, if this 
//  if all row indexes are null then indexes are disabled 
//  This should now throw some useful exception. 
//  execute query, ignore exception if any 
//  Print the column names 
//  Not expected to be encountered for Hive - fail 
//  Clears the dest dir when src is sub-dir of dest. 
/*    * fastSetFromBigIntegerBytes word size we choose is 56 bits to stay below the 64 bit sign bit:   * So, we need a multiplier 2^56   *   *    2^56 =   *      72057594037927936 or   *      72,057,594,037,927,936 or   *      7,2057594037927936  (16 digit comma'd)    */
//  int 
/*      * partitioning Spec      */
//  Virtual relation generated by the reduce sync 
// set value to MIN_VALUE so that -MIN_VALUE overflows and gets set to MIN_VALUE again 
//  test getAllTables 
//  hive server's session input stream is not used   open a per-session file in auto-flush mode for writing temp results and tmp error output 
//  2/ write the size of the map which is a VInt 
//  stageList 
//  The first bounds check requires at least one more byte beyond for 2nd int (hence >=). 
//  There should be 2 original bucket files (000000_0 and 000001_0), a base directory,   plus two new delta directories and one delete_delta directory that would be created due to 
// normalize prop name 
//  The total size of local tables in localWork[i] is unknown. 
//  Need to get a new one, see the comment wrt threadlocals. 
// //////////  First Incremental //////////// 
/*      * Check the 1st-level children and do simple semantic checks: 1) CTLT and     * CTAS should not coexists. 2) CTLT or CTAS should not coexists with column     * list (target table schema). 3) CTAS does not support partitioning (for     * now).      */
//  DATE_STATS 
//  execute the test query 
// --0e 
//  combine two lists 
//  for each alias, add object inspector for filter tag as the last element 
//  it may contain duplicates, remove duplicates 
//  We use a separate metastore client for heartbeat calls to ensure heartbeat RPC calls are   isolated from the other transaction related RPC calls. 
//  This could be brittle. 
//  We assume ranges in "ranges" are non-overlapping; thus, we will save next in advance. 
/*      * The virtual columns available under vectorization.  They may not actually     * be used in this query.  Unused columns will be null, just like unused data and partition     * columns are.      */
//  Traverse through all the source files and see if any file is not copied or partially copied.   If yes, then add to the retry list. If source file missing, then retry with CM path. if CM path 
// ************************************************************************************************   Initialize (fastSetFrom*). 
//  production is: typedef DefinitionType() this.name 
// check that all calls were recorded. 
//    Operations involving/returning day-time intervals   
/*      * 6. Project      */
//  Insert overwrite on unpartitioned table 
//  M_STRING_STRING 
//  Tests for int add_partitions_pspec(PartitionSpecProxy partitionSpec) method 
//  For stripe-level streams we don't need the extra refcount on the block. 
//  object inspector for serializing input tuples 
//  We handle ReduceSinkOperator here as we can safely ignore table alias   and the current comparator implementation does not.   We can ignore table alias since when we compare ReduceSinkOperator, all   its ancestors need to match (down to table scan), thus we make sure that   both plans are the same. 
/*   Distcp currently does not copy a single file in a distributed manner hence we dont care about  the size of file, if there is only file, we dont want to launch distcp.    */
//  ------ Additional static classes defined after this point ------ 
//  Initialize second mocked filesystem (implement only necessary stuff) 
//  Error was expected 
//  Arithmetic specializations are done in a convoluted manner; mark them as built-in. 
//  clear up any existing databases 
//  we set viewProjectToTableSchema so that we can leverage ColumnPruner. 
//  Do full constant propagation   Only perform expression short-cutting - remove unnecessary AND/OR operators 
//  write value element 
/*    * Used to keep position/length for complex type fields.   * NOTE: The top level uses startPositions instead.    */
// JobID job = new JobID(); 
//  Linear search since this won't take much time from the total execution anyway   lower has the range of [0 .. total-1] 
//  Make sure the path is normalized (we expect validation to pass since we just created it). 
//  propagate input format if necessary 
//  Bean methods 
//  This is for backward compatibility. If the user did not specify the   output column list, we assume that there are 2 columns: key and value.   However, if the script outputs: col1, col2, col3 seperated by TAB, the   requirement is: key is col and value is (col2 TAB col3) 
//  If we don't, create a new separate filter. In most cases there will only be one. 
//  Same primitive category but different qualifiers. 
//  toss all other exceptions, related to reflection failure 
//  The original may have settable info that needs to be added to the new copy. 
// @NOTE this code is very similar to the code at org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:2362 
//  Before Cleaner, there should be 5 items: 
//  has failed because the query was killed from under it. 
/*  * Planner rule that creates a {@code SemiJoinRule} from a * {@link org.apache.calcite.rel.core.Join} on top of a * {@link org.apache.calcite.rel.logical.LogicalAggregate}. * * TODO Remove this rule and use Calcite's SemiJoinRule. Not possible currently * since Calcite doesnt use RelBuilder for this rule and we want to generate HiveSemiJoin rel here.  */
/*  Table giving binary powers of 10.  Entry  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setClob(java.lang.String, java.io.Reader)    */
//  process the third child node,if exists, to get partition spec(s) 
//  optional string src_name = 2; 
//  copy the test files into hadoop if required. 
/*  * Test submission of concurrent job requests.  */
//  Supports keeping a TimestampWritable object without having to import that definition... 
//  Process the normal splits 
//  check that the columns referenced in these comparisons form 
//  Position where we'd write   Position where we'd read (unsafely at write time). 
//  We only need to apply negation to all 3 words when there are 3 words, etc. 
//  thrown when the table to be altered does not exist 
//  Set up mock warehouse 
//  For PARTIAL1 and PARTIAL2 
// value is correct 
//  IMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column="value"[, ...])]]   FROM 'source_path'      [LOCATION 'import_target_path'] 
//  In Hive server mode, we are not able to retry in the FetchTask   case, when calling fetch queries since execute() has returned. 
//  inner join 
//    throw new HiveException("remove is not in sort order and unique");   } 
//  Does not need to be unique, just non-zero distinct value to test against. 
//  Whether we are using an acid compliant transaction manager has already been caught in   UpdateDeleteSemanticAnalyzer, so if we are updating or deleting and getting nonAcid   here, it means the table itself doesn't support it. 
//  Just making sure Date.valueOf() works ok 
//  These are used in getImplicitConvertUDFMethod 
//  how many columns 
//  adding 16KB constant memory for keyBinarySortableDeserializeRead as the rabit hole is deep to implement   MemoryEstimate interface, also it is constant overhead 
// set up the fetch operator for the new input file. 
//  3. INSERT OVERWRITE 
//  Project with the output of our operator. 
//  (Re)allocate larger to be a multiple of 1024 (DEFAULT_SIZE). 
//  there should be 3 calls to create partitions with batch sizes of 23, 15, 8 
//  Init output object inspectors.     The return type for a partial aggregation is still a list of strings.     The return type for FINAL and COMPLETE is a full aggregation result, which is 
//  Create temp table with current connection 
//  2) Drain unused sessions; the close() is sync so delegate to the caller. 
//  Variables for LLAP hash table loading memory monitor 
//  iterate and update masks array 
//  If we are in the system registry and this feature is enabled, try to get it from metastore. 
//  We try to push the full Filter predicate iff:   - the Filter is on top of a TableScan, or   - the Filter is on top of a PTF (between PTF and Filter, there might be Select operators)   Otherwise, we push only the synthetic join predicates   Note : pushing Filter on top of PTF is necessary so the LimitPushdownOptimizer for Rank   functions gets enabled 
//  corresponding branch since only that branch will factor is the reduction 
//        after all the perf changes that we might was well hardcode them separately. 
//  This is fast path for query optimizations, if we can find this info   quickly using directSql, do it. No point in failing back to slow path   here. 
//  with a non null value before trying to alter the partition column type. 
//  root is the start of the operator pipeline we're currently 
//  Extract on date: special handling since function in Hive does   include <time_unit>. Observe that <time_unit> information   is implicit in the function name, thus translation will   proceed correctly if we just ignore the <time_unit> 
//  2b. This is a known incomplete CB caused by ORC CB end boundaries being estimates. 
//  see comment next to the field 
//  The input file has changed - load the correct hash bucket 
//  user specified the memory for local mode hadoop run 
//  restore the local job tracker back to original 
//  Don't log the stack, this is normal. 
//  HADOOP_CLIENT_OPTS is appended to HADOOP_OPTS in HADOOP.sh, so we should remove the old   HADOOP_CLIENT_OPTS which might have the main debug options from current HADOOP_OPTS. A new   HADOOP_CLIENT_OPTS is created with child JVM debug options, and it will be appended to   HADOOP_OPTS agina when HADOOP.sh is executed for the child process. 
/*        * Now new job requests should succeed as status operation has no cancel threads.        */
//  Take all the driver run hooks and post-execute them. 
//  ELSE cast (newInput AS newInputTypeNullable) END 
//  now hook up the children 
//  We do not repartition: take number of splits from children 
//  construct the inner struct 
// c7Value = (Map<?,?>) rowValues[6];  assertEquals(1, c7Value.size());  assertEquals("v", c7Value.get("k")); 
//  Avoid a copy when newTmpJars is null or empty 
//  Either the user or the kill is not done yet. 
//  Find first virtual column and clip them off. 
// if this is co-rrelated we need to make RexCorrelVariable(with id and type)   id and type should be retrieved from outerRR 
//  Final return type that goes back to Hive: a list of structs with n-grams and their   estimated frequencies. 
//  update path in IOContext 
// splits. 
//  Collect newer entry is if a super-set of existing entry, 
/*  id >= 12             */
//  Must fill high word from both middle and lower longs. 
//  next we translate the TezWork to a Tez DAG 
//  Assume only 1 parent for FS operator 
//  Add SMALLINT values 
//  there's no extensive need for this to have its own type - it mirrors   the intent of copy enough. This might change later, though. 
//  The get-session call should also fail. 
//  verify standard case 
//  10 is the length of "compactor." We only keep the rest. 
//  required   optional   optional   required 
//  do a one time initialization 
//  Top level. 
//  Assert that the source and target partitions are equivalent. 
//  For backward compatibility, let the above parameter be used 
//  Verify that getOperationStatus returned only after the long polling timeout 
//  Trying to connect with a non-existent user should still fail with   login failure. 
//  This also gets us around the Enum issue since we just take the value 
//  Table will be queried directly by LLAP   Acquire locks if necessary - they will be released during session cleanup.   The read will have READ_COMMITTED level semantics. 
//  Return itself should be a no-op - the pool went from 6 to 4 with 1 session in the pool. 
//  Prepare the field ObjectInspectors 
//  Only process equivalences found in the join conditions. Processing   Equivalences from the left or right side infer predicates that are   already present in the Tree below the join. 
//  Set unique constraint name if null before sending to listener 
//  Process grouping set for the reduce sink operator 
//  operations other than table rename 
//  Ignore nulls 
//  We would expect 4 entries in TXN_TO_WRITE_ID as each insert would have allocated a writeid   including aborted one. 
//  System environment variables 
//  Return the value 
//  we should look take the parent of fsOp's task as the current task. 
/*    * Using CommonTreeAdaptor because the Adaptor in ParseDriver doesn't carry   * the token indexes when duplicating a Tree.    */
//  Round towards positive infinity. 
//  Check if we have visited this operator 
/*  * Test submission of concurrent job requests with the controlled number of concurrent * Requests and job request execution time outs. Verify that we get appropriate exceptions * and exception message.  */
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getMaxRows()    */
//  Variables for metrics 
//  all resources including HDFS are session based. 
// use setPartitionColumnStatistics instead 
//  When hive.metastore.transactional.event.listeners is set, 
//  Compaction can only be done on the whole table if the table is non-partitioned. 
// commit T1 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getInt(int)    */
//  NEW_TBL 
//  Since our probing method is totally bogus, give up after some time. 
//  Combo 1: Both set to none 
//  The only time this condition should be false is in the case of dynamic partitioning   where the data is bucketed on a dynamic partitioning column and the FileSinkOperator is   being processed.  In this case, the dynamic partition column will not appear in   colInfos, and due to the limitations of dynamic partitioning, they will appear at the   end of the input schema.  Since the order of the columns hasn't changed, and no new   columns have been added/removed, it is safe to assume that these will have indexes   greater than or equal to colInfos.size(). 
//  The join columns should contain all the sort columns   The sort columns of all the tables should be in the same order 
//  Add Spark job id to the Hive History 
//  Set the username/passwd for the Accumulo connection 
//  Map values may be serialized in binary format when they are primitive and binary 
//  Oracle cannot have over 1000 expressions in a in-list 
//  If the child SelectOperator has the ColumnExprMap,   we need to update the ColumnExprMap in the parent SelectOperator. 
//  Partition names are URL encoded. We decode the names unless Hive   is configured to use the encoded names. 
//  All are filtered out 
/*  (id < 10 and id < 11 and id < 12) or (id < 13 and id < 14 and id < 15) or       (id < 16 and id < 17) or id < 18  */
//  these should have viable defaults 
// at this point "show compactions" should have (COMPACTOR_HISTORY_RETENTION_FAILED) failed + 1 initiated (explicitly by user) 
//  1 Convert UDAF Params to ExprNodeDesc 
//  Make a create table statement 
/*    * Compiles the given pattern with a proper algorithm.    */
//  Execute the given sql statement 
//  Add more failed compactions so that the total is exactly COMPACTOR_INITIATOR_FAILED_THRESHOLD 
//  Now remove all BaseWorks in all the childSparkWorks that we created 
//  Serialize each field 
//  cookie passes the validation, return null to the caller. 
//  First allocation of write id (hwm+1) should add the table to the next_write_id meta table. 
//  If all correlation variables are now satisfied, skip creating a value 
//  set second argument to IF 
//  5% tolerance for long range bias and 1% for short range bias 
//  Stripped down version of org.apache.calcite.rel.rules.AggregateExpandDistinctAggregatesRule   This is adapted for Hive, but should eventually be deleted from Hive and make use of above. 
//  get the input & output file formats 
// Work Boundary, stop exploring. 
//  Force a re-read of the configuration file.  This is done because 
//  The reason we use compare-command, rather than simply getting the serialized output and comparing   for partition-based commands is that the partition specification order can be different in different   serializations, but still be effectively the same. (a="42",b="abc") should be the same as (b="abc",a="42") 
//  view entity 
//  Long/double arithmetic 
//  Use Case 4. 
//  Close the stripe reader, we are done reading. 
// commit T2 
//  make a clone of existing hive conf   make a clone of existing hive conf 
//  choose a random sign 
//  TODO: HIVE-14042. Potential blocking call. MRInput handles this correctly even if an interrupt is swallowed. 
//  Do not support grouping set right now 
//  CHECK_CONSTRAINT_COLS 
/*    * The storage arrays for this column vector corresponds to the storage of a Timestamp:    */
//  Different parts of the code rely on this being set... 
//  Failed to init, remove it from cache 
//  MySQL parser 
//  Return MD provider 
//  String type affinity 
/*    * If the init method in HMSHandler throws exception all the times it should be retried until   * HiveConf.ConfVars.HMSHANDLERATTEMPTS is reached before giving up    */
//  dynamic partition 
//  remember the output name of the reduce sink 
//  State has not changed / already registered for notifications. 
//  the types of the expressions for the lateral view generated rows 
//  The AccumuloOutputFormat methods 
//  For Hybrid Grace Hash Join, we need to see if there is any spilled data to be processed next 
// should not happen - this should not get called before this.start() is called 
//  A rewriting was produced, we will check whether it was part of an incremental rebuild   to try to replace INSERT OVERWRITE by INSERT 
//  Acquire lock 
//  Add HBase related configuration to Spark because in security mode, Spark needs it   to generate hbase delegation token for Spark. This is a temp solution to deal with   Spark problem. 
//  non-bean .. 
//  The fourth could be combined again. 
//  field name   field type 
//  try merge join tree from inner most source   (it was merged from outer most to inner, which could be invalid)     in a join tree ((A-B)-C)-D where C is not mergeable with A-B,   D can be merged with A-B into single join If and only if C and D has same join type 
//  Note: GROUPING SETS are not allowed with map side aggregation set to false so we don't have to worry about it 
//  descriptors for subq1 and subq2 are linked. 
// close one connection, verify still two left 
/* writable */
//  Incremental Repl A -> B with alters on db/table/partition 
//  Something else is wrong. 
//  run some queries 
//  Perform any partition expressions.  Results will go into scratch columns. 
//  [-H|--help] 
//  From https://msdn.microsoft.com/en-us/library/ms190476.aspx   e1 % e2   Precision: min(p1-s1, p2 -s2) + max( s1,s2 )   Scale: max(s1, s2) 
//  Thread local configuration is needed as many threads could make changes 
//  4. Bailout if select involves Transform 
/*        * Verify that new job requests should succeed with no issues.        */
//  Test that existing shared_read partition with new shared_write coalesces to 
//  4. Perform a major compaction. 
// LlapIoImpl.LOG.info(prefix(ix) + " adding " + header(log[ix + 2]) + " to "      + getSecondInt(log[ix]) + " before " + log[ix + 3]); 
//  Add the function name as a WriteEntity 
//  Check if the given SparkTask has a child SparkTask that contains the target MapWork   If it does not, then remove the target from DPP op 
//  if this is a valid executable command then add it to the buffer 
//  This helper object deserializes known deserialization / input file format combination into   columns of a row in a vectorized row batch. 
//  construct a new DecimalFormat only if a new dValue 
/*      * Rows with a rank <= rankLimit are output.     * Only the first row with rank = rankLimit is output.      */
/*    * If the cInfo is for an ASTNode, this function returns the ASTNode that it is for.    */
//  DPP indeed, Set parallel edges true 
//  This is no-op, there is no column to assign to and val is expected to be null 
/*      *  For the moment, pretend all matched are selected so we can evaluate the value     *  expressions.     *     *  Since we may use the overflow batch when generating results, we will assign the     *  selected and real batch size later...      */
// if no update or delete in Merge, there is no need to to do cardinality check 
//  Create a dummy key for searching the owid/bucket in the compressed owid ranges. 
//  Immediate retry 
//  create matcher for custom path 
//  Add aggregate A (see the reference example above), the top aggregate 
//  re-read length 
//  create a new batch with one char column (for input) and one long column (for output) 
//  causing each thread to get a different client even if the conf is same. 
//  additional rows corresponding to grouping sets need to be created here. 
//  TODO test changes to mark cleaned to clean txns and txn_components 
//  2/ write the size of the list as a VInt 
//  If we can put multiple group bys in a single reducer, determine suitable groups of   expressions, otherwise treat all the expressions as a single group 
//  This assumes no ranges passed to cache to fetch have data beforehand. 
//  save join type 
//  The sorting order of the child RS is more specific than   that of the parent RS. Assign the sorting order of the child RS   to the parent RS. 
//  1,2,3,4 
// when last txn finished (abortTransaction/commitTransaction) the currentTxnIndex is pointing at that txn  so we need to start from next one, if any.  Also if batch was created but 
//  do not overwrite. 
//  with the values generated and propagated from the right input 
//  Skip the colinfos which are not for this particular alias 
//  or for a map-reduce job 
// create HiveSessionImpl object 
//  pipeline which can cause a cycle after hashjoin optimization. 
//  REVOKE_GRANT_OPTION 
//  return true if current min segment(FetchOperator) has next row 
//  translate work to vertex 
//  Get the positions for partition, bucket and sort columns 
//  Initialize the config 
//  traversing an operator tree 
//  The RPC server will take care of timeouts here. 
//  Handle child tasks here. We could add them directly whereever we need, 
//  positive Unix time 
//  BUCKET_COLS 
//  available and grammar check is there in the language itself. 
//  IntWritable so we can just sum in the reduce 
//  full paths are replaced with base filenames 
//  Test if the user session specific conf overlaying global init conf. 
/*       Due to the limitation that we can only have one instance of Persistence Manager Factory in a JVM      we are not able to create multiple embedded derby instances for two different MetaStore instances.     */
//  Couldn't find reference to expression 
/*  Consider a query like:     *     * select * from     *   (subq1 --> has a filter)     *   join     *   (subq2 --> has a filter)     * on some key     *     * Let us assume that subq1 is the small table (either specified by the user or inferred     * automatically). The following operator tree will be created:     *     * TableScan (subq1) --> Select --> Filter --> DummyStore     *                                                         \     *                                                          \     SMBJoin     *                                                          /     *                                                         /     * TableScan (subq2) --> Select --> Filter      */
// Non partitioned table 
//  by using signatures and encryption. 
//  try a valid alter table 
//  Use table properties in case of unpartitioned tables,   and the union of table properties and partition properties, with partition 
//  Find first non-sign (0xff) byte of input. 
//  INSERT INTO 2 partitions and get the last repl ID 
// retain this digit 
//  For use from within HCatClient.getPartitions(). 
//  In particular, we use size of data in RS x number of uses. 
//  Singleton using DCL. 
//  used by GroupBy 
//  Insert this map into the stats 
//  Go over the subqueries and getMetaData for these 
// as with grant also implies without grant privilege, add without privilege as well 
//  Get the existing table 
//  In case of views, the underlying views or tables are not direct dependencies   and are not used for authorization checks.   This ReadEntity represents one of the underlying tables/views, so skip it.   See description of the isDirect in ReadEntity 
//  Fill up as much of the overflow batch as possible with small table values. 
//  regular CREATE TABLE   CREATE TABLE LIKE ... (CTLT)   CREATE TABLE AS SELECT ... (CTAS) 
//  Set SSL 
//  Now trim the overstuffed histogram down to the correct number of bins 
//  Indicates that we are in test mode. 
//  7 : Drop table T2 => 1 event 
//  TableScan of a non-Hive table - don't support for materializations. 
/*  Returning true does not guarantee that the task will run, considering other queries    may be running in the system. Also depends upon the capacity usage configuration      */
//  tests not setting maxRows 
//  if we don't have column stats, we just assume hash aggregation is disabled 
//  SparkWork 
//  Increase target list pos.   Target list is being drained.   Set delta and refcount. 
//  Look for a hint to not run a test on some Hadoop versions 
/*    * Helper function to create JobConf for specific ReduceWork.    */
//  Methods that create group keys and aggregate calls 
//  required   required   required   required   optional   optional   optional   optional 
//  Run Cleaner.   This run doesn't do anything for the above aborted transaction since   the current compaction request entry in the compaction queue is updated   to have highest_write_id when the worker is run before the aborted   transaction. Specifically the id is 2 for the entry but the aborted   transaction has 3 as writeId. This run does transition the entry 
//  contains aliases from sub-query   we are just converting to a common merge join operator. The shuffle   join in map-reduce case. 
//  The overwhelming majority of cases will go here. Read 3 bytes. Tada! 
//  [==================>>-----] 
//  The entire current slice is part of the split. Note that if split end EQUALS   lastEnd, the split would also read the next row, so we do need to look at the   next slice, if any (although we'd probably find we cannot use it).   Note also that we DO NOT treat end-of-file differently here, cause we do not know   of any such thing. The caller must handle lastEnd vs end of split vs end of file   match correctly in terms of how LRR handles them. See above for start-of-file. 
//  the return values are capped to return ==0, ==1 and >= 2 
//  A launchable task is one that hasn't been queued, hasn't been   initialized, and is runnable. 
//  Has the user enabled merging of files for map-only jobs or for all jobs 
//  partitioned table => stats not updated 
//  Look for InputFileFormat / Serde combinations we can deserialize more efficiently   using VectorDeserializeRow and a deserialize class with the DeserializeRead interface.     Do the "vectorized" row-by-row deserialization into a VectorizedRowBatch in the 
/*            * If this is the last file for this bucket, maxKey == null means the split is the tail           * of the file so we want to leave it blank to make sure any insert events in delta           * files are included; Conversely, if it's not the last file, set the maxKey so that           * events from deltas that don't modify anything in the current split are excluded */
//  Case 5 - Max in list members: 1000; Max query string length: 10KB 
//  if all bits are set, expected should be 0 
//  ((R1.x=R2.x) and R1.z=10)) and rand(1) < 0.1 
//  Create/Delete/Write/Admin to creator 
//  minus 1 here otherwise driver is also counted as an executor 
//  Let uval be the value of the unsigned long with the same bits as x   Two's complement => x = uval - 2*MAX - 2   => uval = x + 2*MAX + 2   Now, use the fact: (a+b)/c = a/c + b/c + (a%c+b%c)/c 
//  VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + " NOMATCH duplicate"); 
//  schema info. 
//  Prepare the list of databases 
// batching is not enabled. Try to drop all the partitions in one call 
//  move any incompatible files to final path 
//  Check if external table property being removed 
//  remove values in key exprs for value table schema   value expression for hashsink will be modified in   LocalMapJoinProcessor 
// do this check recursively on all the parent roles of curRole 
//  deserializer is null in case of VectorMapOperator 
//  TODO When will the queue ever be null.   Pass queue and default in as constructor parameters, and make them final. 
//  set memory usage for the MJ operator 
/*    * the names of the Columns of the input to MatchPath. Used to setup the tpath Struct column.    */
//  create a proxy for the local filesystem   the scheme/authority serving as the proxy is derived   from the supplied URI 
//  Get the sort positions and sort order for the table 
//  Adjust the aggregator argument positions.   Note aggregator does not change input ordering, so the input   output position mapping can be used to derive the new positions 
//  skewed Keys which intersect with join keys 
//  Leaf 
//  Sort does not change input ordering 
//  false for the following cases:   * name is "list", which matches the spec   * name is "bag", which indicates existing hive or pig data 
//  the fromIndex(inclusive) and toIndex(exclusive) for each unique owid. 
//  create a syntax tree for a simple function call "longudf(col0)" 
//  tenScale <= SqlMathUtil.MAX_POWER_TEN_INT31   so, we can make this as a long comparison 
//  look for matches in time based counters 
//  If both are numeric, make sure the new type is larger than the old. 
//  If the database is newer than the create event, then noop it. 
//  No-op. 
//  Use the maximum parallelism from all parent reduce sinks 
//  The GroupByOperator is not initialized, which means there is no   data   (since we initialize the operators when we see the first record).   Just do nothing here. 
//  We use the sign of the reversed-nanoseconds field to indicate that there is a second VInt   present. 
//  we also collect table stats while collecting column stats. 
//  LOG.debug("VectorMapJoinFastBytesHashMap findWriteSlot slot " + slot + " tripleIndex " + tripleIndex + " existing"); 
/*  * Simple one long key map join benchmarks. * * Build with "mvn clean install -DskipTests -Pdist,itests" at main hive directory. * * From itests/hive-jmh directory, run: *     java -jar target/benchmarks.jar org.apache.hive.benchmark.vectorization.mapjoin.MapJoinMultiKeyBench * *  {INNER, INNER_BIG_ONLY, LEFT_SEMI, OUTER} *    X *  {ROW_MODE_HASH_MAP, ROW_MODE_OPTIMIZED, VECTOR_PASS_THROUGH, NATIVE_VECTOR_OPTIMIZED, NATIVE_VECTOR_FAST} *  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBytes(java.lang.String, byte[])    */
/*  Object Inspectors corresponding to the struct returned by TerminatePartial and the     * fields within the struct - "maxLength", "sumLength", "count", "countNulls"      */
//  only one of belows is not-null   total length of sample, prunes splits exceeded   percent to total input, prunes splits exceeded   row count per split, do not prune splits 
// as of (8/27/2014) Hive 0.14, ACID/Orc requires HiveInputFormat 
//  Retrieve all partitions generated from partition pruner and partition column pruner 
//  The type of the Hive column   Cannot store the actual TypeInfo because that would require 
//  Check if this UDF has been provided with type params for the output char type 
//  ALTER TABLE ... ADD COLUMNS 
//  right is larger 
//  [b,a,c,a,b,c] 
//  Test 4 variations of callbacks. 2 increases/2 revokes - do not update the same task again;   Then, increase + decrease and decrease + increase, the 2nd call coming after the message is sent;   the message callback should undo the change. 
//  Append this container to the loaded list 
/*  * Captures the Window processing specified in a Query. A Query may * contain: * - UDAF invocations on a Window. * - Lead/Lag function invocations that can only be evaluated in a *   Partition. * - For Queries that don't have a Group By all UDAF invocations are *   treated as Window Function invocations. * - For Queries that don't have a Group By, the Having condition is *   handled as a post processing on the rows output by Windowing *   processing. * Windowing is a container of all the Select Expressions that are * to be handled by Windowing. These are held in 2 lists: the functions * list holds WindowFunction invocations; the expressions list holds * Select Expressions having Lead/Lag function calls. It may also * contain an ASTNode representing the post filter to apply on the * output of Window Functions. * Windowing also contains all the Windows defined in the Query. One of * the Windows is designated as the 'default' Window. If the Query has a * Distribute By/Cluster By clause; then the information in these * clauses is captured as a Partitioning and used as the default Window * for the Query. Otherwise the first Window specified is treated as the * default. * Finally Windowing maintains a Map from an 'alias' to the ASTNode that * represents the Select Expression that was translated to a Window * Function invocation or a Window Expression. This is used when * building RowResolvers.  */
//  2^30 (we cannot use Integer.MAX_VALUE which is 2^31-1). 
//  Because Pairs give Java the vapors. 
//  returns null always 
//  if level is <0, the return all files/directories under the specified path 
//  If it's not in this property, it won't be in any others 
//  Equivalent to acidSinks, but for DDL operations that change data. 
//  Rewritten grouping function 
// remove currTask from childTasks 
//  65535 and Integer.MAX_VALUE; the check for VARCHAR precision is done in Hive. 
//  Maximum reasonable defragmentation headroom. Mostly kicks in on very small caches. 
//  storing nanosecond interval in 2 longs) produces a timestamp. 
// the the bucket we care about here 
//  start inclusive to infinity inclusive 
//  Table comment 
//  Make sure we cross some buffer boundaries... 
//  -Xmx speficied in GB 
//  have to override it with the new conf since this is where   prewarm gets the conf object 
//  Scale down again. 
//  we need a new object to obey our immutable behavior. 
//  while initializing so this need to be done here instead of constructor 
//  Release, but keep the lock (if present). 
//  Case 7: NO column stats 
//  first call to createPartitions should throw exception 
//  harUri is used to access the partition's files, which are in the archive   The format of the RI is something like: 
//  we are creating filter here so should not be returning NULL.   Not sure why Calcite return NULL 
//  Check en edge case to throw Exception if we can not build a single query for 'NOT IN' clause cases as mentioned at the method comments. 
//  We're sure this part is smaller than memory limit 
//  Replace is essentially renaming a plan to the name of an existing plan, with backup. 
//  SUCCESSFUL 
//  TODO: create this centrally in HS2 case 
/*    * DECIMAL.   *   * NOTE: The scale parameter is for text serialization (e.g. HiveDecimal.toFormatString) that   * creates trailing zeroes output decimals.    */
// now make an ExportTask from temp table 
//  7 pending   3 pending   5 pending 
//  3. For missing Wdw Frames or for Frames with only a Start Boundary, completely 
//  not be able to acquire the lock within that time period 
//  DICTIONARY encoding 
//  The output columns for the destination table should match with the join keys   This is to handle queries of the form:   insert overwrite table T3   select T1.key, T1.key2, UDF(T1.value, T2.value)   from T1 join T2 on T1.key = T2.key and T1.key2 = T2.key2   where T1, T2 and T3 are bucketized/sorted on key and key2   Assuming T1 is the table on which the mapper is run, the following is true:   . The number of buckets for T1 and T3 should be same   . The bucketing/sorting columns for T1, T2 and T3 should be same   . The sort order of T1 should match with the sort order for T3.   . If T1 is partitioned, only a single partition of T1 can be selected.   . The select list should contain with (T1.key, T1.key2) or (T2.key, T2.key2) 
//  ConsoleReader will do the substitution if and only if there   is exactly one valid completion, so we ignore other cases. 
//  Table DDL 
//  The current expression node is a column, see if the column alias is already a part of   the return set, s. If not and we already have an entry in set s, this is an invalid expression 
//  Use Case 5. 
//  Validate the first parameter, which is the expression to compute over. This should be an 
//  default is false 
//  Open txns are already sorted in ascending order. This list may or may not include HWM   but it is guaranteed that list won't have txn > HWM. But, if we overwrite the HWM with currentTxn 
//  get the #nulls 
//  Utility to get patterns from a url, every array element is match for one 
//  the stack is the Table scan operator. 
//  should have rolled over to next transaction batch 
//  First we quickly check if the two RS operators can actually be merged.   We already know that these two RS operators have the same parent, but   we need to check whether both RS are actually equal. Further, we check   whether their child is also equal. If any of these conditions are not 
//  Format 
//  set results to be returned 
//  Next, put row into corresponding hash partition 
//  List of locks to protect the above list 
//  Partitions 
//  they will be different values. 
/*    * Flush any catalog objects held by the metastore implementation.  Note that this does not   * flush statistics objects.  This should be called at the beginning of each query.    */
//  Concurrent increase and revocation, then another increase - after the message is sent. 
//  Add all the names for previous batches. 
/*    * represents Column information exposed by a QueryBlock.    */
//  If the struct is null and level > 1, DynamicSerDe will call   writeNull(); 
//  cached to use serialize data 
//  If the Group by operator has null key 
//  All remaining functions simply delegate to objectStore 
//  Column not found in target table. Its a new column. Its schema is map<string,string> 
//  A printStream that stores messages logged to it in a list. 
//  set default spark configurations. 
//  Test that fetching a non-existent db-name yields ObjectNotFound. 
//  to break into multiple batches, remove duplicates first. 
//  This second attempt to create it should throw 
// recursively remove this task from its children's parent task 
//  If replace flag is not set by caller, then by default set it to true to maintain backward compatibility 
//  Fill up host1 with p2 tasks.   Leave host2 empty   Try running both p1 tasks on host1.   R: Single preemption triggered, followed by allocation, followed by another preemption.   
//  reduce sink row resolver used to generate map join op 
//  There's some access (to get ACLs), so assume it means free for all. 
//  Cleanup 
//  This is expected to be a no-op, so we will return null when we use local metastore. 
//  5. Introduce Project Rel above original left/right inputs if cast is 
//  With Data - Unsupported for the test case 
//  test that overflow produces null 
// ---------------------------------------------------------------------------   Outer join specific members.   
//  Tracks vertices for which notifications have been registered 
//  Descendants tasks who subscribe feeds from this task 
//  Don't fail the query - just return null (caller should skip cache lookup). 
//  CONSIDER: Validation of type information 
// For unit testing, no harm in hard-coding allocator ceiling to LONG.MAX_VALUE 
//  create Parquet file with specific data 
//  Get writable object 
//                           MRConfig.SHUFFLE_SSL_ENABLED_DEFAULT)) {          LOG.info("Encrypted shuffle is enabled.");          sslFactory = new SSLFactory(SSLFactory.Mode.SERVER, conf);          sslFactory.init();        } 
//  Cache patternkey.matcher(path).matches() 
//  We are now positioned at the end of this field's bytes. 
// clean up 
//  The exception has been logged by the lower layer. 
//  Try to construct an object 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBlob(java.lang.String,   * java.io.InputStream)    */
/*      * what is the index of the row beyond the set of rows that match this pattern.      */
//  array level   array level 
//  MiniHS2 cluster is up .. let it run until someone kills the test 
//  TYPE 
//  to check if all field inspectors are initialized 
//  A larger scale is ok -- we will knock off lower digits and round. 
//  Context handler 
//  ignore the first child, since it is the variable 
//  op is a DemuxOperator and it directly connects to childOP.   We will add this MuxOperator between DemuxOperator   and childOP. 
//  Add a partition key & ensure its reflected in the schema. 
//  framework expects MapWork instances that have no physical parents (i.e.: union parent is   fine, broadcast parent isn't) 
//  getXXX returns 0 for numeric types, false for boolean and null for other 
//  1.2. With that in mind, determine disk ranges to read/get from cache (not by stream). 
//  guard against poor configuration of noconditional task size. We let hash table grow till 2/3'rd memory   available for container/executor 
//  This processing only needs to happen for the small tables 
//  Verify that the created table is identical to sourceTable. 
//  required string dest_input_name = 3; 
//  row[0] is the column name 
//  Round fractional must be 0.  Not allowed to throw away digits. 
//  direct heap allocations need to be safer 
/*    * intialize the tables    */
//  if null, all tables/views are returned 
//  It passed the test, load 
//  UnsupportedOperationExceptions happen in the normal course of business,   so no need to log them as errors all the time. 
// ~ Instance fields -------------------------------------------------------- 
//  It is not a cast 
//  We are done. 
//  -a <authType> 
//  Aggregate above to sum up the sub-totals 
//  if oldPath is a subdir of destf but it could not be cleaned 
//  We can always do equality predicate. Just need to make sure we get appropriate   BA representation of constant of filter condition.   We can do other comparisons only if storage format in hbase is either binary   or we are dealing with string types since there lexicographic ordering will suffice. 
//  From the value arrays and our isRepeated, selected, isNull arrays, generate the batch! 
//  Mixed sign cases 
//  must be honored 
//  Make sure the parent directory exists.  It is not an error   to recreate an existing directory 
//  scope opened/closed once (multiple opens do not count) 
//  For now, we don't take any pool action. In future, we might restore the session based   on this and get rid of the logic outside of the pool that replaces/reopens/etc. 
//  Call the executor which will execute the appropriate command based on the parsed options 
//  Order expressions which will only get set and used for RANGE windowing type 
//  Phase 2 - verify we get the expected objects created by all threads. 
//  2) We check whether output works when we merge the operators will collide.       Work1   Work2    (merge TS in W1 & W2)        Work1         \   /                  ->                  | |       X         Work3                                     Work3     If we do, we cannot merge. The reason is that Tez currently does   not support parallel edges, i.e., multiple edges from same work x 
//  Stores big table rows as bytes for native vector map join. 
//  forward compatible 
//  try outer row resolver 
//  check value argument 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#registerOutParameter(int, int,   * java.lang.String)    */
//  Test setting fetch size above max 
//  Test that read blocks exclusive 
// do not authorize dummy readEntity or writeEntity 
//  We are the left-most child. 
//  divide by 10**-tenScale. check for rounding. 
//  Definitely not an int. 
//     runStatementOnDriver("select b from " + Table.ACIDTBL + " where a in (select b from " + Table.NONACIDORCTBL + ")"); 
//  Using utility method above, so that JDODataStoreException doesn't   have to be used here. This helps avoid adding jdo dependency for   hcatalog client uses 
//  Try regular properties; 
//  This is OK because the pending events will be sent via the succeeded/failed messages.   TaskDone is set before taskSucceeded/taskTerminated are sent out - which is what causes the   thread to exit 
//  Read notification from metastore 
//  required string vertex_name = 2; 
//  roundPower < fastScale 
//  Noone can take this buffer out and thus change the level after we lock, and if   they take it out before we lock, then we will fail to lock (same as   prepareOneHeaderForMove). 
//  We defer allocation until we really need it since in the common case there is   no CRLF substitution. 
//  CONCERN: isRepeating 
//  get_partitions will parse out the catalog and db names itself 
/*        * >> Super set of       * If the grouping columns are a,b,c and the sorting columns are a,b       * grouping columns >> sorting columns       * (or grouping columns are a superset of sorting columns)       *       * Similarly << means subset of       *       * No intersection between Sort Columns and BucketCols:       *       * 1. Sort Cols = Group By Cols ---> Partial Match       * 2. Group By Cols >> Sort By Cols --> No Match       * 3. Group By Cols << Sort By Cols --> Partial Match       *       * BucketCols <= SortCols (bucket columns is either same or a prefix of sort columns)       *       * 1. Sort Cols = Group By Cols ---> Complete Match       * 2. Group By Cols >> Sort By Cols --> No Match       * 3. Group By Cols << Sort By Cols --> Complete Match if Group By Cols >= BucketCols       * --> Partial Match otherwise       *       * BucketCols >> SortCols (bucket columns is a superset of sorting columns)       *       * 1. group by cols <= sort cols --> partial match       * 2. group by cols >> sort cols --> no match       *       * One exception to this rule is:       * If GroupByCols == SortCols and all bucketing columns are part of sorting columns       * (in any order), it is a complete match        */
//  See planIndexReading - only read non-row-index streams if involved in SARGs. 
//  The partition was dropped before we got around to cleaning it. 
//  Clean up 
/* for multi-statement txns, you may have multiple events for the same      * row in the same (current) transaction.  We want to collapse these to just the last one      * regardless whether we are minor compacting.  Consider INSERT/UPDATE/UPDATE of the      * same row in the same txn.  There is no benefit passing along anything except the last      * event.  If we did want to pass it along, we'd have to include statementId in the row      * returned so that compaction could write it out or make minor minor compaction understand      * how to write out delta files in delta_xxx_yyy_stid format.  There doesn't seem to be any      * value in this.      *      * todo: this could be simplified since in Acid2 even if you update the same row 2 times in 1      * txn, it will have different ROW__IDs, i.e. there is no such thing as multiple versions of      * the same physical row.  Leave it for now since this Acid reader should go away altogether      * and org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader will be used. */
//  Show we cannot create a child of the directory with no permissions 
// -----------------------------------------------------------------------------------------------   Serialization methods.  ----------------------------------------------------------------------------------------------- 
/*  1. is defined with skewed columns and skewed values in metadata  */
//  For HIVE-18817, the only thing missing is the last stripe index information.   Get the information from the last stripe and append it to the existing index.   The actual stripe data can be written as-is, similar to OrcFileMergeOperator. 
/*    * tests if the beeline behaves like default mode if there is no user-specific connection   * configuration file    */
//  Note: we don't take the scheduling lock here, although the call to queue is still 
//  a schema retriever has been provided as well. Attempt to read the write schema from the   retriever 
//  Equal groups, return what we can handle 
//  Keep buckets from the streaming relation 
//  Wait for startup to complete 
//  Alter db "testDatabaseOps" via ObjectStore 
//  Get a list of aliases for the same column 
//  We won't try to be too strict in checking this because we're comparing   table create intents with observed tables created.   If it does have a location though, we will compare, as with external tables 
//  modify it below as part of imposing view column names. 
//   only tablename no pattern and db 
//  we will put a fork in the plan at the source of the reduce sink 
// -----------------------------------------------------------------------------------------------   Standard overrides methods.  ----------------------------------------------------------------------------------------------- 
//  eventSourceTableDesc, eventSourceColumnName, evenSourcePartKeyExpr move in lock-step.   One entry is added to each at the same time 
//  This likely indicates that an instance has recently restarted 
//  256KB 
//  Float.compare() treats -0.0 and 0.0 as different 
//  The types array tells us the number of columns in the data 
//  Get partition column stats for this table 
//  if one of them is null, replace the old params with the new one 
//  Map keys are required to be primitive and may be serialized in binary format 
//  skip this column 
//  Create a separate thread to send the events. 
//  This is the GenericUDAF name 
//  when column name is specified in describe table DDL, colPath will   will be table_name.column_name 
//  Test that MAX_ADDITIONAL_SECONDS_BITS is really the maximum value of the 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceTest#setUp()    */
// Order matters, assuming later that __time_granularity comes first then __druidPartitionKey 
//  read the contents and make sure they match 
//  Fraction digit continue into highest longword. 
//  If table is dropped when dump in progress, just skip partitions dump 
//  Find the appropriate storage descriptor 
//  we will bail out; we do not want to end up with limits all over the tree 
//  this should probably never happen, see 
//  runCtx and ctx share the configuration, but not isExplainPlan() 
//  outPath will be   non-union case: <table-dir>/<staging-dir>/<partition-dir>/<taskid>   union case: <table-dir>/<staging-dir>/<partition-dir>/<union-dir>/<taskid> 
//  round up to the next MB 
//  this should work as comments are stripped by HiveCli 
//  location: set to null here, can be overwritten by the IMPORT stmt 
//  let's see if we can go one step further and just uber this puppy 
//  Create and delete a temp file 
//  Case 1: If there's no delay for the heartbeat, txn should be able to commit 
//  finalization 
// principal is specified, authorize on it 
//  Original bucket files and delta directory should stay until Cleaner kicks in. 
//  2. use util function to aggr stats 
//  now, correct +1 error and rounding up. 
//  store the new joinContext 
/*  Construct list bucketing location mappings from sub-directory name.  */
//  setup uncaught exception handler for the current thread 
//  changing the tags. 
//  For partitioned tables, get the size of all the partitions after pruning 
//  the reason that we set the txn manager for the cxt here is because each   query has its own ctx object. The txn mgr is shared across the 
//  This is the one we are expecting. 
//  If not visited yet 
//  HIVE_JOB_CREDSTORE_PASSWORD 
// mock operation and opHandle for operationManager 
//  Confirm that the function is now gone 
//  Most tests are done with ANSI SQL mode enabled, set it back to true 
//  Since HiveMetaStoreClient is not threadsafe, hive clients are not  shared across threads.   Thread local variable containing each thread's unique ID, is used as one of the keys for the cache 
//  Update min if min is lesser than the smallest value seen so far 
//  This is rather convoluted... to simplify for perf, we could call getRawKeyValue   instead of writable, and serialize based on Java type as opposed to OI. 
//  Used by DatumReader.  Applications should not call.  
//  No materialized view or not heuristic approach, normal costing 
/*     * update the Work name which referred by Operators in following Works.     */
//  No skew on the table to take care of 
//  Try with ascii chars 
//  recall setup so that we get an object store with the metrics initalized 
//  Note: we must handle downgradedTask after this. We do it at the end, outside the lock. 
//  Note that this is not a typical list accumulator - there's no call to finalize   the last list. Instead we add list to SD first, as well as locally to add elements. 
//  Deserialize the on-disk hash table 
//  4 buildup the ob expr AST 
//  where MAPREDUCE-1501 is not present 
//  Localize the non-conf resources that are missing from the current list. 
//  Is the result Decimal64 precision? 
//  INSERT_DATA 
//  Not sure how to get around that. 
//  ignore the first static partition 
//  Keep a copy of HiveConf so if Session conf changes, we may need to get a new HMS client. 
/*  Get number of partitions  */
//  Descending 
//  clean request 
//  trim line 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#registerOutParameter(int, int, int)    */
//  Generates a sequence list of indexes 
//  Check a random one. These can change w/Hive versions. 
//  If string is entirely whitespace, no need to apply trailingSpaces. 
//     assertHelper(AssertHelperOp.SAME, pm0, pm1, TableScanOperator.class); 
//  The above members are initialized by the constructor and must not be   transient.  --------------------------------------------------------------------------- 
//  Generate SQLOperator for merging the aggregations 
/*    * update the SQL string with parameters set by setXXX methods of {@link PreparedStatement}   *   * @param sql   * @param parameters   * @return updated SQL string   * @throws SQLException     */
//  Fill the column vector with the provided value 
//  Store as a list to have a consistent order between getTests, and the test argument generation. 
//  2*a 
//  Process --help 
//  -i <init file> 
//  Fill up host1 with tasks. Leave host2 empty. 
// actually temp table does not support partitions, cascade is not applicable here 
//  drop tables before dropping db 
//  FETCH_TYPE 
//  where the first event refers to source path and  second event refers to CM path 
//  Generate Spark plan 
//  Don't allow a query that returns everything, it will blow stuff up. 
// Check if the configure job properties is called from input   or output for setting asymmetric properties 
//  There should be one exception message per file 
//  after the stats phase we might have some cyclic dependencies that we need 
//  Insert search in the beginning would have failed if these parents didn't exist. 
//  columnExprMap has the reverse of what we need - a mapping of the internal column names   to the ExprNodeDesc from the previous operation.   Find the key/value where the ExprNodeDesc value matches the column we are searching for. 
// if conflict updates p=1/q=3 else update p=1/q=2  deletes from p=1/q=2, p=2/q=2  insert p=1/q=2, p=1/q=3 and new part 1/1 
//  Only valid if allKeyInputColumnsRepeating is true. 
//  for base type, do nothing. Other types, like structs may initialize   internal data   structures. 
//  No value found. 
//  Every thread created by this thread pool will use the same handler 
//  time part 
//  Test 
//  Check if we are in LLAP, if so it needs to be determined if we should use BMJ or DPHJ 
//  NAMES 
//  inlined> 
//  Skip all the INSERT events 
//  SERDE_NAME 
//  Kryo docs say 0-8 are taken. Strange things happen if you don't set an ID when registering   classes. 
//  create N-1 map join tasks 
//  Username and password are added to the http request header. 
/*  This writer will be created when writing the first row in order to get  information about how to inspect the record data.   */
/*  Constructors  */
//  lastly it should be in deDupedNonDistIrefs 
//  The dispatcher finds SMB and if there is semijoin optimization before it, removes it. 
//  generates initial key 
//  Write to a URI, no locking done for this 
//  production is: i32 
//  Populate the cache. 
//  update the keys to use operator name 
//  We could start a move if it's being evicted, but let's not do it for now. 
//  Test that two shared read locks can share a partition 
/*    * If hive job credstore location is not set, but hadoop credential provider is set   * jobConf should contain hadoop credstore location and password should be from HADOOP_CREDSTORE_PASSWORD    */
//  Go over all the tasks and dump out the plans 
//  Round a double to 8 decimal places. 
//  Store the row. See comments above for why we need a new copy of the row. 
//  eg, group name TaskCounter_Map_7_OUTPUT_Reducer_8, counter name OUTPUT_RECORDS 
//  Go over the list and find if a reducer is not needed 
//  An array of hash map results so we can do lookups on the whole batch before output result 
//  Finally, add the files to the existing AM (if any). The old code seems to do this twice,   first for all the new resources regardless of type; and then for all the session resources   that are not of type file (see branch-1 calls to addAppMasterLocalFiles: from updateSession   and with resourceMap from submit). 
//  5. Add Proj on top of TS 
//  figure out if it is table level or partition level 
// Read the record with existing record reader ID and same **evolved** schema 
//  user did not specfied alias names, infer names from outputOI 
//  If it's a standard map reduce task, check what, if anything, it inferred about 
//  returns true as long as there is more data in mockResultData array 
//  ////// Generate ReduceSink Operator 
//  This data structure is needed to create the new project 
//  Arithmetic with a type date (LongColumnVector storing epoch days) and type interval_year_month (LongColumnVector storing 
//  Report progress for each stderr line, but no more frequently than once   per minute. 
// setBoolean  setBoolean  setShort  setInt  setFloat  setDouble  setString  setLong  setByte  setByte  setString  setTimestamp 
//  5 rows should be returned 
//  5. Run other optimizations that do not need stats 
//  Confirm default managed table paths 
//  Re-enable the node, if a task completed due to preemption. Capacity has become available,   and we may have been able to communicate with the node. 
//  assign 0 to simplify hashcode 
//  1. If we need to sort tuples based on the value of some   of their columns 
//  Unescape the partition name 
//  2. Local scratch dir 
//  2nd cancel 
//  BYTE_VAL 
//  All output columns used for bucketing/sorting of the destination table should   belong to the same input table     insert overwrite table T3     select T1.key, T2.key2, UDF(T1.value, T2.value)     from T1 join T2 on T1.key = T2.key and T1.key2 = T2.key2   is not optimized, whereas the insert is optimized if the select list is either changed to   (T1.key, T1.key2, UDF(T1.value, T2.value)) or (T2.key, T2.key2, UDF(T1.value, T2.value)) 
//  They both require DbTxnManager and both need to recordValidTxns when acquiring locks in Driver 
//  Mapping from expression node to is an expression containing only   partition or virtual column or constants 
//  2nd bit set - element 2 was already in cache.   Should have been replaced 
//  add some multi-byte characters to test length routine later. 
//  spark has its own config for merging 
//  generate pruned column list for all relevant operators 
//  For replicating an HCatPartition definition. 
//  operator native... 
//  ////// 3. Generate ReduceSinkOperator2 
//  from 2/31 to 2/28 
//  Set the conf variable values for this test. 
//  blank " " (1 byte)   Cyrillic Capital DJE U+402 (2 bytes) 
//  HS2 operation handle guid string 
//  infrastructure is in place 
//  nodeMap registration is not required, since there's no taskId association. 
//  it has an open txn in it 
//  set the operation handle information in Driver, so that thrift API users   can use the operation handle they receive, to lookup query information in 
//  First branch is query, second branch is MV 
/*    * If any operator which does not allow map-side conversion is present in the mapper, dont   * convert it into a conditional task.    */
//  This configuration is used only for client side configuration. 
//  the basic premise here is that we will rsync the directory to first working drone   then execute a local rsync on the node to the other drones. This keeps   us from executing tons of rsyncs on the master node conserving CPU 
//  Date comparisons 
//  Create directory 
//  remove the last ',' 
//  2*b 
//  its list of join keys 
//  Check what happens, when we ignore these errors 
//  connection secret show up in the child process's command line. 
//  else ptn already exists, but we do nothing with it. 
//  CONSDIER: We need a type name parser for TypeDescription. 
//  An LRU cache using a linked hash map 
//  message kept around in for debugging 
//  OBJ_TO_REFRESH 
//  lengths stream could be empty stream or already reached end of stream before present stream. 
//  If the catalog name isn't set, we need to go through and set it. 
//  We may have already created the tables and thus don't need to redo it. 
/*    * Result of JobCallable task after successful task completion. This is   * expected to be set by the thread which executes JobCallable task.    */
//  If a failure happens here, the intermediate archive files won't be 
//  Make sure the map does not expand; should be able to find space. 
//  All of the insert, update, and delete tests assume two tables, T and U, each with columns a,   and b.  U it partitioned by an additional column ds.  These are created by parseAndAnalyze   and removed by cleanupTables(). 
//  We remove the tasks above without state checks so just reset all metrics to 0. 
//  if the outputType is a float cast the arguments to float to replicate the overflow behavior   in non-vectorized UDF GenericUDFPosMod 
//  For cardinality values use numRows as default, try to use ColStats if available 
//  Single-Column Long hash table import. 
//  extract the real user from the given token string 
//  Allows for unescaped ASCII control characters in JSON values 
//  Do the same without specifying writer time zone. This tests deserialization of older records 
//  Skip over any leading 0s or 0xFFs 
//  skip the last (partitioning) column since it is always non-null 
//  Make sure to put the instance back again, in case it was removed as part of a 
//  rank the tables. 
//   cars <= 9 
// used for union (all?)  CO_PARTITION_EDGE  PARTITION_EDGE 
//  original transaction 
//  HIVE-14444 pending rename: before 
//  setup stats in the operator plan 
//  capture arguments to authorizer impl call and verify ip addresses passed 
//  Update the NEXT_WRITE_ID for the given table after incrementing by number of write ids allocated 
//        non-vectorized validates that explicitly during UDF init. 
//  unset   distribution of keys is fixed   can change reducer count (ORDER BY can concat adjacent buckets)   can redistribute into buckets uniformly (GROUP BY can)   do not wait for downstream tasks 
//  Set our current entry to null (since it's done) and try again. 
//  RESOURCE_PLAN_NAME 
//  Add hacks for well-known collections and maps to avoid estimating them. 
//  Read dpp outputs 
//  MIN_OPEN_WRITE_ID 
//  in case of "DESCRIBE FORMATTED tablename column_name" statement, colPath   will contain tablename.column_name. If column_name is not specified   colPath will be equal to tableName. This is how we can differentiate   if we are describing a table or column 
//  detect if there are attributes in join key 
// this happens if you change BUCKET_COUNT      } 
//  for now, totalResource = taskResource for llap 
//  Will come here if an Exception was thrown in map() or reduce().   Hadoop always call close() even if an Exception was thrown in map() or   reduce(). 
/* (MILLIS_PER_DAY - 1) */
//  Auto-generate column aliases 
//  COMPRESSED 
//  Recurse. 
/* Compaction preserves location of rows wrt buckets/tranches (for now) */
/*     * If the ndv of the PK - FK side don't match, and the PK side is a filter    * on the Key column then scale the NDV on the FK side.    *    * As described by Peter Boncz: http://databasearchitects.blogspot.com/    * in such cases we can be off by a large margin in the Join cardinality    * estimate. The e.g. he provides is on the join of StoreSales and DateDim    * on the TPCDS dataset. Since the DateDim is populated for 20 years into    * the future, while the StoreSales only has 5 years worth of data, there    * are 40 times fewer distinct dates in StoreSales.    *    * In general it is hard to infer the range for the foreign key on an    * arbitrary expression. For e.g. the NDV for DayofWeek is the same    * irrespective of NDV on the number of unique days, whereas the    * NDV of Quarters has the same ratio as the NDV on the keys.    *    * But for expressions that apply only on columns that have the same NDV    * as the key (implying that they are alternate keys) we can apply the    * ratio. So in the case of StoreSales - DateDim joins for predicate on the    * d_date column we can apply the scaling factor.     */
//  Change the plan to this structure.   Note that the aggregateRel is removed.     Project-A' (replace corvar to input ref from the Join)     Join (replace corvar to input ref from LeftInputRel)       LeftInputRel       RightInputRel(oreviously FilterInputRel) 
//  Serves as lock for itself. 
//  Remote Spark Context property. 
//  This reduce sink has been processed already, so the work for the parentRS exists 
//  order matters in all of these so block 
//  handles nulls in items[] 
/*    * Return Hadoop-native RestCsrfPreventionFilter if it is available.   * Otherwise, construct our own copy of its logic.    */
//  Set to 0 to start with. This will be decremented for all columns for which events   are generated by this source - which is eventually used to determine number of expected 
/*    * Determines whether the current node was actually closed and pushed. This   * should only be called in the final user action of a node scope.    */
//  infer foreign key candidates positions 
/*    * Generate the second GroupByOperator for the Group By Plan   * (parseInfo.getXXX(dest)). The new GroupByOperator will do the second   * aggregation based on the partial aggregation results.   *   * @param mode   *          the mode of aggregation (FINAL)   * @param genericUDAFEvaluators   *          The mapping from Aggregation StringTree to the   *          genericUDAFEvaluator.   * @return the new GroupByOperator   * @throws SemanticException    */
//  Java Primitive Class? 
//  @Signature // XXX 
//  couldn't convince you otherwise? well then let's llap. 
/* best effort */
//  Local scratch dir 
//  Only for incremental load, need to validate if event is newer than the database. 
//  If partKey is a constant, we can check whether the partitions   have been already filtered 
//  try to query stats for a column for which stats doesn't exist 
//  the +1 to the size is because of the main work. 
//  Copy conf file 
//  MY_DOUBLE 
//  Get the bucketing version 
//  Never locked for eviction; Java object. 
//  Expected exception as FileNotFoundException will occur if the partitions have custom   location 
//  verify that the two are equal 
//  1. Initialize aux data structures 
//  3. Determine type of UDAF 
//  make it easy to write .q unit tests, instead of unique id generation.   however, this does mean that in writing tests, we have to be aware that   repl dump will clash with prior dumps, and thus have to clean up properly. 
//  grouping sets at this point 
//  If the replacement is changed, make sure we redo toString again. 
//  We store the total memory that this MapJoin is going to use,   which is calculated as totalSize/buckets, with totalSize 
//  failure hooks are run after HiveStatement is closed. wait sometime for failure hook to execute 
/*    * Tries to optimize FROM clause of multi-insert. No attempt to optimize insert clauses of the query.   * Returns true if rewriting is successful, false otherwise.    */
// have we reached a new key group? 
//  write a delta 
//  Retry with same dump with which it was already loaded should resume the bootstrap load. 
//  Descending   Null last (default for descending order) 
// root must exist already 
//  Total: 5/6 running. 
// this enables vectorization of ROW__ID 
//  invariant: reducerHash != null 
//  check incompatible versions 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#execute()    */
//  depending on the server setup. 
//  need to update the mapCorVarToCorRel Update the output position   for the cor vars: only pass on the cor vars that are not used in 
//  CHAR BETWEEN 
//  Setup the actual ports in the configuration. 
//  Now disregard null in second pass. 
//  Link the list record to the first element. 
//  Dummy final assignments. 
//  Schema that exists in the Avro data file. 
//  remove the current active zk server 
//  Create test database and base tables once for all the test 
//  Non-empty java opts with -Xmx specified in KB 
//  fetch the partition referred to by the message and compare 
//  perform dynamic partition pruning 
//  and restore the state before walking each child 
//  TODO: create immutable copies of all maps 
//  Careful with the range here now, we do not want to read the whole base file like deltas. 
//  records whether delta dir is of type 'delete_delta_x_y...' 
//  Processing files 
/*  Note: In the following section, Metadata-only import handling logic is     interleaved with regular repl-import logic. The rule of thumb being     followed here is that MD-only imports are essentially ALTERs. They do     not load data, and should not be "creating" any metadata - they should     be replacing instead. The only place it makes sense for a MD-only import     to create is in the case of a table that's been dropped and recreated,     or in the case of an unpartitioned table. In all other cases, it should     behave like a noop or a pure MD alter.   */
//  For Tez, we don't use appId to distinguish the tokens. 
//  there's a small number of buffers and they all live in the heap). 
//  If we did not kill this session we expect everything to be present. 
//  Make a copy so that we do not modify hookContext conf. 
//  generate the map join operator; already checked the map join 
//  Stores negative values to count columns. Eventually set to #tasks X #columns after the source vertex completes. 
//  End RelDecorrelator.java 
//  get the tables/views for the desired pattern - populate the output stream 
//  at task startup 
//  right border is the max 
//  Otherwise this is not a sampling predicate. We need to process it. 
//  should detect double 
//  Fail if we try to access an offset out of bounds 
//  On LLAP dynamic value registry might already be cached. 
//  via the environment context. 
//  There's an RPC waiting for a reply. Exception was most probably caught while processing   the RPC, so send an error. 
//  No ranges, use the ranges from the child 
// put something in WRITE_SET 
// ***************************************************************************   Data-related configuration properties.  *************************************************************************** 
//  Remove expression node desc and all children of it from mapping 
//  42000 is the generic SQLState for syntax error. 
//  Determine if all digits below a power is zero. 
// Now populate a structure to use to apply delete events 
/*  Testing for equality of doubles after a math operation is   * not always reliable so use this as a tolerance.    */
//  All must be selected otherwise size would be zero   Repeating property will not change. 
//  A very simple counter to keep track of number of rows processed by the   reducer. It dumps   every 1 million times, and quickly before that 
//  how many records the writer buffers before it writes to disk 
/*        * Clear all the reading variables.        */
//  Were going to update the average variable row size by sampling the current batch 
//  Mark a task as failed due to a comm failure. 
//  If the lock manager is still null, then it means we aren't using a 
//  this is a count(*), transform it to count(nullIndicator)   the null indicator is located at the end 
//  For this one don't specify a location to make sure it gets put in the catalog directory 
// comletor add space after last delimeter 
//  End of synchronized (ti)  
//  Recursively do the first phase of semantic analysis for the subquery 
//  flushed. 
//  If custom dynamic location provided, need to rename to final output path 
//  Check that the table or partition isn't sorted, as we don't yet support that. 
//  Get partition-path. For grid='XYZ', place the partition outside the table-path. 
//  output OI strips off the partition columns and retains other columns 
//  Rewrite all INSERT references (all the node values for this key) 
//  Note: if compaction creates a delta, it won't replace an existing base dir, so the txn ID         of the base dir won't be a part of delta's range. If otoh compaction creates a base,         we don't care about this value because bases don't have min txn ID in the name.         However logically this should also take base into account if it's included. 
//  4. The cookie is secured where as the client connect does not use SSL 
//  go ahead with the estimation 
// below value for a is bucket id, for b - txn id (logically) 
//  Replace any occurrence of dummyVectorOperator with our TableScanOperator. 
//  See heapifyDown comment. 
//  We don't exit the loop early because we want to extract the error code   corresponding to the bottommost error coded exception. 
//  ParseContext 
//  Catch-all rule when none of the others apply. 
//  Register tasks on 2 nodes, with a dependency on vertex1 completing. 
//  Setup the map work for this thread. Pruning modified the work instance to potentially remove   partitions. The same work instance must be used when generating splits. 
//  Perform repl 
//  Here are some positive cases which can be executed as below : 
// if here, it means a concrurrent acquireLock() inserted the 'key' 
//  Input name to position map 
// true here indicates that sq_count_check is for IN/NOT IN subqueries 
//  The third one has the base file, so it shouldn't be combined but could be a base. 
/*    * Read a field that is under a complex type.  It may be a primitive type or deeper complex type.    */
//  Add ReplStateLogTask only if no pending table load tasks left for next cycle 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setTime(int, java.sql.Time)    */
//  We are making a new output vectorized row batch. 
//  but have no nulls initially 
//  For caching TableWrapper objects. Key is aggregate of database name and table name 
//  The MV is outdated, see whether we should consider it for rewriting or not 
/* + cCtx.nextOperatorId() */
//  check if srcf contains nested sub-directories 
//  Put a compaction request in the queue. 
//  This should not be null because we were allowed to bind with this username   safe check in case we were able to bind anonymously. 
//  Otherwise we wouldn't be here. 
//  If any aggregate call has a filter, bail out 
//  schema is diff, return false 
//  Control characeters! According to JSON RFC u0020 
/*  100 files x 100 size for 10 splits  */
//  No access. 
//  This will guarantee file name uniqueness. 
//  must be sync with TOperationState in order 
/*    * NOTE: The VectorPTFDesc has already been allocated and populated.    */
//  propagate new conf to meta store 
//  Release buffers as we are done with all the streams... also see toRelease comment. 
//  for local file and hdfs, key and value are same. 
//  This may be invoked before a container is ever assigned to a task. allocateTask... app decides 
//  Try with regular DECIMAL output type. 
//  Disabled in HIVE-19509   Disabled in HIVE-19509   Disabled in HIVE-19509   Disabled in HIVE-19509   Disabled in HIVE-19509   Disabled in HIVE-19509   Disabled in HIVE-19509 
//  MY_ENUM_STRUCTLIST_MAP 
// private static Properties props; 
//  partitioned tables don't have tableDesc set on the FetchTask. Instead   they have a list of PartitionDesc objects, each with a table desc.   Let's   try to fetch the desc for the first partition and use it's   deserializer. 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.newBuilder() 
//  Partial aggregation is performed on the mapper, no distinct processing at the reducer 
//  taking precedence, in the case of partitioned tables 
//  With nulls 
//  Get the escape information 
//  NOTE: This writes into a scratch buffer within HiveDecimalWritable.   
/*  * The root interface for a vector map join hash map.  */
//  Non-empty java opts with -Xmx specified in GB 
//  of the correlate. 
// a dummy group) 
//  add the finalOp after the union 
//        it's hard to tell w/some code paths like UDFs/OIs etc. that are used in many places. 
//  ideally hadoop should let us know whether map execution failed or not 
//  For LazyStruct 
//  delim does not exist in str 
//  100 > x   neg-infinity to end exclusive 
//  Must be escaped by BinarySortable. 
//  Note: This is a readLock to prevent a race with queryComplete. Operations   and mutations within this lock need to be on concurrent structures. 
//  For the CHAR and VARCHAR data types, the maximum character length of   the column.  Otherwise, 0. 
//  This semaphore provides two functions:   1. Forces a cap on the number of outstanding async writes to channel   2. Ensures that channel isn't closed if there are any outstanding async writes 
//  This is a sub-dir under the hdfsSessionPath. Will be removed along with that dir. 
//  STATS_OBJ 
// compare hosts 
//  to get stats for, the metastore does not break. See HIVE-12083 for motivation. 
//  Set the necessary Accumulo information 
//  Test reverse order. 
//  INSERT_ONLY tables don't have to conform to ACID requirement like ORC or bucketing 
// Hive is more restrictive, so Hive->Pig works 
//  for the argument. 
//  Finally, create session paths for this session   Local & non-local tmp location is configurable. however it is the same across 
//  Walk through the Task Graph and invoke SparkDynamicPartitionPruningDispatcher 
//  We don't want to acquire read locks during update or delete as we'll be acquiring write   locks instead. Also, there's no need to lock temp tables since they're session wide 
// Get the input format 
//  Perform reconnect with the proper user context 
//  generator. 
/*    * Test multi-threaded implementation of checker to find out missing partitions    */
//  This can happen for View or Index 
//  No shuffle is needed. For union only.   HashPartition shuffle, keys are not sorted in any way.   RangePartition shuffle, keys are total sorted.   HashPartition shuffle, keys are sorted by partition. 
/*        * Since we have a result for all rows, we don't need to do conditional NULL maintenance or       * turn off noNulls..        */
//  Reader count already incremented during cache lookup. 
//  if this is not set default value is set during config initialization   Default value can't be set in this constructor as it would refer names in other ConfVars 
//  Add the new ReadEntity that were added to readEntityMap in PlanUtils.addInput 
//  Functionality to remove semi-join optimization 
//  This method only returns the result when activating a resource plan.   We could also add a boolean flag to be specified by the caller to see   when the result might be needed. 
//  generates the plan from the operator tree 
//  [a,b,c,a,b,c] 
//  used for GenericUDAFBridgeEvaluator 
//  Extract only 'split: hdfs://...' 
//  LOG.error("Got " + RecordReaderUtils.stringifyDiskRanges(footerRange)); 
//  Check if DB/table/partition in C doesn't have repl.source.for props. Also ensure, ckpt property 
//  Make sure the jar containing the custom CompositeRowId is included   in the mapreduce job's classpath (libjars) 
/*          * Throw TimeoutException to caller.          */
//           of the queue (esp. in conjunction with (1)) and rerun them. 
//  verify when first argument (boolean flags) is repeating 
// 4 tables 
// pass a flag to hide prefixes 
//  Evaluate the key expressions once. 
//  no-arg constructor to make kyro happy. 
/*    * BOOLEAN.    */
//  P 
//  put it back and one additional table 
//  estimate the number of hash table entries based on the size of each   entry. Since the size of a entry 
//  Use SubmitWorkResponseProto.newBuilder() to construct. 
//  runDropPartitions is the main function that gets called with different options   partCount: total number of partitions that will be deleted   batchSize: maximum number of partitions that can be deleted in a batch      based on the above the test will check that the batch sizes are as expected   exceptionStatus can take 3 values     noException: no exception is expected.     oneException: first call throws exception.  Since dropPartitionInBatches will retry, this                    will succeed after the first failure     allException: failure case where everything fails.  Will test that the test fails after                    retrying based on maxRetries when specified, or based on a decaying factor 
//  Use Case 2. 
//  This will trigger new calls to metastore to collect metadata 
//  not required 
//  Try to find the default db postfix; don't check two last components - at least there   should be a table and file (we could also try to throw away partition/bucket/acid stuff). 
//  We've encountered a new key, must save current one   We can't forward yet, the aggregators have not been evaluated 
// for NULL we just write out the type 
//  Make sure we get exceptions strategies might have thrown. 
/*    * Used as a dummy root operator to attach vectorized operators that will be built in parallel   * to the current non-vectorized operator tree.    */
//  we should use '\0' for COLUMN_NAME_DELIMITER if column name contains COMMA   but we should also take care of the backward compatibility 
//  For now, old class. 
//  allowing this to be increased via config breaks the merge impl   p=10 = ~1kb per vector or smaller 
//  1.2. Fix up the query for materialization rebuild 
// https://docs.oracle.com/cd/E17952_01/refman-5.6-en/select.html 
//  Bail out if RS or TS is encountered. 
// 1. Distinct aggregate rewrite 
//  add constant size for unions tags 
//  Also MIN_HISTORY_LEVEL won't have any entries as no reference for open txns. 
//  We won't write the set   expressions in the rewritten query.  We'll patch that up later.   The set list from update should be the second child (index 1) 
//  Runtime constants + deterministic functions can be folded. 
//  During repl load, NoSuchObjectException in foreign key shall   ignore as the foreign table may not be part of the replication 
//  End: tests that check values from Pig that are out of range for target column 
//  Process --listHAPeers 
//  required   required   required   required   required 
// Log with int input and double base 
//  resFile   pCtx   RootTasks   FetchTask   analyzer   explainConfig   cboInfo 
//  vectorized row batch reader 
//  Subclasses must override this with a function that implements the desired logic. 
/*      * Write information about the old value (which becomes our next) at the beginning     * of our new value.      */
//  Multi-file load is for dynamic partitions when some partitions do not   need to merge and they can simply be moved to the target directory. 
//  Don't validate column count - no encodings for vectors. 
//  In case of Spark the credential provider location is provided in the jobConf when the job is submitted 
//  This is checked by DDLSemanticAnalyzer 
//  1. For Wdw Specs that refer to Window Defns, inherit missing components 
//  2.Row resolvers for input, output 
/*    * Lookup an byte array key in the hash multi-set.   *   * @param keyBytes   *         A byte array containing the key within a range.   * @param keyStart   *         The offset the beginning of the key.   * @param keyLength   *         The length of the key.   * @param hashMultiSetResult   *         The object to receive small table value(s) information on a MATCH.   *         Or, for SPILL, it has information on where to spill the big table row.   *   * @return   *         Whether the lookup was a match, no match, or spilled (the partition with the key   *         is currently spilled).    */
//  The previous rules can pull up projections through join operators, 
//  create 9 dummy partitions 
//  with extra structs 
//  If Call is a redundant cast then bail out. Ex: cast(true)BOOLEAN 
//  We need to "remember" the input object inspector so that we need to know the input type 
//  negative range is bigger than positive range, so there is no risk   of overflow here. 
//  We need to remove those branches that   1, ended with a ReduceSinkOperator, and   2, the ReduceSinkOperator's name is not the same as childReducerName.   Also, if the cloned work is not the first, we remove ALL leaf operators except 
//  Add original files to obsolete list if any 
//  delim is 2 chars 
//  parallelism shouldn't be set for cartesian product vertex 
//    Comparison   
//  we already have the merge work corresponding to this merge join operator 
//  The ELSE expression is either IdentityExpression (a column) or a ConstantVectorExpression 
//  bigint 
// would be nice if there was a way to determine if quotes are needed 
// debatable if this is correct, but that's how it's implemented 
//  we have a prefix with a wildcard 
//  replace right Key input ref 
/*    * Vectorization.    */
//  1 scheduling run will happen, which may or may not pick up this task in the test.. 
//  Using system classloader as the parent. Using thread context 
/* |------+----------------+---------------+----------+-------+-----------------------------------|| Use  | Boundary2.type | Boundary2.amt | Sort Key | Order | Behavior                          || Case |                |               |          |       |                                   ||------+----------------+---------------+----------+-------+-----------------------------------||   1. | CURRENT ROW    |               | ANY      | ANY   | scan forward until row R2         ||      |                |               |          |       | such that R2.sk != R.sk           ||      |                |               |          |       | end = R2.idx                      ||   2. | FOLLOWING      | UNB           | ANY      | ANY   | end = partition.size()            ||------+----------------+---------------+----------+-------+-----------------------------------|    */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setAsciiStream(int, java.io.InputStream,   * long)    */
//  Test the idempotent behavior of Open and Commit Txn 
//  If stats aggregator is not present, clear the current aggregator stats.   For eg. if a merge is being performed, stats already collected by aggregator (numrows etc.)   are still valid. However, if a load file is being performed, the old stats collected by   aggregator are not valid. It might be a good idea to clear them instead of leaving wrong   and old stats.   Since HIVE-12661, we maintain the old stats (although may be wrong) for CBO   purpose. We use a flag COLUMN_STATS_ACCURATE to   show the accuracy of the stats. 
// Local file system is using pfile:/// {@link ProxyLocalFileSystem} 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setClob(int, java.io.Reader)    */
//  All the pending get requests should just be requeued elsewhere.   Note that we never queue session reuse so sessionToReuse would be null. 
//  PARTITIONNAME 
//  get the string representing action type if its non default action type 
//  but kept unchanged throughout the operator tree for one row 
//  CNAMEs/subjectAltName verification 
//  initialize stats publishing table for noscan which has only stats task   the rest of MR task following stats task initializes it in ExecDriver.java 
//  anchor the pattern to the start:end of the whole string. 
//        list need to be refactored out to be done only once. 
//  shortcut, 0 length means no fields 
// get big table 
//  All checks passed, return null. 
//  If is a DPP, check if actually it refers to same target, column, etc. 
//  replace the output expression with the input expression so that 
//  bitset array 
//  Replace the synthetic predicate with true and bail out 
//  For serialization only. 
//  make the conditional task as the child of the current leaf task 
//  unfortunately no quick path. let's do scale up/down 
//  PK/FQ relationship: NDV of selColSourceStat is a superset of what is in tsColStat 
//  If it is not an outer join, or the post-condition filters   are empty or the row passed them 
//  actually temp table does not support partitions, cascade is not   applicable here 
//  PRECONDITION: p should always be a directory 
//  flag to indicate if these counters are subject to change across different test runs 
//  If numRecords = -1, fetch all records.   Hence skip all the below checks when numRecords = -1. 
//  Catch-all due to some exec time dependencies on session state   that would cause ClassNoFoundException otherwise 
//  Write the items to the output stream. 
//  Test using the same cache where first n rows are inserted then cache is cleared.   Next reuse the same cache and insert another m rows and verify the cache stores correctly.   This simulates reusing the same cache over and over again. 
//  The output of a partial aggregation is a list of doubles representing the   histogram being constructed. The first element in the list is the user-specified   number of bins in the histogram, and the histogram itself is represented as (x,y)   pairs following the first element, so the list length should *always* be odd. 
//  Revert to default keystore path 
//  Create partitions for the partitioned table 
//  min we allow tez to pick 
//  We have no data at all for this part of the stream (could be unneeded), skip. 
//  If global contains includes, individual modules can only contain additional includes. 
//  We need to provide the minimum number of columns to be read so   LazySimpleDeserializeRead's separator parser does not waste time. 
//  since we are closing the previous fsp's record writers, we need to see if we can get 
//  Task is currently running 
//  a list of AND expressions that we need to distribute 
//  Verify null output data entry is not 0, but rather the value specified by design, 
//  this class 
//  string length should work after enforceMaxLength() 
//  safety check for L53 to get parentOp, although it is very unlikely that   stack size is less than 2, i.e., there is only one MergeJoinOperator in the stack. 
//  The incoming vectorization context.  It describes the input big table vectorized row batch. 
/* target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands-1519423568221/ export  _metadata  p=1      delta_0000001_0000001_0000          bucket_00000 */
// one more major compaction 
//  limit to that for now. 
//  Column names - we lose the enumness of this schema 
//  LazyBinaryUtils.writeVLongToByteArray   offset   length 
//  3) Get txn tables that are being written 
/*    * Lookup an byte array key in the hash set.   *   * @param keyBytes   *         A byte array containing the key within a range.   * @param keyStart   *         The offset the beginning of the key.   * @param keyLength   *         The length of the key.   * @param hashSetResult   *         The object to receive small table value(s) information on a MATCH.   *         Or, for SPILL, it has information on where to spill the big table row.   *   * @return   *         Whether the lookup was a match, no match, or spilled (the partition with the key   *         is currently spilled).    */
//  point for a linear scan has been identified, at which point this value is unset. 
//  Extrapolation is not needed for columns noExtraColumnNames 
//  Config settings. 
//  3/ update the byte size of the struct 
//  Try to merge rest of operators 
//  this test requires disruptor jar in classpath 
//  map join operator by default has no bucket cols and num of reduce sinks 
//  type interval_day_time (IntervalDayTimeColumnVector storing nanosecond interval in 2 primitives). 
//  Matches only ForwardOperators which are preceded by some other operator in the tree,   in particular it can't be a reducer (and hence cannot be one of the ForwardOperators 
//  Only set the default catalog on the client. 
//  required   required   optional   optional 
/*    * This is the same as the setChildren method below but for empty tables.    */
//  Test that attempting to unlock locks associated with a transaction   generates an error 
//  Move the clock forward and trigger a run. 
//  If there are more than one skewed values, 
//  Run the cleaner thread until cache is cleanUntil% occupied 
//  COPY_FROM 
//  DONE 
//  q is contained within p' - p   set MSB to 0 
//  Update the stats which do not require a complete scan. 
//  The "bottom" of the call stack is at the front of the array. The elements are as follows:     [0] getStackTrace()     [1] shouldSkip()     [2] caller test method 
//  Input value serde needs to be an array to support different SerDe   for different tags 
//  calculate filter propagation directions for each alias 
//  Event 5, 6, 7 
//  TEZ AM will only localize FILE (no script operators in the AM) 
//  no-op - HMSHander not needed by this impl 
//  call-1: listLocatedStatus - mock:/mocktbl2 
//  This is test for llap command AuthZ added in HIVE-19033 which require ZK access for it to pass 
//  set timestamp before moving to cmroot, so we can   avoid race condition CM remove the file before setting 
//  Key design point for REPL DUMP is to not have any txns older than current txn in which dump runs.   This is needed to ensure that Repl dump doesn't copy any data files written by any open txns   mainly for streaming ingest case where one delta file shall have data from committed/aborted/open txns.   It may also have data inconsistency if the on-going txns doesn't have corresponding open/write   events captured which means, catch-up incremental phase won't be able to replicate those txns.   So, the logic is to wait for configured amount of time to see if all open txns < current txn is   getting aborted/committed. If not, then we forcefully abort those txns just like AcidHouseKeeperService. 
//  Evict all results grouped with this index; it cannot be any key further in the batch.   If we evict a key from this batch, the keys grouped with it cannot be earlier that that key. 
//  total 3 entries (2 valid + 1 fake) 
//  no custom vertex or edge   custom vertex and custom edge but single MR Input   custom vertex, custom edge and multi MR Input   custom vertex, no custom edge, multi MR Input 
/*      * Optimize the loops by pulling special end cases and global decisions like isEscaped out!      */
//  the correct result that the blank value is not there. 
//  This will be hit if there's a large number of mapIds in a single request   (Determined by the cache size further up), in which case we go to disk again. 
//  Expressions macro table used when we deserialize the query from calcite plan 
/*  * Specialized class for doing a vectorized map join that is an inner join on Multi-Key * and only big table columns appear in the join result so a hash multi-set is used.  */
//  Not supposed to be a compactable table. 
//  if RS is inserted by enforce bucketing or sorting, we need to remove it   since ReduceSinkDeDuplication will not merge them to single RS.   RS inserted by enforce bucketing/sorting will have bucketing column in   reduce sink key whereas RS inserted by this optimization will have   partition columns followed by bucket number followed by sort columns in   the reduce sink key. Since both key columns are not prefix subset   ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs. 
//  TEMPORARILY: 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createBlob()    */
//  We have 0 bytes of data for this part, for now. 
/*  Create new source files with same filenames  */
//  confirm the batch sizes were 23, 15, 8 in the three calls to create partitions 
//  call-1: listLocatedStatus - mock:/mocktbl1 
//  TypeInfo 
//  COMMENTS 
//  2) test with txn.Abort() 
//  If an expression does not have a where clause, there can be no common filter 
//  Writer for producing row from input batch 
/*    * This map maintains the PTFInvocationSpec for each PTF chain invocation in this QB.    */
//  Simulate an unknown type 
// check that files produced by compaction still have the version marker 
//  We are trying to adding map joins to handle skew keys, and map join right   now does not work with outer joins 
//  "-blah -foo bar" form 
//  After closing the path is set to null... 
//  "A comma separated list of work names used as prefix. 
//  Create table and insert two file of the same content 
//  We only support changing the SerDe mapping and the state. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getRowId(int)    */
//  Looks for the most encrypted table location   It may return null if there are not tables encrypted, or are not part of HDFS 
//  LRU extreme, frequency of accesses should be ignored, only order matters. 
/*    * If this the first PPTF in the chain and there is no partition specified   * then assume the user wants to include the entire input in 1 partition.    */
//  Filters are using an index which should match 3 rows 
//  test case sensitivity 
//  done processing so far 
//  We are just a relay; send pause to encoded data producer. 
//  No equivalent Java type for the backing structure, need to recurse and build a list 
/*  @bgen(jjtree) FieldRequiredness  */
// close with null reporter 
//  these 2 flags are intended only for the big-key map work 
//  We can only push down stuff which appears as part of   a pure conjunction:  reject OR, CASE, etc. 
// invoking init method of baseHandler this way since it adds the retry logic  in case of transient failures in init method 
//  We only support COUNT/SUM/MIN/MAX for the "single" count distinct optimization 
//  for bucket join testing 
//  Not completely accurate, since OOB heartbeats could go out. 
//  This get should succeed because its variance ((10-9)/9) is within past MAX_VARIANCE (0.5) 
//  convert to DPHJ 
//  maintain a list of non-NULL column IDs 
//  reporter is a member variable of the Operator class. 
//  ~ 1 week 
//  Do we want to end the binary search 
//  for each bucket file in big table, get the corresponding bucket file   name in the small table.   more than 1 partition in the big table, do the mapping for each partition 
//  find all leaf tasks and make the DDLTask as a dependent task on all of them 
//  If we've found it and it's already been marked acquired, 
//  then definitely this will end up zero. 
//  Create the map if needed 
//  Note: interestingly this would exclude LLAP app jars that the session adds for LLAP case.         Of course it doesn't matter because vertices run ON LLAP and have those jars, and         moreover we anyway don't localize jars for the vertices on LLAP; but in theory         this is still crappy code that assumes there's one and only app jar. 
//  An update error for some session that was actually already killed by us. 
//  Check if all fields start with "key." or "value."   If so, then unflatten by adding an additional level of nested key and value structs   Example: { "key.reducesinkkey0":int, "key.reducesinkkey1": int, "value._col6":int }   Becomes 
//  return null since this will be handled as a special case in VectorizationContext 
//  check the contents of the first row 
//  No filter if any TS has no filter expression 
//  it was a empty stream 
//  consider approximate map side parallelism to be table data size 
//  Giving up. 
//  -main entry_point_name 
//  Strings are interned and can thus be compared like this. 
//  and so should the delete_delta directory. 
// return the current block's length 
//  Should have some AST   when(explainWork.getAstStringTree()).thenReturn(AST); 
//  binary join 
/*      * name      */
//  Try the first again, it would not be combined and we'd retain the old base (less files). 
//  the sargs are closely tied to hive.optimize.index.filter 
// delete the table from the database 
//  so the new partition should be similar to the original partition 
//  Note - above looks funny because it seems like we're instantiating a static var, and   then a non-static var as the rule, but the reason this is required is because Rules   are not allowed to be static, but we wind up needing it initialized from a static   context. So, bcompat is initialzed in a static context, but this rule is initialized   before the tests run, and will pick up an initialized value of bcompat. 
//  Big table value expressions apply to ALL matching and non-matching rows. 
//  this set is a copy of the arguments objects - avoid serializing 
//  check table only, should not exist in ms 
//  If the result is not null, the buffer was evicted during the move. 
//  3. Insert ReduceSide GB2 
//  Case 6 - No parenthesis 
// so that we create empty bucket files when needed (but see HIVE-17138) 
//  Execute all implementation variations. 
/*    * @param partInfoList   *  @return The size of the list of partitions.   * @throws HCatException,ConnectionFailureException   * @see org.apache.hive.hcatalog.api.HCatClient#addPartitions(java.util.List)    */
//  null projection 
//  4. Join Selectivity = 1/NDV 
//  if the operator is a groupby and we are referencing the grouping 
// fast pass worked, i.e. all txns we were asked to heartbeat were Open as expected 
//  Multiply the integer quotient back out so we can subtract it from the original to get 
/*  (non-Javadoc)   * @see org.apache.hadoop.io.Writable#write(java.io.DataOutput)    */
//  Group by contains the columns needed - no need to aggregate from children 
//  2/ serialize the struct 
// columnName to column position map 
//  3. Insert ReduceSide GB1 
/*        * Single-Column String specific declarations.        */
//  add primitive types 
//  @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.hooks.proto.HiveHookEventProto) 
//  append filter tag 
//  generate SQL stmts to execute 
//  Get "<some number>" 
//  Connect any edges required for min/max pushdown 
//  UNIFORM || AUTOPARALLEL (maxed out) 
//  Only remove information of a column if it is not a key, 
//  test that overflow produces NULL 
//  PARTITIONNAMES 
//  Constructs a standard group by plan if:   There is no other subquery with the same group by/distinct keys or   (There are no aggregations in a representative query for the group and   There is no group by in that representative query) or   The data is skewed or   The conf variable used to control combining group bys into a single reducer is false 
// this table needs to be converted to MM 
//  collect name of output columns which is result of function 
//  set up backup task 
//  CLIENT/TOOL END     The tasks have completed, control is back at the tool 
//  logging configuration 
//  Lower this for big key testing. 
/*          * killThreads option has already done force shutdown. No need to do again.          */
//  no nulls possible 
//  Now we trigger some needed optimization rules again 
//  In test mode we want the operation logs regardless of the settings 
//  For har files 
//  Create a Conf for n-way HybridHashTableContainers 
//  test right input repeating 
//  sync to start 
//  Test failure if user not set 
//  GRANTOR_NAME 
//  c15:struct<r:int,s:struct<a:int,b:string>> 
//  Add to map all the referenced positions (relative to each input rel). 
//  scale the raw data size to split level based on ratio of split wrt to file length 
//  sqlState, errorCode should be set to appropriate values 
//  Just to translate 
//  only right input repeating 
//  get the partitions for the table and populate the output 
//  Create a new SparkTask for the specified SparkWork, recursively compute 
//  See the path in FSOP that calls fs.exists on finalPath. 
//  Handle minimum integer case that doesn't have abs(). 
//  We remembered the offset of just after the key length in the list record.   Read the absolute offset to the 2nd value. 
//  The session cannot have been killed just now; this happens after all the kills in   the current iteration, so we would have cleared sessionToReuse when killing this. 
//  Strings including single escaped characters. 
//  2.5. Remember the bad estimates for future reference. 
//  the metric value must be zeroed: 
//  CTAS 
//  forward conditional on the survival of the corresponding key currently in indexes. 
//  runs 
//  100 == x   single row 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.operation.Operation#getResultSetSchema()    */
//  Probably don't have a JobConf here, but we can still try... 
//  check that destination does not exist otherwise we will be   overwriting data 
//  If this is run as a pre or post execution hook, it writes a message to SessionState.err   (causing it to be cached if a CachingPrintStream is being used).  If it is run as a failure   hook, it will write what has been cached by the CachingPrintStream to SessionState.out for   verification. 
//  Push or next 
//  Update creation metadata 
//  Overflow. 
//  Note: do not rename to ..service.acl; Hadoop generates .hosts setting name from this,   resulting in a collision with existing hive.llap.daemon.service.hosts and bizarre errors.   These are read by Hadoop IPC, so you should check the usage and naming conventions (e.g.   ".blocked" is a string hardcoded by Hadoop, and defaults are enforced elsewhere in Hive) 
//  non-temp tables should use underlying client. 
/*      * expressions in SubQ that are joined to the Outer Query.      */
//  Link the update repl state task with dependency collection task 
//  sourceTask for TS is not changed (currently) but that of FS might be changed   by various optimizers (auto.convert.join, for example) 
//  Delete some data -> this will generate only delete deltas and no insert deltas: delete_delta_5_5 
//  Now lost a finishable state. 
//  figure out how many tasks we want for each bucket 
//  repeated string group_vertices = 2; 
//  CHAR tests 
//  When deleteRecordId > currRecordIdInBatch, we have to move on to look at the   next record in the batch.   But before that, can we short-circuit and skip the entire batch itself   by checking if the deleteRecordId > lastRecordInBatch? 
//  For file sink operator, change the directory name 
//  No task for given input, return empty list with -1 as index 
// "Update tab2" 
//  RENAME_PARTITION EVENT to partitioned table 
//  Should not reach here 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareStatement(java.lang.String, int, int)    */
//  Check if these belong to the same task, and work with withinDagPriority 
//  Make sure we don't reference any old buffer. 
//  Copy the data over, so that the internal state of Text will be set to 
//  to prevent infinite loop 
//  No truncate (ASCII) -- maximum length large. 
//  AVG_COL_LEN 
//  create list with variables that match some of the regexes 
//  required   required   required   optional   required 
/*          * Non-repeating input column. Use any non-NULL values for unassigned rows.          */
//  Note: this is not the calling user, but rather the user under which this session will 
//  Note that, output storage handlers never sees partition columns in data   or schema. 
//  After analyzeInternal() Hiveop get set as Query   since we are passing in AST for select query, so reset it. 
//  Retrieving CD can be expensive and unnecessary, so do it only when required. 
//  Containers likely to come up soon. 
//  for each directory add it once 
//  Find the JDBC driver 
//  For bucket columns   If all the columns match to the parent, put them in the bucket cols   else, add empty list.   For sort columns   Keep the subset of all the columns as long as order is maintained. 
/*  * An multi-key value hash map optimized for vector map join. * * The key is stored as the provided bytes (uninterpreted).  */
// determine if partition level privileges should be checked for input tables 
//  NULL is considered numeric type for arithmetic operators. 
//  Best-effort check. We cannot do a good check against caller thread, since   refCount could still be > 0 if someone else locked. This is used for asserts and logs. 
//  find the jar in local maven repo for testing 
//  Shall never happen 
//  in this case, current task is in the root tasks   so add this new task into root tasks and remove the current task from root tasks 
//  If any event is there and db name is known, then dump the start and end logs 
//  Create a list of operator nodes to start the walking. 
//  3. Now handle actual returns. Sessions may be returned to the pool or may trigger expires. 
//  Each column in the row 
//  connect the work correctly. 
//  this is being read because it is a dependency of a view). 
//  Either we have user functions, or metastore is old version - filter names locally. 
//  First call to resultSet.next() should return true 
//  handle null with selected in use 
//  Insert the number of elements plus 2, to trigger 2 evictions. 
//  if lesser than both NEXT_TXN_ID.ntxn_next and min(MIN_HISTORY_LEVEL .mhl_min_open_txnid). 
//  Builds and starts the mini dfs and mapreduce clusters 
//  If we cannot backtrack any of the columns, bail out 
//  in fact, we're adding this table as a map table 
//  -D : process these first, so that we can instantiate SessionState appropriately. 
//  Map of String to String, LazyMap allows empty-string style key, e.g., {"" : null}   or {"", ""}, but not null style key, e.g., {null:""} 
//  The FK table name might be null if we are retrieving the constraint from the PK side 
//  Try to store the first key.   if TopNHashes aren't active, always forward 
//  remove them from current spark work 
//  even with tez on some jobs are run as MR. disable the flag in   the conf, so that the backend runs fully as MR. 
/*    * References to keys of the hashtable. The index is hash of the key; collisions are   * resolved using open addressing with quadratic probing. Reference format   * [40: offset into writeBuffers][8: state byte][1: has list flag]   * [15: part of hash used to optimize probing]   * Offset is tail offset of the first record for the key (the one containing the key).   * It is not necessary to store 15 bits in particular to optimize probing; in fact when   * we always store the hash it is not necessary. But we have nothing else to do with them.   * TODO: actually we could also use few bits to store largestNumberOfSteps for each,   *      so we'd stop earlier on read collision. Need to profile on real queries.    */
//  @@protoc_insertion_point(class_scope:VertexOrBinary) 
//  SCALE 
//  Writes data out as a series of chunks in the form <chunk size><chunk bytes><chunk size><chunk bytes>   Closing the output stream will 0send a final 0-length chunk which will indicate end of input. 
//  Add temp table info to current session 
//  check if the existing entry contains the new 
//  Query specific info 
//  Add Ctrl-B delimiter between the fields. This is necessary because for structs in case no   delimiter is provided, hive automatically adds Ctrl-B as a default delimiter between fields 
//  Orc store rows inside a root struct (hive writes it this way).   When we populate column vectors we skip over the root struct. 
//  LlapIoImpl.LOG.debug("Adding batch " + batch); 
//  store the vertex name in the operator pipeline 
// this column doesn't come from any table 
//  NOTE:  these references are with respect to the output 
//  TODO Reduce the duplicated code 
//  Get the last repl ID corresponding to all insert events except RENAME. 
//  Should we try to tolerate corruption? Default is No. 
//  alias1, alias2, alias3 all can be selected but overriden by biggest one (alias3) 
//  Write the current set of valid write ids for the operated acid tables into the conf file so 
//  Partition by the bucketing column 
//  2^62*2^63 - 1 
//  Create the consumer of encoded data; it will coordinate decoding to CVBs. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setBinaryStream(int, java.io.InputStream,   * long)    */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#executeUpdate()    */
/*     Comparisons using Strings for event Ids is wrong. This should be numbers since lexical string comparison    and numeric comparision differ. This requires a broader change where we return the dump Id as long and not string    fixing this here for now as it was observed in one of the builds where "1001".compareTo("998") results    in failure of the assertion below.      */
//  AINT 
/*    * Method to de-serialize AllocWriteIdMessage instance.    */
//  If the current buffer contains multiple parts, split it. 
//  Avoid storing headers with data since we expect binary size allocations. 
/*      * Make null safe Check if the job submission has gone through and if job is valid.      */
//  DriverContext could be released in the query and close processes at same 
//  result object 
//  done processing the operator 
//  This is in place to be compatible with the MR ShuffleHandler. Requests from ShuffleInputs   arrive with a job_ prefix. 
//  This TableScanOperator could be part of semijoin optimization. 
//  HDFS counters should be relatively consistent across test runs when compared to   local file system counters 
/*    * After processing all the non-streaming group's batches with evaluateGroupBatch and   * isGroupResultNull is false, the aggregation result value (based on getResultColumnVectorType).    */
//  call the operator specific close routine 
//  Set up the data structures, before any notifications come in. 
//  class HCatPartitionSpec; 
//  Finds column by name in HCatSchema, if not found returns null. 
/*      * if there are any LeadLag functions in this Expression Tree: - setup a     * duplicate Evaluator for the 1st arg of the LLFuncDesc - initialize it     * using the InputInfo provided for this Expr tree - set the duplicate     * evaluator on the LLUDF instance.      */
//  First process the current key. 
//  generating the final row count relying on the basic comparator evaluation methods 
//  Put 1 record into COMPACTION_QUEUE and do nothing 
/*  100 files x 100 size for 1 splits  */
//  otherwise we need to make sure that there is no subquery at any level 
//  Cleanup structures 
//  if this operator is the last operator, we summarize the non-inlined 
//  sample path: /llap-sasl/hiveuser/hostname/workers/worker-0000000   worker-0000000 is the sequence number which will be retained until session timeout. If a   worker does not respond due to communication interruptions it will retain the same sequence   number when it returns back. If session timeout expires, the node will be deleted and new   addition of the same node (restart) will get next sequence number 
//  define schema 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#closeSession(org.apache.hive.service.cli.SessionHandle)    */
//  use SubStructObjectInspector to serialize the non-partitioning columns in the input row 
//  The logger 
/*    * Create information for vector map operator.   * The member oneRootOperator has been set.    */
//  tables with various types 
//  For partitioned tables, get the size of all the partitions 
//  col1 ... col5 
//  Start the heartbeat after a delay, which is shorter than  the HIVE_TXN_TIMEOUT 
//  error code "08006" indicates proper shutdown 
//  In all of the getters, we try the metastore value name first.  If it is not set we try the   Hive value name. 
//  if the cluster is running in MR2 mode, return null 
//  The number of Hive operations that are waiting to enter the compile block 
//  Benefit (rows filtered from ts): (1 - selectivity) * # ts rows 
/*    * Element for Key: byte[] x Hash Table: HashSet    */
//  } 
//  Input fullTableName is expected to be of format <db_name>.<table_name> 
//  no grantor type for public role, hence the null check 
//  get the largest table alias from order 
//  check only when it's in terminal state 
//  table node 
//  We support some virtual columns in vectorization for this table scan. 
//  fetch the data from parquet data page for next call 
//  Hive code has AssertionErrors in some cases - we want to record what happens 
//  Find the record identifier column (if there) and return a possibly new ObjectInspector that 
//  Now rememember what is supported for this query and any support that was 
//  If in test mode then the LogDivertAppenderForTest created an extra log file containing only 
//  Take filter pushdown into account while calculating splits; this   allows us to prune off regions immediately.  Note that although   the Javadoc for the superclass getSplits says that it returns one   split per region, the implementation actually takes the scan   definition into account and excludes regions which don't satisfy 
//  in the waitQueue. Avoid Object comparison. 
//  optional string eventType = 1; 
//  If datasource is in the table properties, it is an INSERT/INSERT OVERWRITE as the datasource   name was already persisted. Otherwise, it is a CT/CTAS and we need to get the name from the   job properties that are set by configureOutputJobProperties in the DruidStorageHandler 
//  clean up the mapred work 
// No token, so remove the placeholder arg 
//  mapTask and currTask should be merged by and join/union operator   (e.g., GenMRUnion1) which has multiple topOps.   assert mapTask == currTask : "mapTask.id = " + mapTask.getId()   + "; currTask.id = " + currTask.getId(); 
//  Add inputs to ops to remove 
// major compaction + check data + files 
//  Implied. 
//  transformation of the join operation 
//  Make sure the hidden keys didn't get published 
//    testtable1.key    S 
//  Event 10, 11, 12 
/*  Maps application identifiers (jobIds) to the associated user for the app  */
//  For now, expecting a single row (min/max, aggregated bloom filter), or no rows 
//  verify that the new configs are in effect 
//  This could be overridden thru the jobconf. 
// http://dev.mysql.com/doc/refman/5.7/en/select.html 
//  check input object's length, if it doesn't match   then output a new primitive with the correct params. 
//  (FETCH_NEXT) execute the same sql again, 
//  Check if the table file has header to skip. 
//  Add constants if there is no SELECT on top 
//  trigger failover on miniHS2_1 
/*    * Setup our outer join specific members.    */
//  see http://download.oracle.com/javase/6/docs/api/constant-values.html#java.lang.Float.MAX_EXPONENT 
//  digit  measuring stick:                  12345678901234567890123456789012345678 
//  return the child directly if the conversion is redundant. 
//  set output column 
//  optional bytes initial_event_bytes = 10; 
//  For CBO 
/*  alternate2 = useBinarySortableCharsNeedingEscape  */
//  which could lead to a lot of extra unnecessary scratch columns. 
//  we're starting a new command 
//  convert the absolute big decimal to string 
//  Form result from lower and high words. 
//  window based aggregations are handled differently 
//  No dynamic partition pruning 
//  Add string value to NumDistinctValue Estimator 
//  execution mode 
//  Check for Map which occupies 2 levels (key separator and key/value pair separator). 
/*  first_name is null   */
/*  resetValueColumnsOnly  */
//  In most of the cases, this is a column reference 
// load hive-site.xml from classpath 
//  we should be careful when authorizing table based on just the   table name. If columns have separate authorization domain, it 
//  NUM_DVS 
//  Note: we assume this never happens for SerDe reader - the batch would never have vectors. 
//  Fill values down for equal value series. 
// Methods for metrics integration.  Each thread-local PerfLogger will open/close scope during each perf-log method. 
//  oops this should have been caught before trying to componentize 
//  cannot convert to complex types 
// test invalid table name 
// ~ Constructors ----------------------------------------------------------- 
//  programmatically set root logger level to INFO. By default if log4j2-test.properties is not   available root logger will use ERROR log level 
//  Temporarily re-using the TEZ AM View ACLs property for individual dag access control.   Hive may want to setup it's own parameters if it wants to control per dag access.   Setting the tez-property per dag should work for now. 
//  type 
/*  Exponent that derives from the fractional				 * part.  Under normal circumstatnces, it is				 * the negative of the number of digits in F.				 * However, if I is very long, the last digits				 * of I get dropped (otherwise a long I with a				 * large negative exponent could cause an				 * unnecessary overflow on I alone).  In this				 * case, fracExp is incremented one for each				 * dropped digit.  */
//  need to do some conversions here 
//  host3 dead before scheduling 
// this also ensures that txn is still there in expected state 
//  Do nothing   No migration, just validate that the tables   Automatically determine if the table should be managed or external   Migrate tables to external tables   Migrate tables as managed transactional tables 
//  all partitions are filtered by partition pruning 
//  There should be 1 new directory: base_xxxxxxx. 
//  if all the entries of map are representing null, then return true 
//  we're done processing events 
//  In SemanticAnalyzer we inject SEL op before aggregation. The columns   in this SEL are derived from the table schema, and do not reflect the   actual columns being selected in the current query.   In this case, we skip the merge and just use the path from the child ops. 
//  Load the defaults. 
//  unknown field, we return. We'll continue from the next field onwards. 
//  The first version of RCFile used the sequence file header. 
//  @@protoc_insertion_point(builder_scope:QueryCompleteRequestProto) 
//  Moving files depends on the parentTask (we still want the dependencyTask to depend   on the parentTask) 
// check that aborted operation didn't become committed 
//  SR.SR.wait Lock we are examining is waiting.  In this case we keep   looking, as it's possible that something in front is blocking it or 
//  Each iteration of this loop tries to split blocks from one level of the free list into   target size blocks; if we cannot satisfy the allocation from the free list containing the 
//  this is to prevent dropping archived partition which is archived in a 
/*  We don't test all the combinations because (at least currently)     * the logic is inherited to be the same as testColLower, which checks all the cases).      */
//  ISNULL 
//  Note: Do not change without changing the corresponding reference in llap-daemon-log4j2.properties 
//  This stream will be restarted with the same random seed over and over. 
// to create some partitions 
//  Use Case 3. 
//  Integer digits; stop on zeroes above. 
//  last one for union column. 
//  junk the destination for the 1st pass 
//  With bucketing using two different versions. Version 1 for exiting   tables and version 2 for new tables. All the inputs to the SMB must be   from same version. This only applies to tables read directly and not 
//  The merge file being currently processed. 
//  Get the clusterby aliases - these are aliased to the entries in the   select list 
//  We have a row for current group, so we indicate not the last batch. 
//  Copy data values over 
//  Query might have been canceled. Stop the background processing. 
//  create an invalid table which has wrong column type 
//  initialize all forward operator 
//  2.5 Add to field collations 
//  finally add a project to project out the 1st columns 
//  SERVER_PROTOCOL_VERSION 
//  source: complexpb.proto 
//  timebased 
//  alias confict should not happen here. 
//  Wait a while for tasks to respond to being cancelled 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setClob(int, java.io.Reader, long)    */
//  If aggregate B had a COUNT aggregate call the corresponding aggregate at   aggregate A must be SUM. For other aggregates, it remains the same. 
//  Initialize with data row type conversion parameters. 
//  pass the null value along to the escaping process to determine what the dir should be 
//  Create Embedded MetaStore 
//  Recursively clone the children 
//  case the statement is a CREATE TABLE AS 
//  Not expected, access by the same class. 
//  Determine if we need to read this slice for the split. 
//  Convert dynamic arrays and maps to simple arrays. 
//  Cached copy is valid 
//  DOT is not affected 
//  offset within compression buffer where the row group begins 
//  In this case, both should be DHJ operators as pRSChildMJ can only guarantee 
//  Hash-join 
//  a failed event should not create a new notification 
//  DATABASE 
//  If there is branch, remove prune sink operator branch in the baseWork   If there is no branch, remove the whole baseWork 
//  configured not to ignore this 
//  Traverse the TXN_TO_WRITE_ID to see if any of the input txns already have allocated a   write id for the same db.table. If yes, then need to reuse it else have to allocate new one   The write id would have been already allocated in case of multi-statement txns where 
//  if grouping sets are involved do early return 
//  uses string type for binary before HIVE_CLI_SERVICE_PROTOCOL_V6 
//  so let's just go the safe, expensive way. 
//  CONSIDER: Should be do this for all vector expressions that can 
//  Emit the rest of word1 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setTime(java.lang.String, java.sql.Time)    */
// test some extreme cases. 
// we need to do this to get the task path and set it for mapred implementation  since it can't be done automatically because of mapreduce->mapred abstraction 
/*  Divide by a power of 10 equal to 10**scale to logically shift the digits     * places right by "scale" positions to eliminate them.      */
//  I have to create the tables here (rather than in setup()) because I need the Hive   connection, which is conveniently created by the semantic analyzer. 
//  reply copy only happens for jars on hdfs not otherwise. 
//  Need to close() because in some FileSystem   implementations flush() is no-op.   Close the file handle if it is a hdfs file.   But if it is stderr/stdout, skip it since   WebHCat is not supposed to close it 
//  This is the first batch we process after switching from hash mode 
//  Default to lexicographicalasc 
//  check that the partition folders exist on disk 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#nativeSQL(java.lang.String)    */
//  In http mode we use NOSASL as the default auth type 
//  no implicit casts possible 
//  Name of the class to report for 
//  verify whether the sql operation log is generated and fetch correctly. 
//  Note: The following test will fail if you are running this test as root. Setting   permission to '0' on the database folder will not preclude root from being able   to create the necessary files. 
//  row 1 
//  This is default case with setugi off for both client and server 
//  comment passed as table params 
//  XXX: From o.a.zk.t.ClientBase 
//  Avoid the NPE. 
//  Overwhelmingly executes once. 
//  Utility UDFs 
//  in-place progress update related variables 
//  with no nulls 
/*    * provide an Iterator on the rows in a Partition.   * Iterator exposes the index of the next location.   * Client can invoke lead/lag relative to the next location.    */
//  No ranges, just return the empty list 
//  when converting from char to string/varchar, strip any trailing spaces 
//  Re-align the columns appearing on or after startPostion(say, column 1) such that   column 2 becomes column (2+offset), column 3 becomes column (3+offset) and so on. 
//  2.2 Classify leaf predicate as Equi vs Non Equi 
//  all outside elements should be ignored from stat estimation 
//  One live session 
//  move to the next child in FROM tree 
//  Wait for all futures to complete. Check for an abort while waiting for each future. If any of the futures is cancelled / aborted - cancel all subsequent futures. 
//  new number of register bits for higher accuracy 
//  Emit the rest of word2 
//  LBStruct needs ByteArrayRef 
//  Brute force may discard up to twice as many buffers. 
//  Is the field the configured string representing NULL? 
//  There should be 2 delta dirs in the location 
//  hive streaming ingest settings 
//  for a select or create-as-select query, populate the partition to column   (par2Cols) or   table to columns mapping (tab2Cols) 
//  currently druid supports only MapBasedRow as Jackson SerDe so it should safe to cast without check 
//  LINT 
//  row 3 
//  Minimum number of OR clauses needed to transform into IN clauses 
/*  Evaluate result for position i (using bytes[] to avoid storage allocation costs)   * and set position i of the output vector to the result.    */
//  check that /mpart4 does not exist, but /mpart5 still does. 
//  Use bucketized hive input format - that makes sure that one mapper reads the entire file 
//  doesn't have a notion of tiny, and saves the full value as an int, so no overflow   expected:<null> but was:<300> 
//  it must be on the same file system as the current destination 
//  Atlas would be interested in lineage information for insert,load,create etc. 
//  row 2 
//  Supervised kafka tasks should respect KafkaSupervisorIOConfig.completionTimeout instead of   handoffConditionTimeout 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.newBuilder() 
//  ** Methods that need a data object **   In this function, key has to be of the same structure as the Map expects.   Most cases key will be primitive type, so it's OK.   In rare cases that key is not primitive, the user is responsible for   defining 
//  clear out the set. we don't need it anymore. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBinaryStream(java.lang.String,   * java.io.InputStream, int)    */
/*  All the code paths below propagate nulls even if arg2 has no nulls.     * This is to reduce the number of code paths and shorten the     * code, at the expense of maybe doing unnecessary work if neither input     * has nulls. This could be improved in the future by expanding the number     * of code paths.      */
//  Check if the join columns contains all bucket columns.   If a table is bucketized on column B, but the join key is A and B,   it is easy to see joining on different buckets yield empty results. 
//  since the exceptions and the range in question overlap, count the 
//  Strip leading zeroes -- although there shouldn't be any for a decimal. 
//  Still valid, nothing more to do 
//  A simple case. 
/*  there are NULLs in the structColumnVector  */
//  Initialize transaction table properties with default string value. 
//  based on SecurityUtil.getAuthenticationMethod() 
//  Calling close() explicitly to clean up the staging dirs 
/*    * Verify table for Key: byte[] x Hash Table: HashMultiSet    */
//  ts str 
//  add dummy data for not removed by CombineHiveInputFormat, etc. 
/* the mutex pools should ideally be somewhat larger since some operations require 1           connection from each pool and we want to avoid taking a connection from primary pool           and then blocking because mutex pool is empty.  There is only 1 thread in any HMS trying           to mutex on each MUTEX_KEY except MUTEX_KEY.CheckLock.  The CheckLock operation gets a           connection from connPool first, then connPoolMutex.  All others, go in the opposite           order (not very elegant...).  So number of connection requests for connPoolMutex cannot           exceed (size of connPool + MUTEX_KEY.values().length - 1). */
//  The colList is the output columns used by child operators, they are   different   from input columns of the current operator. we need to find out which   input columns are used. 
//  Which big table and small table columns are ByteColumnVector and need have their data buffer   to be manually reset for some join result processing? 
//  Prepend column names with '_o_' if it starts with '_c' 
//  this mapper operator is used to initialize all the operators 
//  Minimum values 
//  HIVE-19332: external tables should not be considered to have up-to-date stats. 
//  it possible that some other HMS instance could have created the guid   at the same time due which this instance could not create a guid above   in such case return the guid already generated 
//  host. 
//  set up kafka producer 
//  neither side is repeating 
//  Extract rows and call process per row 
//  The first small table; 
//  this exception has been handled 
//  show that their positions are recorded 
//  if exception is thrown in scheduled tasks, no further tasks will be scheduled, hence this ugly catch 
//  Thread killing the query 
/*      * any additions to the SubQueries Select Clause.      */
//  offsetLimit is smaller than rowCount of the input operator   thus, we return the offsetLimit 
//  Add the part to lastBatch to track the parition being dropped 
//  if there are no more nodes. Signal timeout monitor to start timer 
//  checksum failure 
//  Test that 2 separate databases don't coalesce. 
//  All parsing is done, we're now good to start the export process 
//  check access columns from readEntity 
//  Second child node could be partitionspec or column 
//  Figure out and encode what files we need to read.  We do this here (rather than in   getSplits below) because as part of this we discover our minimum and maximum transactions,   and discovering that in getSplits is too late as we then have no way to pass it to our   mapper. 
// Write operation always start a txn 
// we just ran Major compaction so we should have a base_x in tblName2 that has the new files 
//  copy full bucket context 
//  a 
// need to disable these so that automatic merge doesn't merge the files 
//  Existing table is not a view 
//  This is a semijoin branch. The stack should look like, 
// try to delete other directories if possible 
//  process the list 
//  Divide up rows array into different sized batches.   Modify the rows array for isRepeating / NULL patterns.   Provide iterator that will fill up a VRB with the divided up rows. 
//  add dependency between the two work items 
//  Test that existing shared_write partition with new shared_read coalesces to 
//  close shouldn't matter 
//  StringsCompleter matches against a pre-defined wordlist 
//  if any of filter is sorted filter, it's sorted filter 
//  Do the intersection so only support in both is kept. 
//  2. Make sure there is no dynamic addition of virtual cols 
//  Can be converted to a Tez event, if this is sufficient to decide on pre-emption 
//  with vectorization 
//  ACCUMULO-2962 Iterators weren't getting serialized into the InputSplit, but we can   compensate because we still have that info. 
//  ORC is in ql, so we cannot do anything here. For now, all the logic is in the proxy. 
//  For string/char/varchar buffering when there are escapes. 
//  column names also can be inferred from result of UDTF 
//  Note: TOK_WINDOWVALUES means RANGE type, TOK_WINDOWRANGE means ROWS type 
//  build the final custom path string by replacing each column name with 
//  We will move all the files in the table/partition directories into the first MM   directory, then commit the first write ID. 
//  short-circuit quickly - eat all rows 
//  2. We gather the common factors and return them 
//  Create a list of topop nodes 
//  If caller is looking for temp table, handle here. Otherwise pass on to underlying client. 
//  reference is already accounted for in the directSize. 
/*  This is to fulfill the contract of the interface which states that an exception shall             be thrown when a SaslServer cannot be created due to an error but null should be             returned when a Server can't be created due to the parameters supplied. And the only             thing PlainSaslServer can fail on is a non-supported authentication mechanism.             That's why we return null instead of throwing the Exception  */
//  production is: i16 
//  normal 
//  For external tables, we do not need to do anything else 
// simulate concurrent session 
//  Wait for the delete to complete 
//  Gather columns used by aggregate operator 
//  After INSERT INTO operation, get the last Repl ID 
//  The sender will take care of this.   The value didn't change. 
//  Obtain the byte buffer from the input string so we can traverse it code point by code point 
//  Reset the parameters, so we can compare 
//  anti-symmetric 
//  overwrite buffer size and stripe size for delta writer, based on BASE_DELTA_RATIO 
//  For Arrow SerDe 
//  make sure miniHS2_1 is not leader 
//  write estimated count 
//  We have propagated the value to the task. 
//  call-2: open to read data - split 2 => mock:/mocktable4/0_1 
//  make sure NullScanFileSystem can be loaded - HIVE-18442 
//  get the forward op 
//  This is the last range for this compression block. Yay! 
//  if not dynamic partitioning then bail out 
//  folder. 
/*  If null, then the major version number should match  */
//  Must be deterministic order set for consistent q-test output across Java versions 
//  Conversion requires source be placed in writable so we can call upon   VectorAssignRow to convert and assign the row column. 
//  Walk the tree. As long as the operators between the union and the filesink   do not involve a reducer, and they can be pushed above the union, it makes   sense to push them above the union, and remove the union. An interface   has been added to the operator 'supportUnionRemoveOptimization' to denote whether 
//     assertTrue(dependentTasksForOne.iterator().next() instanceof DependencyCollectionTask);      assertTrue(dependentTasksForTwo.iterator().next() instanceof DependencyCollectionTask);      assertTrue(dependentTasksForThree.iterator().next() instanceof DependencyCollectionTask); 
//  This version of Hadoop does not support getPassword(), just retrieve password from conf. 
//  CREATE MATERIALIZED VIEW ... 
// write data to the target 
//  Some tests control the execution of the background update thread 
//  Copy within row. 
//  Do this WITHOUT checking for parents 
//  tail 
//  boolean 
//  Just alter the table 
/*          * If the input to the GBy has a tab alias for the column, then add an         * entry based on that tab_alias. For e.g. this query: select b.x,         * count(*) from t1 b group by x needs (tab_alias=b, col_alias=x) in the         * GBy RR. tab_alias=b comes from looking at the RowResolver that is the         * ancestor before any GBy/ReduceSinks added for the GBY operation.          */
//  d 
/*    * Write a NULL field.    */
//  add empty string to the list of aliases. Some operators (ex. GroupBy) add 
//  Mark Partitions with new schema with different blurb. 
//  FileCache might be empty; see if we can remove it. "tryWriteLock" 
//  Backing store for this cache 
//  We assume that there are no locked blocks in the list; or if they are, they can be dropped.   Therefore we always evict one contiguous sequence from the tail. We can find it in one pass,   splice it out and then finalize the eviction outside of the list lock. 
//  if this is zero, we need to check if o might become zero after   scaling down.   this is not easy because there might be rounding. 
//  be committed anymore 
//    Rewrite logic:     1. If a Filter references a correlated field in its filter   condition, rewrite the Filter to be     Filter       Join(cross product)         OriginalFilterInput         ValueGenerator(produces distinct sets of correlated variables)   and rewrite the correlated fieldAccess in the filter condition to   reference the Join output.     2. If Filter does not reference correlated variables, simply   rewrite the filter condition using new input.   
//  nothing 
//  don't need to update tmp paths when there is no depth difference in paths 
// // end add_partitions tests 
/*  * Assumes that a getMapValueElement on object2 will work with a key from * object1. The equality is implemented fully, the greater-than/less-than * values do not implement a transitive relation.   */
//  2. Convert NONACIDORCTBL to ACID table with split_update enabled. (txn_props=default) 
//  Populated if column is struct, array or map types.   If struct type, contains schema of the struct.   If array type, contains schema of one of the elements. 
/*    * Example dump dirs we need to be able to handle :   *   * for: hive.repl.rootdir = staging/   * Then, repl dumps will be created in staging/<dumpdir>   *   * single-db-dump: staging/blah12345 will contain a db dir for the db specified   *  blah12345/   *   default/   *    _metadata   *    tbl1/   *      _metadata   *      dt=20160907/   *        _files   *    tbl2/   *    tbl3/   *    unptn_tbl/   *      _metadata   *      _files   *   * multi-db-dump: staging/bar12347 will contain dirs for each db covered   * staging/   *  bar12347/   *   default/   *     ...   *   sales/   *     ...   *   * single table-dump: staging/baz123 will contain a table object dump inside   * staging/   *  baz123/   *    _metadata   *    dt=20150931/   *      _files   *   * incremental dump : staging/blue123 will contain dirs for each event inside.   * staging/   *  blue123/   *    34/   *    35/   *    36/    */
//  if files exists in input path then it has to be 1 as this code path gets triggered only   of order by queries which is expected to write only one file (written by one reducer) 
//  NULL 1   0 NULL 
//  batches will be sized 11,3 
//  First, "give up" on this task and put it back in the original list. 
//  AccumuloOutputFormat complains if you re-set an already set value. We just don't care. 
//  We only support query-time merge for MM tables, so don't handle this. 
//  do DDL time munging if thrift mode 
//  Parse out the number of histogram bins only once, if we haven't already done   so before. We need at least 2 bins; otherwise, there is no point in creating   a histogram. 
//  data stream could be empty stream or already reached end of stream before present stream.   This can happen if all values in stream are nulls or last row group values are all null. 
//  Make sure we fail the channel if something goes wrong.   We internally handle all the "expected" exceptions, so log a lot of information here. 
//  [A: 1, B: 0, B.x: 1, B.y: 0, C: 0] 
//  if can't find hadoop home we can at least try /usr/bin/hadoop 
//  delete any contents in the warehouse dir 
//  Nothing to be done here. 
//  optional string group_name = 1; 
//  big input cumulative row count 
//  No tasks qualify as preemptable 
//  Not removing this here. Will be removed when taken off the queue and discovered to have 0   pending tasks. 
//  for OR condition independently compute and update stats. 
//  Read table stats via CachedStore 
//  Comment out these checks when we are happy.. 
//  Check if MetaException has one of InvalidObjectException or NoSuchObjectExcetion or any exception thrown from ObjectStore , which means that the   authorization checks passed. 
//  disable filter pushdown for mapreduce when there are more than one table aliases, 
//  columns. 
//  Clone the fileSinkDesc of the final fileSink and create similar fileSinks at 
//  set 'n' if we haven't done so before 
//  permissions set. 
// these are insert events so (original txn == current) txn for all rows 
//  add all data for an element in ListColumnVector, get out the loop if there is no data or the data is for new element 
/*        * c. Rebuilt the QueryDef.       * Why?       * - so that the ExprNodeDescriptors in the QueryDef are based on the       *   Select Operator's RowResolver        */
//  It is expected NEXT_WRITE_ID doesn't have entry for this table and hence directly insert it. 
//  User1 privileges:   testdb1:            S     testtable1.*:     SU     testtable2.*:     S     testtable3.*:     S     testtable4.*:   testdb2:            S 
// by checkLock() - makes diagnostics easier. 
//  Don't release if the buffer contains any data beyond the acceptable boundary. 
//  not using the path child cache here as there could be more than 1 path per host (worker and slot znodes) 
//  Include shim and admin specified libjars 
//  If we cannot merge, we bail out 
//  Ignore if a file with same content already exist in cmroot 
//  Try to serialize 
//  normalized display width is based on maximum of display size   and label size 
//  First lookup the file sink operator from the load work. 
//  test code path 
//  Always create partition and virtual columns. 
//  The validatePTFOperator method will update vectorPTFDesc. 
//  ordinal position of this column in the schema 
//  ENTITY_NAME 
//  sanity check. all tasks must submit events for us to succeed. 
//  null signifies nodes that are irrelevant to the generation 
//  This config contains all the configuration that master node wants to provide 
//                                   having key in (select .. where a = min(b.value) 
//  Oid for kerberos principal name 
//  Next, we verify that the destination table is not offline, or a non-native table 
//  baseCommitter.cleanupJob failed, try to clean up the   metastore 
//  if here, there is attempt to set transactional to something other than 'true'   and NOT the same value it was before 
/*    * A Unit Test convenience method for putting key and value into the hash table using the   * actual types.    */
//  VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + " MATCH duplicate"); 
//  last function name is replicated, null if function replication was in progress when we created this state. 
//  used for LIST_PROVIDED cases 
//  SW.SR.acquired Lock we are examining is acquired;  We need to keep   looking, because there may or may not be another shared write in front 
//  binary keys, values and hashCodes of rows, lined up by index 
//  Prepare a new query string. 
//  and convert it to a string. Yay! 
/*    * Call this to reinitialize the node stack. It is called automatically by the   * parser's ReInit() method.    */
//  Both are not empty. Merge two lists. 
//  left and right repeat and right is null 
//  Transaction manager used for the query. This will be set at compile time based on 
//  get the path names for the 1st row only 
// here each group looks something like "Map 2: 2/4" "Reducer 3: 1(+2)/4"  just parse the numbers and ignore one from "Map 2" and from "(+2)" if it's there 
//  If there is only one element in tuple contained in bag, we throw away the tuple. 
//  Put the script content in a temp file 
//  New state is not running (user not active) any more 
//  bucketed/sorted columns for the destination table 
//  default is that it's not in a repl scope   default is full export/import, not metadata-only 
//  Nothing to do if it is a lock mode 
//  Set to non-zk lock manager to avoid trying to connect to zookeeper 
//  Data structures to store original values 
//  not all partitions are scanned in all mappers, so this could be null. 
//  two int 
//  This wraps an instance of an ExprNodeDesc, and makes equals work like isSame, see comment on 
//  2. _success file is required for Oozie to indicate availability of data source 
//  Create the databases, and reload them from the MetaStore 
//  start all mr/multi-mr inputs 
//  1 file, 1 dir for each, for now. Plus export "data" dir.   This could be changed to a flat file list later. 
//  UPDATE 
//  same 
//  Old API assumed all partitions belong to the same table; keep the same assumption 
//  The writing thread has given us an object to wait on. 
//  a(string,int,int) can be matched with methods like   a(string,int,int), a(string,int,Integer), a(string,Integer,int) and a(string,Integer,Integer) 
//  open 5 connections 
//  Tez session settings 
//  Now vary isRepeating   nulls possible only on left 
//  in case of external table, drop the table contents as well 
//  only one result column 
//  The heartbeat is consistent with what we have. 
//  Get the order by aliases - these are aliased to the entries in the   select list 
//  PRIMARY_KEY_COLS 
//                    12345678901234567890123456789012345678                               1         2         3 
//  ------ Inner classes defined after this point ------ 
// do the authorization check 
//  Determine if we are dealing with a numeric or date arithmetic operation 
//  Maintain count of outstanding requests for tokenIdentifier. 
//  Remove entries from txn_components, as there may be aborted txn components 
/*        * For better performance on LONG/DOUBLE we don't want the conditional       * statements inside the for loop.        */
//  Acquire lock for the given materialized view. Only one rebuild per materialized   view can be triggered at a given time, as otherwise we might produce incorrect   results if incremental maintenance is triggered. 
//  Prevent view cycles 
// to aid in testing through .q files, authenticator is passed as argument to   the interface. this helps in being able to switch the user within a session.   so we need to check if the user has changed 
//  Second request for host. Single invocation since the last has not completed. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getResultSetHoldability()    */
//  Next one is a new key, we bail out from the inner loop 
//  D3-D7. Loop on iulRindex - obtaining digits one-by-one, as "in paper" 
/*    * @return A new hash set result implementation specific object.   *   * The object can be used to access access spill information when the partition with the key   * is currently spilled.    */
/*      * Descending.      */
//  Ignore the outdated updates; for the same version, ignore non-null updates because   we assume that removal is the last thing that happens for any given version. 
/*    * getConvertedOI with caching to store settable properties of the object   * inspector. Caching might help when the object inspector   * contains complex nested data types. Caching is not explicitly required for   * the returned object inspector across multiple invocations since the   * ObjectInspectorFactory already takes care of it.    */
/*  1 files x 1000 size for 99 splits  */
//  Check if the comparison is supported for this type 
//  Convert the constants available as strings to the corresponding objects 
//  Try preempting a lower priority task in any case. 
//  support Decimal64. 
//  Abstract class to hold info required for the implementation 
//  Add listener to handle any cleanup for when the connection is closed 
/*    * FLOAT.    */
//  1 for "_" after tname; 3 for ".qv" at the end. Version is in between. 
//  creates scratch directories needed by the Context object 
/*    * The following conditions are for native Vector ReduceSink.    */
/*    * Handles expr like struct(key,value).key   * Follows same rules as TypeCheckProcFactory::getXpathOrFuncExprNodeDesc()   * which is equivalent version of parsing such an expression from AST    */
//  MetadataTypedColumnsetSerDe does not need type conversions because it 
//  Setup the hadoop vars to specify the user. 
//  that already exist (existingParts), will not generate notifications. 
//  v[8] -- since integer #5 is always 0, some products here are not included. 
//  no support for statistics yet 
//  write the empty base 
//  QUERY,LOAD op can contain an insert & overwrite, 
//  3. Calculate IN selectivity 
//  If we weren't interrupted, just propagate the error 
//  HIVE_OBJECT 
//  SSL conf 
//  optional .EntityDescriptorProto merged_input_descriptor = 3; 
/*      * For both graceful or forceful shutdown, wait for tasks to terminate such that     * appropriate exceptions are raised and stored in JobRunnable.exception.      */
//  Generate the map work for this alias_id   pass both confirmed and unknown partitions through the map-reduce 
//  Do safety checks 
//  If url was not specified with -u, but -r was present, use that. 
//  Timeseries query results 
//  Sets delimiter to tab (ascii 9) 
//  Throw an error if the user asked for bucketed mapjoin to be enforced and 
//  Reset the time, we only want to count it in the loop. 
//  Set permissions appropriately for each of the partitions we just created 
//  this is only there for the use case where we are doing table only replication and not database level 
//  Setting REPL DUMP and REPL LOAD as all requiring ADMIN privileges.   We might wind up loosening this in the future, but right now, we do not want   to do individual object based checks on every object possible, and thus, asking   for a broad privilege such as this is the best route forward. REPL STATUS   should use privileges similar to DESCRIBE DB/TABLE, and so, it asks for no 
/*  This class is used to verify that HiveMetaStore calls the non-transactional listeners with the    * current event ID set by the DbNotificationListener class  */
//  The group by operator has already been processed 
//  spilled tables are loaded always (no sharing), so clear it 
/*    * Truncate a byte array to a maximum number of characters and   * return a byte array with only truncated bytes.    */
// set lineage info 
//  remove the tag for in-memory side of mapjoin 
//  The data does not belong to a recognized chunk, or is split wrong. 
//  referencing another table instead of self for the primary key. 
//  derby fails creating multiple stats aggregator concurrently 
//  MY_BINARY 
//  HEADER_NAMES 
//  to go a few % over. 
//  Get information from YARN Service 
// MetaException 
// in MSSQL this means Communication Link Failure 
//  Initially all columns are projected and in the same order 
//  MAX_WEIGHT 
//  lock. 
//  We are good. 
//  While updating local structures   Note: this is actually called under the epic writeLock in schedulePendingTasks 
//  each parent 
//  This works, assuming the executor is running within YARN. 
//  done 
//  clear vertices list 
//  Print out the un/compressed sizes of each column 
//  here only deals with non-partition columns. We deal with partition columns next 
//  test the last day of month 
//  Subtract 1 for two's compliment adjustment. 
//  Gives a list of any new paths that may have been created to maintain the persistent ephemeral node 
//  MY_32BIT_INT 
// write the delimiter to the stream, which means we don't need output.format.string anymore 
//  UNION_MSTRING_STRING 
//  Red Hat: /sys/kernel/mm/redhat_transparent_hugepage/enabled            /sys/kernel/mm/redhat_transparent_hugepage/defrag   CentOS/Ubuntu/Debian, OEL, SLES: /sys/kernel/mm/transparent_hugepage/enabled                                    /sys/kernel/mm/transparent_hugepage/defrag 
//  If we couldn't create the tracker node, don't create the main node. 
//  new session 
//   1 for left outer join (+-) : join and skip further LO 
//  NOT on boolean columns is possible. in which case return false count. 
//  executed twice: once with the typed ps setters, once with the generic setObject 
//  the last 2 columns are VCol and c 
//  verify outputs 
//  The avg() result type has the same number of integer digits and 4 more decimal digits. 
//  LOG4J2-1292 utilize gc-free Layout.encode() method: taken care of in superclass 
//  Test valid case 
//  The roundPower same as scale means all zeroes below round point. 
//  Get an empty container when the small table is empty. 
//  TODO: Extend TaskRunner2 or see if an API with callbacks will work 
//  Set the value of the writable from the decimal digits that were written with no dot. 
//  Asian character U+24B62 (4 bytes) 
//  a mixture of input big table columns and new scratch columns. 
//  Add some margin to the wait to avoid rechecking close to the boundary. 
//  The data is not in cache. 
//  We drive the doProcessBatch logic with the same batch but different   grouping set id and null variation.   PERFORMANCE NOTE: We do not try to reuse columns and generate the KeyWrappers anew... 
//  invoke DELETE /leader endpoint for failover 
//  Check the schema of a table with one field & no partition keys. 
/*    * Walk to Hive AST and translate the hive column names to their equivalent mappings. This is basically a cheat.   *    */
//  5. Resolve Parse Tree 
//  Whether the method takes an array like Object[],   or String[] etc in the last argument. 
//  delete source   overwrite destination 
//  otherwise it is count(null), should directly return 0. 
//  NOTE: only used from index related classes 
// store the partitions temporarily until processed 
//  days/hours/minutes/seconds all represented as seconds 
/*      * BOOLEAN .. LONG: Min and max.      */
//  The number of tez tasks executed by the HiveServer2 since the last restart 
//  NUM_TXNS 
//  LVmerge.. in the above order 
//  Force Spark configs to be cloned by default 
//  canHandleQbForCbo returns null if the query can be handled. 
//  Temporary work arrays. 
//  get a list with the current classpath components 
//  specifies db 
//  replace the commar. 
/*  keyCount  */
//  Pad output 
//  LocalDateTime is immutable. 
/*            * This case can happen with LLAP.  If it is able to deserialize and cache data from the           * input format, it will deliver that cached data to us as VRBs.            */
//  Quote always, for fields that mimic SQL keywords, like DESC 
//  Need to register minimum open txnid for current transactions into MIN_HISTORY table. 
//  Get the vector description from the operator. 
//  Divide and round up. 
//  best answer would be 1. 
//  Path to file is specified (can be relative), so treat target as a file name (hadoop fs -put behavior) 
//  delete partition level column stats if it exists 
//  required   required   optional   required   optional   optional 
/*  New method that distributes the Scan query by creating splits containing   * information about different Druid nodes that have the data for the given   * query.  */
/*  nUnknown hive type infoot a map  */
//  shallow copy aliasToOpInfo, we won't want to clone the operator tree here 
//  it is DESCRIBE table partition 
//  3, 7, 9 
//  The writer is local to the process. 
//  Generate the list bucketing pruning predicate as 2 separate IN clauses   containing the partitioning and non-partitioning columns. 
//  clean up the temp files 
//  Could be null if there's a race between the threads processing requests, with a   dag finish processed earlier. 
//  Counters for debugging, we cannot use existing counters (cntr and nextCntr) 
// if metastore has no record of this lock, it most likely timed out; either way  there is no point tracking it here any longer 
//  10^16 - 1 
//  This would refresh any conf resources and also local resources. 
//  The JobTracker has exceeded its threshold and is doing a GC. 
//  We are modifying divisor here, so make a local copy. 
//  DC_NAME 
//  Check for a list. If found, recursively init its members 
//  No op, this thread was interrupted   In this case, the call might return sooner than long polling timeout 
//  Update description 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setObject(int, java.lang.Object, int, int)    */
//  In future, this may examine ReadEntity and/or config to return   appropriate HCatReader 
//  10^-15 
//  Within the proxy object, we wrap the method call in a UserGroupInformation#doAs 
//  no token found 
//  Offset truncation. 
//  At the beginning of the list record will be the value length. 
//  OPEN_TXNS 
//  all keys + VCol + c + VCol*c 
//  check for this pattern   The pattern matching could be simplified if rules can be applied   during decorrelation,     CorrelateRel(left correlation, condition = true)     LeftInputRel     Project-A (a RexNode)       Aggregate (groupby (0), agg0(), agg1()...) 
//  see paper for alpha initialization. 
//  if size of sparse map excess the threshold convert the sparse map to   dense register and switch to DENSE encoding 
//  if query is not in compiled state, or executing state which is carried over from   a combined compile/execute in runInternal, throws the error 
//  Heartbeat on the lockid first, to assure that our lock is still valid.   Then look up the lock info (hopefully in the cache).  If these locks 
//  number of distribution keys of cRS is chosen only when numDistKeys of pRS   is 0 or less. In all other cases, distribution of the keys is based on   the pRS which is more generic than cRS.   Examples:   case 1: if pRS sort key is (a, b) and cRS sort key is (a, b, c) and number of   distribution keys are 2 and 3 resp. then after merge the sort keys will   be (a, b, c) while the number of distribution keys will be 2.   case 2: if pRS sort key is empty and number of distribution keys is 0   and if cRS sort key is (a, b) and number of distribution keys is 2 then   after merge new sort key will be (a, b) and number of distribution keys   will be 2. 
//  test string->string version 
//  get pushdown predicates for this operator's predicate 
//  Check if there exists bloom filter size entry 
//  Initialization isn't finished until all parents of all operators   are initialized. For broadcast joins that means initializing the 
//                   ROW__ID 
//  validate principals 
//  First shot without waiting. 
//  We have more input but did we start with something valid? 
//  Precision/scale exceed maximum precision. Result must be adjusted to HiveDecimal.MAX_PRECISION. 
//  @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.hooks.proto.MapFieldEntry) 
//  Check if the group by operator has already been processed 
//  Merge the result of last evaluate to previous evaluate. 
// throws exception if not initialized 
//  Evaluate the aggregation functions over the group batch. 
//    this entry in the pathenv is a directory.   see if the required file is in this directory   
//  1. Insert a row to Non-ACID table 
//  Can not judge, so assuming replication is not enabled. 
//  List? 
//  This is simply to verify that the hooks were in fact run 
//  row resolver for the operator 
//  Yarn ATS 
//  this is just for SMB join use-case. The numBuckets would be equal to that of the big table   and the small table could have lesser number of buckets. In this case, we want to send the   data from the right buckets to the big table side. For e.g. Big table has 8 buckets and small   table has 4 buckets, bucket 0 of small table needs to be sent to bucket 4 of the big table as 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getCatalogs(org.apache.hive.service.cli.SessionHandle)    */
//  Two objects. 
//  Decimal64 
//  It is a writable class 
// test get_table_objects_by_name functionality 
//  1,2,3 
//  invariant: bucket-col IN literals of type bucketField 
// this breaks all the tests in .q files 
//  The one big table row's values repeat. 
// key_dne = "50" 
//  Map of Avro's primitive types to Hives (for those that are supported by both) 
//  This close() function does not need to be synchronized   since it is called by its parents' main thread, so no 
//  Assumption - batchIndex is increasing; startVectorizedBatch was called 
//  2. INSERT OVERWRITE 
//  Use Complex.newBuilder() to construct. 
//  TBL_NAMES 
//  EOF: 
//  Select query results 
//  Zero dividend 
//  10^-16 
//  Adding mysql jdbc driver if exists 
//  we should also remove it when pulling it from Accumulo 
//  get input path and remove this alias from pathToAlias 
// column indexes of corresponding data in storage layer 
//  HIVE-9644 Attempt to fold expression like :   where (case ss_sold_date when '1998-01-01' then 1=1 else null=1 end);   where ss_sold_date= '1998-01-01' ; 
//  indicates if read buffer has data   number of rows in the temporary read buffer   cursor during reading   total number of objects in 0output 
//  verify 
//  Add jars containing the specified classes 
/*    * This method takes in an input operator and a subset of its output   * column names, and generates the input column names for the operator   * corresponding to those outputs. If the mapping from the input column   * name to the output column name is not simple, the method returns   * false, else it returns true. The list of output column names is   * modified by this method to be the list of corresponding input column   * names.    */
//  After Load from this dump, all target tables/partitions will have initial set of data. 
/*  This method returns the flip big-endian representation of value  */
//  v[7] -- since integer #5 is always 0, some products here are not included. 
//  only precision is specified 
//  Make a deep copy of the aliases so that they are not changed in the context 
//    Rewrite logic:     1. Permute the group by keys to the front.   2. If the input of an aggregate produces correlated variables,      add them to the group list.   3. Change aggCalls to reference the new project.   
//  Some of the strings can be passed in as unicode. For example, the   delimiter can be passed in as \002 - So, we first check if the   string is a unicode number, else go back to the old behavior 
//  has the user explicitly asked not to sample this table 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getBytes(int)    */
//  No children means we're at the bottom. If there are more operators to scan 
//  B is maxed out on capacity, so this move should fail the session 
//  This will also move the iterator ahead by one code point 
//  End of the month behavior 
//  Look for a file to which we can move the existing file. With external services,   it's possible for the service to be marked complete after each fragment. 
//  continue; 
/*  doClipping  */
//  Converts Timestamp to TimestampTZ. 
//  re-set TABLE_PROPS with reloaded tblproperties 
//  pRS-pJOIN-cRS 
//  production is: i64 
//  10^-18 
//  tables have been replicated over, and verified to be identical. Now, we do a couple of   alters on the source 
//  We skip DB check for import here because we already handle it above   as a CTAS check. 
//  Store the ugi in transport and then continue as usual. 
//  For unpartitioned table, partitionVals are not specified 
//  delete. 
//  Extract the launcher job submit/start time and use that to scope down   the search interval when we look for child jobs 
//  finishTime 
//  Then, finishable must always precede non-finishable. 
//  Still in the local JVM, use the username+password or Kerberos credentials 
//  Init 
//  override with user defined properties 
//  Apply isnull and instr (not supported by pushdown) via name filtering. 
//  no locks present 
//  lets take a look at the operators. we're checking for user   code in those. we will not run that in llap. 
//  The registration znode. 
//  First try to quickly lock some of the correct-sized free lists and allocate from them. 
//  Adjust file column index for ORC struct. 
//  Remove this server instance from ZooKeeper if dynamic service discovery is set 
//  We do not want to keep state in two separate places so remove from hive table properties. 
// done 
//  for temporary tables we set the location to something in the session's scratch dir   it has the same life cycle as the tmp table 
// toEpochMilli() returns UTC time regardless of TZ 
//  resulting output object inspector can be used to make the RowResolver 
//  The new ObjectInspector is the same as the old one, directly return   true 
// Power with double power 
//  replace it back 
//  OrcRecordUpdater.ROW. This is somewhat fragile...   Note: this guarantees that physical column IDs are in order. 
//  We don't want to include the root struct in ACID case; it would cause the whole   struct to get read without projection. 
//  Refer to org.apache.tez.mapreduce.hadoop.MRHelpers.processDirectConversion. 
//  negative means the key didn't exist in the original    stream (i.e.: we changed the tree) 
//  Determine minimum of all non-null decimal column values; maintain isGroupResultNull. 
//  Can't be escaped, it is the separator 
//  Skip the auth in embedded mode or if the auth is disabled 
//  Same file, different offset 
//  restore repeating and no nulls indicators 
//  For inner joins, we may apply the filter(s) now. 
// HCatFieldSchema.getName()->position 
//  2) If we need to create some extra sessions, we'd do it just like startup does. 
//  improved later. 
//  TODO: If this test method is the first to run, then the parameters does not contain totalSize   and numFiles, if this runs after other tests (setUp/dropDatabase is successful), then the 
// we've added Insert clauses in order or WHEN items in whenClauses 
//  Most operations cannot run asynchronously. 
//  should emit an exception 
//  Callback on a separate thread so that when a task completes, the thread in the main queue 
//  filter the list of locations to those that have at least 80% of the 
//  2 X 2 events for V1. 3 X 1 events for V2 
//  ignore empty strings 
//  when partition column type is not string, the values from __HIVE_DEFAULT_PARTITION__ will be NULL 
//  Apply the plan again - enable WM. 
//  2) To unpartitioned table 
//  test February of non-leap year, 2/28 
//  class PartitionSpecProxy; 
//  Get failed attempts from jobfailures.jsp 
//  SIMD loop 
//  Get list of temp table names 
//  In case thread count is set to 0, use single thread. 
//  2. We then iterate through all the operators that have candidate FKs again.   We assume the PK is first joining with the FK that we just selected.   And we apply the PK-FK relationship when we compute the newrows and ndv.   After that, we join the result with all the other FKs.   We do not assume the PK-FK relationship anymore and just compute the 
//  in case of io error, reset the file system object 
//  Replicate all the events happened after bootstrap 
//  ExecutorRunTime, etc.) 
//  runAsync, queryTimeout makes sense only for a SQLOperation   Pass the original statement to SQLOperation as sql parser can remove comments by itself 
//  if the big table has more buckets than the current small table,   use "MOD" to get small table bucket names. For example, if the big   table has 4 buckets and the small table has 2 buckets, then the   mapping should be 0->0, 1->1, 2->0, 3->1. 
//  the reader pointer has moved to the end of next block, or the end of   current record. 
//  Need some extra checks   Get the running owner 
//  years/months represented in months 
//  Buddy block is allocated, or it is on higher level of allocation than we are, or we   have reached the top level. Add whatever we have got to the current free list. 
//  Not public since we must have the field count or column sort order information. 
//  create a simple table and test create, drop, get 
//  Repeated NULLs -- skip this input column. 
//  Changes the owner to a group and verify the change 
//  The default is different on the client and server, so it's null here. 
//  compare set fields 
//  In the cases of create partition, by the time this event fires, the partition   object has not yet come into existence, and thus will not yet have a   location or an SD, but these are needed to create a ql.metadata.Partition,   so we use the table's SD. The only place this is used is by the   authorization hooks, so we will not affect code flow in the metastore itself. 
//  And finally return them in a flat array 
//  The following 'deltas' includes all kinds of delta files including insert & delete deltas. 
//  Drop table will clean the table entry from the compaction queue and hence cleaner have no effect 
//  Many restrictions. 
//  Check if user have erroneously specified non-existent partitioning columns 
//  Manually modify the underlying metastore db to reflect statistics corresponding to 
//  The RangeInputSplit *should* have all of the necesary information contained in it   which alleviates us from re-parsing our configuration from the AccumuloStorageHandler   and re-setting it into the Configuration (like we did in getSplits(...)). Thus, it should   be unnecessary to re-invoke configure(...) 
//  for asc, nulls first; for desc, nulls last 
//  if all is good 
//  Fully specified partition spec 
//  Ensure Data structures are updated in the main TaskScheduler 
//  Retry with CM path 
//  Filter) are in the output and in the same position. 
// txnid:idTxnUpdate2 
// Execution mode: vectorized 
//  such as "abc%" 
//  special case: if both constants are not equal then return 0 
//  honor custom location for external table apart from what metadata specifies 
//  We don't use the cache's copy of the buffer. 
//  Is the decimal value currently valid? 
//  update paths[i], e.g., from "QUERY:id" to "id" 
// Read should get 10 + 20 rows if immutable, 50 (10+20+20) if mutable 
//  Search like binary search to minimize comparisons. 
//  This is doing a lot of copying here, this could be improved by enforcing length   at the same time as escaping rather than as separate steps. 
//  When data is prematurely ended the fieldPosition will be 1 more than the end. 
//  See if the alias has a lateral view. If so, chain the lateral view   operator on 
/*    * Element for Key: Long x Hash Table: HashSet    */
//  we need openssl 
//  only input 1 side has nulls 
//  Sort columns to make output deterministic 
//  if cRS is being used for distinct - the two reduce sinks are incompatible 
//  Replicate the changes to the replicated-table. 
//  Add the distinct aggregate column(s) to the group-by columns, 
//  Design Note: In the future, if this function can be implemented   directly to translate input to output without creating new   objects, performance can probably be improved significantly.   It's implemented in the simplest way now, just calling the   existing built-in function. 
/*  @bgen(jjtree) FlagArgs  */
// txnid:idTxnUpdate1 
//  we do not need to connect its parent to its counterpart, as they have the same parents. 
//  Archived partitions have har:/to_har_file as their location.   The original directory was saved in params 
/*  Patterns that are included in execution logging level.     * In execution mode, show only select logger messages.      */
//  Return a constant vector expression 
//  We do not include the dummy grouping set column in the output.  So we pass outputKeyLength   instead of keyExpressions.length 
//  The tokens we should ignore when we are trying to do table masking. 
//  This batch is full: break out of for loop to execute 
//  Given a byte array consisting of a serialized BloomKFilter, gives the offset (from 0)   for the start of the serialized long values that make up the bitset. 
//  not be read. Thus 0 records. 
// txnid:idTxnUpdate4 
//  the jsonObject for this operator 
// Basic case 
//  Intentionally using deprecated method 
//  Disable LLAP IO wrapper; doesn't propagate extra ACID columns correctly. 
//  default executors is 4, max slots is 3. so 3 * 20% of noconditional task size will be oversubscribed 
//  Delete the incorrectly copied file and retry with CM path 
//  Map of Integer to Integer 
//  If HIVE_CONF_DIR is not defined or file is not found in HIVE_CONF_DIR then check HIVE_HOME/conf 
//  create partCount dummy partitions 
//  Nothing to be done 
//  Go over all the aggregation classes and and get the size of the fields of   fixed length. Keep track of the variable length 
// check how much memory left memory 
//  Unregister for the dirWatcher for the specific dagIdentifier in either case. 
// txnid:idTxnUpdate3 
/*    * Tests with queries which cannot be executed with directSQL, because of type mismatch. The type   * of the num column is string, but the parameters used in the where clause are numbers. After   * falling back to ORM the number of partitions cannot be fetched by the   * ObjectStore.getNumPartitionsViaOrmFilter method. They are fetched by the   * ObjectStore.getPartitionNamesPrunedByExprNoTxn method.    */
//  while ! done 
//  Current vector map operator read type and context. 
//  1.4. Set needed cols in TSDesc 
//  serialize some data in the schema before it is altered. 
//  Logger with default base 
//  another single call to get all the partition objects 
//  no rows qualify 
//  only user belonging to admin role can list role 
//  This should not happen. 
//  A DemuxOperator should have at least one child 
//  Currently we do not support merging windowing functions with other   windowing functions i.e. embedding windowing functions within each   other 
//  Set of values to look for. 
//  3) We check whether we will end up with same operators inputing on same work.           Work1        (merge TS in W2 & W3)        Work1         /   \                  ->                  | |       X     Work2   Work3                                 Work2     If we do, we cannot merge. The reason is the same as above, currently   Tez does not support parallel edges.     In the check, we exclude the inputs to the root operator that we are trying 
//  both were the same, and can be replaced by the new db we're loading into. 
//  This join has been converted to a SMB join by the hive optimizer. The user did not   give a mapjoin hint in the query. The hive optimizer figured out that the join can be 
//  Test that existing shared_write table with new exclusive coalesces to 
//  BINARY conversions supported by GenericUDFDecimal, GenericUDFTimestamp. 
//  TODO: we could/should trace seek destinations; pps needs a "peek" method 
//  source prep 
//  get the scale factor to turn big decimal into a decimal < 1   This relies on the BigDecimal precision value, which as of HIVE-10270 
//  Cancel the timer 
//  cache the results for table authorization 
// only set output dir if partition is fully materialized 
//  column buffers, otherwise at the end when closeOp() is called, things get printed multiple times. 
//  Fall through to delta 
//  not -1   not -1 
//  keep column expression map, explain plan uses this to display 
// this is set in ZooKeeperStorage.create() 
//  Proceed with a binary search 
//  For once we actually want reference equality in Java. 
//  Should we convert the datetime to the format Hive understands by default   - either yyyy-mm-dd HH:MM:SS or seconds since epoch?   Date d = dateparser.parse(c.rdatetime);   c.rdatetimeepoch = d.getTime() / 1000; 
//  8 
//  First row is all nulls 
//  We don't need this for now, so do not support it. 
//  following mapping is to enable UDFName to UDF while generating expression for default value (in operator tree) 
//  We have a repeated value.  The sum increases by value * batch.size. 
/*  id <= 4              */
//  Always set the EXPLAIN conditions. 
//  Verify that the table does not already exist   dumpTable is only used to check the conflict for non-temporary tables 
//  objectinspector 
// data from delta_200_200 
//  Setup cache if enabled. 
/*    * If the init method of HMSHandler throws exception for the first time   * while creating RetryingHMSHandler it should be retried    */
//  2^128 - 1 
//  as a setting. 
/*    * Use this constructor when there is an output column.    */
//  generate a groupby operator (HASH mode) for a map-side partial 
//  return true if this task is an ancestor of itself of parameter desc 
//  Skip all the events belong to other DBs/tables. 
//  LOG.debug("VectorMapJoinFastLongHashMap lookup " + key + " hashCode " + hashCode); 
//  no need of merging if the move is to a local file system 
//  i.e. the sequence must be pRS-SEL*-cRS 
//  These operators need to be linked to enable runtime statistics to be gathered/used correctly 
//  Remove bloomfilter if no stats generated 
//  for now, disable operating on decimal64 column vectors for semijoin reduction as   we have to make sure same decimal type should be used during bloom filter creation   and bloom filter probing 
// since Repeating only happens when offset length is 1. 
//  optional authzid 
//    testLazyBinaryFast(         source, rows,         serde, rowStructObjectInspector,         serde_fewer, writeRowStructObjectInspector,         primitiveTypeInfos,         /* useIncludeColumns */ true, /* doWriteFewerColumns */ true, r);   } 
//  Exact same state as above. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#executeBatch()    */
//  optional string hiveQueryId = 2; 
//  GroupBy1 for the grouping set (corresponding to the rollup). 
/*      * Optimize by running value expressions only over the matched rows.      */
//  not ok to have a location locked by someone else. 
//  Look at the current session's setting 
/*    * - tree form is   *   ^(TOK_PTBLFUNCTION name alias? partitionTableFunctionSource partitioningSpec? arguments*)   * - a partitionTableFunctionSource can be a tableReference, a SubQuery or another   *   PTF invocation.    */
//  boolean that says whether tez auto reduce parallelism should be used 
//  this plug-in to avoid getting a serialized event at run-time. 
//  Leaving this as the 'hive' catalog (rather than choosing the default from the   configuration) because all the default UDFs are in that catalog, and I think that's   would people really want here. 
/*    * BYTE.    */
// if needs refresh param is passed, it should return new object 
//  Create an SSL socket and connect 
//  If HiveTableScan is not found, e.g., not sequence of Project and   Filter operators, execute the original getUniqueKeys method 
//  inpPartSpec is a mapping from partition column name to its value. 
//  Writer should match the orc configuration from the original file 
//  We will remember completed DAG for an hour to avoid execution out-of-order fragments. 
//  get column type 
//  backtrack key exprs of child to parent and compare it with parent's 
// Highest priority at this point should have come out. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setURL(java.lang.String, java.net.URL)    */
//  is   always   positive 
//  Remove branch 
//  We are going to create the map for each view in the given database 
//  r--r----- 
//  Returns true if the aggregation result will be streamed. 
//  old bucketing logic 
//  Hive "insert overwrite local directory" uses task id as dir name   Setting the id in jobconf helps to have the similar dir name as MR 
//  Make sure no one calls this 
//  else osx gives ugly temp path which screws up approvals 
//  do a group by on the list to dedup 
//  blank " " (1 byte)   blank " " (1 byte)   blank " " (1 byte)   Asian character U+4824 (3 bytes) 
//  this will stop run() from pushing records, along with potentially   blocking initialization. 
//  How much we can read from current read buffer, out of what we need. 
//  input path --> {operator --> context} 
//  Wait for scheduling to run a few times. 
//  We enforce a certain order when we do the reutilization.   In particular, we use size of table x number of reads to 
/*            * Single-Column Long get key.            */
//  Message must be transacted before we return. 
//  Push filter on top of children for retainable 
//  Mark any scratch small table scratch columns that would normally receive a copy of the   key as null, too. 
//  The input should be in TextInputFormat. 
//  Set to non-zk lock manager to prevent HS2 from trying to connect 
//  Retry logic 
//  Test append_partition_by_name 
//  Verify moveOnlyTask is optimized 
//  keep the dynamic partition context in conditional task resolver context 
/*   public boolean dropPartition(String dbName, String tableName, String partName, boolean deleteData, boolean ifPurge)      throws NoSuchObjectException, MetaException, TException {    return dropPartition(dbName, tableName, partName, deleteData,                         ifPurge? getEnvironmentContextWithIfPurgeSet() : null);  }   */
//  Lowest field. 
// /////////////////////////////////////////////  ////////////////// other failure testcases  //////////////////////////////////////////// 
//  Cannot validate the list, it may be unset 
//  New state is changed to running from something else (user is active) 
//  Deal with the last entry 
//  The number of reduce tasks per job. Hadoop sets this value to 1 by default   By setting this property to -1, Hive will automatically determine the correct 
/*         we set it to completed if there is nothing the server has to report        for example, DDL statements       */
//  Step 2.4: Remove this MapJoin task 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getPrimaryKeys(org.apache.hive.service.cli.SessionHandle)    */
//  If the to string has more code points, make sure to traverse it too 
//  The log 
//  App Base Dir / ${dagDir}   appBase/output/   appBase/output/attemptDir 
//  The method is not exposed, and we don't use it. 
//  Name required for routing. Error out if it is not set. 
//  This call is accessed from client (jdbc) side 
//  Use HiveHookEventProto.newBuilder() to construct. 
//  Process grouping set for the reduce sink operator   For eg: consider: select key, value, count(1) from T group by key, value with rollup.   Assuming map-side aggregation and no skew, the plan would look like:     TableScan --> Select --> GroupBy1 --> ReduceSink --> GroupBy2 --> Select --> FileSink     This function is called for ReduceSink to add the additional grouping keys introduced by 
/*  10 files x 100 size for 11 splits  */
//                    99999999.99999999999999999999999999999949999 
//  Coalesce is a special case because it can take variable number of arguments.   Nvl is a specialization of the Coalesce. 
//  Use Case 1. 
/*         try to render in place update progress bar only if the operations logs is update at least once        as this will hopefully allow printing the metadata information like query id, application id        etc. have to remove these notifiers when the operation logs get merged into GetOperationStatus       */
//  Convert the first param into a DateWritableV2 value 
// tries to get X lock on T1 and gets Waiting state 
// BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(new QueryState(queryState.getConf()), input); 
//  source table input format 
//  Change the location, and see the results 
//  If the current child struct expression is a constant struct. 
//  0 
//  Try to create a table with all of the parameters set 
//  If there's no base file, do a major compaction 
//  Now replace the old evaluators with our own 
//  Recurse over the subqueries to fill the subquery part of the plan 
// We don't want to push cross join 
//  do nothing for null struct 
//  mapOutputInfoMap is used to share the lookups with the caller 
//  query 1 
//  Attempt to create the table, taking EXTERNAL into consideration 
//  Found all possible rows which will not be filtered 
//  first 3 stripes will satisfy the predicate and merged to single split, last stripe will be a 
//  finally throw an exception 
//  Test partition listing with a partial spec - hr is specified but ds is not 
//  of this operator. This is used for connecting them later. 
//  This is the first call, open the session 
//  Set these two to invalid values so any attempt to use them 
//  set fastScale to 0. 
//  startIx is inclusive, endIx is exclusive. 
//        Not sure if these large cols could be in resultSchema. Ignore this for now 0_o 
//  Now run a lower priority task. 
//  Remote dirs 
//  Keep backward compat in explain for single-file copy tasks. 
// p=0 exists and is not empty  p=2 exists and is empty  p=3 doesn't exist 
//  1 integer digit; 2 fraction digits.   Trailing zeroes are suppressed.  --------------------------------------------------- 
//  text -> byte[] -> value 
//  query 2 
//  RESOURCE_URIS 
//  Sleep for longer than server's idletimeout and execute a query 
//  can be stored in String, Text, or some other classes. 
//  Can we add to current batch? 
/*  do nothing  */
//  Test that existing exclusive db with new shared_write coalesces to 
//  long-running application. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getWarnings()    */
//  Request task1 
//  The globalHook stuff. There's no proper way to insert this, so we add it everywhere. 
// create 2 rows in a file 000001_0 (and an empty 000000_0) 
//  right now we assume that the group by is an ArrayList object. It may 
//  since tasks themselves can be graphs we want to limit the number of created 
//  The serialize routine uses this to build serializedKeyLengths. 
//  Empties the projection columns. 
//  0 or -1 implies fetch everything 
//  partitions to be dropped in this batch 
//  This is called only during move session handling, removing session already checks this.   So this is not expected as remove failing will not even invoke this method 
//  The collectable stats for the aggregator needs to be cleared.   For eg. if a file is being loaded, the old number of rows are not valid 
//  it's parent hierarchy. selfAndUpstreamComplete indicates how many of these have completed. 
//  same above 
//  REQ 
//  Drop them from the proper catalog 
//  more reliable then attempting to parse the error message from the SQLException. 
//  0. Check if we can handle the SubQuery; 
//  @@protoc_insertion_point(class_scope:LlapOutputSocketInitMessage) 
/*                * Single-Column String specific save key and lookup.                */
//  Read cost 
//  @@protoc_insertion_point(class_scope:PurgeCacheRequestProto) 
//  Cannot open the lock file for writing, must be held by a live process 
//  repl noop export on non-existant table, has repl.noop, does not error   import repl noop dump, no error 
//  call-1: open to read - split 1 => mock:/mocktable3/0_0 
//  We rely on NDC information in the logs to map counters to attempt.   If that is not available, appId should either be passed in, or extracted from NDC. 
//  Aggregation buffer methods. 
//  For SMB, the key column(s) in RS should be same as bucket column(s) and sort column(s)` 
//  make sure delta_0000001_0000001_0000 appears before delta_0000002_0000002_0000 
//  3 
//  JAVA64_META + JAVA64_REF   JAVA64_ARRAY_META + JAVA64_REF 
//  Replicate drop partition event and verify 
//  test batchsize is not divisible by decaying factor 
//  All new versions of Acid tables created after the introduction of Acid version/type system   can have TRANSACTIONAL_PROPERTIES property defined. This parameter can be used to change   the operational behavior of ACID. However if this parameter is not defined, the new Acid   tables will still behave as the old ones. This is done so to preserve the behavior   in case of rolling downgrade. 
//  These are the possible types referenced by 'type' below. 
// since we take the RHS of set exactly as it was in Input, we don't need to deal with quoting/escaping column/table names 
//  create a union above all the branches   the schema of union looks like this 
//  let hashtable Op be the child of this parent 
//  Fill high long and middle from some of lower long. 
//  Visible for testing 
//  Project nulls for the extra fields. (Maybe a sub-class table has 
//  Invariants. 
//  cancel the deleg. tokens that were acquired for this job now that   we are done - we should cancel if the tokens were acquired by   HCatOutputFormat and not if they were supplied by Oozie.   In the latter case the HCAT_KEY_TOKEN_SIGNATURE property in   the conf will not be set 
// multiple transactions only happen for streaming ingest which only allows inserts 
//  No overlap between the two ranges 
//  found a source which is not to be stored in memory 
//  evaluate children 
//  TopN query results as records (types defined by metastore) 
//  Sequence is the following:   1) INSERT with no segments -> Original segment still present in the datasource   2) INSERT OVERWRITE with no segments -> Datasource is empty   3) INSERT OVERWRITE with no segments -> Datasource is empty   4) INSERT with no segments -> Datasource is empty   5) INSERT with one segment -> Datasource has one segment   6) INSERT OVERWRITE with one segment -> Datasource has one segment   7) INSERT with one segment -> Datasource has two segments   8) INSERT OVERWRITE with no segments -> Datasource is empty 
//  Same depth, using natural order 
/*    * Context for using VectorDeserializeRow to deserialize each row from the Input File Format   * into the VectorizedRowBatch.    */
//  a map that keeps track of work that need to be linked while 
//  Use only 1 reducer if order by is present 
//  1 0   1 1 
//  I can't do this with @Before because I want to be able to control when the thread starts 
/*  == long in Avro  */
//  Try to expire the session if it's not in use; if in use, bail. 
//  INTERMEDIATE REDUCTION:   Case 1: NO column stats, NO hash aggregation, NO grouping sets  numRows   Case 2: NO column stats, NO hash aggregation, grouping sets  numRows * sizeOfGroupingSet   Case 3: column stats, hash aggregation, NO grouping sets  Min(numRows / 2, ndvProduct * parallelism)   Case 4: column stats, hash aggregation, grouping sets  Min((numRows * sizeOfGroupingSet) / 2, ndvProduct * parallelism * sizeOfGroupingSet)   Case 5: column stats, NO hash aggregation, NO grouping sets  numRows   Case 6: column stats, NO hash aggregation, grouping sets  numRows * sizeOfGroupingSet 
//  LOG.debug("generateHashMultiSetResultSingleValue with big table..."); 
// mapside 
//  initialize map operator 
//  put them in and also roll them up while we're at it 
//  All the tasks should have deallocated their stuff. Make sure we can allocate everything. 
//  Regular vertices 
//  Depending on whether we are using beeline or sqlline the line endings have to be handled   differently. 
//  Reset key and value columns; and batch.size 
//  Extrapolation is needed for extraColumnNames. 
// assume # can only be used at the beginning of line. 
//  CORS check 
//  set r.genericUDAFEvaluator 
//  computes the sig of every object 
//  we have just added a new node. Signal timeout monitor to reset timer 
//  Configure header size 
//  Field not included in query. 
//  set up operator plan 
//  left outer join produced a list with no values 
//  The default encoding for this table when not otherwise specified 
//  Recreate the database 
//  ill-formed query like select * from t1 having c1 > 0; 
/*  id <> 12 and       first_name in ('john', 'sue') and       id in (34,50)  */
//  Query does not contain CUBE, ROLLUP, or GROUPING SETS, and thus,   grouping should return 0 
//  Kick out previous overflow batch results. 
// output noNulls is set to false, we need to reset the isNull array 
//  Storage vars 
//  Process per vertex counters that are available only via vertex Progress 
//  This ArrayList holds the max/min N   This is the N 
/*  canRetainByteRef  */
//  Start with the following locks:   [path1, shared]   [path1, exclusive]   [path2, shared]   [path2, shared] 
//  Select query results as records (types defined by metastore) 
//  add the new aggregate column and recompute data size 
//  alter the table 
//  column level statistics are required only for the columns that are needed 
//  Read the warehouse dir, which can be changed so multiple MetaStore tests could be run on 
//  At this point, we are going to make a copy if needed to avoid array boundaries. 
/*    * how is this different from the OutputShape set on the TableDef.   * This is the OI of the object coming out of the PTF.   * It is put in an output Partition whose Serde is usually LazyBinarySerde.   * So the next PTF (or Operator) in the chain gets a LazyBinaryStruct.    */
//  Only primitive fields supports for now. 
//  Case 2 - Max in list members: 10; Max query string length: 1KB 
//  Linux:yes, Windows:no 
//  of the user assumptions. 
//  Avoid a copy when oldTmpJars is null or empty 
//  semijoin case 
//  current random sampling implementation in InputSampler always returns   value of index 3, 5, 8, which can be same with previous partition key.   That induces "Split points are out of order" exception in TotalOrderPartitioner causing HIVE-7699 
//  vertex 
//  Now clone the tree above selOp 
//  To be populated for SMB joins only for all the small tables 
//  non-transitive 
//  Reset the bytebuffer 
//  Set the specific parameters if needed 
//  run the map join task, set task tag 
//  Instantiate the chosen transaction manager 
//  Run count times and get average 
//  2. If we cannot, we swap the inputs so we can try 
//  IDE support for running tez jobs 
//  Need to set the client's KeyProvider to the NN's for JKS,   else the updates do not get flushed properly 
//  name of child class 
//  view column access info is carried by this.getColumnAccessInfo(). 
//  (i.e. too large to have an effect). 
//  The table containing the partitions is not yet loaded in cache 
//  Use reflection to set LockManager since creating the object using the   relection in DummyTxnManager won't take Mocked object 
//  A mapping from an operator to the columns by which it's output is bucketed 
//  generating join output results. 
// the token file location as initial hiveconf arg 
//  1.1 First Add original GB Keys 
//  First try without qualifiers - would resolve builtin/temp functions.   Otherwise try qualifying with current db name. 
//  ReduceSink also needs MapJoin as child 
//  SD 
//  D4. Multiply and subtract: (some digits of) R -= D * QH 
//  This resets vectors in both batches. 
//  The storage root 
/*    * - If the SubQuery has no where clause, there is nothing to rewrite.   * - Decompose SubQuery where clause into list of Top level conjuncts.   * - For each conjunct   *   - Break down the conjunct into (LeftExpr, LeftExprType, RightExpr,   *     RightExprType)   *   - If the top level operator is an Equality Operator we will break   *     it down into left and right; in all other case there is only a   *     lhs.   *   - The ExprType is based on whether the Expr. refers to the Parent   *     Query table sources, refers to the SubQuery sources or both.   *   - We assume an unqualified Column refers to a SubQuery table source.   *     This is because we require Parent Column references to be qualified   *     within the SubQuery.   *   - If the lhs or rhs expr refers to both Parent and SubQuery sources,   *     we flag this as Unsupported.   *   - If the conjunct as a whole, only refers to the Parent Query sources,   *     we flag this as an Error.   *   - A conjunct is Correlated if the lhs refers to SubQuery sources and rhs   *     refers to Parent Query sources or the reverse.   *   - Say the lhs refers to SubQuery and rhs refers to Parent Query sources; the   *     other case is handled analogously.   *     - remove this conjunct from the SubQuery where clause.   *     - for the SubQuery expression(lhs) construct a new alias   *     - in the correlated predicate, replace the SubQuery   *       expression(lhs) with the alias AST.   *     - add this altered predicate to the Join predicate tracked by the   *       QBSubQuery object.   *     - add the alias AST to a list of subQueryJoinAliasExprs. This   *       list is used in the case of Outer Joins to add null check   *       predicates to the Outer Query's where clause.   *     - Add the SubQuery expression with the alias as a SelectItem to   *       the SubQuery's SelectList.   *     - In case this SubQuery contains aggregation expressions add this SubQuery   *       expression to its GroupBy; add it to the front of the GroupBy.   *   - If predicate is not correlated, let it remain in the SubQuery   *     where clause.   * Additional things for Having clause:   * - A correlation predicate may refer to an aggregation expression.   * - This introduces 2 twists to the rewrite:   *   a. When analyzing equality predicates we need to analyze each side   *      to see if it is an aggregation expression from the Outer Query.   *      So for e.g. this is a valid correlation predicate:   *         R2.x = min(R1.y)   *      Where R1 is an outer table reference, and R2 is a SubQuery table reference.   *   b. When hoisting the correlation predicate to a join predicate, we need to   *      rewrite it to be in the form the Join code allows: so the predict needs   *      to contain a qualified column references.   *      We handle this by generating a new name for the aggregation expression,   *      like R1._gby_sq_col_1 and adding this mapping to the Outer Query's   *      Row Resolver. Then we construct a joining predicate using this new   *      name; so in our e.g. the condition would be: R2.x = R1._gby_sq_col_1    */
//  The list of table names could contain duplicates. RawStore.getTableObjectsByName()   only guarantees returning no duplicate table objects in one batch. If we need 
// check that compacted base dir has a version file with expected value 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareCall(java.lang.String)    */
//  performed as a smb join, based on all the tables/partitions being joined. 
//  this is the last key we need to process 
//  Initialize set of unprocessed small tables 
//  This code has been only added for testing 
// ---------------------------------------------------------------------------   Single-Column Long specific members.   
// create input ByteArrayRef 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#closeOperation(org.apache.hive.service.cli.OperationHandle)    */
//  + 
//  we need to convert these to the generated column names we can see in the Join operator 
/*  Spot check correctness of decimal scalar subtract decimal column. The case for   * addition checks all the cases for the template, so don't do that redundantly here.    */
/*    * Deliver a vector batch to the operator tree.   *   * The Vectorized Input File Format reader has already set the partition column   * values, reset and filled in the batch, etc.   *   * We pass the VectorizedRowBatch through here.   *   * @return Return true if the operator tree is not done yet.    */
/*  One of the digits was the point.  */
//  dummy private constructor, since this class is a collection of static utility methods. 
//  remember which parent belongs to which tag 
//  introduce derived table above one child, if this is self-join   since user provided aliases are lost at this point. 
//  Get column names and types 
// -----------------------------------------------------------------------------------------------   Math methods.  ----------------------------------------------------------------------------------------------- 
//  cast (40 + 50 as string) 
//  Whether the files output by this FileSink can be merged, e.g. if they are to be put into a 
//  reset the data 
//  We create converters beforehand, so that the converters can reuse the   same object for returning conversion results. 
//  First branch should have the query (with write ID filter conditions) 
//  Test a set of random divisions at high precision. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#getMetaData()    */
//  use default values from fs.default.name 
//  also check a config that has default in hiveconf 
//  initialize input 
//  Pass lineageState when a driver instantiates another Driver to run   or compile another query 
//  select with grant for exporting contents 
//  Neither select nor compaction (which is a select) wil work after this. 
//  statistics object that is combination of statistics from all 
//  The wrapper for byte array 
//  Overlay the ConfVars. Note that this ignores ConfVars with null values 
//  code below does not deal with the Connection Server.object. 
//  if there is a transaction then this lock will be released on commit/abort/rollback instead. 
//  ignore this exception, actually this exception won't be thrown from getPartitionColumnStatistics 
//  so that beeline won't kill the JVM 
//  finally remove the role 
//   write the given version to metastore 
//  Test in http mode with ssl properties specified in url 
//  Test getColumns() 
//  this is the key less than the lowest key we need to process 
/*    * MAP.    */
//  predicate expression: userid <= 100 and subtype <= 1000.0 
//  literals. 
// , 'transactional_properties'='default' 
//  If it's multi-expr filter (e.g. a='5', b='2012-01-02'), AND with previous exprs. 
//  If this is the first expression 
// Generate vectorized expression 
//  Force the cache clear so we know its empty 
//  If udf is requiring additional jars, we can't determine the result in 
/*       Since there can be multiple rounds of this run all of which will be tied to the same      query id -- generated in compile phase , adding a additional UUID to the end to print each run      in separate files.        */
//  Extract input refs. They will serve as input for the function invocation 
/*        * Here, we can be in one of 4 states.       *       * 1. If map join work is null implies that we have not yet traversed the big table side. We       * just need to see if we can find a reduce sink operator in the big table side. This would       * imply a reduce side operation.       *       * 2. If we don't find a reducesink in 1 it has to be the case that it is a map side operation.       *       * 3. If we have already created a work item for the big table side, we need to see if we can       * find a table scan operator in the big table side. This would imply a map side operation.       *       * 4. If we don't find a table scan operator, it has to be a reduce side operation.        */
//  max. This table is either the big table or we cannot convert. 
//  Create the LazyStruct from the LazyStruct...Inspector 
//  The layout for ACID files is table|partname/base|delta|delete_delta/bucket   We will always only be writing delta files ( except IOW which writes base_X/ ).   In the buckets created by FileSinkOperator   it will look like original_bucket/delta|delete_delta/bucket   (e.g. .../-ext-10004/000000_0/delta_0000014_0000014_0000/bucket_00000).  So we need to   move that into the above structure. For the first mover there will be no delta directory,   so we can move the whole directory.   For everyone else we will need to just move the buckets under the existing delta   directory. 
/*    * For T1 join T2 on T1.x = T2.y if we identify 'y' s a key of T2 then we can   * infer the join cardinality as: rowCount(T1) * selectivity(T2) i.e this is   * like a SemiJoin where the T1(Fact side/FK side) is filtered by a factor   * based on the Selectivity of the PK/Dim table side.   *   * 1. If both T1.x and T2.y are keys then use the larger one as the PK side.   * 2. In case of outer Joins: a) The FK side should be the Null Preserving   * side. It doesn't make sense to apply this heuristic in case of Dim loj Fact   * or Fact roj Dim b) The selectivity factor applied on the Fact Table should   * be 1.    */
//  Find the next chunk size 
/*  * The data written and read withing the same M/R job, thus should never be written by one  * version of Hive and read by another. * @see org.apache.hive.hcatalog.data.ReaderWriter  */
//  unable to parse the use command 
//  ------r-- 
//  If the table is partitioned we have to put the partition() clause in 
// for Update/Delete we always write exactly (at most actually) the partitions we read 
//  The AM can be used for multiple queries. This is an indication that a single query is complete.   We don't have a good mechanism to know when an app ends. Removing this right now ensures 
//  Error already occurred, but we still want to get the   error code of the child process if possible. 
//  0-based index of which row we are on. 
//  In this particular SparkSessionManager implementation, we don't recycle   returned sessions. References to session are still valid. 
//  Make fully qualified path for further use. 
//  3. Deduce Resultset Schema 
//  We have valid pruning expressions, only retrieve qualifying partitions 
//  Take the pattern and split it on the | to get all the composing   patterns 
//  just an individual column 
// there is a change here - prev version had 'transadtional', one beofre' acid' 
//  get the last operator for processing big tables 
//  Do the authorization 
//  If based on the new key count, keyCount is smaller than a threshold,   then just load the entire restored hashmap into memory. 
//  This call checks under lock if we can actually preempt the task.   It is possible to have a race where the update (that's also under lock) makes the   task finishable or guaranteed between the remove and kill, but it's the same timing 
//  When true indicates that this object is being read as part of an update or delete.  This is   important because in that case we shouldn't acquire a lock for it or authorize the read. 
//  Make sure there's a default database associated with each catalog 
//  1 Add order by token 
//  Now prepare partnames with 10 partitions: [tab1part11...tab1part20], but with no overlap 
//  Combined, all good.   Don't combine with that, but may combine with others.   Don't combine with with that, and make that a base for 0new combines. 
/*  @bgen(jjtree) HeaderList  */
//  The partition that has to be exchanged 
//  it represents the column name corresponding to distinct aggr, if any 
//  Multi parents, cant handle that.   Right now, we do not remove projection on top of   LateralViewForward operators. 
//  LOG.debug("generateHashMultiSetResultMultiValue allMatchesIndex " + allMatchesIndex + " duplicateCount " + duplicateCount + " count " + count); 
//  Trimming is again necessary, because rounding may introduce new trailing 0's. 
//  test for timestamp type 
//  Populate byte[] from Timestamp 
//  Get the last repl ID corresponding to all insert/alter/create events except DROP. 
// "may be used (ACID table)"; 
// lastAccessTime > 90 
//  Test for both colNames and partNames being empty: 
//  if client proxies connection, use forwarded ip-addresses instead of just the gateway 
//  For now, we just use this to hold the object inspector.  There are no writeValue,   setValue, etc methods yet... 
//  log which resources we're adding (apart from the hive exec) 
//  Javadoc for Statement interface states that if the value is zero   then "fetch size" hint is ignored.   In this case it means reverting it to the default value. 
//  3. Run exhaustive PPD, add not null filters, transitive inference, 
//    Properties   
//  Since the user didn't supply a customized type-checking context,   use default settings. 
//  mingle existing tblproperties with those specified on the ALTER TABLE command 
//  left and right repeat 
//  Validation on the bitset size/3 hash functions. 
//  One less integer digit... 
//  if HIVE_STATS_COLLECT_SCANCOLS is enabled, check. 
//  Should only down-cast and elimination precision if within valid range. 
//  Ignore the char after escape_char 
//  positive test 
/*  Get the big table row bytes container for native vector map join  */
//  need to maintain the unique ID so that target map works can   read the output 
//  the function should support both short date and full timestamp format 
//  TableDesc needed for dynamic partitioned hash join 
//  Send out the preempted request outside of the lock. 
//  Only small table values appear in join output result. 
//  Modulo operator with overflow/zero-divide check. 
// now both txns concurrently updated TAB2 but different partitions. 
//  Scale up with 5. This is done via #multiplyDestructive(int). 
/*  Use list bucketing prunner's path.  */
// 5)  test extra field names 
//  because at that point we need access to the objects. 
//                              1         2         3 
//  skip the row. 
//  TODO: for now, we get the secure username out of UGI... after signing, we can take it 
//  Write the necessary config info to hadoop-site.xml 
//  For UDTF's, skip the function name to get the expressions 
//  E.SW: Lock we are examining is shared write 
/*  this may happen in one of the following case:      TS[0], FIL[26], SEL[2], DUMMY_STORE[30], MERGEJOIN[29]]                                              /                                    TS[3], FIL[27], SEL[5], ---------------       */
//  DEST_DB 
//  Make it possible for tests to check that the right type of PartitionExpressionProxy was 
//  If a join operator contains a big subtree, there is a chance that its 
/*    * A PTF Function must provide the 'external' names of the columns in its Output.   *    */
//  Add this parent to the children 
//  record reader 
//  Nope, so look to see if Hive's home dir has been explicitly set 
/*  Release all transient locks, by simply closing the client  */
//  At this point, we know that the extracted files are in the intermediate   extracted dir, or in the the original directory. 
// you are an admin! You have all privileges, no missing privileges 
//  initialize unionExpr for reduce-side   reduce KEY has union field as the last field if there are distinct 
//  now really scan... 
//  The calculation is strongly dependent on the assumption that all splits   came from the same file 
//  If the delimiter is seen, and the line isn't inside a quoted string, then treat   line[lastDelimiterIndex] to line[index] as a single command 
/*  All the code paths below propagate nulls even arg3 has no     * nulls. This is to reduce the number of code paths and shorten the     * code, at the expense of maybe doing unnecessary work if neither input     * has nulls. This could be improved in the future by expanding the number     * of code paths.      */
//  support the old syntax "hivemetastore [port]" but complain 
//  Wrap the static AccumuloInputFormat methods with methods that we can   verify were correctly called via Mockito 
//  shared plan utils for spark 
//  we need '=' 
//  use this function to make the union "flat" for both execution and explain 
//  add_partitions(5) : ok 
//  GUID 
//  no match for workerIdentity 
//  TableFunctionResolver tResolver = FunctionRegistry.getTableFunctionResolver(def.getName()); 
//  checks if default hs2 connection configuration file is present   and uses it to connect if found   no-op if the file is not present 
//  The implementation may or may not set output it isRepeting. 
//  Can ignore - the check failed. 
//  availableSlots * waves => desired slots to fill   sizePerBucket/totalSize => weight for particular bucket. weights add   up to 1. 
//  Implementation of row iterator 
//  subqueryToRelNode might be null if subquery expression anywhere other than    as expected in filter (where/having). We should throw an appropriate error   message 
// can happen in a race condition where another process adds a zLock under this parent  just before it is about to be deleted. It should not be a problem since this parent  can eventually be deleted by the process which hold its last child zLock 
//  Given a byte array consisting of a serialized BloomFilter, gives the offset (from 0)   for the start of the serialized long values that make up the bitset. 
// be called many times. 
//  SQL_STATE 
/*    * Setup the context for reading from the next partition file.    */
//  Even if the state has changed, don't log it twice. 
/* now process the delta files.  For normal read these should only be delete deltas.  For    * Compaction these may be any delta_x_y/.  The files inside any delta_x_y/ may be in Acid    * format (i.e. with Acid metadata columns) or 'original'. */
//  testcase.testWithColumnNumber(count, 50, checkCorrect, codec);   testcase.testWithColumnNumber(count, 80, checkCorrect, codec); 
// Else, recurse up the parents. 
//  Skip compaction if there's no delta files AND there's no original files 
//  real object 
// not relevant for LOAD 
// for RU this may be null so we should default it to 'u' which is most restrictive 
//  been processed. 
//  Consider the query: select a,b, count(1) from T group by a,b with cube;   where it is being executed in a single map-reduce job   The plan is TableScan -> GroupBy1 -> ReduceSink -> GroupBy2 -> FileSink   GroupBy1 already added the grouping id as part of the row   This function is called for GroupBy2 to add grouping id as part of the groupby keys 
//  no-arg ctor required for Kyro 
//  TODO: we could also vectorize some formats based on hive.llap.io.encode.formats if LLAP IO         is enabled and we are going to run in LLAP. However, we don't know if we end up in         LLAP or not at this stage, so don't do this now. We may need to add a 'force' option. 
//  Alter the table/partition stats and also notify truncate table event 
//  byte. 
//  It is assumed throughout the code that a reducer has a single child, add a 
//  reference HDFS based resource directly, to use distribute cache efficiently. 
//  Druid's time column is always not null. 
//  Find the last stripe. 
//  This means that we do not need a value generator. 
//  hadoop might return null if it cannot locate the job.   we may still be able to retrieve the job status - so ignore 
// For runtime, query may have finished. 
//  1 WriteEntity: default@acidtblpart@p=p2 Type=PARTITION WriteType=INSERT isDP=false 
//  Determine maximum of all non-null long column values; maintain isGroupResultNull. 
//  tests expect configuration changes applied directly to metastore 
//  This piece of code runs in master node and gets necessary context. 
//  Release the background thread. 
//  We're dealing with input that is an array of strings 
//  avoid allocation/copy of nulls, because it potentially expensive.   branch instead. 
//  All of the versions should be place in this list. 
//  Drop a table/partition; corresponding records in TXN_COMPONENTS and COMPLETED_TXN_COMPONENTS should disappear 
/*      * For multiplicands with scale 0, trim trailing zeroes.      */
//  constant projection 
//  The modification cannot affect an active plan. 
//  For each of the GB op in the logical GB this should be called seperately; 
//  @@protoc_insertion_point(class_scope:SubmitWorkResponseProto) 
//  1.b. Generate reduce sink and project operator 
//  Both are partitioned tables. 
//  Bucketing and sorting keys should exactly match 
// 2 distinct partitions modified 
//  We could derive the expected number of AMs to pass in.   Note: we pass a null token here; the tokens to talk to plugin endpoints will only be         known once the AMs register, and they are different for every AM (unlike LLAP token). 
//  Start a third batch, abort everything, don't properly close it 
// Store credentials in a private hash map and not the udf context to 
//  Overlay the SASL transport on top of the base socket transport (SSL or non-SSL) 
/*  100 files x 1000 size for 10 splits  */
//  filter disabled, injection enabled, exception not expected 
//  Only used for semijoin with residual predicates 
//  And UDF, (rowId >= 'h' and (rowId <= 'd' or rowId >= 'q')) 
//  All Sources are initialized up front. Events from different sources will end up getting added to the same list.   Pruning is disabled if either source sends in an event which causes pruning to be skipped 
//  Rest of cases 
//  get the OperationLog object from the operation 
//  Do first comparison as unsigned. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#rollback()    */
//  test that the all columns will be read by default 
//  optional int64 first_attempt_start_time = 5; 
/* 32M */
//  test that setting read column ids set read all columns to false 
/*      * process Wdw functions      */
//  1.1 Add Column info for non partion cols (Object Inspector fields) 
//  Optimizing for readField? 
//  Create the event and send it tez. Tez will route it to appropriate processor 
//  1 (test #readFully(3)): 
//  Add maxLength parameter to UDFs that have CHAR or VARCHAR output. 
//  change to TextInputFormat 
//  Is the existing entry and newer entry are subset of one another ? 
//  Projection mode is not yet supported for [NOT] BETWEEN. Return null so Vectorizer   knows to revert to row-at-a-time execution. 
//  We trim the trailing zero fraction digits so we don't cause unnecessary precision   overflow later. 
//  function class will not cause Exception 
//  a vectorized row batch is being created. 
//  1 anonymous element "array_element" 
//  The position of this table 
//  Verify scratch dir paths and permission 
//  Uses a pattern 
//  properties for remote driver RPC, and yarn properties for Spark on YARN mode. 
//  Convert to a MapReduce job id 
//  The cached buffer is in the middle of the requested range.   The remaining tail of the latter may still be available further. 
//  All good 
//  leave work's output may be read in further SparkWork/FetchWork, we should not combine   leave works without notifying further SparkWork/FetchWork. 
//  filter enabled, injection disabled, exception expected 
//  Not supported for MM tables for now. 
//  scanning the filesystem to get file lengths. 
//  sort-based aggregations 
//  Our hash tables are immutable.  We can safely do by reference STRING, CHAR/VARCHAR, etc. 
//  if this is an insert into statement we might need to add constraint check 
//  we want to remove the DPP with bigger data size 
//  This can only come from a brute force discard; for now we don't discard blocks larger   than the target block. We could discard it and add remainder to free lists.   By definition if we are fragmented there should be a smaller buffer somewhere. 
//  Adjust right input fields in nonEquiConds if previous call modified the input 
/*  HCat Input Format related errors 1000 - 1999  */
//  Run an hcat expression and return just the json outout. 
// looks as much as possible like original query 
//  continue merging with next alias 
//  Open the log file, and read in a line. Then feed the line into 
//  all rows from left side will be present in resultset 
//  TODO: periodically reload a new HiveConf to check if stats reporting is enabled. 
//  Regrettable that we have to wrap the HCatException into a RuntimeException,   but throwing the exception is the appropriate result here, and hasNext()   signature will only allow RuntimeExceptions. Iterator.hasNext() really   should have allowed IOExceptions 
// 2 from insert + 1 for each update stmt 
//  Some of the information in the source is complete. Don't need to fetch it from the context. 
//  Back to seconds. 
//  this internalName represents a constant parameter in aggregation parameters 
//  its parents also 
//  test with same schema with include 
//  We at least know they are not equal.  The one with the larger scale has non-zero digits   below the other's scale (since the scale does not include trailing zeroes). 
//  authorize for the old   location, and new location 
/*          * Get our Multi-Key hash map information for this specialized class.          */
//  Today, ACID tables are only ORC and that format is vectorizable.  Verify these   assumptions. 
//  Add the Hadoop token back to the Job, the configuration still has the necessary 
//  TODO: for ordinal types you can produce a range (BETWEEN 1444442100 1444442107) 
//  Make sure the Accumulo token is set in the Configuration (only a stub of the Accumulo   AuthentiationToken is serialized, not the entire token). configureJobConf may be   called multiple times with the same JobConf which results in an error from Accumulo 
//  Walk through the tree to decide value.   Example: skewed column: C1, C2 ;   index: (1,a) ;   expression tree: ((c1=1) and (c2=a)) or ((c1=3) or (c2=b)) 
//  If the size is present in the metastore, use it 
//  Do implicit conversion from source type to target type. 
/*  In this branch we had already a cookie that did expire        therefore we need to resend a valid Kerberos challenge */
// need a separate stmt for executeUpdate() otherwise it will close the ResultSet(HIVE-12725) 
//  so aggregationIsDistinct is a boolean array instead of a single number. 
//  Wrap around at the end of buffer. 
// This is required otherwise correct work object on repl load wont be created. 
//  10^-31 
//  The last line didn't match a pattern, it is probably an error message, part of   a string of stack traces related to the same error message so add it to the stack   trace 
//  Map of valid write ids list for all the tables read by the current txn 
//  Send the DelegationToken down to the Configuration for Accumulo to use 
//  For caching column stats for a partitioned table 
//  if map tasks and reduce tasks are in finishable state then priority is given to the task in   the following order   1) Dag start time   2) Within dag priority   3) Attempt start time   4) Vertex parallelism 
//  If order of events (i.e. dagstart and fragmentstart) was guaranteed, we could just   create the cache when dag starts, and blindly return it to execution here. 
//  If the log4j.configuration property hasn't already been explicitly set,   use Hive's default log4j configuration 
//  if there is at least one record, put it in the map 
//  Add the entry in mapredwork 
//  Now clean them and check that they are removed from the count. 
//  we have found the colName 
//  Its count(col) case 
/*  GenericUDFCase and GenericUDFWhen are implemented with the UDF Adaptor because             * of their complexity and generality. In the future, variations of these             * can be optimized to run faster for the vectorized code path. For example,             * CASE col WHEN 1 then "one" WHEN 2 THEN "two" ELSE "other" END             * is an example of a GenericUDFCase that has all constant arguments             * except for the first argument. This is probably a common case and a             * good candidate for a fast, special-purpose VectorExpression. Then             * the UDF Adaptor code path could be used as a catch-all for             * non-optimized general cases.              */
//  start column # for DP columns   array of values corresponding to DP columns 
//  level OB; top level OB will have RexCall kept in a map.) 
//  Create ReduceSink operator 
//  Interrupt the runner thread 
//  Union is hard to handle. For instance, the following case:    TS    TS    |      |    FIL   FIL    |      |    SEL   SEL      \   /      UNION        |        RS        |       JOIN   If we treat this as a MJ case, then after the RS is removed, we would   create two MapWorks, for each of the TS. Each of these MapWork will contain   a MJ operator, which is wrong.   Otherwise, we could try to break the op tree at the UNION, and create two MapWorks   for the branches above. Then, MJ will be in the following ReduceWork. 
//  The join alias is modified before being inserted for consumption by sort-merge   join queries. If the join is part of a sub-query the alias is modified to include 
/*    * If this QB represents a  SubQuery predicate then this will point to the SubQuery object.    */
//  We're dealing with input that is an array of arrays of strings 
// cq_state 
//  The join keys are available in the reduceSinkOperators before join 
//  Setup the table column stats 
//  Create database in specific location (absolute non-qualified path) 
//  second table in union query with view as parent 
//  Base means originals no longer matter. 
//  Load from modified dump event directories. 
//  optional 
/*    * Amount of time a thread can be alive in thread pool before cleaning this up. Core threads   * will not be cleanup from thread pool.    */
//  Create expressions for Project operators before and after the Sort 
//  The map is overloaded to keep track of mapjoins also 
// not found 
//  10^-32 
//  add added archives 
/*  Remove a task from the pending list  */
//  init output object inspectors 
//  Indexes have only one entry per value, could go linear from here, if we want to   use this for any sorted table, we'll need to continue the search 
//  In cases where column expression map or row schema is missing, just pass on the parent column   stats. This could happen in cases like TS -> FIL where FIL does not map input column names to 
//  We need a new connection object as we'll check the cache size after connection close 
//  future 
//  Store the required fields information in the UDFContext so that we 
//  the number of reducers (set by user or inferred) is <=1 
//  If we are going to validate the schema, make sure we don't create it 
//  Object inspectors for the tags for the input and output unionss 
//  Start the service 
// when a token is created the renewer of the token is stored  as shortName in AbstractDelegationTokenIdentifier.setRenewer()  this seems like an inconsistency because while cancelling the token  it uses the shortname to compare the renewer while it does not use  shortname during token renewal. Use getShortUserName() until its fixed  in HADOOP-15068 
//  fallback to mapjoin no bucket scaling 
/*            * We convert to an array of TypeInfo using a library routine since it parses the           * information and can handle use of different separators, etc.  We cannot use the           * raw type string for comparison in the map because of the different separators used.            */
//  compute keys and values as StandardObjects. Use non-optimized key (MR). 
//  If this file sink desc has been processed due to a linked file sink desc, 
//  Have to force a cleanup of all expired entries here because its possible that the   expired entries will still be counted in Cache.size().   Taken from: 
//  Remove from all tables 
//  null, we just add it 
//  Bootstrap test 
//  The number of non-NULL keys.  They have associated hash codes and key data.  
//  Create a temporary function using the jar 
//  Query hooks that execute before compilation and after execution 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setDate(java.lang.String, java.sql.Date,   * java.util.Calendar)    */
//  Creates the command-line parameters for distcp 
//  This should not happen 
// add shutdown hook to cleanup the beeline for smooth exit 
//  Insert a row to ACID table 
//  The mapping that doesn't exist still shouldn't work. 
//  If we have eager evaluators anywhere below us, then we are eager too. 
//  Test select root.col1 from root:struct<col1:struct<a:boolean,b:double>,col2:double> 
/*    * Return a short string with the parameters of the vector expression that will be   * shown in EXPLAIN output, etc.    */
//  Find one session dir to remove 
// driverRun("insert overwrite table orc5318 select * from inpy"); 
/*       we set progress bar to be completed when hive query execution has completed     */
//  4. Warn user if we could get stats for required columns 
//  skip the name and metadata 
//  Expected exception - Remote MetaStore 
//  OWNER_NAME 
//  10^-37 
//  error out. 
//  Make sure row-7 is in the output. 
//  instance. 
//  The cleaner will removed aborted txns data/metadata but cannot remove aborted txn2 from TXN_TO_WRITE_ID   as there is a open txn < aborted txn2. The aborted txn1 < open txn and will be removed.   Also, committed txn > open txn is retained. 
/*          * Cancelled the job request and return to client.          */
//  The current element in a1   The current element in a2 
//  Loading the extra configuration options 
//  In case the query is served by HiveServer2, don't pad it with spaces,   as HiveServer2 output is consumed by JDBC/ODBC clients. 
//  non nulls 
//  NOOP 
//  Nothing to copy and cache. 
/*        *  Restriction.7.h :: SubQuery predicates can appear only as top level conjuncts.        */
//  if the Hive configs are received from WITH clause in REPL LOAD or REPL STATUS commands. 
//  unknown | unknown 
//  Restore the old path information back   This is just to prevent incompatibilities with previous versions Hive 
//  Set dynamic partitioning to nonstrict so that queries do not need any partition   references.   todo: this may be a perf issue as it prevents the optimizer.. or not 
//  1st level GB: create a GB(R1 on all keys + VCol + count() as c) for each 
//  the task in. On MR: The cache is a no-op. 
//  may be null 
//  ok even if there is not data 
/*    * Initialize the conversion related arrays.  Assumes initTopLevelField has already been called.    */
//  We store CHAR and VARCHAR without pads, so write with STRING. 
//  The fastBigIntegerBytes method returns 3 56 bit (7 byte) words and a possible sign byte. 
//  Send a state update for vertex1 completion. This triggers a status update to be sent out. 
//  The join outputs a concatenation of all the inputs. 
//  return positive modulo 
//  Otherwise, handle like a normal generic UDF. 
//  Regression test for defect reported in HIVE-6399 
//  assume the archive is in the original dir, check if it exists 
//  10^-39 
//  Tests may leave this unitialized, so better set it to 1 
/*    * Config name used to find the maximum time job request can be executed.    */
//  join. 
//  cast string group to string (varchar to string, etc.) 
//  Replace expression 
//  VALUES 
/*  1-replica memory  */
//  10^-38 
/*    * NOTE: There is an expectation that all fields will be read-thru.    */
//  The mapjoin has the same schema as the join operator 
//  For thread safety, we allocate private writable objects for our use only. 
//  Return true if the partition is bucketed/sorted by the specified positions   The number of buckets, the sort order should also match along with the 
//  double NOT BETWEEN 
//  small table 
// https://dev.mysql.com/doc/refman/5.5/en/error-messages-server.html 
// minor comp, so we ignore 'base_0000100' files so all Deletes end up first since 
//  optional string hive_query_id = 4; 
//  Update expectedEntries based on factor and minEntries configurations 
/*  is 10^2^i.  Used to convert decimal  */
//  Write the mutation 
// Preliminary checks.  In the MR version of the code, these used to be done via another walk,  here it is done inline. 
/*        * Get the job request time out value. If this configuration value is set to 0       * then job request will wait until it finishes.        */
// 0 since base_x doesn't have a suffix (neither does pre acid write) 
//  Now that bootstrap has dumped all objects related, we have to account for the changes   that occurred while bootstrap was happening - i.e. we have to look through all events   during the bootstrap period and consolidate them with our dump. 
//  3. We populate the filters and filterMap structure needed in the join descriptor 
// template, <ClassName>, <ValueType>, <IfDefined> 
//  User specified a row limit, set it on the Query 
//  Nobody can see this exception on the threadpool; just log it. 
// reached EndOfFile 
//  The RPC library takes care of timing out this. 
//  Add to the queue only the first time this is registered, and on 
/*    * {@link VectorizedOrcAcidRowBatchReader} is always used for vectorized reads of acid tables.   * In some cases this cannot be used from LLAP IO elevator because   * {@link RecordReader#getRowNumber()} is not (currently) available there but is required to   * generate ROW__IDs for "original" files   * @param hasDeletes - if there are any deletes that apply to this split   * todo: HIVE-17944    */
//  Mark this small table as being processed 
//  specified 
//  Some columns from tables are not used. 
//  If the previous character isn't an escape characters, it's the separator 
//  check that data has moved 
//  all children are base classes 
//  Its either count (*) or count() case 
//  assigning higher priority than FileSystem shutdown hook so that streaming connection gets closed first before   filesystem close (to avoid ClosedChannelException) 
//  Break if it encountered a union 
//  toStandardDuration assumes days are always 24h, and hours are always 60 minutes,   which may not always be the case, e.g if there are daylight saving changes. 
//  Only refresh once. 
//  Password must be present 
//  Create the non-deferred realArgument 
//  the mapjoin has already been handled 
//  There should be 1 delta dir, plus a base dir in the location 
//  Set the table properties 
/*  * An single long value hash map based on the BytesBytesMultiHashMultiSet. * * We serialize the long key into BinarySortable format into an output buffer accepted by * BytesBytesMultiHashMultiSet.  */
//  doDisplayFields(newerFields, newerClass); 
//  Check if there already exists a semijoin branch 
//  the row key column becomes a STRING 
//  Now, try it as the table owner and see if we get better luck. 
//  Bail out ungracefully - we should never hit   this here - but would have hit it in SemanticAnalyzer 
//  Already processed. See backtracking. 
//  PRIVILEGES 
//  Decrement batch size.  When this gets to 0, the batch will be executed 
//  Handle remaining middle long word digits. 
/*    * test LazyMap with bad entries, e.g., empty key or empty entries   * where '[' and  ']' don't exist, only for notation purpose,   * STX with value of 2 as entry separator, ETX with 3 as key/value separator   *  */
// TOK_SUBQUERY_EXPR should have either 2 or 3 children 
//  set memory threshold on memory used after GC 
// test with predicates such that partition pruning doesn't kick in 
// here means the last branch of the multi-insert is Cardinality Validation 
//  Test a set of random subtracts at high precision. 
//  we need to keep predicate kind e.g. EQUAL or NOT EQUAL   so that later while decorrelating LogicalCorrelate appropriate join predicate   is generated 
//  infer PK-FK relationship in single attribute join case 
//  initialize pathToTableInfo 
//  add the property only if it exists in table properties 
//  Ambiguous call: two methods with the same number of implicit 
//  1. We extract the information necessary to create the predicate for the 
//  if map tasks and reduce tasks are in finishable state then priority is given to the task   that has less number of pending tasks (shortest job) 
//  we don't push down any expressions that refer to aliases that can;t   be pushed down per getQualifiedAliases 
//  Serde may not have this optional annotation defined in which case be conservative   and say conversion is needed. 
//  all we can handle is LimitOperator, FilterOperator SelectOperator and final FS     for non-aggressive mode (minimal)   1. sampling is not allowed   2. for partitioned table, all filters should be targeted to partition column 
//  Backtrack SEL columns to pRS 
//  remove the biggest key 
//  Transient members initialized by transientInit method.   temporary location for building number string 
//  Free output columns if inputs have non-leaf expression trees. 
//  If it is a floor <date> operator, we need to rewrite it 
//  check null cols schemas for a partition 
//  Column the record identifier is in, -1 indicates no record id  unique within a transaction 
//  Invalid paths 
// load partition that doesn't exist in T 
//  Do the per-batch setup for an inner big-only join. 
//  All partitions should miss in target as it was marked virtually as dropped 
//  need to either move tmp files or remove them 
//  TODO Nothing else should be done for this task. Move on. 
//  Drop connection without calling close. HMS thread deleteContext 
//  Map key will be list of [typeInfo, isEscaped, escapeChar] 
//  Note: this could be made more generic; it may be a common problem for the endpoints that         can move around dynamically. For now we only handle this for the update. 
/*      * 2. convert from node.      */
//  v[5] -- since left integer #5 is always 0, some products here are not included. 
//  create hiveconf again to run initialization code, to see if value changes 
//  then we continue to use this perf logger 
//  process map joins with no reducers pattern 
//  if log4j configuration file found successfully, use HiveConf property value 
//  No conflicts. Module configuration is what will be used.   We've already verified that includes and excludes are not present at the same time for   individual modules. 
//  Should not happen, edit check is false. 
//  If we fail to remove, it's probably an internal error. We'd try to handle it the same way   as above - by restarting the session. We'd fail the caller to avoid exceeding parallelism. 
//  At this point we're dealing with all return types, except ScheduleResult.SCHEDULED. 
//  Ignore errors cleaning up miniMR 
//  so we don't create an extra SD in the metastore db that has no references. 
//  Entries should be in LRU order in the keyset iterator. 
//  check if a map-reduce job is needed to merge the files   If the current size is smaller than the target, merge 
//  Not much we can do about it here. 
// generate enough delta files so that Initiator can trigger auto compaction 
//  Append the separator if needed. 
//  Test regular inputformat 
//  LOG.info("Allocated " + allocCount + " of " + size + "; " + a.debugDump()); 
//  Now multiply by 2 and add 1 sign bit. 
/*    * STRUCT.    */
// should've done several heartbeats 
// fall through 
//  The output column projection of the vectorized row batch.  And, the type infos of the output 
//  The createTime will be set on the server side, so the comparison should skip it 
//  TXN_HIGH_WATER_MARK 
//  KEY_SEQ 
//  Specific test for HIVE-18744 --   Tests Timestamp assignment. 
//  Create only needed/included columns data columns. 
//  Recurse over all the source tables 
// if there is no prefix then we don't cut anything 
//  Look for databases but do not find any 
//  Set up codahale if enabled; we cannot tag the values so just prefix them for the JMX view. 
//  Cached buffer has the same (or lower) offset as the requested buffer. 
// each of these should fail 
//  ///////////////////////////// 
// now metastore connection should fail 
//  Use the default separators [0, 1, 2, 3, ..., 7] 
//  later... 
//  tablename is either TABLENAME or DBNAME.TABLENAME if db is given 
// OK, so now we have a lock 
/*    * A WindowFrame specifies the Range on which a Window Function should   * be applied for the 'current' row. Its is specified by a <i>start</i> and   * <i>end</i> Boundary.    */
/*  Convert an integer value representing a timestamp in nanoseconds to one   * that represents a timestamp in seconds (since the epoch).    */
//  Pairwise: ColumnHasNulls, ColumnIsRepeating 
//  Major compact to create a base that has ACID schema. 
//  the seed port 
//  blank " " (1 byte)   NEW TAI LUE LETTER LOW QA U+1981 (3 bytes) 
//  If the threshold is 100 percent, then there is no throttling 
//  It's not a column or a table alias. 
//  GenericUDF 
//  For some attempts, check inheritance. 
/*    * Verify table for Key: Long x Hash Table: HashMap    */
//  assert that the table created still has no hcat instrumentation 
//  TODO: call setRemoteUser in ExecuteStatementOperation or higher. 
//  the data that we need for this RG. 
//  A vectorized expression that selects no rows. 
//  check that the agg is on the entire input 
/*  We want a,b,x to come from a finite field of size 0 to k, where k is a prime number.   * 2^p - 1 is prime for p = 31. Hence bitvectorSize has to be 31. Pick k to be 2^p -1.   * If a,b,x didn't come from a finite field ax1 + b mod k and ax2 + b mod k will not be pair wise   * independent. As a consequence, the hash values will not distribute uniformly from 0 to 2^p-1   * thus introducing errors in the estimates.    */
//  No-op -- this is needed to be able to instantiate the   class from the name. 
//  SCHEMA_NAME 
//  the corresponding ReduceSinkOperator. 
//  Find the highest failure count 
//  Always use index 0 so the write methods don't write a separator. 
// look at top 3 bits and return appropriate enum 
//  Validate resultset columns 
//  Init fails, but the session is also killed by WM before that. 
//  If all of the agg expressions are distinct and have the same 
//  The path to the tracking root 
//  set the escape 
//  get the table names out 
//  Look for it under the old Hive name 
//  Left pad longer strings with multi-byte characters. 
//  find this parentColName in its parent's rs 
// All the vectors have the same length, 
//  before making changes or copy-pasting these. 
//  second time will complete silently 
//  Whether there are more than 0 rows. 
//  the partition doesn't qualify the global limit optimization for some reason 
//  fail early if the columns specified for column statistics are not valid 
//  Normalize the case for source of replication parameter 
//  later. 
/*        * Clip off one byte and expect to get an EOFException on the write field.        */
//  test second IF argument repeating 
//  End sync stuff. 
// get the new nextKVReader with lowest key 
//  From this point on, session creation will wait for the default pool (if # of sessions > 0). 
//  If sort does not contain a limit operation or limit is 0, we bail out 
//  Test altering the table 
//  Second branch should only have the MV 
//  total characters = 3; byte length = 9 
//  This is for transactional tables. 
//  Drop the partitions and get a list of locations which need to be deleted 
//  while we are in the process of setting it to valid. 
//  issue a command with bad options 
//  Given the data in a partition, evaluate the result for the next row for   streaming and batch mode 
//  no interpolation needed because lower position and higher position has the same key 
/*  comment for reviewers -> updateTab2Cols needed to be separate from tab2cols because if I    pass tab2cols to getHivePrivObjects for the output case it will trip up insert/selects,    since the insert will get passed the columns from the select.      */
//  response is true 
//  This is the vectorized row batch description of the output of the native vectorized PTF   operator.  It is based on the incoming vectorization context.  Its projection may include 
//  Nothing updated yet. 
//  clear the mask for array reuse (this is to avoid masks array allocation in inner loop) 
//  ptf node form is:   ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)   guaranteed to have an alias here: check done in processJoin 
//  that if we do something we didn't expect to do, it'd be more likely to fail. 
//  run the operator pipeline 
//  optional bytes work_spec_signature = 2; 
//  And no metadata gets created. 
//  all keys + VCol + c 
/*      * todo: Longer term we should pass this from client somehow - this would be an optimization;  once     * that is in place make sure to build and test "writeSet" below using OperationType not LockType     * With Static Partitions we assume that the query modifies exactly the partitions it locked.  (not entirely     * realistic since Update/Delete may have some predicate that filters out all records out of     * some partition(s), but plausible).  For DP, we acquire locks very wide (all known partitions),     * but for most queries only a fraction will actually be updated.  #addDynamicPartitions() tells     * us exactly which ones were written to.  Thus using this trick to kill a query early for     * DP queries may be too restrictive.      */
//  do not put the TAB for the last column 
//  {Small Value Bytes} 
//  between has 4 args here, but can be vectorized like this 
//  Filters are using an index which should match 2 rows 
//  i.e. this column is not appearing in keyExprs of the RS 
//  Flush the last record when reader is out of records 
//  rename based on output schema of join operator 
//  only dealing with special join types here. 
//  The AccumuloOutputFormat will look for it there. 
//  in case of dynamic partitioning lock the table 
//  Update max executors now that cluster info is definitely available. 
//  Current replication state must be set on the Partition object only for bootstrap dump.   Event replication State will be null in case of bootstrap dump. 
//  propagate nulls 
//  Adjust the compression block position. 
//  jobClose needs to execute successfully otherwise fail task 
//  No stats to delete, forgivable error. 
//  Step 1 : Create a temp table object 
//  Execute optimization 
/*    * Tests single threaded implementation of checkMetastore    */
// rename the data directory 
// owner like ".*Owner.*" and owner like "test.*" 
//  if there are union all operators, it means that the walking context contains union all operators.   please see more details of context.currentUnionOperator in GenTezWorkWalker 
//  base  = JAVA64_OBJECT + PRIMITIVES1 * 4 + JAVA64_FIELDREF * 3 + JAVA64_ARRAY;   entry = JAVA64_OBJECT + JAVA64_FIELDREF + PRIMITIVES1 
//  For now, we don't go higher than the default batch size unless we do more work   to verify every vectorized operator downstream can handle a larger batch size. 
//  Rewriting cannot be performed 
//  this matches the list structure that Hive writes 
/*       This sets up dependencies such that a child task is dependant on the parent to be complete.    */
//  hashMap += JAVA32_FIELDREF + PRIMITIVES1   hashMap.entry += JAVA32_FIELDREF * 2 
//  we are in secure mode. Login using keytab 
//  Join key exprs are represented in terms of the original table columns, 
//  concatenate. Keeping the old logic for non-MM tables with temp directories and stuff. 
/*    * Infer Uniquenes if: - rowCount(col) = ndv(col) - TBD for numerics: max(col)   * - min(col) = rowCount(col)   *    * Why are we intercepting Project and not TableScan? Because if we   * have a method for TableScan, it will not know which columns to check for.   * Inferring Uniqueness for all columns is very expensive right now. The flip   * side of doing this is, it only works post Field Trimming.    */
//  if there is nothing to project or if we are projecting everything   then no need to introduce another RelNode 
//  Make sure it has a chance to dump it. 
/*  Minimum value seen so far  */
// vectorized because there is INPUT__FILE__NAME 
//  At this point, tablePath is part of HDFS and it is encrypted 
/*  100 files x 1000 size for 1 splits  */
//  total characters = 3; byte length = 5 
//  for any exception in conversion to decimal, produce NULL 
//  LocalJobRunner does not work with mapreduce OutputCommitter. So need 
//  Accumulo instance name with ZK quorum 
//  Pad with empty rows if the number of values in group is less than TOP num 
//  Default, all columns that are not metrics or timestamp, are treated as dimensions 
//  Calculate the length of the UTF-8 strings in input vector and place results in output vector. 
//  daemon 
//  1. analyze and process the position alias   step processPositionAlias out of genResolvedParseTree 
//  TXN_ID 
//  Check common conditions for both Optimized and Fast Hash Tables. 
//  If MM wants to create a new base for IOW (instead of delta dir), it should specify it here 
//  Clear all in memory partitions first 
//  check if a predicate is needed   predicate is needed if either input pruning is not enough   or if input pruning is not possible 
//  Now checkFailedCompactions() will return true 
//  Exhausted reading all records, close the reader. 
//  pass the message to the user - essentially something about   the table   information passed to HCatOutputFormat was not right 
//  The delegation token is not applicable in the given deployment mode   such as HMS is not kerberos secured 
//  3. attach this SEL to the operator right before FS 
//  Create backtrack SelectOp 
//  We want to send the heartbeat at an interval that is less than the timeout. 
//  for negative tests, which is succeeded.. no need to print the query string 
//  Virtual columns start after the last partition column. 
//  Get the id the Spark job that was launched, returns -1 if no Spark job was launched 
//  Do this check in case the decrypted plaintext actually makes sense in some way. 
//  not map reduce task or not conditional task, just skip 
//  Go by position, not field name, as field names aren't guaranteed.  The order of fields   in RecordIdentifier is writeId, bucketId, rowId 
//  check if column is defined or not 
//  transition to Success state 
//  MY_STRINGSET 
//  check filter condition type First extract the correlation out   of the filter 
//  Test that existing shared_write db with new shared_read coalesces to 
//  PAYLOAD 
//  create the index table if it does not exist 
//  Netty defaults to no of processors * 2. Can be changed via -Dio.netty.eventLoopThreads 
//  We cannot just deallocate the buffer, as it can hypothetically have users. 
//  Just give each table the same amount of memory. 
/*  Remove the lock specified  */
//  set some values to use for getting conf. vars 
//  There should be no blocking operation in RecordProcessor creation,   otherwise the abort operation will not register since they are synchronized on the same 
//  empty list 
//  partitions archived before introducing multiple archiving 
//  if HIVE_TXN_TIMEOUT is defined, heartbeat interval will be HIVE_TXN_TIMEOUT/2 
//  List to maintain the incremental dumps for each operation 
//  If the app state is running, get additional information from YARN Service 
//  use lowercase table name as prefix here, as StatsTask get table name from metastore to fetch counter. 
//  Nothing got modified 
//  set the root of the temporary path where dynamic partition columns will populate 
//  -p port 
//  We can safely remove the condition by replacing it with "true" 
//  We will increase the size of the array on demand 
/*  @bgen(jjtree) Namespace  */
//  If any of the table requests are null, then I need to pull all the 
//  Test string 
//  Friday 30th August 1985 02:47:00 AM 
//  serialize dense/sparse registers. Dense registers are bitpacked whereas 
// short 
//  mergeIsDirectFlag, need to merge isDirect flag even newInput does not have parent 
//  A reference to the current row. 
//  Adding mssql jdbc driver if exists 
/*  100 files x 100 size for 111 splits  */
//  Adjust all longs using power 10 division/remainder. 
/*  * An multi-key hash multi-set optimized for vector map join. * * The key is stored as the provided bytes (uninterpreted).  */
//  it out. 
//  Buffer needed to bridge. 
//  This handles the common logic for destroy and return - everything except   the invalid combination of destroy and return themselves, as well as the actual   statement that destroys or returns it. 
//  See if someone else evicted this in parallel. 
//  1 1   NULL 0 
// Verify vectorized expression 
//  Should the cache size be updated here, or after the result data has actually been deleted? 
//  Test that exclusive lock blocks shared reads 
//  Bug - the field has to be in ms, not sec. Override only if set precisely to sec. 
//  Now if there are more than 1 sources then we have a join case 
//  Have to use this if-else since switch-case on String is supported Java 7 onwards 
//  For static partitions, values would be obtained from partition(key=value...) clause. 
//  ptf node form is: ^(TOK_PTBLFUNCTION $name $alias?   partitionTableFunctionSource partitioningSpec? expression*)   guranteed to have an lias here: check done in processJoin 
//  GCE settings 
//  clear out the mapjoin set. we don't need it anymore. 
//  located at the same position as the input newProject. 
//  Primarily for debugging purposes a.t.m, since there's some unexplained TASK_TIMEOUTS which are currently being observed. 
//  To make it more explicit below that processHooks needs to be called last. 
//  join positions for even index, filter lengths for odd index 
//  Fourth is lower priority as a result of canFinish being set to false. 
//  c1-c10 
//  1.1. Fix up the query for insert/ctas/materialized views 
//  Don't use the userName member, as it may or may not have been set.  Get the value from   conf, which calls into getUGI to figure out who the process is running as. 
//  Use GetTokenResponseProto.newBuilder() to construct. 
/*      * validate and setup resultExprStr      */
//  No change. 
//  date str 
//  Heartbeat indicates task has a duck - this must be reverted. 
//  If we have failed before building newCacheData, deallocate other the allocated. 
//  Test that two different partitions don't collide on their locks 
//  Reset the previously stored rootNode string 
/*  noscan uses hdfs apis to retrieve such information from Namenode.       */
//  Create inline SQL operator 
//  Number of columns in the aliases does not match with number of columns   generated by the lateral view 
//  add column expression for bloom filter 
/*    * This hashtable stores "references" in an array of longs;  index in the array is hash of   * the key; these references point into infinite byte buffer (see below). This buffer contains   * records written one after another. There are several simple record formats.   * - single record for the key   *    [key bytes][value bytes][vlong value length][vlong key length][padding]   *    We leave padding to ensure we have at least 5 bytes after key and value.   * - first of multiple records for the key (updated from "single value for the key")   *    [key bytes][value bytes][5-byte long offset to a list start record]   *  - list start record   *    [vlong value length][vlong key length][5-byte long offset to the 2nd list record]   *    Lengths are preserved from the first record. Offset is discussed above.   *  - subsequent values in the list   *    [value bytes][value length][vlong relative offset to next record].   *   * In summary, because we have separate list record, we have very little list overhead for   * the typical case of primary key join, where there's no list for any key; large lists also   * don't have a lot of relative overhead (also see the todo below).   *   * So the record looks as follows for one value per key (hash is fixed, 4 bytes, and is   * stored to expand w/o rehashing, and to more efficiently deal with collision   *   *             i = key hash   *           ._______.   * REFS: ... |offset | ....   *           `--\----'   *               `-------------.   *                            \|/   *          .______._____._____'__.__._.   * WBS: ... | hash | key | val |vl|kl| | ....   *          `------'-----'-----'--'--'-'   *   * After that refs don't change so they are not pictured.   * When we add the 2nd value, we rewrite lengths with relative offset to the list start record.   * That way, the first record points to the "list record".   *                         ref .---------.   *                         \|/ |        \|/   *       .______._____._____'__|___.     '__.__.______.   * WBS:  | hash | key | val |offset| ... |vl|kl|      |   *       `------'-----'-----'------'     '--'--'------'   * After that refs don't change so they are not pictured. List record points to the 2nd value.   *                         ref .---------.        .---------------.   *                         \|/ |        \|/       |              \|/   *       .______._____._____'__|___.     '__.__.__|___.     ._____'__._.   * WBS:  | hash | key | val |offset| ... |vl|kl|offset| ... | val |vl|0|   *       `------'-----'-----'------'     '--'--'------'     '-----'--'-'   * If we add another value, we overwrite the list record.   * We don't need to overwrite any vlongs and suffer because of that.   *                         ref .---------.         .-------------------------------.   *                         \|/ |        \|/        |                              \|/   *       .______._____._____'__|___.     '__.__.___|__.     ._____.__._.     ._____'__.______.   * WBS:  | hash | key | val |offset| ... |vl|kl|offset| ... | val |vl|0| ... | val |vl|offset|   *       `------'-----'-----'------'     '--'--'------'     '-----:--'-'     '-----'--'--|---'   *                                                               /|\                     |   *                                                                `----------------------'   * And another value (for example)   * ... ---.         .-----------------------------------------------------.   *       \|/        |                                                    \|/   *        '__.__.___|__.     ._____.__._.     ._____.__.______.     ._____'__.______.   * ...    |vl|kl|offset| ... | val |vl|0| ... | val |vl|offset| ... | val |vl|offset|   *        '--'--'------'     '-----:--'-'     '-----'--:--|---'     '-----'--'--|---'   *                                /|\                 /|\ |                     |   *                                 `-------------------+--'                     |   *                                                     `------------------------'    */
//  This loop fills up the selected[] vector with all the index positions that are selected. 
//  Indicate that the query will use a cached result. 
//  HIVE-16985: try to find the fake merge work for SMB join, that is really another MapWork. 
/*    * UNION.    */
//  shouldl return nulls at end 
//  Note : Not testing table rename because table rename replication is not supported for table-level repl. 
//  Use a separate method to make it easier to create a SaslHandler without having to 
//  Aggregation buffer definition and manipulation methods 
//  verify number of digits is <= 38 and each number has 1 or more digits 
//  current dest has no distinct keys. 
//  for tez. used to remember which position maps to which logical input 
//  Gather RS operators that 1) belong to root works, i.e., works containing TS operators,   and 2) share the same input operator.   These will be the first target for extended shared work optimization 
//  Include column names from SerDe, the partition and virtual columns. 
//  we are checking to the desired action. 
// verify the serialized format for dtype 
//  A MuxDesc is only generated from a corresponding ReduceSinkDesc. 
//  Cancel again if current thread also interrupted 
//  Set the parameters 
//  if CREATE or DROP priv requirement is there, the owner should have WRITE permission on 
//  The special case zero logic at the beginning should have caught this. 
//  inverse of partSpecToFileMapping, populated at runtime 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#isReadOnly()    */
//  Update LRU 
//  UNION_ENTRY 
// this is helpful when Sqoop is installed on each node in the cluster to make sure  relevant jars (JDBC in particular) are present on the node running the command 
//  Shared between threads (including SessionState!) 
//  used for rehashing to get last set of values   " "   current array size   have minimum 40% fill factor 
//  There should be 1 delta dir, plus a base dir in the location   steve 
//  no privileges to filter 
//  into ORIGINAL_VERSION 
//  check if hiveserver2 config gets loaded when HS2 is started 
/*  * An single byte array value hash map optimized for vector map join.  */
//  In normal case, we evict the items from the list. 
//  Special cases for Avro. As with ORC, we make table properties that   Avro is interested in available in jobconf at runtime 
//  E.SR: Lock we are examining is shared read 
//  Write the output to temporary directory and move it to the final location at the end 
//  There is hint but none of the operators removed. Throw error 
//  If Non-Acid case, then all files would be in the base data path. So, just return it. 
//  Sort the queue - we may have put items here out of order. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getOperationStatus(org.apache.hive.service.cli.OperationHandle)    */
//  work can still be null if there is no merge work for this input 
//  This key evaluator translates from the vectorized VectorHashKeyWrapper format 
//  Using parseInto() avoids throwing exception when parsing, 
//  Iterate thru the file cache. This is best-effort. 
//  Get column names and sort order 
//  Skip the 0-th column, since it won't have a vector after reading the text source. 
//  add x to it 
// ************************************************************************************************   Decimal to Integer conversion. 
//    Getters   
//  Nothing to do if it is not a MapReduce task. 
//  Reduce sink is needed if the query contains a cluster by, distribute by, 
//  These DS are used to cache previously created String 
//  optional .EntityDescriptorProto processor_descriptor = 9; 
//               cost of transferring map outputs to Join operator 
//  In the case of tablesample, the input paths are pointing to files rather than directories.   We need to get the parent directory as the filtering path so that all files in the same   parent directory will be grouped into one pool but not files from different parent   directories. This guarantees that a split will combine all files in the same partition   but won't cross multiple partitions if the user has asked so.   path is not directory 
//  Skip "standard_conforming_strings" command which is read-only in older   Postgres versions like 8.1   See: http://www.postgresql.org/docs/8.2/static/release-8-1.html 
/*  write string itself  */
//  template, <ClassNamePrefix>, <ReturnType>, <OperandType>, <FuncName>, <OperandCast>, 
//  Now for each path that is for the given versionNumber, delete the znode from ZooKeeper 
//  For caching Database objects. Key is database name 
//  command should be redacted to avoid to logging sensitive data 
//  replace all ReduceSinkOperators which are not at the bottom of 
//  tested 
//  user has told us to run in local mode or doesn't want auto-local mode 
//  Set the new value to the output string variable 
//  getAllUrls will parse zkJdbcUrl and will plugin the active HS2's host:port 
//  Now test tblproperties specified on ALTER TABLE .. COMPACT .. statement 
//  must be TezWork 
//  look for matches in vertex specific counters 
//  stage 2 
//  need a SEMI-SHARED. 
//  create a dummy MapReduce task 
//  Single-Column String specific imports. 
//  Note that regular SerDe doesn't tolerate fewer columns. 
//  1.3 Build row type from field <type, name> 
//  Max number of nodes when converting to CNF 
//  check entries beyond first one 
//  Need to override this one too or dropTable breaks because it doesn't find the table when checks 
//  HiveDecimal -> Number -> Double 
//  Note: both full-ACID and insert-only sinks. 
//  trim off the ending ",", if any 
//  LOG.info("got "+t); 
//  Test testing that the filters introduced by EventUtils are working correctly. 
//  stage 1 
//  the start/stop row conditions (HBASE-1829). 
//  Using a previously cached result. 
//  Size of bigtable 
//  Calculate TypeInfo 
//  If so, we need not perform this optimization and we should bail out. 
//  For queries using script, the optimization cannot be applied without user's confirmation 
//  the materialization was created. Otherwise, query returns 0 rows. 
//  Output from the script 
//  convert the mapjoin to a bucketized mapjoin 
//  Check if session files are removed 
//  check that hook to disable transforms has been added 
//  2.5 and later way of finding sasl property 
//  Pass job to initialize metastore conf overrides for embedded metastore case   (hive.metastore.uris = ""). 
//  If table is null on either of these, then they are claiming to   lock the whole database and we need to check it.  Otherwise, 
//  Ignore all the other events; logged above. 
//  By default, TezSessionPoolManager handles this for both pool and non-pool session. 
//  Add the Hadoop token to the JobConf 
//  Overlord and coordinator both run in same JVM. 
//  Also dependent on the UDFExampleAdd class within that JAR. 
//  If child is not a RexCall instance, we can bail out 
//  XXX: this could easily become a hot-spot 
//  Non-empty java opts with -Xmx specified in MB 
//  The following code should be gone after HIVE-11075 using topological order 
//  set load server conf booleans to false 
//  These represent the sorted columns 
//  in presence of grouping sets we can't remove sq_count_check 
//  Expr is built by DDLSA, it should only contain part cols and simple ops 
//  Now, try to find the file based on SHA and name. Currently we require   exact name match.   We could also allow cutting off versions and other stuff provided that 
// process hosts from which doAs requests are authorized 
//  for now 
//  multi-threaded file statuses and split strategy 
//  This should now go fine, since we increased the configured header size 
/*  * An bytes key hash map optimized for vector map join. * * This is the abstract base for the multi-key and string bytes key hash map implementations.  */
//  Only select operators among the allowed operators can cause changes in the   column names 
//  In future, this may examine WriteEntity and/or config to return   appropriate HCatWriter 
//  get the next byte 
//  Is there integer room above? 
//  load multiple random sets of Long values 
//  2. Build RelOptAbstractTable 
//  MockResultSet 
//  Note that while this is an improvement over static initialization, it is still not,   technically, valid, cause nothing prevents us from connecting to several metastores in 
//  Add this to the list of top operators - we always start from a table 
//  Runtime.getRuntime().exec(wrappedCmdArgs); 
//  verify that drops were replicated. This can either be from tables or ptns   not existing, and thus, throwing a NoSuchObjectException, or returning nulls   or select * returning empty, depending on what we're testing. 
//  map-side aggregation should reduce the entries by at-least half 
//  If this partition's set of columns is the same as the parent table's,   use the parent table's, so we do not create a duplicate column descriptor,   thereby saving space 
//  order of forwarded ips per X-Forwarded-For http spec (client, proxy1, proxy2) 
//  Special cases for ORC   We need to check table properties to see if a couple of parameters,   such as compression parameters are defined. If they are, then we copy   them to job properties, so that it will be available in jobconf at runtime   See HIVE-5504 for details 
//  Construct using org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto.newBuilder() 
//  Although an instance of ObjectStore is accessed by one thread, there may   be many threads with ObjectStore instances. So the static variables   pmf and prop need to be protected with locks. 
//  Set insiderView so that we can skip the column authorization for this. 
//  Check the location of the result partition. It should be located in the destination table 
//  if no entries were added via conf, we initiate our defaults 
//  SQL standard - return null for zero or one elements 
/*    * This method is invoked for unqualified column references in join conditions.   * This is passed in the Alias to Operator mapping in the QueryBlock so far.   * We try to resolve the unqualified column against each of the Operator Row Resolvers.   * - if the column is present in only one RowResolver, we treat this as a reference to   *   that Operator.   * - if the column resolves with more than one RowResolver, we treat it as an Ambiguous   *   reference.   * - if the column doesn't resolve with any RowResolver, we treat this as an Invalid   *   reference.    */
//  Verify no more invocations in case of success. 
//  The total size of local tables may not be under   the limit after we merge mapJoinLocalWork and childLocalWork.   Do not merge. 
// This means that the derived colAlias collides with existing ones. 
//  Add the left hand side of the IN clause which contains the struct definition. 
//  This can be relaxed in the future if there is a requirement. 
//  Return true if this is a custom UDF or custom GenericUDF. 
//  scalar/column IF 
//  1. Trigger transformation 
//  Then, non-finishable must always precede finishable. 
/*  Required to build against 0.23 Reporter and StatusReporter.  */
//  3. Insert another row to newly-converted ACID table 
/*    * Lookup an long in the hash map.   *   * @param key   *         The long key.   * @param hashMapResult   *         The object to receive small table value(s) information on a MATCH.   *         Or, for SPILL, it has information on where to spill the big table row.   *   * @return   *         Whether the lookup was a match, no match, or spilled (the partition with the key   *         is currently spilled).    */
//  Lets see if this constant was folded because of optimization. 
/*    * now a Column can have an alternate mapping.   * This captures the alternate mapping.   * The primary(first) mapping is still only held in   * invRslvMap.    */
//  If the input table is bucketed, choose the first bucket 
//  Create an empty database to load 
//  Validation has limited evaluatorInputExprNodeDescLists to size 1. 
//  t1-> 1 entry and t2-> 2 entries (1 per partition) 
//  Since skew join optimization makes a copy of the tree above joins, and   there is no multi-query optimization in place, let us not use skew join   optimizations for now. 
// For now assume no partition may have > 10M files.  Perhaps better to count them. 
/*    * Verify table for Key: byte[] x Hash Table: HashSet    */
/*  Handle default case for isRepeating setting for output. This will be set to true     * later in the special cases where that is necessary.      */
//  Used by serialization only 
//  1. Create RS and backtrack Select operator on top 
//  Explicit pool specification - invalid - there's no mapping that matches. 
//  we convert the databaseNameOrPattern to lower case because events will have these names in lower case. 
//  Part of lowest word survives. 
//  get the key and value 
//  test when second argument has nulls and repeats 
//  Otherwise, for each child run this method recursively 
//  drop table and check that trash works 
//  Ordered columns are the output columns. 
//  So far we have all the data from the beginning of the part. 
/*  Not all messages are parametrized even those that should have been, e.g {@link #INVALID_TABLE}.     INVALID_TABLE is usually used with {@link #getMsg(String)}.     This method can also be used with INVALID_TABLE and the like and will match getMsg(String) behavior.     Another example: {@link #INVALID_PARTITION}.  Ideally you want the message to have 2 parameters one for     partition name one for table name.  Since this is already defined w/o any parameters, one can still call     {@code INVALID_PARTITION.format("<partName> <table Name>"}.  This way the message text will be slightly     different but at least the errorCode will match.  Note this, should not be abused by adding anything other     than what should have been parameter names to keep msg text standardized.      */
//  There should be no residual since we already negotiated that earlier in   HBaseStorageHandler.decomposePredicate. However, with hive.optimize.index.filter   OpProcFactory#pushFilterToStorageHandler pushes the original filter back down again.   Since pushed-down filters are not omitted at the higher levels (and thus the   contract of negotiation is ignored anyway), just ignore the residuals.   Re-assess this when negotiation is honored and the duplicate evaluation is removed.   THIS IGNORES RESIDUAL PARSING FROM HBaseStorageHandler#decomposePredicate 
//  This is the Job Tracker URL 
/*      * Finally write out the pieces (sign, power, digits)      */
//  captured by WritableComparableHiveObject.hashCode() function. 
//  Parse the rewritten query string 
/*  (non-Javadoc)   * This provides a LazyBoolean like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  only one bucket file 
// important to remove after unlock() in case it fails 
//  There's no buffer and another move is reserving this. 
//  Skip the 0th column that is the root structure. 
//  Find out number of partitions for each small table (should be same across tables) 
//  double IN 
//  3) Add the expressions that correspond to the aggregation 
//  binary join   n-way join, first (biggest) small table   We unconditionally create a hashmap for the first hash partition 
/*    * Sets the job state to FAILED. Returns true if FAILED status is set.   * Otherwise, it returns false.    */
//  finally add a project to project out the 1st column 
//  Column stats in hiveColStats might not be in the same order as the columns in   nonPartColNamesThatRqrStats. reorder hiveColStats so we can build hiveColStatsMap   using nonPartColIndxsThatRqrStats as below 
//  required string fragment_id = 1; 
//  Make sure the partitions directory is not in hdfs. 
//  Primary entry point is a factory method instead of ctor 
//  Lock states 
//  causing it to not sort the entire table due to not knowing how selective the filter is. 
//  If there's no pending fragments, queue some of the cleanup for a later point - locks, log rolling. 
//  deep copy a new mapred work 
//  shared_read 
//  Add another db via ObjectStore 
//  tinyint 
//  Now try with hive/_HOST principal 
//  11 primitive1 fields, 2 refs above with alignment 
//  projRel 
//  If something happened and we were not able to rename the temp file, attempt to remove it 
//  Try to fold if it is a constant expression 
/*  one second before and after  */
//  Strings test 
//  add to list of running jobs to kill in case of abnormal shutdown 
//  Indicates if this instance of beeline is running in compatibility mode, or beeline mode 
// dirs 1/, 2/, 3/ 
//  Parse numrecords to an integer 
//  A type timestamp (TimestampColumnVector) minus a type timestamp produces a 
//  close the reader for this entry 
//  if above returned a null, then the db does not exist - probably a   "drop database if exists" clause - don't try to authorize then. 
//  2. Get RexNodes for original Projections from below 
//  Dropping by expressions. 
//  We do not call endGroup on operators below because we are batching rows in   an output batch and the semantics will not work.   super.endGroup(); 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#releaseSavepoint(java.sql.Savepoint)    */
//  column pruner 
/*  system defaults (usually 3-replica disk)  */
//  if the username is not already available in the URL add the one provided 
//  Throw away lowest word. 
//  The accessed columns of query 
//  try the repeating case 
//  Save the original selected vector 
//  Assume including everything means the VRB will have everything.   TODO: this is rather brittle, esp. in view of schema evolution (in abstract, not as          currently implemented in Hive). The compile should supply the columns it expects         to see, which is not "all, of any schema". Is VRB row CVs the right mechanism         for that? Who knows. Perhaps resolve in schema evolution v2. 
//  QUERY_ID 
//  Make sure we can actually get a session still - parallelism/etc. should not be affected. 
//  test LastColumnTakesRest 
/*      * Generate a floating-point number that represents the exponent.     * Do this by processing the exponent one bit at a time to combine     * many powers of 2 of 10. Then combine the exponent with the     * fraction.      */
//  The (pool-sized) list is being fully drained. 
//  remove the pwd from conf file so that job tracker doesn't show this 
//  We can't eliminate stripes if there are deltas because the   deltas may change the rows making them match the predicate. todo: See HIVE-14516. 
//  Validate the first parameter, which is the expression to compute over. This should be an   array of strings type, or an array of arrays of strings. 
//  Sort the files 
//  Patch the optimized query back into original FROM clause. 
//  uses default TZ 
//  to store the partitions that are currently being processed 
//  Create, initialize, and test the SerDe 
//  \N or -1 (allow latter) 
//  Map of dbName.TblName -> TSOperator 
//  Union (extra fields) 
/*    * Increased visibility of this method is only for providing better test coverage    */
//  Check that the directory is created 
//  for the UDTF operator 
//  create formatter that includes all of the input patterns 
/*  10 files x 1000 size for 11 splits  */
//  additional bits to pad long array to block size 
//  setup list of conf vars that are not allowed to change runtime 
//  We can continue 
//  is actually available for execution and will not potentially result in a RejectedExecution 
//  for use as wildcard pattern to test LIKE 
//  If there is a materialized view update desc, we create introduce it at the end   of the tree. 
//  add user metadata to footer in case of any 
//  the path used above should not be used on a second try as each dump request is written to a unique location.   however if we want to keep the dump location clean we might want to delete the paths 
//  Larger allocations will be special-cased and will not use the normal buffer.   buffer/nextFree will be set to a newly allocated array just for the current row. 
//  used in the join(as field access). 
//  transform the operator tree 
//  For PARTIAL1 and COMPLETE: ObjectInspectors for original data 
//  difference from standard File Appender:   locking is not supported and buffering cannot be switched off 
//  Check and transform group by *. This will only happen for select distinct *.   Here the "genSelectPlan" is being leveraged.   The main benefits are (1) remove virtual columns that should   not be included in the group by; (2) add the fully qualified column names to unParseTranslator   so that view is supported. The drawback is that an additional SEL op is added. If it is   not necessary, it will be removed by NonBlockingOpDeDupProc Optimizer because it will match 
//  Remove all the entries from the parameters which are added by repl tasks internally. 
//  Nothing to process. 
//  The heap over the keys, storing indexes in the array. 
// do this before next update so that delte_delta is properly sorted 
//  Set of GenericUDFs which require need implicit type casting of decimal parameters.   Vectorization for mathmatical functions currently depends on decimal params automatically   being converted to the return type (see getImplicitCastExpression()), which is not correct 
// start a 2nd (overlapping) txn 
//  child 3 is the optional constraint 
//  found no files under a sub-directory under table base path; it is possible that the table   is empty and hence there are no partition sub-directories created under base path 
//  check all elements are contained in g 
//  same, but repeating input is not null 
/*        * The Vectorized Input File Format reader is responsible for setting the partition column       * values, resetting and filling in the batch, etc.        */
// if here then we must be compacting 
//  Insert the records in DB to simulate a hive table 
//  Update the memory monitor info for LLAP. 
// looking for map 100% reduce 100% 
//  ascii string 
/*      * 4. give Evaluator chance to setup for Output execution; setup Output shape.      */
//  frac >= tezMaxReserveFraction 
/* schema  */
//  2^56 * 2^56 - 1 
/*    * Attempts to make a connection using default HS2 connection config file if available   * if there connection is not made return false   *    */
//  Tracks running fragments, and completing fragments.   Completing since we have a race in the AM being notified and the task actually 
//  Pass false for canRetainByteRef since we will NOT be keeping byte references to the input   bytes with the BytesColumnVector.setRef method. 
//  User specified perms in invalid format. 
//  configurations are always published to ServiceRecord. Read/apply configs to JDBC connection params 
//  we always remove the condition by replacing it with "true" 
//  couldn't find proper parent column expr 
/*    * Returns the number of children on the stack in the current node scope.    */
//  since local mode only runs with 1 reducers - make sure that the 
//  introduce RS and EX before FS. If the operator tree already contains 
/*    * SHORT.    */
// create RexSubQuery node 
/*    * NOTE: The VectorGroupByDesc has already been allocated and will be updated here.    */
//  execute the driver locally? 
// r = 31 * r + hashCode(listOI.getListElement(o, ii), elemOI); 
//  override 
// don't combine if inputformat is a SymlinkTextInputFormat 
//  "Tez Merge File Work" will become "Tez Merge File.." 
//  CHAIN pattern 
//  Make sure big table BytesColumnVectors have room for string values in the overflow batch... 
//  This is necessary as sometimes semantic analyzer's mapping is different than operator's own alias. 
//        for TOK_JOIN and TOK_FULLOUTERJOIN. 
//  keep record all the input path for this alias 
//  session vars 
//  Whether any of the node outputs is unknown   Whether all of the node outputs are divided 
//  Member variables 
//  start exclusive to infinity inclusive 
// traverse the given node to find all correlated variables   Note that correlated variables are supported in Filter only i.e. Where & Having 
//  Initialize the first value in the delete reader. 
// the first "split" is for base/ 
//  set the regular provided qualifier names 
//  This file is unknown to metastore. 
//  create map join task and set big table as i 
//  filter out the deleted records 
//  Infer column stats state 
//  Look for functions with empty pattern 
//  whether any ACID table or Insert-only (mm) table is involved in a query 
//  Lifted from org.apache.hadoop.util.hash.MurmurHash... but supports offset. 
//  See Marker class comment. 
//  We for these input with exponents, we have at this point an intermediate decimal,   an exponent power, and a result:                         intermediate     input               decimal      exponent        result   701E+1            701 scale 0        +1            7010 scale 0   3E+4              3 scale 0          +4               3 scale 0   3.223E+9          3.223 scale 3      +9      3223000000 scale 0   0.009E+10         0.009 scale 4      +10       90000000 scale 0   0.3221E-2         0.3221 scale 4     -2               0.003221 scale 6   0.00223E-20       0.00223 scale 5    -20              0.0000000000000000000000223 scale 25   
//  Sum all non-null decimal column values; maintain isGroupResultNull. 
//  optional string aString = 2; 
//  any of condition contains non-NS, non-NS 
/*    * close will move the temp files into the right place for the fetch   * task. If the job has failed it will clean up the files.    */
//  Set other configurationOverlay parameters 
/*      * We build up the Value Reference Word we will return that will be kept by the caller.      */
//  reset defaults 
//  If any of the children contains null, then return a null 
//  NOTE: Currently, read variations only apply to top level data types... 
//  INVALID_WRITE_IDS 
/*  first_name <> 'sue'  */
//  be able to acquire the lock and finish successfully 
//  Add shutdown hook. 
//  2^32 -- needs 2 32 bit words 
//  table is sampled. In some situation, we really can leverage row   limit. In order to be safe, we do not use it now. 
//  this transformation needs to be first because it changes the work item itself. 
/*      * This batch is used by vector/row deserializer readers.      */
// Futureproofing: the parser will actually not allow this 
// cleanup just in case something is left over from previous run 
//  USER_DEFINED_TYPE_ENTRY 
// ========================== 40000 range starts here ========================// 
//  In case this has not been initialized elsewhere. 
//  test for double type 
//  the aliases that are allowed to map to a null scan. 
//  Only this table has spilled big table rows 
// <<<<<<< HEAD 
// following matcher.group() would fail anyway and we don't want to skip files since that  may be a data loss scenario 
// run Worker to execute compaction 
//  We may not have an active resource plan in the start. 
//  Restore the settings 
/*      * Now suck up the digits in the mantissa.  Use two integers to     * collect 9 digits each (this is faster than using floating-point).     * If the mantissa has more than 18 digits, ignore the extras, since     * they can't affect the value anyway.      */
//  America/Los_Angeles DST dates - 2015-03-08 02:00:00/2015-11-01 02:00:00 
//  corresponding to the input file is stored to name the output bucket file appropriately. 
//  No rows? 
//  serialize some data in the schema after it is altered. 
//  in theory, the below call isn't needed in non thrift_mode, but let's not 
//  fetchColumns is not called because we had no columns to fetch 
/*      * A note on retries.     *     * We've configured a total of 4 attempts.     * 5 - 4 == 1 connect failure simulation count left after this.      */
//  delete a dependency only if no other resource depends on it. 
//  NUM_BUCKETS 
//  Close the PipedOutputStream before we close the outermost OutputStream. 
//  The EXPLAIN VECTORIZATION option was specified. 
/*    * If hive job credential provider is set but HIVE_JOB_CREDSTORE_PASSWORD is not set, use   * HADOOP_CREDSTORE_PASSWORD in the jobConf    */
//  genGroupByPlanMapAggrNoSkew 
// https://dev.mysql.com/doc/refman/5.0/en/select.html 
//  Set a scratch dir permission 
//  Test EventUtils.getDbTblNotificationFilter - this is supposed to restrict   events to those that match the dbname and tblname provided to the filter.   If the tblname passed in to the filter is null, then it restricts itself 
//  Single-Column String hash table import. 
//  there is a valid bucket pruning filter 
//  Test an invalid case with multiple versions 
/*  predicate is not deterministic  */
//  We could not parse the view 
//  Need to explicitly update ProxyUsers 
//  the begin the real elements 
/*      * Look at evaluator to get output type info.      */
//  Retry from same dump when the database is empty is also not allowed. 
//  Happens due to AM side pre-emption, or the AM asking for a task to die.   There's no hooks at the moment to get information over. 
// check that we get the right files on disk 
//  Try to put the most common first 
/*          * This code is a modified version of BinarySortableSerDe.deserializeText that lets us         * detect if we can return a reference to the bytes directly.          */
//  Retry with same dump with which it was already loaded also fails. 
//  2. Restart pool sessions. 
//  should already be true 
//  for these positions, some variable primitive type (String) is used, so size 
//  Allow only keys that start with hive.*, hdfs.*, mapred.* for security   i.e. don't allow access to db password 
//  hope this is respected properly 
//  Verify that cleanNotificationEvents() cleans up all old notifications 
//  when renaming a partition, we should update   1) partition SD Location   2) partition column stats if there are any because of part_name field in HMS table PART_COL_STATS 
//  the nanosecond part fits in 30 bits 
//  Not used yet - since the Writable RPC engine does not support this policy. 
//  public Long rdatetimeepoch; // The format Hive understands by default, 
//  SR.SW.acquired Lock we are examining is acquired;  We can acquire   because a read can share with a write, and there must be 
//  -1 means that there is no stats 
//  blank " " (1 byte)   RING ABOVE U+02DA (2 bytes) 
//  reload if DS related configuration is changed 
//  tests whether the RS needs automatic setting parallelism 
//  Save the info that is required at query time to resolve dynamic/runtime values. 
//  add a struct 
//  We always need the base row 
// SQLState for cancel operation 
//  Generate the aggregate B (see the reference example above) 
//  union consisted on a bunch of map-reduce jobs, and it has been split at   the union 
//  This is either an alter table add foreign key or add primary key command. 
//  For unregistered patterns, fail. 
//  Class members for cookie based authentication. 
//  Verify that the sourceTable was created successfully. 
//  Do not allow embedded metastore in LLAP unless we are in test. 
// todo: api changed in 3.0 
//  We haven't processed all the parent sinks, and we need   them to be done in order to compute the parallelism for this sink.   In this case, skip. We should visit this again from another path. 
// ? is Type - when implemented 
//  jobConf will hold all the configuration for hadoop, tez, and hive 
//  multiple hash tables with Hybrid Grace partitioning. 
//  has to be a separate first step because we don't set the default values in the config object. 
/*  Get the small table key/value container  */
//  limit = 0 means that we do not need any task. 
//  As we said before, here we use genSelectLogicalPlan to rewrite AllColRef 
//  operator 
// but there should be no more active calls. 
//  Run worker. 
//  parsing elements one by one 
/*  Set List Bucketing context.  */
//  Drop one database, see what remains 
//  estimated hash table size 
//  Clear the value of sparkCloneConfiguration 
//  Per JDBC spec, the request defines a hint to the driver to enable database optimizations.   The read-only mode for this connection is disabled and cannot be enabled (isReadOnly always returns false). 
//  Set to start + 10000 which is the timeout 
//  match 
//  though we clone the op tree, we're still using the same MapWork/ReduceWork. 
// generate ID so that we can make an entry in COMPLETED_COMPACTIONS 
//  Comma-separated intervals without brackets 
//  TCP Server 
//  ReduceSinkMapJoinProc logic does not work unless the ReduceSink is connected as   a parent of the MapJoin, but at this point we have already removed all of the   parents from the MapJoin. 
//  We just return in that case, no drop needed. 
//  For Serialization only. 
// now that we let Calcite process subqueries we might have more than one 
//  GenericUDAF 
//  Typical line length 
/*      * user grants      */
//  toDigitsOnlyBytes. 
/*  skewed value.  */
//  a child of TableScan, so there is no need to push this predicate. 
//  get the join keys from old parent ReduceSink operators 
//  Create a Standard java object Inspector 
//  test February of non-leap year, 2/31 is viewd as 3/3 due to 3 days diff 
//  It's a table alias.   We will process that later in DOT. 
//  We flip the highest-order bit of the seven-byte representation of seconds to make negative   values come before positive ones. 
//  should have a new root now 
//  The result of the comparison of the last row processed 
// restore the original HDFS root 
// this is needed specifcally for Hive on Tez (in addition to 
//  End SqlSumAggFunction.java 
// ************************************************************************************************   Take Integer or Fractional Portion. 
// other writeTypes related to DMLs 
//  The digits must fit without rounding. 
//  NAME_TO_TYPE_PTR 
//  resFile 
//  call the method recursively over all the internal fields of the given avro 
//  1. Determine Join Type   TODO: What about TOK_CROSSJOIN, TOK_MAPJOIN 
//  After processing all the group's batches with evaluateGroupBatch, is the non-streaming 
//  With this map we project the big table batch to make it look like an output batch. 
/*  @bgen(jjtree) ConstValue  */
//  decaying where the batchSize keeps reducing by half 
//  Both are non null.   First compare the table names. 
// required for HiveRelDecorrelator 
//  The default works, no bug. 
//  propagate this change till the next RS 
//  TODO : jackson-streaming-iterable-redo this 
//  Different instance, same value 
//  Used for logDir, failure messages etc. 
//  We can merge 
//  It should not be a materialized view 
//  CM path itself is missing, cannot recover from this error 
//  required   optional   optional 
//  store data 
/*      * This TableScanDesc flag is strictly set by the Vectorizer class for vectorized MapWork     * vertices.      */
// the ref must be a table, so look for column name as right child of DOT 
//  This map records such information 
// JobSubmissionConstants.TOKEN_FILE_ARG_PLACEHOLDER) 
//  query should pass and create the table 
//  Finish the last query. 
/*    * StreamingEval: wrap regular eval. on getNext remove first row from values   * and return it.    */
//  Determine the length of storage for value and key lengths of the first record. 
//  schema/counter name validation will be done in grammar as part of HIVE-17622 
//  No writable needed for this data type. 
//  Make sure all the numbers are converted to long for size estimation. 
// --------------------------------------------------- 
//  Replication done, we need to check if the new property is added 
//  only red qualifies, and it's in entry 0 
//  start with the keywrapper itself 
//  EVENTS_COUNT 
//  [-offline|--offline] 
//  Compute knownPending tasks. selfAndUpstream indicates task counts for current vertex and 
//  Should allocate since H2 is not known. 
//  MAX_EVENTS 
//  only remove MVs first 
//  check if there is capacity in dest pool, if so move else kill the session 
//  if table and all partitions have the same schema and serde, no need to convert 
//  Return session to the pool; we can do it directly here. 
//  DEFAULT_CONSTRAINTS 
//  Test deprecated mapred.dfsclient.parallelism.max 
// do some operations with new format 
//  Lateral view AST has the following shape:   ^(TOK_LATERAL_VIEW     ^(TOK_SELECT ^(TOK_SELEXPR ^(TOK_FUNCTION Identifier params) identifier* tableAlias))) 
//  knows where to look to compact. 
//  reduce sink does not have any kids - since the plan by now has been   broken up into multiple   tasks, iterate over all tasks.   For each task, go over all operators recursively 
//  must be different 
// if destPath's parent path doesn't exist, we should mkdir it 
//  Since we remove reduce sink parents, replace original expressions 
//  Rejected 
//  The following two are public for any external users who wish to use them. 
//  Are we running tests? 
//  -------rwx   ----rwx---   -rwx------   -rwxr-xr-x   -rwxrwxrwx   --wx------   -r-x------   -r-xr-xr-x 
//  Don't timeout because of retry delay 
//  We only support pattern matching via jdo since pattern matching in Java   might be different than the one used by the metastore backends 
// compare schemes 
//  Patterns that match the middle/end of stack traces 
//  in strict mode, in the presence of order by, limit must be specified 
//  Deny if this is black-list filter (excludeMatches = true) and it   matched or if this is whitelist filter and it didn't match 
//  Small table indices has priority over retain. 
//  if partSpec doesn't exists in DB, return a delegate one   and the actual partition is created in MoveTask 
/*      * Can the LazyBinary format really tolerate writing fewer columns?      */
//  first try to get it from select 
//  return null; 
//  The extra parameters will be added on server side, so check that the required ones are 
/*        * A Windowing specification get added as a child to a UDAF invocation to distinguish it       * from similar UDAFs but on different windows.       * The UDAF is translated to a WindowFunction invocation in the PTFTranslator.       * So here we just return null for tokens that appear in a Window Specification.       * When the traversal reaches up to the UDAF invocation its ExprNodeDesc is build using the       * ColumnInfo in the InputRR. This is similar to how UDAFs are handled in Select lists.       * The difference is that there is translation for Window related tokens, so we just       * return null;        */
//  2. Collect Grouping Set info 
//  attempts at execute will be made using batchsizes 11, 3, 1, throws retry exception 
//  Create the layout for the queryId appender 
//  LATIN SMALL LETTER TURNED A U+0250 (2 bytes) 
//  If it is a column reference, we will try to resolve it 
//  group them into the chunks we want 
// Query using the hive command line 
//  Expression for the operation. e.g. N^2 > 10 
//  each table creation itself takes more than one task, give we are giving a max of 1, we should hit multiple runs. 
//  Check case insensitive search 
//  Reset this before calling positionToFirst. 
//  Pow(col, P) and Power(col, P) are special cases implemented separately from this template 
//  The caller is responsible for destroying the session. 
//  NOTE: Previously, we did OldHiveDecimal.setScale(scale), called OldHiveDecimal         unscaledValue().toByteArray(). 
//  Execute -i init files (always in silent mode) 
//  initialize source table/partition 
//  For dynamic partitioned hash join, run the ReduceSinkMapJoinProc logic for any 
//  The hive key and bytes writable value needed to pass the key and value to the collector. 
/*    * (non-Javadoc)   *   * @see javax.sql.CommonDataSource#getLoginTimeout()    */
//  populate definedRestrictedSet with parameters defined in hive.conf.restricted.list 
/*  * This class annotates each operator with its traits. The OpTraits class * specifies the traits that are populated for each operator.  */
//  NOTE: Multi Insert is not supported 
//  PARTNAME 
// being acquired now 
//  themap might be reused by the Protocol. 
//  The char type info need to be set prior to initialization,   and must be preserved when the plan serialized to other processes. 
// fall through; doesn't map to Hive/Hcat type; here for completeness 
//  If a union occurs before the sort-merge join, it is not useful to convert the the   sort-merge join to a mapjoin. The number of inputs for the union is more than 1 so   it would be difficult to figure out the big table for the mapjoin. 
//  selectivity(RS-3) = numRows(RS-3)/numRows(JOIN) * selectivity(JOIN) 
//  4. See if all the field expressions of the left hand side of IN are expressions    containing constants or only partition columns coming from same table. 
//  All the tables/partitions columns should be sorted in the same order   For example, if tables A and B are being joined on columns c1, c2 and c3   which are the sorted and bucketed columns. The join would work, as long 
//  Note: fullFromMResroucePlan needs to be called inside the txn, otherwise we could have         deduplicated this with getActiveMWMResourcePlan. 
//  continue on to the next exprNode to find a match 
/*          * Get our Single-Column Long hash map information for this specialized class.          */
// MAPPING: bucket_file_name_in_big_table->{alias_table->corresonding_bucket_file_names} 
//  Cap WriteBufferSize to avoid large preallocations   We also want to limit the size of writeBuffer, because we normally have 16 partitions, that 
//  Do division/remainder to get lower binary word; quotient will either be middle decimal   or be both high and middle decimal that requires another division/remainder. 
// return the current block's compressed key length 
//  first row/call or a new partition 
//  Ignore the struct column and just copy all the following data columns. 
//  Counter may exceed limitation 
//  add_table 
//  Add rounding and handle carry. 
//  testing acid usage when no masking/filtering is present 
//  Multi-Key hash table import. 
//  i.e. Overflow. 
//  To be compatible with the OldHiveDecimal version, zero has factor 1. 
/*      * Get the HIVE counters     *     * HIVE     *  CREATED_FILES=1     *  DESERIALIZE_ERRORS=0     *  RECORDS_IN_Map_1=550076554     *  RECORDS_OUT_INTERMEDIATE_Map_1=854987     *  RECORDS_OUT_Reducer_2=1      */
//  Otherwise, load lazily via StorageHandler at query time. 
//  FIXME 
/*      * Basic algorithm:     *     * 1. Scale away fractional digits if present.     * 2. Clear integer rounding portion.     *      */
//  Lookup values needed for numeric arithmetic UDFs 
//  union type currently not totally supported. See HIVE-2390 
//  max heap size 
//  set output vector 
//  get acl provider for most outer path that is non-null 
//  The number of map reduce tasks executed by the HiveServer2 since the last restart 
//  no distinct processing at the reducer   A query like 'select count(distinct key) from T' is transformed into 
/*  Get the in memory hashmap  */
//  compatibility mode enabled 
//  Validate the second parameter, which is either a solitary double or an array of doubles. 
// this seems odd, but we wan to make sure that to run CompactionTxnHandler.cleanEmptyAbortedTxns() 
//  Note: this is backward compat only. Should be removed with createUnallocated. 
//  a common base class or not. 
//  check split 
//  validate/fill-in scheme and authority. this follows logic   identical to FileSystem.get(URI, conf) - but doesn't actually   obtain a file system handle 
//  finally move recovered file to actual file 
//  use the serialization option switch to write primitive values as either a variable   length UTF8 string or a fixed width bytes if serializing in binary format 
//  Things we log in the jobconf 
//  Must set this key even if differences is empty otherwise client and AM will attempt 
//  First value for next column 
// HIVE-16952 
//  create a new setop whose children are the filters created above 
//  KILLED or FAILED state 
//  optional string value = 2; 
//  always round down to the previous period (for timestamps prior to origin) 
/*    * Tests if null value returned when file is not present in any of the lookup locations    */
//  we can set the traits for this join operator 
//  precision 6 
//  If it is not a SEL operator, we bail out 
/*            * Optionally, the next value's small length could be a 2nd integer...            */
/*  * This is a pluggable policy to chose the candidate map-join table for converting a join to a * sort merge join. The largest table is chosen based on the size of the tables.  */
/*          * Initialize Multi-Key members for this specialized class.          */
/*  @bgen(jjtree) FieldValue  */
//  Per MapObjectInpsector.getMapSize(), -1 length means null map. 
// Get fields out of the lazy struct and check if they match expected   results 
//  we want to signal an error if the table/view doesn't exist and we're   configured not to fail silently 
//  store column name in map-targetWork 
//  precision 5 
//  Populate the names and order of columns for the first partition of the   first table 
/*    * To be able to combine a parent join and its left input join child,   * the left keys over which the parent join is executed need to be the same   * than those of the child join.   * Thus, we iterate over the different inputs of the child, checking if the   * keys of the parent are the same    */
//  get the tmp URI path; it will be a hdfs path if not local mode 
//  Cannot deserialize => throw the specific exception. 
//  4. Get Join Condn 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getMaxFieldSize()    */
// bucket_num_reducers_acid.q, TestTxnCommands.testMoreBucketsThanReducers() 
//  For null and true values, return every partition 
//  maintain the stack of operators encountered 
//     Configuration conf = context.getConfiguration();      Credentials creds = context.getCredentials(); 
//  Database DDL 
//  precision 4 
//  update primary and secondaryKey 
//  required. 
//  Summary for top column values 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getFetchDirection()    */
//  This should fail with given HTTP response code 431 in error message, since header is more 
//  For now, don't push anything into HBase, nor store anything special in HBase 
//  turn on db notification listener on metastore 
//  MIN_OPEN_TXN 
//  Prepare IN (blah) lists for the following queries. Cut off the final ','s. 
//  Blocking execute 
//  convert the filter to one that references the child of the project 
//  use the parser to get the output operators of RS 
//  Parse the rewritten query string   check if we need to ctx.setCmd(rewrittenQuery); 
//  One fewer byte. 
//  Not supported by CBO 
//  We already have the TableScan for one side of the join. Check this now. 
//  the correlated variables. 
//  create N TableScans 
//    trim=true 
//  If metadata-only dump, then the state of the dump shouldn't be the latest event id as   the data is not yet dumped and shall be dumped in future export. 
//  Possible overflow once 
//  create final load/move work 
//  matching rows must be in the final block, so we can end the binary search. 
//  Drain the first Row, which just contains column names 
//  Do not give out the capacity of the initializing sessions to the running ones;   we expect init to be fast. 
// noop 
//  create a batch with two string ("Bytes") columns 
//  Or UDF 
//  close + write 
//  Assuming TaskAttemptId and FragmentIdentifierString are the same. Verify this. 
//  if the task is done, all operators are done as well 
//  Do not delete the data 
//  add "execute" permission to downloaded resource file (needed when loading dll file) 
//  standard ObjectInspector 
//  negative number, flip all bits 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#openSession(java.lang.String, java.lang.String, java.util.Map)    */
//  The process method was not called -- no big table rows. 
//  1 seconds wait until subsequent status   5 minutes timeout for watch mode 
//  revert output cols of SEL(*) to ExprNodeColumnDesc 
//  estimate size of aggregation buffer 
//  is it a filter or a join condition   if it is filter see if it can be pushed above the join   filter cannot be pushed if   * join is full outer or   * join is left outer and filter is on left alias or   * join is right outer and filter is on right alias 
//  We need to operate on sorted data to fully test BinarySortable. 
//  chosen. 4 * 20% of noconditional task size will be oversubscribed 
//  production is: Map<FieldType(),FieldType()> 
//  offsets 
//  The user can specify the hadoop memory 
//  ReduceSinkOperators. 
//  Force the underlying db to initialize. 
/*        * The issue with caching in case of bucket map join is that different tasks       * process different buckets and if the container is reused to join a different bucket,       * join results can be incorrect. The cache is keyed on operator id and for bucket map join       * the operator does not change but data needed is different. For a proper fix, this       * requires changes in the Tez API with regard to finding bucket id and       * also ability to schedule tasks to re-use containers that have cached the specific bucket.        */
//  15% for io cache 
//  nothing at the moment 
//  Replace filter in current FIL with new FIL 
/*  The result is empty string if a negative start is provided         * whose absolute value is greater than the string length.          */
//  random sampling 
/*        * The scratch column information was collected by the task VectorizationContext.  Go get it.        */
//  we are going to eliminate 
//  Here we rewrite the * and also the masking table 
//  Hide this so it doesn't look like a simple property. 
//  If the parameters does not define any transactional properties, we return a default type. 
/* cc_id is monotonically increasing so for any entity sorts in order of compaction history,        thus this query groups by entity and withing group sorts most recent first */
//  in multithreaded mode - do cleanup/initialization just once 
//  find the branch on which this processor was invoked 
//  Set a custom prefix for hdfs scratch dir path 
//  Get checksum of a file 
//  FetchWork's sink is used to hold results, so each query needs a separate copy of FetchWork 
//  New attempt path crated. Add a watch on it, and scan it for existing files. 
//  There should still now be 5 directories in the location 
//  be sent over the wire from the AM, and will take the place of AppId+dagId in QueryIdentifier. 
//  If a file is copied from CM path, then need to rename them using original source file name   This is needed to avoid having duplicate files in target if same event is applied twice 
//  partition column value is null. 
//  This lock is used to mutex commit/abort and heartbeat calls 
//  HAS_MORE_ROWS 
//  verify that flattening and unflattening "isRepeating" works 
//  Fall back to regular API and create states without ID. 
//  Tests for getTable in other catalogs are covered in TestTablesCreateDropAlterTruncate. 
//  "jdbc:hive2://localhost:10000/default"   "hive",   "" 
// write  baseSplit into output 
//  set the default configs in whitelist 
//  if zk HA is enabled get hosts property 
//  in a vectorized row batch. 
//  The return value of 0 indicates success, 
/*        * Single-Column Long specific declarations.        */
//  Make sure we run through the loop once before checking to stop as this makes testing   much easier.  The stop value is only for testing anyway and not used when called from   HiveMetaStore. 
//  If the 'metastore.partition.inherit.table.properties' property is set in the metastore   config, the partition inherits the listed table parameters.   This property is not set in this test, therefore the partition doesn't inherit the table   parameters. 
//  a composite key class was provided. But neither the types   property was set and   neither the getParts() method of HBaseCompositeKey was   overidden in the   implementation. Flag exception. 
//  First look for all the compactions that are waiting to be cleaned.  If we have not   seen an entry before, look for all the locks held on that table or partition and   record them.  We will then only clean the partition once all of those locks have been   released.  This way we avoid removing the files while they are in use,   while at the same time avoiding starving the cleaner as new readers come along.   This works because we know that any reader who comes along after the worker thread has   done the compaction will read the more up to date version of the data (either in a   newer delta or in a newer base). 
//  do not need to update column stats if alter partition is not for rename or changing existing columns 
//  Trigger query hooks after query completes its execution. 
//  No exceptions expected 
//  the raw data size. 
//  Simple implementation for now, this will later expand to do DAG evaluation. 
// associated with a txn and is handled by performTimeOuts() 
//  initialize output 
// TODO: poll periodically 
// ************************************************************************************************   Emulate BigInteger deserialization used by LazyBinary and others. 
//  not using FieldSchema.equals as comments can be different 
//  walk through the operator tree 
//  Adding column types used later by org.apache.hadoop.hive.druid.serde.DruidSerDe 
//  remember mapping of plan to input 
// owner = "testOwner1" and (lastAccessTime = 30 or test_param_1 = "hi") 
//  attempt retrieving the schema from the data 
//  Reorder fields in record based on the order of columns in the table 
//  existing 
//  Generate the output column info's / row resolver using internal names. 
/*           acid file would have schema like <op, owid, writerId, rowid, cwid, <f1, ... fn>> so could          check it this way once/if OrcRecordUpdater.ACID_KEY_INDEX_NAME is removed          TypeDescription schema = reader.getSchema();          List<String> columns = schema.getFieldNames();          */
//  at this point we have Druid segments from reducers but we need to atomically   rename and commit to metadata   Moving Druid segments and committing to druid metadata as one transaction. 
// create 1 row in a file 000000_0_copy1 and 1 row in a file 000001_0_copy1 
//  We've eliminated the highest digit already with HiveDecimal.MAX_PRECISION check above. 
//  Atomically move temp file to the destination file 
//  required   required   required   optional 
//  This is primarily for testing to avoid the host lookup 
//  an equivalent algorithm exists in   com.google.common.primitives.UnsingedLongs 
//  round(Col, N) is a special case and will be implemented separately from this template 
// 'targetPath' is table root of un-partitioned table or partition  'sourcePath' result of 'select ...' part of CTAS statement 
//  direct hand-off 
//  customized log4j config log file to be: /${test.tmp.dir}/TestHiveLogging/hiveLog4jTest.log 
//  test fields 
//  non-vectorized record reader is created below. 
//  If we are running tests, we are going to verify that the contents of the cache   correspond with the contents of the plan, and otherwise we fail.   This check always run when we are running in test mode, independently on whether 
//  vector expression doesn't support checked execution   hold on to it in case there is no available checked variant 
//  don't go  through Initiator for user initiated compactions) 
//  if last row group is set to true, it means the above row group spans compression buffer 
//  Need to instantiate the realInputFormat 
/*     todo: parse     target/tmp/org.apache.hadoop.hive.upgrade.acid.TestUpgradeTool-1527286026461/convertToAcid_1527286063065.sql     make sure it has:    ALTER TABLE default.tacid SET TBLPROPERTIES ('transactional'='true');    ALTER TABLE default.tacidpart SET TBLPROPERTIES ('transactional'='true');      */
//  Repeated NULL fill down column. 
//  either input 1 or input 2 may have nulls 
//  COMPATIBILITY 
//  create matchers for custom path string as well as actual dynamic partition path created 
//  walk operator tree to create expression tree for list bucketing 
//  Rewrite the agg calls. Each distinct agg becomes a non-distinct call   to the corresponding field from the right; for example,   "COUNT(DISTINCT e.sal)" becomes   "COUNT(distinct_e.sal)". 
//  needed so that the file is actually loaded into configuration. 
//  The name used in the service registry may not match the host name we're using.   Try hostname/canonical hostname/host address 
//  LOCKS 
//  Remove view-based rewriting rules from planner 
//  if true return true 
//  Ensure that we have the correct identifier as the column name 
/*  useIncludeColumns  */
//  Explicitly use {@link org.apache.hadoop.hive.ql.metadata.Table} when getting the schema,   but store @{link org.apache.hadoop.hive.metastore.api.Table} as this class is serialized   into the job conf. 
//  Wait for the initial resource plan to be applied. 
//  MY_64BIT_INT 
//  We check for two different classes below because initially the result   is IfExprLongColumnLongColumn but in the future if the system is enhanced 
// try converting to the enum to verify that this is a valid privilege type 
//  catch all errors (throwable and execptions to prevent subsequent tasks from being suppressed) 
//  For join operators that can generate small table results, reset their   (target) scratch columns. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setClientInfo(java.util.Properties)    */
//  for ValidCompactorWriteIdList, everything in exceptions are aborted 
/*  Last try to get a port.  Just use default 50111.   */
//  Get the MWMTrigger object from DN 
//  quoted string 
//  The only concurrent change that can happen when we hold the heap lock is list removal; 
//  NOSUCH 
//  check if the grantor matches current user 
//  We need to provide a different record reader for every type of Druid query.   The reason is that Druid results format is different for each type. 
//  for casting integral types to boolean 
//  Modify a copy, not the original 
//  For remote JDBC client, try to set the hive var using 'set hivevar:key=value' 
//  trigger bugs in anyone who uses this as a hostname 
//  Table name or pattern 
//  WriteID for table1, way in the future 
//  get the column path   return column name if exists, column could be DOT separated.   example: lintString.$elem$.myint 
//  If the table/partition exist and is older than the event, then just apply 
// it is assumed that parent directory of the destf should already exist when this  method is called. when the replace value is true, this method works a little different  from mv command if the destf is a directory, it replaces the destf instead of moving under 
//  v[9] -- since left integer #5 is always 0, some products here are not included. 
//  nothing to register (return host-<hostname>) 
/*  doWriteFewerColumns  */
/*    * called during deserialization of a QueryDef during runtime.    */
//  This would be the case for obscure tasks like truncate column (unsupported for MM). 
//  Not matching the regex? 
//  transactionalListener.onAddNotNullConstraint(addcheckConstraintEvent);  } 
//  drop any partitions 
/*  We are not starting zookeeper server here because     * QTestUtil already starts it.      */
//  push down projections. 
//  add it's locations to the list of paths to delete 
//  Assigning can be a subset of columns, so this is the projection --   the batch column numbers. 
//  optional string queue = 6; 
//  Verify privilege objects 
// ************************************************************************************************   Decimal Scale Up/Down. 
/*      * the rankValue of the last row output.      */
//  Authorization DDL 
//  We populate inputInspectors for all children of this DemuxOperator.   Those inputObjectInspectors are stored in childInputObjInspectors. 
//  This will call one of the specific notifyEvicted overloads. 
//  0. Collect AggRel output col Names 
//  Backtrack key columns of cRS to pRS 
//  Note: we are called after preReadUncompressedStream, so it doesn't have to do nearly as much         as prepareRangesForCompressedRead does; e.g. every buffer is already a CacheChunk. 
//  set the fetchSize 
//  LOG.info("Get input " + a.getClass() + ":" + a + " " + b.getClass() + ":"   + b); 
//  remove from HADOOP_CLIENT_OPTS any debug related options 
//  If a system property that matches one of our conf value names is set then use the value 
//  Column stats generates "select compute_stats() .." queries.   Disable caching for these. 
//  Clean up System properties that were set by this test 
//  folding the constant 
/*        * if partition is smaller than the lagAmt;       * the entire partition is in lagValues.        */
//  For the mapper processing C, The SMJ is not initialized, no need to close it either. 
//  Note: do not cancel any user actions here; user actions actually interact with kills. 
//  There should be 2 delta dirs, plus a base dir in the location 
//  The intermediate rename would've failed as bootstrap dump in progress 
//  If any of the elements was not referenced by every operand, we bail out 
//  prefix = work.getAggKey(); 
//  Table scan operators to DPP sources 
//  Keys for all tables are the same, so only the first has to deserialize them. 
//  right border is the min 
//  When we generate results into the overflow batch, we may still end up with fewer rows   in the big table batch.  So, nulSel and the batch's selected array will be rebuilt with   just the big table rows that need to be forwarded, minus any rows processed with the 
//  wish to add new line here. 
//  Adjustment only needed when we exceed max precision 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setInt(int, int)    */
//  Assume it is not streamjng by default. 
//  String including a surrogate pair character 
//  The column names and order (ascending/descending) matched   The first 'n' sorted columns should be the same as the joinCols, where   'n' is the size of join columns.   For eg: if the table is sorted by (a,b,c), it is OK to convert if the join is   on (a), (a,b), or any combination of (a,b,c):     (a,b,c), (a,c,b), (c,a,b), (c,b,a), (b,c,a), (b,a,c) 
//  Now we check if though the schemas are the same,   the operator changes the order of columns in the   output 
//  Converts the negative byte into positive index 
//  Get the children of the set clause, each of which should be a column assignment 
//  QBJoinTree from innermost to outer 
//  prune any nulls present in map values - this is the typical case. 
//  Set fetch size 
//  No need for this if all sub-queries are map-only queries 
// --------------------------- Windowing handling: PTFInvocationSpec to PTFDesc -------------------- 
//  generate the output records 
//  Order on outputColumn. 
// written by Major compaction 
//  and a notification comes from the LlapTaskReporter 
//  Assume 
//  if orc table, restrict changing the serde as it can break schema evolution 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setTimestamp(int, java.sql.Timestamp,   * java.util.Calendar)    */
//  first load the defaults from spark-defaults.conf if available 
//  optional bytes credentials_binary = 8; 
//  part of the partition specified   Create a DummyPartition in this case. Since, the metastore does not store partial   partitions currently, we need to store dummy partitions 
// standardize to lower case 
//  execute and exit 
//  If the caller is not within a mapper/reducer (if reading from the table via CliDriver),   then TaskAttemptID.forname() may return NULL. Fall back to using default constructor. 
//  This is a test 
//  one VInt with nanos 
//  This is a root task 
//  Also note that any delete_deltas in between a given delta_x_y range would be made   obsolete. For example, a delta_30_50 would make delete_delta_40_40 obsolete.   This is valid because minor compaction always compacts the normal deltas and the delete   deltas for the same range. That is, if we had 3 directories, delta_30_30,   delete_delta_40_40 and delta_50_50, then running minor compaction would produce   delta_30_50 and delete_delta_30_50. 
//  Assuming the ObjectInspector represents exactly the same type as this   struct.   This assumption should be checked during query compile time. 
//  Since we allow write operations on cache while prewarm is happening:   1. Don't add tables that were deleted while we were preparing list for prewarm 
//  modified.  
//  SMB Join. 
//  More complex methods which may wrap multiple operations 
//  The max for 18 - 1 digits. 
//  Initialize the cookie based authentication related variables. 
//  Following parameters slated for removal, prefer usage of enum above, that allows programmatic access. 
//  this shouldn't happen 
//  End of the record is part of the data 
//  We must be able to take away the requisite number; if we can't, where'd the ducks go? 
//  row specific RecordWriters 
//  test perfectly divisible batchsize and decaying factor 
//  tells you what that mapping is. 
//  1786135888657847525803324040144343378.09799306448796128931113691624 
//  initialize with estimated element size 35 
//  Just return the object. We need no further operation on it 
//  TODO: potential DFS call 
//  Go over all the children 
//  There is an overlap of ranges - create combined range. 
//  If the global limit optimization is triggered, we will   estimate input data actually needed based on limit rows.   estimated Input = (num_limit * max_size_per_row) * (estimated_map + 2)   
/*    * When neither HADOOP_CREDSTORE_PASSWORD nor HIVE_JOB_CREDSTORE_PASSWORD   * are not set jobConf should contain only the credential provider path    */
//  @@protoc_insertion_point(builder_scope:GroupInputSpecProto) 
//  no pos change - no need since we've shrunk the string with same pos 
//  A fair reentrant lock 
// if still equal, compare by lock ids 
//  Not using SASL/Kerberos -- use the password 
//  Set OUT parameters 
//  If it contains non-equi join conditions, we bail out 
//  cars >= 2 
//  After using selected to generate spills, generate non-matches, if any. 
//  These parameters must match for all orc files involved in merging. If it   does not merge, the file will be put into incompatible file set and will   not be merged. 
//  get connection 
//  Http transport mode. 
/*      * joinCond represents a correlated predicate.     * leftIsRewritten, rightIsRewritten indicates if either side has been replaced by a column alias.     *      * If a side is not rewritten, we get its text from the tokenstream.      * For rewritten conditions we form the text based on the table and column reference.      */
//  See: SPARK-21187 
//  Finally. 
/*    * Deserializes 64-bit decimals up to the maximum 64-bit precision (18 decimal digits).    */
//  CONSIDER unwinding ValidatorVectorSelectOperator as a subclass of VectorSelectOperator. 
//  PARTITION_ORDER 
// traling ',' 
//  currently all Spark work is on the cluster 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getOperationStatus(org.apache.hive.service.cli.OperationHandle)    */
//  add new item to the tez work 
//  run the script using SqlLine 
//  check if the partition spec is valid 
//  Note : List<Partition> getPartitions() removed with HIVE-9609 because it will result in OOM errors with large add_partitions. 
//  get privileges for this user and its roles on this object 
//  cancel. 
/*    * This operator is allowed before mapjoin. Eventually, mapjoin hint should be done away with.   * But, since bucketized mapjoin and sortmerge join depend on it completely. it is needed.   * Check the operators which are allowed before mapjoin.    */
/*      * 3. give Evaluator chance to setup for RawInput execution; setup RawInput shape      */
//  otherwise return the arguments 
//  Pop (struct, union) 
//  Try with with DECIMAL_64 input and desired output type. 
//  Now do a union of the select operators: selectOp and selectOpClone   Store the operator that follows the select after the join, we will be 
//  rows will be 1 
//  Both the handlers should be same 
//  Compute the geometric average of the ratios of all of the factors   which are non-zero and finite. 
// supports AcidInputFormat which uses the KEY pass ROW__ID info 
//  Restart the sessions until one of them refuses to restart. 
//  Restore the original selected vector 
/*        * d. Construct PTF Operator.        */
//  Test partition listing with a partial spec 
// need the WHERE clause below to ensure consistent results with READ_COMMITTED 
//  go over all aggregation buffers and see they implement estimable 
//  check isRepeating propagation 
//  Note: we only look at the schema here to deal with complex types. Somebody has set up the         reader with whatever ideas they had to the schema and we just trust the reader to         produce the CVBs that was asked for. However, we only need to look at top level columns. 
//  store a path prefix in this TestFilter 
//  Attempt dynamic partitioned hash join   Since we don't have big table index yet, must start with estimate of numReducers 
//  All will be filtered out 
//  Serialize AggBuffer 
//  Not supported for MM tables right now. 
//  read baseSplit from input 
//  Check if this is table name is qualified or not 
//  Short circuit and return the current number of rows if this is a 
//  A failure here will cause this query to not be added to the cache. 
//  tables in test db 
//  increments file modification time 
//  Exceeds max value 
//  show table properties - populate the output stream 
//  is well formed. 
//  Configure http client for SSL 
//  dummy for backtracking 
//  string BETWEEN 
//  intentionally corrupt some files 
//  Literal double. 
//  Try to find valid entry, but settle for pending entry if that is all we have. 
//  get a set of RecordWriter based on the DP column values 
//  Add the queryId appender to the queryId based route 
//  regular case of accessing nested field in a column 
//  START_TIME 
//  bad files don't pollute the filesystem 
//  Locked, or invalidated, buffer was in the list - just drop it;   will be re-added on unlock. 
//  remove all the candidate filter operators   when we get to the TS 
//  enforce is required to retain the buffer sizes of old files instead of orc writer   inferring the optimal buffer size 
//  repeated .GroupInputSpecProto grouped_input_specs = 12; 
// should not matter which txnMgr is used here 
//  Use url param indirectly - as the name of an env var that contains the url   If the urlParam is "default", we would look for a BEELINE_URL_DEFAULT url 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getSQLXML(java.lang.String)    */
//  Testing multiByte string with reference set to mid array 
// we don't need to cancel this token as the TokenRenewer for JT tokens  takes care of cancelling them 
//  POSITION 
//  We copy the join and the top sort operator 
//  TEST - repeating NULL & no selection 
//  Definitely a long; most longs fall here 
//  Map-only subqueries can be optimized in future to not write to a file in 
//  to the formulae in JLS, Section 20.10.22. 
//  If any table has bad size estimate, we need to fall back to sizing each table equally 
//  TODO HIVE-14042. abort handling: Handling of mergeMapOp 
//  Now all the column values should always return NULL! 
//  an exception 
// Currently only column related changes can be cascaded in alter table 
//  When people forget to quote a string, op1/op2 is null.   For example, select * from some_table where ds > 2012-12-1 or ds < 2012-12-2 . 
//  gather statistics for the first time and the attach it to table scan operator 
//  a new value so the last isn't clobbered 
// now 4 
/*        * the SubQuery predicate has been hoisted as a Join. The SubQuery predicate is replaced       * by a 'true' predicate in the Outer QB's where/having clause.        */
//  If the current record needs to be returned based on the   filter conditions specified by the user, increment the counter 
//  Bad luck! Handle the corner cases where 3 bytes are in multiple blocks. 
/*        * Do careful maintenance of the outputColVector.noNulls flag.        */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBlob(java.lang.String,   * java.io.InputStream, long)    */
//  return a loaned session goes back to tez am pool 
//  Return true if the remove was successful, false otherwise 
//  Distincts are not allowed with an additional mr job 
//  This is likely a shutdown 
//  even though there are only two delta_3_3, delta_4_4 and one delete_delta_5_5. 
//  Type affinity does not help when type affinity does not match input args 
// now 3 
//  Tuesday 1st January 1985 12:00:00 AM 
//  File named by path doesn't exist; nothing to validate. 
//  Index into partitionSpecs. 
//  NN_NAME 
//  Number of variables declared with the same data type and default 
//  isSame 
//  2^p number of bytes for register 
//  Everything should be same as before 
//  Latin Capital Letter Z U+005A (1 bytes)   blank " " (1 byte) 
//  Should we intercept here for a possible Decimal64 vector expression class? 
//  If delegation token is passed from the client side, do not set the principal 
//  from ParentRunner, retried under exception (notified only after exhausting retryCount) 
//  Clear the thread locals 
//  for these positions, some variable primitive type (String) is used for the 
//  iterate for the first time to get all the names of stages. 
//  Cancel the maintenance thread. 
//  partial column statistics on grouping attributes case.   if column statistics on grouping attribute is missing, then   assume worst case.   GBY rule will emit half the number of rows if ndvProduct is 0 
//  TODO: add allocate overload with an offset and length 
//  TODO: parts of this should be moved out of TezSession to reuse the clients, but there's         no good place for that right now (HIVE-13698). 
//  This null check is specifically done as the same class is used to handle both incremental and   bootstrap replication scenarios for create function. When doing bootstrap we do not have   event id for this event but rather when bootstrap started and hence we pass in null dmd for   bootstrap.There should be a better way to do this but might required a lot of changes across   different handlers, unless this is a common pattern that is seen, leaving this here. 
//  in dynamic usecase, called through FileRecordWriterContainer 
//  clone rhsSemijoin 
//  Use the RowResolver from the input operator to generate a input   ObjectInspector that can be used to initialize the UDTF. Then, the 
/*    * @param conf /* @throws HCatException,ConnectionFailureException   *   * @see   * org.apache.hive.hcatalog.api.HCatClient#initialize(org.apache.hadoop.conf.   * Configuration)    */
//  give me more 
//  if string can not be parsed converter will return null 
//  allow NOT but throw an error for rest 
// now 5 
//  INPUT_FORMAT 
//  Get all items into an array and sort them. 
//  Now prepare partnames with only 5 partitions: [tab1part1...tab1part5] 
//  use linear extrapolation. more complicated one can be added in the 
//  Given that we do not delete, an empty slot means no match.   LOG.debug("VectorMapJoinFastLongHashTable findReadSlot key " + key + " slot " + slot + " pairIndex " + pairIndex + " empty slot (i = " + i + ")"); 
//  We will store all the new /changed properties in the job in the   udf context, so the the HCatOutputFormat.setOutput and setSchema 
//  --------------------------------------------------------- Private Methods 
//  the owner will also have select with grant privileges on new view 
//  Year granularity 
// use only control chars that are very unlikely to be part of the string   the following might/likely to be used in text files for strings   9 (horizontal tab, HT, \t, ^I)   10 (line feed, LF, \n, ^J),   12 (form feed, FF, \f, ^L),   13 (carriage return, CR, \r, ^M),   27 (escape, ESC, \e [GCC only], ^[). 
// disable the expandEvents for the purpose of backward compatibility 
//  Also make sure it is a constant. 
//  Process min/max 
//  hash from the fetcher 
//  We decompose an AND expression into its parts before checking if the   entire expression is a candidate because each part may be a candidate   for replicating transitively over an equijoin condition. 
//  Remove cast of BOOLEAN NOT NULL to BOOLEAN or vice versa. Filter accepts   nullable and not-nullable conditions, but a CAST might get in the way of 
//  key = (key << 21) - key - 1; 
/*  * Defines an interface for re-execution logics. * * FIXME: rethink methods.  */
/*  Exponent read from "EX" field.  */
//  input OI includes table columns + partition columns 
//  Check that we have two deltas. 
/*        * Repeating ELSE expression?        */
//  Methods to set/reset getPartition modifier 
//  During the DFS traversal of the AST, a descendant of nd likely set an   error because a sub-tree of nd is unlikely to also be a group by   expression. For example, in a query such as   SELECT *concat(key)* FROM src GROUP BY concat(key), 'key' will be   processed before 'concat(key)' and since 'key' is not a group by   expression, an error will be set in ctx by ColumnExprProcessor. 
//  TRIGGER 
//  The following information is for creating VectorizedRowBatch and for helping with   knowing how the table is partitioned.     It will be stored in MapWork and ReduceWork. 
//  1. Get file metadata from cache, or create the reader and read it. 
//  A map-only job can be optimized - instead of converting it to a   map-reduce job, we can have another map   job to do the same to avoid the cost of sorting in the map-reduce phase.   A better approach would be to   write into a local file and then have a map-only job.   Add the limit operator to get the value fields 
//  Stop, as we won't be able to go forward. 
//  Maintain count of outstanding requests for tokenIdentifier.   If count goes to 0, it is safe to remove the token. 
//  ColumnizedDeleteEventRegistry reads all the delete events into memory during initialization   and it closes the delete event readers after it. If an exception gets thrown during   initialization, we may have to close any readers that are still left open. 
//  NOTE: We do conditional vector expression so we do not call super.evaluateChildren(batch). 
//  Calcite day-time interval is millis value as BigDecimal   Seconds converted to millis 
//  if we've seen both root and child, we can bail. 
//  Should have files 
//  Remove default partition from partition names and get aggregate 
//                  123456789012345678901234567890123456789012345                            1         2         3         4 
//  Otherwise, we tried .. 
//  If anything remains, this is where it starts. 
//  Iterating through the buffer should not cause the next buffer to be fetched 
// now 1 
//  QUERY_TIMEOUT 
//  Check for dynamic partitions. 
//  Second check - the tasks we looked for must not have been accessed more than   once as a result of the traversal (note that we actually wind up accessing   2 times , because each visit counts twice, once to check for existence, and   once to visit. 
//  add role privileges 
//  How much can the fraction be moved up? 
//  Check if any of the txns in the list is aborted. 
//  Add another db via CachedStore 
//  Tez internals may register the same task as completing multiple times. 
/*      * The conversion to standard object inspector was necessitated by HIVE-5973. The issue     * happens when a select operator preceeds this operator as in the case of a subquery. The     * select operator does not allocate a new object to hold the deserialized row. This affects     * the operation of the SMB join which puts the object in a priority queue. Since all elements     * of the priority queue point to the same object, the join was resulting in incorrect     * results.     *     * So the fix is to make a copy of the object as done in the processOp phase below. This     * however necessitates a change in the object inspector that can be used in processing the     * row downstream.      */
//  provider setup 
//  make sure delta files are written with no indexes, no compression and no dictionary 
//  Translate the positions of correlated variables to be relative to   the join output, leaving room for valueGenFieldOffset because   valueGenerators are joined with the original left input of the rel 
//  also test getting a table from a specific db 
//  Let's try to parse it as timestamp with time zone and transform 
//  Note that here we ignore nanos part of Hive Timestamp since nanos are dropped when   reading Hive from Pig by design. 
/*    * is this PTFDesc for a Map-Side PTF Operation?    */
// test with predicates such that partition pruning works 
//  10^17 - 1 
//  offer accepted, r1 evicted 
//  pick up unknown case 
//  These remaining combinations below definitely result in overflow. 
//  I don't entirely believe that everything is cleaned up   when close is called based on watching the TRACE logs 
//  Replication dest will not be external - override if set 
//  All join tables have 0 keys, doesn't matter what we generate. 
//  this will trigger the column pruner to collect view column   authorization info. 
//  special handling for SQL "reload function" 
// why even compute syntheticProps if !isOriginal??? 
//  notifyReused implies that buffer is already locked; it's also called once for new   buffers that are not cached yet. Don't notify cache policy. 
//  missing stripe stats (old format). If numRows is 0 then its an empty file and no statistics   is present. We have to differentiate no stats (empty file) vs missing stats (old format). 
//  expected 
//  check the VirtualColumn directly. 
//  have to make sure there's slash after .har, otherwise resolve doesn't work 
//  Attempting to get the password should throw an exception 
//  Try outer Row resolver 
/*  Disregard nulls for processing. In other words,     * the arithmetic operation is performed even if one or     * more inputs are null. This is to improve speed by avoiding     * conditional checks in the inner loop.      */
// First non-null item. 
//  call getsplits 
//  Make sure we got the right implementation of a ColumnMapping 
//  add the new entry 
//  Return our mocked objects 
//  Trigger an update if needed. 
/*    * refactored out of the Equality case of parseJoinCondition   * so that this can be recursively called on its left tree in the case when   * only left sources are referenced in a Predicate    */
//  Used for testing to simulate method timeout. 
// newer versions (2012 and later) support OFFSET/FETCH  https://msdn.microsoft.com/en-us/library/ms189463.aspx 
//  Mainly for verification 
//  This object should be the same as in cache, otherwise, it must be removed due to init error 
//  Build an input to output position map. 
//  Test if rpc_server_address is configured 
//  MODIFIER LETTER UP ARROWHEAD U+02C4 (2 bytes) 
//  Use the tableAlias to generate keyBaseAlias 
//  replace SEL(*) to SEL(exprs) 
//  t1=5+1(insert) and t2=5+2(insert) 
//  clone filterMap 
//  If we have a plan, prefer its logical result schema if it's   available; otherwise, try digging out a fetch task; failing that,   give up. 
//  acid 
/*      * Instantiate Decimal64 vector expression.     *     * The instantiateExpression method sets the output column and type information.      */
//  non-space character 
//  Randomly pick the character corresponding to the field id and convert it to byte array 
//  This string reader should simply redirect to its own seek (what other types already do). 
// {"" : null} 
//  stddev_pop(x) ==>     power(       (sum(x * x) - sum(x) * sum(x) / count(x))       / count(x),       .5)     stddev_samp(x) ==>     power(       (sum(x * x) - sum(x) * sum(x) / count(x)) 
//                              1         2         3         4 
//  optional bool is_external_submission = 14 [default = false]; 
//  Flush any pending scheduler runs which may be blocked. Wait 2 seconds for the run to complete. 
//  For now, we don't support group by on DECIMAL_64 keys. 
//  uppercase first letter 
//  As above, we enforce a certain order when we do the reutilization. 
// if target table has cols c1,c2,c3 and p1 partition col and we had "SET c2 = 5, c1 = current_date()" we want to end up with  insert into target (p1) select current_date(), 5, c3, p1 where .... 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.newBuilder() 
//  make sure we don't modify the config in RpcConfiguration 
/*    * use to check if a set of columns are all partition columns. true only if: -   * all columns in BitSet are partition columns.    */
//  This will only send update if it's necessary. 
// disallow subqueries which HIVE doesn't currently support 
//  If it went through correctly, then s is also a HCatRecord,   and also equal to the above, and a deepcopy, and this holds   through for multiple levels more of serialization as well. 
//  Shuffle write metrics. 
// txn X write to bucket1   txn X + 1 write to bucket0 + bucket1 
//  run this rule at later stages, since many calcite rules cant deal with semijoin 
// Take a row from the current buffered batch 
/*  * The general idea here is to create * /created/1 * /created/2 * /created/3 .... * for each job submitted.  The node number is generated by ZK (PERSISTENT_SEQUENTIAL) and the  * payload is the JobId. Basically this keeps track of the order in which jobs were submitted, * and ZooKeeperCleanup uses this to purge old job info. * Since the /jobs/<id> node has a create/update timestamp  * (http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#sc_zkStatStructure) this whole * thing can be removed. */
//  LOG.debug(CLASS_NAME + " processOp all " + displayBytes(keyBytes, 0, keyLength)); 
//  Incase the join has extra keys other than bucketed columns, partition keys need to be updated   on small table(s). 
//  Values within the column type bounds. 
//  Init output object inspectors.     The return type for a partial aggregation is still a list of doubles, as in   GenericUDAFHistogramNumeric, but we add on the percentile values requested to the   end, and handle serializing/deserializing before we pass things on to the parent   method.   The return type for FINAL and COMPLETE is a full aggregation result, which is also 
//  (all partitions and all but default) 
//  ResultSet#next blocks until the async query is complete 
//  green and red qualify 
//  create map join task and set big table as bigTablePosition 
//  String variable 
//  and remind ourselves to perform incremental normalization 
//  JAAS login from ticket cache to setup the client UserGroupInformation 
//  value. Therefore it is always LogicalCorrelate's left input which is outer query 
//  createDirectories creates all non-existent parent directories 
//  Log a warning if hive-default.xml is found on the classpath 
// add post script 
//  number of output columns   array of path expressions, each of which corresponds to a column   array of returned column values  object pool of non-null Text, avoid creating objects all the time 
//  We are now on some intersecting buffer, find the first intersecting buffer. 
//  replace stdout and run command 
//  Set query timeout to 1 second 
//  Reject any clauses that are against a column that isn't the rowId mapping or indexed 
//  Do the per-batch setup for an inner join. 
//  Swap column vectors to simulate change in data 
//  For negative number, initializing bits to 1 
//  put it into the cache BEFORE it is initialized to make sure we can catch 
//  If the fourth parameter -- precision factor 'pf' -- has been specified, make sure it's 
//  Splits are not shared across different partitions with different input formats. 
//  If the database is newer than the drop event, then noop it. 
//  test set and append methods 
//  Get the positions for sorted and bucketed columns   For sorted columns, also get the order (ascending/descending) - that should   also match for this to be converted to a map-only job.   Get the positions for sorted and bucketed columns   For sorted columns, also get the order (ascending/descending) - that should 
//  done! 
//  all other exceptions are considered as emanating from   unauthorized accesses 
//  LocalClientProtocolProvider expects both parameters to be 'local'. 
//  If location is specified - ensure that it is a full qualified name 
//  COMMENT 
//  Always allow the operation if it is not in replication scope. 
//  need to add this branch to the key + value info 
//  b.c 
//  Implement vectorized function Hex(string) returning string 
//  Move on to the next bit vector 
//  Run without cascade 
//  bucketing isn't consistent or there are >1 bucket columns   optimizer does not extract multiple column predicates for this 
//  Debug display. 
//  The number of reducer(s) should be used for those bottom layer ReduceSinkOperators 
//  Copy all the histogram bins from us and 'other' into an overstuffed histogram 
//  View already exists, thus we should be replacing 
//  Larger than max compile duration used in test 
//  Delete any tables other than the source tables 
//  Pass the row though the operator tree. It is guaranteed that not more than 1 row can   be produced from a input row. 
//  First write chunk length 
//  TODO: ExprNodeDesc is an expression tree, we could just use that and be rid of Filter.g. 
//  NULL 0   NULL 1 
//  The update has failed; we'd try with another candidate first, but only at the same priority. 
//  Metrics are first dumped to a temp file which is then renamed to the destination 
//  This is a mapping of the values in the small table hash table that will be copied to the   small table result portion of the output.  That is, a mapping of the LazyBinary field order 
// 1st and only stmt in implicit txn and uses acid resource 
//  new input column numbers 
//  check if it is a reduce vertex and its children is more than 1;   check if all the child ops are reduce output operators 
//  High word must be zero.  Check for overflow digits in middle word. 
/*    * Returns job status for list of input jobs as a list.    */
/*            * Single-Column String outer null detection.            */
//  6. Build Rel for OB Clause 
//  all rows from both side will be present in resultset 
//  The position in the column keys of the dummy grouping set id column. 
//  Essentially, partition values are represented as strings, but we want the actual object type associated 
//  Txn was committed (but notification was not received) or it was aborted.   Either case, we can clean it up 
//  using metrics.  Thus we should always check whether this is non-null before using. 
//  16 without HIVE-19588, 8 with HIVE-19588 
//  Just move the marker according to delta.   We hit the limit - the list was exhausted. 
//  1. If WM is not present just go to unmanaged. 
//  Method that provides similar filter functionality to filter-holder above, useful when 
//  (Otherwise, sub-directories produced by Hive UNION operations won't be readable.) 
//  The current* members of deserializeRead have the field value. 
//  We do not try to finish and flush an in-progress group because correct values require the   last group batch. 
//  set one null value for possible later use 
//  TODO: move into the if below; account for release call 
//     runStatementOnDriver("delete from " + Table.ACIDTBL + " where a in(select a from " + Table.NONACIDORCTBL + ")"); 
//  to use as the move operation that created it is atomic. 
// This will work only if the files are local files on webhcat server   (which is not very useful since users might not have access to that file system).  This is likely the HIVE-5188 issue 
// truncate on external table is not allowed 
//  sleep twice the TTL interval - things should have been cleaned by then. 
//  Get the key column names for each side of the join,   and check if the keys are all constants 
//  set resolver and resolver context 
//  GSS name for server 
//  See comments for next method. 
//  we can generate multi-phase aggregates   one distinct aggregate   no filter   sum/min/max/count in non-distinct aggregate   one or more non-distinct aggregates 
//  if password is available in url it needs to be striped 
/*  Spot check correctness of decimal column subtract decimal scalar. The case for   * addition checks all the cases for the template, so don't do that redundantly here.    */
//  Need to get a Connector so we look up the user's authorizations if not otherwise specified 
//  otherwise we find the row groups that were selected on the client 
//        + "are not unique keys for " 
//  is enabled. 
//  For expr "*", aliases should be iterated in the order they are specified   in the query. 
//  override this method to forward its outputs 
//    Implementation notes.     1. Since only local file systems are supported, there is no need to use Hadoop      version of Path class.   2. java.nio package provides modern implementation of file and directory operations      which is better then the traditional java.io, so we are using it here.      In particular, it supports atomic creation of temporary files with specified      permissions in the specified directory. This also avoids various attacks possible      when temp file name is generated first, followed by file creation.      See http://www.oracle.com/technetwork/articles/javase/nio-139333.html for      the description of NIO API and      http://docs.oracle.com/javase/tutorial/essential/io/legacy.html for the      description of interoperability between legacy IO api vs NIO API.   3. To avoid race conditions with readers of the metrics file, the implementation      dumps metrics to a temporary file in the same directory as the actual metrics      file and then renames it to the destination. Since both are located on the same      filesystem, this rename is likely to be atomic (as long as the underlying OS      support atomic renames.     NOTE: This reporter is very similar to         org.apache.hadoop.hive.metastore.metrics.JsonReporter.         It would be good to unify the two.   
//  Ok, try the UDF. 
//  PRINCIPAL_NAME 
//  disallow udfs that can potentially allow untrusted code execution 
// $NON-NLS-1$ 
//  Task Hash Map 
//  Perform any value expressions.  Results will go into scratch columns. 
//  This is akin to CBO cumulative cardinality model 
//  Child #1 is the IF boolean expression. 
//  if all dp columns got constant folded then disable this optimization 
//  The worker should remove the subdir for aborted transaction 
//  Verify query result 
// Check the union field's length and offset 
//  test non-vectorized, non-acid, combine 
//  close + abort 
//  tez blurs the boundary between map and reduce, thus it has it's own config 
//  FILES_ADDED_CHECKSUM 
//  number of rows for the key in the given table 
//  WORKERID 
//  for whatever failure reason including that trash has lower encryption zone   retry with force delete 
// if acid write, add to plan output, else don't bother 
//  56 bits minus 1 byte. 
//  verify that driver fails due to older version schema 
/*  Count of number of false values seen so far  */
//  alter table with invalid column type 
//  Get full objects. For Oracle/etc. do it in batches. 
//  CAST AS INT does not work as expected (ID is still considered as STRING in ORDER BY for some reason) 
//  Test that existing shared_read partition with new shared_read coalesces to 
//  We will start a new key group. We can call flush the buffer   of children from lastChildIndex (inclusive) to the last child and   propagate processGroup to those children. 
//  TODO HIVE-14042. Propagating abort to dummyOps. 
//  A is maxed out on capacity, so this move should fail the session 
//  HiveServer2 auth configuration 
//  for use by Column-Scalar and Scalar-Column arithmetic for null propagation 
//  DB_NAME 
//  Mapping from constraint name to list of foreign keys 
//  remove any existing entries that are contained by the new one 
/*      * If OrcSplit.isAcid returns true, we know for sure it is ACID.      */
// if here, ther were no locks that blocked any locks in 'locksBeingChecked' - acquire them all 
/*  256 files x 100 size for 11 splits  */
//  If there are no inputs; the Execution engine skips the operator tree.   To prevent it from happening; an opaque  ZeroRows input is added here - when needed. 
//  sp column names   dp column names   default partition name in case of null or empty value   maximum dynamic partitions created per mapper/reducer 
//  Spark memory per task, and total number of cores 
//  We will always release copies at the end. 
//  schema name 
//  specifies db and pattern 
//  There should be only 1 directory left: base_0000001. 
//  Create the row resolver for this operator from the output columns 
//  Create the db 
//  The selected array is already filled in as we want it. 
/*      * The sign is encoded as the least significant bit.     *     * We need to adjust our decimal before conversion to binary.     *     * Positive:     *   Multiply by 2.     *     * Negative:     *   Logic in SerializationUtils.writeBigInteger does a negate on the BigInteger. We     *   do not have to since FastHiveDecimal stores the numbers unsigned in fast0, fast1,     *   and fast2.  We do need to subtract one though.     *     *   And then multiply by 2 and add in the 1 sign bit.     *     *   CONSIDER: This could be combined.      */
//  Metastore SSL settings 
//  An optional group containing a repeated anonymous group "map", containing 
//  Set Checkpoint task as dependant to create table task. So, if same dump is retried for 
//  Set reducer parallelism 
/*        Below are the properties that need to be set based on what database this test is going to be run      */
//  Keep track of S[0..x] 
//  Test that committing unlocks 
//  For backwards compatibility reasons we honor the older   HIVEMAPJOINBUCKETCACHESIZE if set different from default. 
//  tinyint   smallint   int   bigint   double   float   string   string   struct   array   map   bool   complex  decimal(5,2)  char(10)  varchar(20)  date  timestamp  binary 
/*    * Gets file data for particular offsets. The range list is modified in place; it is then   * returned (since the list head could have changed). Ranges are replaced with cached ranges.   * In case of partial overlap with cached data, full cache blocks are always returned;   * there's no capacity for partial matches in return type. The rules are as follows:   * 1) If the requested range starts in the middle of a cached range, that cached range will not   *    be returned by default (e.g. if [100,200) and [200,300) are cached, the request for   *    [150,300) will only return [200,300) from cache). This may be configurable in impls.   *    This is because we assume well-known range start offsets are used (rg/stripe offsets), so   *    a request from the middle of the start doesn't make sense.   * 2) If the requested range ends in the middle of a cached range, that entire cached range will   *    be returned (e.g. if [100,200) and [200,300) are cached, the request for [100,250) will   *    return both ranges). It should really be same as #1, however currently ORC uses estimated   *    end offsets; we do in fact know in such cases that partially-matched cached block (rg)   *    can be thrown away, the reader will never touch it; but we need code in the reader to   *    handle such cases to avoid disk reads for these "tails" vs real unmatched ranges.   *    Some sort of InvalidCacheChunk could be placed to avoid them. TODO   * @param base base offset for the ranges (stripe/stream offset in case of ORC).    */
//  simple division.   Yes, we should do something like this:   http://www.hackersdelight.org/divcMore.pdf   but later... (anyway this will be eventually replaced by   intrinsics in Java 8) 
// ---------------------------------------------------------------------------   Process Single-Column String Inner Join on a vectorized row batch.   
/*        *  Hashing the string is potentially expensive so is better to branch.       *  Additionally not looking at values for nulls allows us not reset the values.        */
//  Create estimators 
//  the stage that this vertex belongs to 
//  Re-enable the node. If a task succeeded, a slot may have become available.   Also reset commFailures since a task was able to communicate back and indicate success. 
//  Don't use streaming for distinct cases 
//  only persist input/output format to metadata when it is explicitly specified. 
//  Some mix tests (STRING, CHAR), (VARCHAR, CHAR), (VARCHAR, STRING)... 
/*      * We need to use a cleanup interval, which is how often the cleanup thread will kick in     * and go do a check to see if any of the connections can be expired. We don't want to     * do this too often, because it'd be like having a mini-GC going off every so often,     * so we limit it to a minimum of DEFAULT_HIVE_CACHE_EXPIRY_TIME_SECONDS. If the client     * has explicitly set a larger timeout on the cache, though, we respect that, and use that      */
//  avoid adding group by for correlated IN/EXISTS queries   since this is rewritting into semijoin 
//  insert new filter between RS and parent of RS 
//  unknown | F | unknown 
//  not from this 
//  See comments for sqlPrecision. 
//  Insert the select operator in between. 
//  Make sure reuse changes the FIFO order of the session. 
//  The first line check should handle the OOB. 
// get the bucket id 
//  Set the last query time 
//  for output rows of this operator 
//  Already used. Update the port and retry. 
//  Delete all things. 
/* the silly looking call to Builder below is to get the default value of session timeout    from Curator which itself exposes it as system property */
/*    * We allow CTE definitions in views. So we can end up with a hierarchy of CTE definitions:   * - at the top level of a query statement   * - where a view is referenced.   * - views may refer to other views.   *   * The scoping rules we use are: to search for a CTE from the current QB outwards. In order to   * disambiguate between CTES are different levels we qualify(prefix) them with the id of the QB   * they appear in when adding them to the <code>aliasToCTEs</code> map.   *    */
//  Keep partKeyValMap in synch as well. 
//  partSpec is a mapping from partition column name to its value. 
//  VALID_TXN_LIST 
// Testing substring index starting with 0 and length equal to array length 
//  init input object inspectors 
//  Second, check if this work has multiple reduceSinks. If so, do split. 
//  TODO enable MetadataFilter by using readFooter(Configuration configuration, Path file,   MetadataFilter filter) API 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getArray(java.lang.String)    */
//  Return the settable equivalent object inspector for primitive categories   For eg: for table T containing partitions p1 and p2 (possibly different   from the table T), return the settable inspector for T. The inspector for 
//  parsing the keys and values one by one 
//  Reduce side of optimized plan) 
//  set command is currently not authorized through the API 
//  throw a HiveException if the table/partition is bucketized 
/*  100 files x 1000 size for 9 splits  */
//  Note: after this, the caller MUST send the downgrade message to downgradedTask         (outside of the writeLock, preferably), before exiting. 
//  ERRORS 
//  we're the only node, just clear out the expression 
//  user explicitly specified queue name 
//  Obtain inspector for schema 
//  get the size of cache AFTER 
//  restore 
//  Byte values. 
//  Now, test again with a seed. 
//  EXPRS 
//  skip the first, which is always required 
//  Convert millis result back to days 
//  indexColumnVector includes the keys of Map 
//  JAVA64_OBJECT + PRIMITIVES1 * 2 + JAVA64_ARRAY; 
//  Populate the names and order of columns for the first table 
//  Input files can be pruned 
//  Please take a look at the instructions in HiveAuthorizer.java before 
//  Allowed operations:   IntervalYearMonth + IntervalYearMonth = IntervalYearMonth   IntervalYearMonth + Date = Date (operands reversible)   IntervalYearMonth + Timestamp = Timestamp (operands reversible)   IntervalDayTime + IntervalDayTime = IntervalDayTime   IntervalDayTime + Date = Timestamp (operands reversible)   IntervalDayTime + Timestamp = Timestamp (operands reversible) 
//  try using 2nd permanent function and verify its only 2nd one that shows up 
//  The join task is converted to a mapjoin task. This can only happen if   hive.auto.convert.join.noconditionaltask is set to true. No conditional task was 
//  TODO HIVE-10236 Report a fatal error over the umbilical 
//  alias to filter mapping 
//  inner Map 
//  5. Extract rest of join predicate info. We infer the rest of join condition      that will be added to the filters (join conditions that are not part of 
//  The expiring session may or may not be in the pool. 
/*          * Hive treats names that start with '_c' as internalNames; so change         * the names so we don't run into this issue when converting back to         * Hive AST.          */
//  We do not proactively remove locked items from the heap, and opportunistically try to   remove from the list (since eviction is mostly from the list). If eviction stumbles upon   a locked item in either, it will remove it from cache; when we unlock, we are going to   put it back or update it, depending on whether this has happened. This should cause   most of the expensive cache update work to happen in unlock, not blocking processing. 
//  for jackson 
//  Don't remove the old entry - in SparkPartitionPruningSink it still   refers to the old TS, and we need to lookup it later in   processPartitionPruningSink. 
//  remove the pwd from conf file so that job tracker doesn't show this   logs 
//  add to n-gram estimation only if the context matches 
//  routines can't be used directly. 
//  partition columns 
//  when type is not set, init hasn't been called yet 
//  Don't wait for the thread. 
//  create reduce 
//  can happen if this operator does not carry forward the previous bucketing columns   for e.g. another join operator which does not carry one of the sides' key columns 
//  Read lock for get operation 
//  All the fields of this event are final, so no reason to create a new one for each   listener 
//  The SIMD optimized form of "a < b" is "(a - b) >>> 63" 
//  Validation's been done at compile time. no validation is needed here. 
//  single double value 
//  TABLE_DB 
//  check projRel only projects one expression   check this project only projects one expression, i.e. scalar 
//  Our reading is positioned to the key. 
//  Get the MapredLocalWork 
/*      * Restriction.13.m :: In the case of an implied Group By on a     * correlated SubQuery, the SubQuery always returns 1 row.     * An exists on a SubQuery with an implied GBy will always return true.     * Whereas Algebraically transforming to a Join may not return true. See     * Specification doc for details.     * Similarly a not exists on a SubQuery with a implied GBY will always return false.      */
//  'n' columns where 'n' is the length of the bucketed columns. 
//  A config variable set via a System Property, a config variable set in the CLI,   a config variable not in the default List of config variables, and a config variable in the   default list of config variables, but which has not been overridden 
//  At this point we are done processing the input. Close the record processor 
//  Init output object inspectors.     The return type for a partial aggregation is still a list of doubles, as in   GenericUDAFHistogramNumeric, but we add on the percentile values requested to the   end, and handle serializing/deserializing before we pass things on to the parent   method.   The return type for FINAL and COMPLETE is a full aggregation result, which is a 
//  a list of columns names and maps them to 0..n-1 indices. 
//  3. Perform a major compaction. Nothing should change.   Both deltas and base dirs should have the same name. 
//  This is the vectorized row batch description of the output of the native vectorized map join   operator.  It is based on the incoming vectorization context.  Its projection may include 
// having a non null create table grants privileges causes problems in   the tests that compares underlying thrift Table object of created   table with a table object that was fetched from metastore.   This is because the fetch does not populate the privileges field in Table 
//  TODO: the below assumes that all the arguments to IN are of the same type; 
//  cancel the delegation token 
//  if all drones where abandoned on a host, try replacing them. 
//  HBase API is convoluted. 
//  There's always just one file that we have merged.   The union/DP/etc. should already be account for in the path. 
//  Tell our super class FilterStringColumnInList it will be evaluating our scratch   BytesColumnVector. 
//  Verify that field changes are consistent with what Hive does. Note: we could handle this. 
//  Finalize the previous record. 
//  find the firing rule   find the rule from the stack specified 
//  Following is the main loop which iterates through all the cookies send by the client.   The HS2 generated cookies are of the format hive.server2.auth=<value>   A cookie which is identified as a hiveserver2 generated cookie is validated   by calling signer.verifyAndExtract(). If the validation passes, send the   username for which the cookie is validated to the caller. If no client side 
//  currently getImportedKeys always returns an empty resultset for Hive 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getColumns(org.apache.hive.service.cli.SessionHandle)    */
//  fallthrough 
//  special case for handling false constants 
//  UNDONE: Until Fast BinarySortable supports complex types -- disable. 
//  update non-distinct key aggregations : "KEY._colx:t._coly" 
//  Finally exclude preds that are already in the subtree as given by the metadata provider   Note: this is the last step, trying to avoid the expensive call to the metadata provider 
//  Lookup the state in the map. 
//  Changing string value; getCharacterLength() should update accordingly 
//  CREATE_TABLE EVENT on partitioned table 
//  create a file in the folder to mark it 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#commit()    */
//  We remove the discardable RS operator 
// for jackson to instantiate 
//  We need to use the OpParseContext of the child SelectOperator to replace the 
//  Free up any previously allocated buffers that are referenced by vector 
//  First apply configuration applicable to both Hive Cli and HiveServer2   Not adding any authorization related restrictions to hive cli   grant all privileges for table to its owner - set this in cli as well so that owner   has permissions via HiveServer2 as well. 
//  generate n-grams wherever the context matches 
//  First just check that this translates 
//  this method contains various asserts to warn if environment variables are in a buggy state 
//  Convert skewed table to non-skewed table. 
//  OK, fall through. 
//  Aggr checks for sorted argList. 
//  Executed if relevant, and used to contain all the other details about the table if not. 
//  metastore and so some partitions may have no data based on other filters. 
//  CREATE TABLE AS SELECT statement 
//  launch a tez job 
//  no-op 
//  A mapper can span multiple files/partitions.   The serializers need to be reset if the input file changed 
//  Need to test this with LLAP settings, which requires some additional configurations set. 
//  Check single column for repeating. 
//  this table is not good to be a big table. 
//  an individual column becomes a STRING 
//  Now, we load data into the tables, and see if an incremental   repl drop/load can duplicate it. 
//  UNDONE: Performance problem with conversion to String, then bytes... 
//  0 1 
//  large to fit into main memory. 
//  after deserialization, need to recreate the txnToWriteIdList as its not under JsonProperty. 
/*  * Comment from BooleanWritable evaluate(DateWritable d) *     // date value to boolean doesn't make any sense. * So, we always set the output to NULL.  */
//  if a DPP is with MJ, the tree won't be split and so we don't have to remove it 
//  1) Don't call postRead - we will have checked everything here.   2) Ignore GenericUDFBridge, it's checked separately in LlapUdfBridgeChecker.   Run post-hook. 
//  this will abort initializeOp() 
//  skip the first node, which is always required 
//  View DDL 
/*    * The length of each field. If the value repeats for every entry, then it is stored   * in vector[0] and isRepeating from the superclass is set to true.    */
//  Following remote job may refer to classes in this jar, and the remote job would be executed   in a different thread, so we add this jar path to JobContext for further usage. 
// failed 
//  Add new residual preds 
//  3. We will extract the literals to introduce in the IN clause. 
// w/o order by likely currently running compactions will be first (LHS of Union) 
//  Read aggregated stats for one column 
//  NEW TAI LUE LETTER LOW XA U+1986 (3 bytes) 
//  Test basic truncate to vector. 
//  0 0 
//  We start with an empty wordlist and build it up 
//  key index to nullsafe join flag 
//  adjustment array 
//  add constant object overhead for union 
//  2019-01-02 00:00:00 GMT is 1546387200000 milliseconds after epoch 
//  Check if the source file unmodified even after copy to see if we copied the right file 
//  META_INFO 
//  4. Run Cleaner. It should remove the 2 delta dirs. 
//  We have some disk data. Separate it by column chunk and put into cache. 
//  Create a list of top op nodes 
//  There should be no rows in the delete_delta because there have been no delete events. 
//  Count. 
//  test mode in hive mode 
// variable access should not be done and use exportRootDir() instead. 
//  This means that the lock is being committed at this instant, hence   the cleaner should not remove it even if it times out. If transaction 
//  It is fired only once: on the original node 
//  We have the UDF, or we are in the session registry (or both). 
//  Partition path can be null in the case of a new create partition - in this case,   we try to default to checking the permissions of the parent table.   Partition itself can also be null, in cases where this gets called as a generic 
//  Not synchronizing creation of mapOp with an invocation. Check immediately   after creation in case abort has been set.   Relying on the regular flow to clean up the actual operator. i.e. If an exception is   thrown, an attempt will be made to cleanup the op.   If we are here - exit out via an exception. If we're in the middle of the opeartor.initialize 
// this is NULL for minor compaction  it may also be null if there is no base - only deltas 
//  FOREIGN_SCHEMA_NAME 
//  Replicate all the events happened so far. It should fail as the data files missing in 
//  Remove default partition from partition names and get aggregate stats again 
//            refs 
//  Since the input 
//  we are using oracle schema because it is simpler to parse, no quotes or backticks etc 
//  If ckeys or pkeys have constant node expressions avoid the merge. 
//  TopN query results as records 
//  Schedule Major compaction on all the partitions/table to clean aborted data 
//  Iterate over each group of subqueries with the same group by/distinct keys 
//  take one's complement' 
//  If the driver context has been shutdown (due to query cancellation) kill the Spark job 
//  99999999.99999999999999999999999999999949999 
//  Tests wait queue behaviour for fragments which have reported to the AM, but have not given up their executor slot. 
//  / XXX: From o.a.zk.t.ClientBase 
//  If we're updating, add the ROW__ID expression, then make the following column accesses   offset by 1 so that we don't try to convert the ROW__ID 
//  does the query have a using clause 
//  Now, insert this record into the list. 
//  Serialize the rest of the values in the AggBuffer 
//  This block exists for debugging purposes: we want to check whether   the col stats cache is working properly and we are retrieving the 
//  1 means asc, could really use enum here in the thrift if 
//  TODO: For now only support sampling on up to two columns   Need to change it to list of columns 
//  errored 
//  Error getting children of node -- probably node has been deleted 
//  3. Download resources dir 
//  token was not a storage format token 
//  Argh, HCatRecord doesnt implement equals() 
//  x events to insert, last repl ID: replDumpId+3x 
//  add the move task 
//  All small writes to the first buffer should be in contiguous memory 
// here if start of explicit txn 
//  output OI should have varchar type params 
//     Table tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, likeTbl);      assertEquals(likeTbl,tbl.getTableName());      List<FieldSchema> cols = tbl.getSd().getCols();      assertEquals(1, cols.size());      assertEquals(new FieldSchema("a", "int", null), cols.get(0));      assertEquals("org.apache.hadoop.hive.ql.io.RCFileInputFormat",tbl.getSd().getInputFormat());      assertEquals("org.apache.hadoop.hive.ql.io.RCFileOutputFormat",tbl.getSd().getOutputFormat());      Map<String, String> tblParams = tbl.getParameters();      assertEquals("org.apache.hadoop.hive.hcat.rcfile.RCFileInputStorageDriver", tblParams.get("hcat.isd"));      assertEquals("org.apache.hadoop.hive.hcat.rcfile.RCFileOutputStorageDriver", tblParams.get("hcat.osd"));        hcatDriver.run("drop table junit_sem_analysis");      hcatDriver.run("drop table "+likeTbl); 
//  the serdes for the partition columns 
//  For example,      SELECT deptno, COUNT(DISTINCT sal), SUM(DISTINCT sal)      FROM emp      GROUP BY deptno     becomes        SELECT deptno, COUNT(distinct_sal), SUM(distinct_sal)      FROM (        SELECT DISTINCT deptno, sal AS distinct_sal        FROM EMP GROUP BY deptno)      GROUP BY deptno 
//  unsupported conversion 
//  Assign ids to all vertices,   targets at first, then sources. 
//  Values needed for numeric arithmetic UDFs 
//  Size of string buffer in bytes 
//  4. Gen RS 
//  Load the column stats and table params with 30 TB scale 
//  and retry 
/*    * @return A new hash multi-set result implementation specific object.   *   * The object can be used to access the *count* of values when the key is contained in the   * multi-set, or access spill information when the partition with the key is currently spilled.    */
//  Handle type casts that may contain type parameters 
//  instance of shared utils 
//  INTEGER_BITMASK 
//  No specifications default to UTF8 String storage for backwards compatibility 
//  test using loadTableWork 
//  2nd condition occurs when the input has 0 rows (possible due to   filtering, joins etc). 
//  1. Create column schema 
// Map column number to type (this is always non-null for a useful vec context) 
//  This is not a config that users set in hive-site. It's only use is to share information   between the java component of the service driver and the python component. 
/*  * breakup the original WindowingSpec into a set of WindowingSpecs. * Each WindowingSpec is executed in an instance of PTFOperator, * preceded by ReduceSink and Extract. * The logic to componentize is straightforward: * - distribute Window Fn. Specs from original Window Spec into a set of WindowSpecs, *   based on their Partitioning. * - A Group of WindowSpecs, is a subset of the Window Fn Invocations in the QueryBlock that *   have the same Partitioning(Partition + Order spec). * - Each Group is put in a new WindowingSpec and is evaluated in its own PTFOperator instance. * - the order of computation is then inferred based on the dependency between Groups. *   If 2 groups have the same dependency, then the Group with the function that is *   earliest in the SelectList is executed first.  */
//  Example handling of dirs with a .   shims/hadoop-2.6     -> moduleName=shims.hadoop-.2.6 
//  used for the analyze command (statistics)   used for the analyze command (statistics) (noscan) 
//  reset rowcontainer's serde, objectinspector, and tableDesc. 
//  Find the closest pair of bins in terms of x coordinates. Break ties randomly. 
//  Reopening (close + open) allowed in opened state: 
//  no NOT keyword 
//  write to file 
//  for local directory - we always write to map-red intermediate   store and then copy to local fs 
//  The source table 
//  Prepare future cache buffer. 
// Since the user running the test belongs to the group  obtained above the call to getDelegationTokenStr will succeed 
//        See setLoadFileType and setIsAcidIow calls elsewhere for an example. 
//  True if this statement creates or replaces a materialized view 
/*  * An single LONG key hash multi-set optimized for vector map join.  */
//  Concurrent force-eviction - ignore. 
//  Lost the duck. 
/*            * last_value: when an Sort Key is specified, then last_value should return the           * last value among rows with the same Sort Key value.            */
//  Break if iteration times out 
//  Cleanup the scratch dir before starting 
//  size for each input alias. 
//  This records how many rows have been inserted or deleted.  It is separate from insertedRows 
//  Reader-specific incompatibility like SMB or schema evolution. 
// this is not used for DELETE commands (partitionEval is not set up correctly   (or needed) for that 
//  If there's no lock, we don't need to do heartbeat   Start heartbeat for read-only queries which don't open transactions but requires locks.   For those that require transactions, the heartbeat has already been started in openTxn. 
//  Since there is no concept of a group, we don't invoke   startGroup/endGroup for a mapper 
// This implementation completely and exhaustively reverses the addGauge method above. 
//  @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.hooks.proto.HiveHookEventProto) 
//  Row format (SerDe) 
//  Is smbJoin possible? We need correct bucketing 
//  revert partSpecToFileMapping to inputToPartSpecMapping 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#clearBatch()    */
//  that SemanticAnalyzer finds are in use 
//  double scalar/column IF 
//  only create bucket files only if no dynamic partitions,   buckets of dynamic partitions will be created for each newly created partition  todo IOW integration. Full Acid uses the else if block to create Acid's RecordUpdater (HiveFileFormatUtils)   and that will set writingBase(conf.getInsertOverwrite()) 
//  Drop the table if it exists 
//  Note: we assume that workers run on the same threads repeatedly, so we can set up         the session here and it will be reused without explicitly storing in the worker. 
/*  1 files x 100 size for 11 splits  */
//  no metrics gathered by default 
/*  (non-Javadoc)   * @see org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer#getHiveAuthorizationTranslator()    */
//  Parameters used for JMX 
//  We have to create a new object, because the object o belongs   to the code that creates it and may get its value changed. 
//  Preserve operator before the GBY - we'll use it to resolve '*' 
//  Determine minimum of all non-null long column values; maintain isGroupResultNull. 
//  Checks whether txn list has been invalidated while planning the query.   This would happen if query requires exclusive/semi-shared lock, and there   has been a committed transaction on the table over which the lock is 
//  hashMap += JAVA64_OBJECT 
//  isDynamicFunction used to indicate the function is not deterministic between queries. 
//  Date + day-time interval = timestamp 
//  This semijoin optimization should be removed. Do it after we're done iterating 
//  For now, just small table values... 
//  if column statistics for a column is already found then merge the statistics 
// if here it maybe compaction or regular read or Delete event sorter  in the later 2 cases we should do:  HIVE-17320: we should compute a SARG to push down min/max key to delete_delta 
//  commit the changes 
//  or if this is the last key-value pair 
// ---------------------------------------------------------------------------   Single-Column String specific members.   
//  FMSketch treats the ndv of all nulls as 1 but hll treates the ndv as 0.   In order to get rid of divide by 0 problem, we follow FMSketch 
//  Allocate write ids for both tables t1 and t2 for all txns 
//  set the number of instances on which llap should run 
//  already registered 
//  Clarification of terms:   - The originalDir directory represents the original directory of the     partitions' files. They now contain an archived version of those files     eg. hdfs:/warehouse/myTable/ds=1/   - The source directory is the directory containing all the files that     should be in the partitions. e.g. har:/warehouse/myTable/ds=1/myTable.har/     Note the har:/ scheme 
//  if nothing matches, try creating a custom counter 
//  when it executes a non-ResultSet query (like create) 
//  The following method introduces a cast if x or y is DECIMAL_64 and parent expression (x % y) is DECIMAL. 
//  set to true 
//  Create the SerDe 
//  check whether operation log file is deleted. 
//  this might seem counter intuitive but some queries like query   SELECT YEAR(Calcs.date0) AS yr_date0_ok FROM druid_tableau.calcs Calcs WHERE (YEAR(Calcs.date0) IS NULL)   LIMIT 1   is planed in a way where we only push a filter down and keep the project of null as hive project. Thus empty   columns 
//  If we are getting the resources externally, don't relocalize anything. 
//  negative numbers, flip all bits 
/*      * Fallback for the case when OrcSplit flags do not contain hasBase and deltas      */
//  TABLE_TYPE 
// small table file size 
//  optional int32 underscore_int = 3; 
//  Make this method final to improve performance. 
//  ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan; 
//  sub-tree). 
//  workaround for DN bug in persisting nulls in pg bytea column   instead set empty bit vector with header. 
//  1st pass at marking invalid candidates   Checks based on variance and TTL 
//  Package visible so that HMSMetricsListener can see them. 
//  Add granularity to the row schema 
//  5% tolerance for long range bias and 3% for short range bias 
//  long scalar/scalar IF 
// In the case of truncate table, we set the stats to be 0. 
//  The divided-by-2 logic is consistent to MapJoinOperator.reloadHashTable 
//  @@protoc_insertion_point(class_scope:PurgeCacheResponseProto) 
//  Hive normalizes partition spec for dates to yyyy-mm-dd format. Some versions of Java will   accept other formats for Date.valueOf, e.g. yyyy-m-d, and who knows what else in the future;   some will not accept other formats, so we cannot test normalization with them - type check   will fail before it can ever happen. Thus, test in isolation. 
//  end while 
//  trigger failover on miniHS2_1 and make sure the connections are closed 
/*    * Uses DeleteDelegator to kill a job and ignores all exceptions.    */
/*  * A console SQL shell with command completion. * <p> * TODO: * <ul> * <li>User-friendly connection prompts</li> * <li>Page results</li> * <li>Handle binary data (blob fields)</li> * <li>Implement command aliases</li> * <li>Stored procedure execution</li> * <li>Binding parameters to prepared statements</li> * <li>Scripting language</li> * <li>XA transactions</li> * </ul> *  */
//  then this is considered like the metastore server case 
//  production is: string 
//  check for two way join 
//  Request :          "{\"queryType\":\"segmentMetadata\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},"          + "\"intervals\":{\"type\":\"intervals\","          + "\"intervals\":[\"-146136543-09-08T00:30:34.096-07:52:58/146140482-04-24T08:36:27.903-07:00\"]},"          + "\"toInclude\":{\"type\":\"all\"},\"merge\":true,\"context\":null,\"analysisTypes\":[],"          + "\"usingDefaultInterval\":true,\"lenientAggregatorMerge\":false,\"descending\":false}"; 
//  we need to keep the path part only because the Hadoop CombineFileInputFormat will   pass the path part only to accept().   Trailing the path with a separator to prevent partial matching. 
//  into the row-mode MapJoinKey 
//  now add pre-configured users to admin role 
//  Stats setup: 
//  Delete the data in the partitions which have other locations 
/*    * Generate the GroupByOperator for the Query Block (parseInfo.getXXX(dest)).   * The new GroupByOperator will be a child of the reduceSinkOperatorInfo.   *   * @param parseInfo   * @param dest   * @param reduceSinkOperatorInfo   * @param mode   *          The mode of the aggregation (MERGEPARTIAL, PARTIAL2)   * @param genericUDAFEvaluators   *          The mapping from Aggregation StringTree to the   *          genericUDAFEvaluator.   * @param groupingSets   *          list of grouping sets   * @param groupingSetsPresent   *          whether grouping sets are present in this query   * @param groupingSetsNeedAdditionalMRJob   *          whether grouping sets are consumed by this group by   * @return the new GroupByOperator    */
//  Commit each partition so it gets moved out of the job work   dir 
/*  Patterns that are excluded in verbose logging level.     * Filter out messages coming from log processing classes, or we'll run an infinite loop.      */
//  Set signum before; if result is zero, fastMultiply will set signum to 0. 
/*  16 > id or  */
// obtain delegation tokens for the job 
//  In case we are stuck in consume. 
//  compare the column names and the order with the first table/partition. 
// By the time we enter here the byteValues.lentgh and isNull must have already been compared 
//  It is enough to compare the last level sub-directory which has the name as event ID 
//  Request LLAP splits for a table. 
//  Set the catalog name if it hasn't been set in the new table 
// will fail 
//  POOL 
//  Construct list bucketing location mappings from sub-directory name. 
//  1. Generate Resolved Parse tree from syntax tree 
//  test the connection metastore using the config property 
//  Else return it as it is. 
//  Run Join releated optimizations 
//  4. Build Rel for GB Having Clause 
//  Need to initialize to 0 to make sure if nobody modified this table, then current txn   shouldn't read any data.   If there is a conversion from non-acid to acid table, then by default 0 would be assigned as 
/*    * A definite node is constructed from a specified number of children. That   * number of nodes are popped from the stack and made the children of the   * definite node. Then the definite node is pushed on to the stack.    */
//  initialize the converter 
//  create filters on top of each setop child, modifying the filter   condition to reference each setop child 
//  two VInt without nanos 
//  should not occur since second parameter to getTableWithQN is false 
//  subsequent requests 
//  Start the timer thread for cancelling the query when query timeout is reached 
//  Successfully scheduled. 
//  4) the cumulative cardinality is equal, but the size is bigger, 
//  We keep flushing until the memory is under threshold 
//  Since we merge multiple operation paths, we assign new tags to bottom layer   ReduceSinkOperators. This mapping is used to map new tags to original tags associated 
//  Found the proper columns. 
//  D6. Add back - probability is 2**(-31). R += D. Q[digit] -= 1 
//  @@protoc_insertion_point(builder_scope:PurgeCacheRequestProto) 
//  Copy credentials 
//  write out integer part first   then write out fractional part 
//  There are many ways to have AUX jars in Hive... sigh 
//  Make sure the new partition has the catalog value set 
//  Verify that the schema now within the configuration is the one passed 
//  Write the current set of valid transactions into the conf file 
//  read-only maps (initialized once) 
//  through Hive 
//  dummy HS2 instance just to trigger failover 
//  Expected events not updated yet - vertex SUCCESS notification not received. 
//  Nothing needs to be done. 
//  Now vary isRepeating   nulls possible only on right 
//  Get key element information 
//  Check the affinity of the argument passed in with the accepted argument,   based on the PrimitiveGrouping 
//  are any of the new transactions ones that we care about? 
//  back-check to force at least 1 output; and this should have a partial which is empty 
//  All the tables/partitions columns should be sorted in the same order   For example, if tables A and B are being joined on columns c1, c2 and c3   which are the sorted and bucketed columns. The join would work, as long   c1, c2 and c3 are sorted in the same order. 
//  If this is a trivial query block return 
//  No existing ranges, just accept the current 
//  UGI for the http/_HOST (SPNego) principal 
//  The real expression 
//  2. Perform a MINOR compaction. Since nothing was aborted, subdirs should stay. 
//  In vector mode, we store CHAR as unpadded. 
//  reduce only if the parameters are significant 
//  Tests that in the absence of stats for partitions, and/or absence of columns 
//  Only accept parse results if we parsed the entire string 
//  org.apache.hadoop.hive.ql.parse.MetaDataExportListener preevent listener 
//  Must be struct because List and Map need to be ParameterizedType 
//  can be boolean column in which case return true count 
//  init output object inspectors   The output of a partial aggregation is a list 
/*    * Test retries when InvocationException wrapped in MetaException wrapped in JDOException   * is thrown    */
//  initialize QB 
//  add spark job metrics. 
//  Only proceed if the THEN/ELSE types were aligned. 
//  this path points to a symlink path 
//  When we have repeating values, we can unset the whole bitset at once   if the repeating value is not a valid write id. 
//  if grantee is a role, check if it exists 
//  needed to avoid file name conflict when big table is partitioned 
//  If same return one of them 
//  Every 8 fields we write a NULL byte. 
//  Make sure the catalog name is set in the new partition 
//  Note that the code removes the data from the field as it's passed to the consumer,   so we expect to have stuff remaining in there only in case of errors. 
/*  allowRounding  */
//  Update the number of entries that can fit in the hash table 
//  Further, the DPP value needs to be generated from same subtree 
/*    * Returns true of thread pool is created and can be used for executing a job request.   * Otherwise, returns false.    */
//  change smallint and tinyint to int 
/*  * An single STRING key hash set optimized for vector map join. * * The key will be deserialized and just the bytes will be stored.  */
//  Initialize column buffers 
//  The dispatcher fires the processor corresponding to the closest 
//  The boolean predicate. 
//  validate is false by default if we disable the constraint 
//  Create and add AST node with position of grouping function input   in group by clause 
//  Simple case - CB fits entirely in the disk range. 
//  Cancel should be a no-op in either cases 
//  empty out side file 
//  F | T 
/*    * (non-Javadoc)   *   * @see org.apache.hadoop.hive.ql.lib.Node#getChildren()    */
//  We recursively create the exprNodeDesc. Base cases: when we encounter   a column ref, we convert that into an exprNodeColumnDesc; when we   encounter   a constant, we convert that into an exprNodeConstantDesc. For others we   just 
//  Need to reconnect 
//  If we have a valid batchIter and it has more elements, return them. 
//  Add a sort by clause so that the row ids come out in the correct order 
//  Drop one table, see what remains 
//  Add the semijoin branch to the map 
//  This version of Hadoop does not support HdfsAdmin.getEncryptionZoneForPath().   Hadoop 2.6.0 introduces this new method. 
//  to be a CONSTANT node with value to be the agreed result. 
//  Key definitions related to replication 
//  LOG.info(queryPlan); 
//  We disable web UI in tests unless the test is explicitly setting a   unique web ui port so that we don't mess up ptests. 
//  precision 9 
//  Note: Hive ops do not use the normal Future failure path, so this will not happen         in case of actual failure; the Future will just be done.   The background operation thread was aborted 
//  parse the schema file to determine the tables that are expected to exist 
//  We replace the path in the file status by the input path as Parquet   may use the path in the file status to open the file 
//  the output position should not change since there are no corVars 
//  Close the connection as soon as the error message is sent. 
//  LOG.info("First tail offset to create list record is " + firstTailOffset); 
//  Get the name of a file and look at its properties to see if orc.compress.size was respected. 
//  copy is a nop, so skip it--this is important for avoiding   spurious overlap assertions 
//  Enable or disable test scheduling control. 
//  Add HiveConf variable with 3 modes:     1) adaptor: Always use VectorUDFAdaptor for IF statements.       2) good: Vectorize but don't optimize conditional expressions       3) better: Vectorize and Optimize conditional expressions.   
//  but we should have already checked for that before this point. 
//  EVENT_TYPE 
//  update the target map work of the second 
//  Process --failover 
//  The threshold where we should use a repeating vectorized row batch optimization for 
//  4. Construct JoinPredicateInfo 
//  precision 8 
//  if table is already in there, don't recreate. 
//        RightInputRel 
//  never includes a join condition. The code was not modified for brevity. 
//  Desired schema does not include virtual columns or partition columns. 
//  create the dummy operators 
//  if user is not an admin user, allow the request only if the user is   requesting for privileges for themselves or a role they belong to 
//  two VInt with nanos 
//  Set the option for tolerating corruptions. The read should succeed. 
//  ss can be null in unit tests 
//  table is newer, leave it be. 
//  Unsecure case? 
//  Clean post-conditions 
//  Error out & exit - we were not able to parse the args successfully 
// Pig which uses HCat will pass this to HCat so that it can find the metastore 
//  Write some garbage to the buffer that should be erased 
/*  * When constructing the Evaluator Tree from an ExprNode Tree * - look for any descendant LeadLag Function Expressions * - if they are found: *   - add them to the LLInfo.leadLagExprs and *   - add a mapping from the Expr Tree root to the LLFunc Expr in LLInfo.mapTopExprToLLFunExprs  */
//  precision 7 
//  Suppress the STAGES if vectorization is off. 
//  For types with parameters (like varchar), we need to determine the type parameters   that should be added to this type, based on the original 2 TypeInfos. 
//  validate all tasks 
/*      * We always set the null flag to false when there is a value.      */
//  Update the queryId to use the generated applicationId. See comment below about   why this is done. 
// aid in testing 
//  Some value is set. 
// first see if it is gbkeys 
//  Import table t1 to C 
//  So, need not validate inpPartSpec here. 
//  which will cause the scheduler loop to continue. 
//  use partition level authorization 
//  The 'magic' bytes at the beginning of the RCFile 
//  Check if work has an explain annotation 
//  print attr 
//  change these parameters 
//  this should be database usage privilege once it is supported 
//  should honor the ordering of records provided by ORDER BY in SELECT statement 
//  exhausted the batch, no longer have to heartbeat 
//  lists of clauses. 
//  unlink connection between FS and its parent 
//  Fill the child of ListColumnVector with valueList 
//  If the line doesn't end with the delimiter or if the line is empty, add the cmd part 
//  Grab sign bit and shift it away. 
//  The HiveDecimalWritable set method will quickly copy the deserialized decimal writable fields. 
//  This is a test calling. 
//  Padding leading zeroes/ones. 
//  test stats deletion at partition level 
//  Create the list of serialized splits for each bucket. 
//  joins alias1, alias2, alias3 (alias1 was not eligible for big pos)   Must be deterministic order map for consistent q-test output across Java versions 
// Buffer one batch at a time, for row retrieval 
//  Test that existing shared_read db with new shared_read coalesces to 
//  First allocation of write id should add the table to the next_write_id meta table   The initial value for write id should be 1 and hence we add 1 with number of write ids allocated here 
//  there are 2 survivor spaces 
//  Offset to where this RG begins. 
//  This method implementation is preserved for backward compatibility. 
//  offer accepted and r4 gets evicted 
//  can't do much, expression is not in context of filter, so we can't treat null as equivalent to false here. 
//  plus any correlated variables the input wants to pass along. 
//  TOK_SKEWED_LOCATION_LIST 
//  Add struct field data to the Row 
//  priority = 10 / (200 - 100) = 0.01 
//  Short 
//  Log error if the acid table is missing from the ValidWriteIdList conf 
//  are associated with a transaction then heartbeat on that as well. 
//  set list task 
//  call process once we have a full batch 
//  found a unused port, exit 
//  Do not put the unused duck back; we'd run the tasks below, then assign it by priority. 
//  We merge filters from previous scan by ORing with filters from current scan 
// ensures txn is still there and in expected state 
//  We are only vectorizing Reduce under Tez/Spark. 
//  If partition is specified, get pruned partition list 
//  the constant value into it and add the struct as part of exprNodeStructs. 
// Environment Variables long values 
//  'or' nodes need to be merged 
//  Success! 
//  remove all TOK tokens 
/*      *  if there was a pre-existing work generated for the big-table mapjoin side,     *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)     *  with the pre-existing work.     *     *  Otherwise, we need to associate that the mapjoin op     *  to be linked to the RS work (associated with the RS-MJ pattern).     *      */
/*  * TODO:<br> * 1. Could we use combined RR instead of list of RR ?<br> * 2. Why not use GB expr ?  */
//  Error; default value 
//  if the size is unavailable, we need to assume a size 1 greater than   localTableTotalSizeLimit this implies that merge cannot happen   so we will return false. 
//  The only case where duplicate elements matter... the others are handled by the below. 
//  Create scratch columns to hold small table results. 
//  If the parsed statement contained a db.tablename specification, prefer that. 
//  Primitive Writable class? 
//  failed to infer PK-FK relationship for row count estimation fall-back on default logic   compute denominator  max(V(R,y1), V(S,y1)) * max(V(R,y2), V(S,y2))   in case of multi-attribute join 
//  Mandatory Property 
//  If CBO did not optimize the query, we might need to replace grouping function 
//  Check multi-set count. 
//  Secondary DropTableCommand test for testing repl-drop-tables' effect on partitions inside a partitioned table   when there exist partitions inside the table which are older than the drop event.   Our goal is this : Create a table t, with repl.last.id=157, say.   Create 2 partitions inside it, with repl.last.id=150 and 160, say.   Now, process a drop table command with eventid=155.   It should result in the table and the partition with repl.last.id=160 continuing to exist,   but dropping the partition with repl.last.id=150. 
//  do nothing by default 
//  it doesn't seem like you should have to do this, but java serialization is wacky, and doesn't call the default constructor. 
//  DP in the form of T   partition (ds, hr) 
//  stripe position within file 
//  server sets createtime 
//  Create KV result cache object, add one (k,v) pair and retrieve them. 
//  Threshold percentage to trigger the GC warning 
//  Convert to lowercase for the comparison 
//  Use the keytab if one was provided 
//  ^(TOK_CREATEFUNCTION identifier StringLiteral ({isTempFunction}? => TOK_TEMPORARY)) 
//  Two events: one for create db and other for drop db 
// directory to output results to 
//  Create GroupingID column 
//  print job stages. 
//  nulls and repeating 
//  complain about the deprecated syntax -- but still run 
/*  Get row size of small table  */
// check elements of the innermost array 
// delete the bucket files so now we have empty delta dirs 
//  Load next batch from disk 
//  We pretend to add trailing zeroes, EVEN WHEN it would exceed the HiveDecimal.MAX_PRECISION. 
//  Right now the work graph is pretty simple. If there is no   Preceding work we have a root and will generate a map   vertex. If there is a preceding work we will generate 
//  Create table in database without location clause 
//  Convert all NaN, and optionally infinity values in vector v to NULL. 
//  Have we already set the enabled conditions not met? 
//  at the table level and the storage handler level. 
// clear the username 
//  This is not needed beyond compilation, so it is transient. 
//  Initialize the result. 
//  not all argument elements need to hold true 
//  used by the framework at runtime. initialize is the real initializer at runtime 
//  SetOp Rel 
//  Commit the open txn, which lets the cleanup on TXN_TO_WRITE_ID. 
//  close any open readers, if there was some exception during initialization.   rethrow the exception so that the caller can handle. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#closeOperation(org.apache.hive.service.cli.OperationHandle)    */
//  Create default route where events go without queryId 
//  application-specific typecodes 
//  Prepare StringBuilder for "PART_ID in (...)" to use in future queries. 
//  The reason why we can get a list of split strategies here is because for ACID split-update   case when we have a mix of original base files & insert deltas, we will produce two   independent split strategies for them. There is a global flag 'isOriginal' that is set 
//  so this is set by DriverContext in runtime 
//  optional string requestUser = 5; 
//  extract the values. 
//  CONSIDER: For large n, fill n or all of isNull array and use the tighter ELSE loop. 
//  Chose the table descriptor if none of the partitions is present.   For eg: consider the query:   select /*+mapjoin(T1)*/ count(*) from T1 join T2 on T1.key=T2.key   Both T1 and T2 and partitioned tables, but T1 does not have any partitions   FetchOperator is invoked for T1, and listParts is empty. In that case,   use T1's schema to get the ObjectInspector. 
//  Happens in case of TezDummyStoreOperator 
//  Current number of open txns 
//  Adds the missing scheme/authority for the new partition location 
//  Helper class to submit fragments to LLAP and retry rejected submissions. 
//  This is map of which vectorized row batch columns are the key columns. 
//  The math.min, and the fact that maxAllocation is an int, ensures we don't overflow. 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.newBuilder() 
//  validate close/reopen 
//  Metrics throws an Exception if we don't do this when the key already exists 
// easier to read logs 
//  Indicates whether cache statistics should be collected. 
//  exactly the same. 
//  Use magic length value to indicate big. 
/*    * init method in HMSHandler should not be retried if there are no exceptions    */
//  we can save ourselves from spilling once we have join emit interval worth of rows. 
//  One Writable per row. 
//  privileges obtained indirectly via roles 
//  Override what's placed in the Configuration by setup() 
//  immutable maps 
//  Generates plan for min/max when dynamic partition pruning is ruled out. 
//  The fastRoundInteger* methods remove all fractional digits, set fastIntegerDigitCount, and 
//  re-instantiate the parent expression with new arguments 
//  smallint 
//  GRANTOR_PRINCIPAL_TYPE 
//  Regardless of our matching result, we keep that information to make multiple use   of it for a possible series of equal keys. 
//  FUTURE: We be more sophisticated in our conversion check. 
//  for short range use linear counting 
//  Release buffers as we are done with all the streams... also see toRelease comment.\ 
//  Heartbeat is from a task that we are not currently tracking. 
//  1 static partition created during setup + 10 dynamic partitions 
//  Output top values 
//  stores datastore (jpox) properties, 
//  Let's not guess which one is correct. 
//  create table and pupulate with kv1.txt 
//  lengths stream could be empty stream or already reached end of stream before present stream.   This can happen if all values in stream are nulls or last row group values are all null. 
//  publish DP columns to its subscribers 
/*    * Responsible for capturing SubQuery rewrites and providing the rewritten query   * as SQL.    */
//  start offset of compression buffer corresponding to above row group 
//  DAG scratch dir. We get a session from the pool so it may be different from Tez one. 
/*    * if {@code true} it means current transaction is started via START TRANSACTION which means it cannot   * include any Operations which cannot be rolled back (drop partition; write to  non-acid table).   * If false, it's a single statement transaction which can include any statement.  This is not a   * contradiction from the user point of view who doesn't know anything about the implicit txn   * and cannot call rollback (the statement of course can fail in which case there is nothing to   * rollback (assuming the statement is well implemented)).   *   * This is done so that all commands run in a transaction which simplifies implementation and   * allows a simple implementation of multi-statement txns which don't require a lock manager   * capable of deadlock detection.  (todo: not fully implemented; elaborate on how this LM works)   *   * Also, critically important, ensuring that everything runs in a transaction assigns an order   * to all operations in the system - needed for replication/DR.   *   * We don't want to allow non-transactional statements in a user demarcated txn because the effect   * of such statement is "visible" immediately on statement completion, but the user may   * issue a rollback but the action of the statement can't be undone (and has possibly already been   * seen by another txn).  For example,   * start transaction   * insert into transactional_table values(1);   * insert into non_transactional_table select * from transactional_table;   * rollback   *   * The user would be in for a surprise especially if they are not aware of transactional   * properties of the tables involved.   *   * As a side note: what should the lock manager do with locks for non-transactional resources?   * Should it it release them at the end of the stmt or txn?   * Some interesting thoughts: http://mysqlmusings.blogspot.com/2009/02/mixing-engines-in-transactions.html.    */
//  Guaranteed to be just 1 because each DummyStoreOperator can be part of only one work. 
// ---------------------------------------------------------------------------------------------- 
//  Filter does not change the input ordering.   Filter rel does not permute the input.   All corvars produced by filter will have the same output positions in the 
//  increment cursor for per-query IN-clause list 
//  Note : edge-case here in interaction with table-level REPL LOAD, where that nukes out   tablesUpdated. However, we explicitly don't support repl of that sort, and error out above 
//  1) The set of operators in the works that we are merging need to meet   some requirements. In particular:   1.1. None of the works that we are merging can contain a Union   operator. This is not supported yet as we might end up with cycles in   the Tez DAG.   1.2. There cannot be more than one DummyStore operator in the new resulting   work when the operators are merged. This is due to an assumption in   MergeJoinProc that needs to be further explored.   If any of these conditions are not met, we cannot merge. 
//  Used by sort-based GroupBy: Mode = COMPLETE, PARTIAL1, PARTIAL2, 
//  Set up zookeeper dynamic service discovery configs 
//  The current record should not be included in the output detailList. 
//  set the values of totalInputFileSize and totalInputNumFiles, estimating them   if percentage block sampling is being used 
//  total characters = 10; byte length = 26 
//  flatten OR 
//  partition   column 
//  STORED_AS_SUB_DIRECTORIES 
// insert data in "legacy" format 
//  example code 
//  if this operator is a Map Join Operator or a Merge Join Operator 
//  first hash is used to locate start of the block (blockBaseOffset) 
//  child operator --> object inspector (converted OI if it's needed) 
//  the tableDesc will be null in the case that all columns in that table   is not used. we use a dummy row to denote all rows in that table, and   the dummy row is added by caller. 
//  this operator can be removed. 
//  AccumuloInputFormat complains if you re-set an already set value. We just don't care. 
//  for DELETE statements NOT NULL constraint need not be checked 
//  Bootstrap dump with open txn timeout as 1s. 
//  write totalMonths to DataOutput 
//  Now change the resource plan - change the mapping for the user. 
//  Only columns and constants can be selected 
//  CASCADE 
/*            * Determine the data and partition columns using the first partition descriptor's           * partition count.  In other words, how to split the schema columns -- the           * allColumnNameList and allTypeInfoList variables -- into the data and partition columns.            */
//  skip map-aggr GBY 
//  RuleRegExp rules are used to match operators anywhere in the tree   RuleExactMatch rules are used to specify exactly what the tree should look like   In particular, this guarantees that the first operator is the reducer   (and its parent(s) are ReduceSinkOperators) since it begins walking the tree from 
//  Check additional constraint. 
//  Stay with MULTI_KEY 
//  encoded filename/checksum of files, write into _files 
//  The join is the root, but we should always end up with a Project operator 
/*    * State of the Rolling Partition   *    * x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17   * ^                    ^                                     ^   * |                    |                                     |   * |--preceding span--numRowsProcessed---followingSpan --numRowsRecived   *    * a. index x7 : represents the last output row   * b. so preceding span rows before that are still held on for subsequent rows processing.   * c. The #of rows beyond numRowsProcessed = followingSpan    */
//  A HiveDecimalWritable version. 
//  FUNCTIONS 
// set the configuration up such that proxyUser can act on  behalf of all users belonging to the group foo_bar_group ( 
//  get the table from the client, verify the name is correct 
//  This relies on findKeyRefToRead doing key equality check and leaving read ptr where needed. 
/*        This is used to print the progress information as pure text , a sample is as below:          Map 1: 0/1	Reducer 2: 0/1          Map 1: 0(+1)/1	Reducer 2: 0/1          Map 1: 1/1	Reducer 2: 0(+1)/1          Map 1: 1/1	Reducer 2: 1/1      */
/*      * Limit can be pushed down to Map-side if all Window Functions need access     * to rows before the current row. This is true for:     * 1. Rank, DenseRank and Lead Fns. (the window doesn't matter for lead fn).     * 2. If the Window for the function is Row based and the End Boundary doesn't     * reference rows past the Current Row.      */
//  check if any of the roles of this user is an owner 
//  The BinarySortable serialization of the saved key for a possible series of equal keys. 
//  x events to insert, last repl ID: replDumpId+2x 
//  Does it require a new MR job for grouping sets 
//  temporarily, mark it as child of all the TS 
//  should not affect schema conversion 
/*    * Lookup an long in the hash multi-set.   *   * @param key   *         The long key.   * @param hashMultiSetResult   *         The object to receive small table value(s) information on a MATCH.   *         Or, for SPILL, it has information on where to spill the big table row.   *   * @return   *         Whether the lookup was a match, no match, or spilled (the partition with the key   *         is currently spilled).    */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getRef(java.lang.String)    */
//  We ignore refcounts (and errors) for now. 
//  surprise) that at midnight UTC it was 20:00 in local. So far we are on firm ground. 
//  Bootstrap Repl B -> C 
//  Cache indexList 
/*      * Use common conversion method we share with fastSerializationUtilsWrite.      */
//  For primitive object, serialize to plain string 
//  Split starting at row start will not read that row. 
//  Test a single, high-precision multiply of random inputs.   Arguments must be integers with optional - sign, represented as strings.   Arguments must have 1 to 37 digits and the number of total digits 
//  Handle decimal separately. 
//  Remove operator   Add ops of new collection 
//  Concurrent increase and termination, increase fails. 
//  Exchange single partitions using complete partition-spec (all partition columns) 
//  column in the target table that will be pruned against 
//  we need a column expression on other side. 
//  call-5: open - mock:/mocktbl1/0_1 
//  Always collect input file formats. 
// if not using position alias and it is a number. 
//  Make a new big table scratch column for the small table value. 
//  Nope, so look to see if Hive's conf dir has been explicitly set 
//  UNDONE: Currently, negative exponent is not supported. 
//  months) produces a type timestamp via a calendar calculation. 
//  An optional long array of length FastHiveDecimal.FAST_SCRATCH_LONGS_LEN. 
//  UNDONE: For now, a 1-element list with a null element is a null list... 
//  Use IOSpecProto.newBuilder() to construct. 
//        but for now just rely on the cache put to lock them before we send them over. 
//  references. 
//  A type timestamp (TimestampColumnVector) plus/minus a type interval_day_time (TimestampColumnVector 
/*        * See if the execution thread has just completed operation and result is available.       * If result is available then return the result. Otherwise, raise exception.        */
//  Monotonic iff its first argument is, but not strict. 
//  leap year 
//  globStatus() API returns empty FileStatus[] when the specified path   does not exist. But getFileStatus() throw IOException. To mimic the   similar behavior we will return empty array on exception. For external   tables, the path of the table will not exists during table creation 
/*               Implicit assumption here is that database level is processed first before table level,              which will depend on the iterator used since it should provide the higher level directory              listing before providing the lower level listing. This is also required such that              the dbTracker /  tableTracker are setup correctly always.            */
//  If any table/partition directory is not owned by hive,   then assume table is using storage-based auth - set external.   Transactional tables should still remain transactional, 
//  already stopped, or else it was never   started (eg another service failing canceled startup) 
//  nothing to start 
//  set up temporary path to communicate between the small/big table 
//  3. Destroy the sessions that we don't need anymore. 
//  Filters are not over the rowid, therefore scan everything 
//  Switching tables between catalogs is not allowed. 
//    throw new HiveException("difference is not in sort order and unique");   } 
//  According to the Java documentation this does nothing, but just in case 
/* next call will eventually end up in HiveEndPoint.createPartitionIfNotExists() which    makes an operation on Driver    * and starts it's own CliSessionState and then closes it, which removes it from ThreadLoacal;    * thus the session    * created in this class is gone after this; I fixed it in HiveEndPoint */
// see Arrays.binarySearch() JavaDoc 
//  @@protoc_insertion_point(builder_scope:EntityDescriptorProto) 
//  get non-SSL socket transport 
// Compactor generated a split for a bucket that has no data? 
//  create a row per database name 
//  add the last one 
//  2. If the outputOI has all fields settable, return it 
//  in case we decided to run everything in local mode, restore the 
//  before failover, check if we are getting connection from miniHS2_1 
//  Naked dot. 
//  DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; 
//  Get credentials using the configuration instance which has HBase properties 
// deserialize path, offset, length using FileSplit 
//  Test that adding multiple versions of the same schema 
//  Authenticate or deny based on its context completion 
//  No operation emitter will be used by some internal druid classes. 
//  set all properties specified on the command line 
/*      * Once enough rows have been output, there is no need to process input rows.      */
// it's key that this is a per HCatStorer instance object 
//  Can't add NULL. 
/*            * No other columns provided non-NULL values.  We *may* be able to finish all rows           * with this input column...            */
//  http (over thrift) transport settings 
//  Set child expressions 
//  Make sure the bucketId is at max the numBuckets 
// run major compaction 
//  no need to look further for a checked variant of this expression 
//  TODO Is it possible for heartbeats to come in from lost tasks - those should be told to die, which   is likely already happening. 
//  just 1 VInt 
//  if there is anything wrong happen, we bail out. 
//  -select- should return a ResultSet 
//  AND hash with mask to 0 out sign bit to make sure it's positive.   Then we know taking the result mod n is in the range (0..n-1).   Include salt as argument so this hash function can be varied   if we need to rehash. 
// produce this sequence 
//  scSize == 1 
//  Working on the assumption that a single DAG runs at a time per AM. 
//  Now, create a delete delta that has rowIds divisible by both 3 and 2. This will produce 
//  Make sure the allocation is transfered correctly on return. 
//  example file names are input1.q.out_mr_0.17 or input2.q.out_0.17 
//  Jersey uses java.util.logging - bridge to slf4 
//  seek directly to first record 
//  not (e.g.: group by) 
//  Currently there is no way to stop the MetaStore service. It will be stopped when the   test JVM exits. This is how other tests are also using MetaStore server. 
//  Note that it shouldn't show t14 from db2 
//  Open a session 
//  if it is a reference to a boolean column, covert it to a truth test. 
//  Set up CredentialProvider 
//  Rows we looked up as one repeated key need to spill.  But filtered out rows   need to be generated as non-matches, too. 
//  For operator, the function name is the operator text, unless it's in   our special dictionary 
//  Iterate over the Path -> Partition descriptions to find the partition   that matches our input split. 
//  The character set name starts with a _, so strip that 
//  Check for an escape character before the colon 
//  this colon is escaped, search again after it 
/*      * No joining condition.      */
//  l4j.info("ReduceRecordSource processVectorGroup keyBytes " + keyLength + " " +       VectorizedBatchUtil.displayBytes(keyBytes, 0, keyLength)); 
//  (1) Because we have operator.supportUnionRemoveOptimization() for   true only in SEL and FIL operators,   this rule will actually only match UNION%(SEL%|FIL%)*FS%   (2) The assumption here is that, if   operator.getChildOperators().size() > 1, we are going to have   multiple FS operators, i.e., multiple inserts.   Current implementation does not support this. More details, please   see HIVE-9217. 
//  http mode 
//  Parse digits. 
//  This will throw an exception in case of the response from druid is not an array   this case occurs if for instance druid query execution returns an exception instead of array of results. 
//  Remove all parts that are not partition columns. See javadoc for details. 
//  Only keep the most significant decimalDigits digits. 
//  we copy all of the values to avoid creating more objects   TODO: it might be cheaper to always preserve data or reset existing objects 
//  them 
//  prepare arguments for createVectorExpression 
//  composite key types is a comma separated list of different parts of the   composite keys in   order in which they appear in the key 
//  Note: this should never happen for mm tables. 
//  if data size still could not be determined, then fall back to filesytem to get file 
//  We currently include all data, partition, and any vectorization available   virtual columns in the VRB. 
//  for use in DDL operations that only need a shared lock, such as creating a table   for use in DDL statements that do not require a lock 
//  Set up the rules for the graph walker for group by and join operators 
//  We're hijacking the big table evaluators and replacing them with our own custom ones 
//  if there are no files/partitions to read, we need to skip trying to read 
//  EXPR AS (ALIAS,...) parses, but is only allowed for UDTF's   This check is not needed and invalid when there is a transform b/c the 
//  Add the additional postprocessing transformations needed if 
/*   uses the authorizer from SessionState will need some more work to get this to run in parallel,  however this should not be a bottle neck so might not need to parallelize this.    */
//  ReduceSink parents that we missed. 
//  Note: we assume here that the data that was returned to the caller from cache will not   be passed back in via put. Right now it's safe since we don't do anything. But if we   evict proactively, we will have to compare objects all the way down. 
//  wrong expression: 
//  Capacity exists. 
//  Allow implicit String to Double conversion 
//  Couldn't determine common type, don't cast 
//  When we have yet another child beyond the current one... save unselected. 
//  object overhead + 4 bytes for int (nanos) + 4 bytes of padding 
//  Set UGI to use Kerberos   Have to use the string constant to support hadoop 1 
//  The creation time is changed, so we do not check that 
//  Ensure that we get the right concrete ColumnMapping 
//  cast only needed for Hadoop 0.17 compatibility 
//  If it has a limit, we use it and we do not distribute the query 
//  TYPE_NAME   DATA_TYPE   PRECISION   LITERAL_PREFIX   LITERAL_SUFFIX   CREATE_PARAMS   NULLABLE   CASE_SENSITIVE   SEARCHABLE   UNSIGNED_ATTRIBUTE   FIXED_PREC_SCALE   AUTO_INCREMENT   LOCAL_TYPE_NAME   MINIMUM_SCALE   MAXIMUM_SCALE   SQL_DATA_TYPE, unused   SQL_DATETIME_SUB, unused  NUM_PREC_RADIX 
//  optional string fragment_identifier_string = 2; 
//  NOTE: To support pruning the grouping set id dummy key by VectorGroupbyOpeator MERGE_PARTIAL   case, we use the keyCount passed to the constructor and not keyExpressions.length. 
//  We can proceed with the conversion 
//  We expect evicted, but not failed. 
//  Boolean is special case. 
//  read each child node, add to results 
//  looks like some network outrage, reset the file system object and retry. 
//  HiveServer2 WebUI 
//  Only do the lightweight stuff in ctor; by default, LLAP coordinator is created during 
//  Current usage looks like it's only for metadata columns, but if that changes then   this method may need to require a type qualifiers aruments. 
//  timeout 
//  Boolean must come before the integer family. It's a special case. 
//  cascade only occurs with partitioned table 
//  the function should support both string and binary input types 
//  TODO: For LLAP, assumption is off-heap cache. 
//  No match 
//  Hash is ineffective, disable. 
//  authorize drops if there was a drop privilege requirement, and   table is not external (external table data is not dropped) or   "hive.metastore.authorization.storage.check.externaltable.drop" 
//  other rewrites. 
//  (1,a,x), (2,b,x), (1,c,x), (2,a,y) 
//  Verify corrupted cache value gets replaced. 
//  then remove the table 
//  colStatIndex=12,13,14 respond to "AVG_LONG", "AVG_DOUBLE", 
//  a>0 && 2a=b 
//  number of register bits 
//  Preparation for hybrid grace hash join 
//  override the db name if provided in repl load command 
//  oldDbName and newDbName *will* be the same if we're here 
// this needs to be manually set, under normal circumstances MR Task does this 
//  For each file, figure out which bucket it is. 
// should never happen 
//  Discard context that is cached for reuse per thread to avoid allocating lots of arrays,   and then resizing them down the line if we need a bigger size. 
/*      * all filters were executed during partition pruning      */
//  nothing in front of this one to prevent acquisition. 
//  set the file owner to hive (or the id metastore run as) 
/*    * This method must return the decimal TypeInfo for what getCastToDecimal will produce.    */
//  clean test space 
/* |------+----------------+----------------+----------+-------+-----------------------------------|| Use  | Boundary1.type | Boundary1. amt | Sort Key | Order | Behavior                          || Case |                |                |          |       |                                   ||------+----------------+----------------+----------+-------+-----------------------------------||   1. | PRECEDING      | UNB            | ANY      | ANY   | start = 0                         ||   2. | CURRENT ROW    |                | ANY      | ANY   | scan backwards until row R2       ||      |                |                |          |       | such R2.sk != R.sk                ||      |                |                |          |       | start = R2.idx + 1                ||------+----------------+----------------+----------+-------+-----------------------------------|    */
//  This no longer does expansions of run commands in the files as it used to.  Instead it   depends on the developers to have already unrolled those in the files. 
//  Find out if this synthetic predicate belongs to the current cycle 
//  special entry for non-DP case 
//  If pruning sink operator is with map join, then pruning sink need not be split to a   separate tree.  Add the pruning sink operator to context and return 
//  If there is one and only one limit starting at op, return the limit   If there is no limit, return 0 
//  2 original files, 1 original directory, 1 base directory and 1 delta directory 
//  First pass will drop the materialized views 
//  Deserialize 
//  check null handling 
// normalize name for mapping 
//  Currently the long must fit 2 markers. Setting these bit sizes determines the balance   between max pool size allowed and max concurrency allowed. This balance here is not what we   want (up to 254 of each op while only 65535 objects limit), but it uses whole bytes and is   good for now. Delta and RC take the same number of bits; usually it doesn't make sense to   have more delta. 
//  Cartesian product 
//  Cache the hints before CBO runs and removes them. 
//  Error because of thrift client, we have to recreate base object 
//  let's see if we can convert aggregate into projects 
//  null or NULL 
//  base since the presence of a base will make the originals obsolete. 
//  This registration has to be done after knownTasks has been populated.   Register for state change notifications so that the waitQueue can be re-ordered correctly   if the fragment moves in or out of the finishable state. 
//  require db ownership, if there is a file require SELECT , INSERT, and DELETE 
// test for and or precedence 
//  When split-update is enabled for ACID, we initialize a separate deleteEventWriter   that is used to write all the delete events (in case of minor compaction only). For major   compaction, history is not required to be maintained hence the delete events are processed   but not re-written separately. 
//  read the null byte again 
// in both cases, this will be the next day in GMT 
//  figure out which factory we're instantiating from HiveConf iff it's not been set on us directly. 
//  blank " " (1 byte)   blank " " (1 byte)   blank " " (1 byte)   hyphen-minus "-" U-002D (1 byte)   blank " " (1 byte)   grave accent "-" U-0060 (1 byte)   BLACK SUN WITH RAYS U+2600 (3 bytes) 
/*  * JavaCC - OriginalChecksum=67039445e12d18e18e63124a33879cd3 (do not edit this * line)  */
//  Check repeating 
//  ////////////////////////   Command methods follow   //////////////////////// 
//  Make sure that the '#' wasn't escaped 
//  add any other header info 
//  If table doesn't exist, allow creating a new one only if the database state is older than the update.   This in-turn applicable for partitions creation as well. 
//  DPP work is considered a descendant because work needs   to finish for it to execute 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getCatalogs(org.apache.hive.service.cli.SessionHandle)    */
//  64 bytes is the overhead for a reference 
// using target.a breaks this 
//  COMPONENT 
//  update the parentOp 
//  If column is not column reference , we bail out 
//  make sure b will have less digits than A 
//  This port is already in use, try to use another. 
//  ensure size is >= 1, otherwise try again 
//  sleep half a second 
//  Table exists and is older than the update. Now, need to ensure if update allowed on the   partition. 
//  Handled via adminPrivOps (see above). 
// /////////////////////////////////////////////  ////////////////// correct testcase  ////////////////// executed twice: once with the typed ps setters, once with the generic setObject 
//  the Map vertex. 
//  these are non standard version numbers. can't perform the   comparison on these, so assume that they are incompatible 
//  Test that locking a table prevents locking of partitions of the table 
//  input and output are the same 
// don't create splits for anything past logical EOF 
//  Pinged before. Log only occasionally.   5 seconds elapsed. Log again. 
// this implies that no locks are needed for such a command 
//  its value, if provided 
//  used to load columns' value into memory 
//  Add as is; it would become a recursive split. 
//  call-2: open to read - split 2 => mock:/mocktable3/0_1 
//  note that we do not set location - for repl load, we want that auto-created. 
/*          * Get our Single-Column String hash set information for this specialized class.          */
//  or the old_pruner_pred and the new_ppr_pred 
//  get delegation token for the given proxy user 
//  Node disable timeout higher than locality delay. 
//  If the query contains windowing processing 
//  If not, we will start to transform the operator tree. 
//  Deep one is bigger, i.e. less to the top 
//  Allow create table only on t1. Create should fail for rest of the tables and hence constraints 
/* Here we want to encode the error in machine readable way (e.g. JSON)     * Ideally, errorCode would always be set to a canonical error defined in ErrorMsg.     * In practice that is rarely the case, so the messy logic below tries to tease     * out canonical error code if it can.  Exclude stack trace from output when     * the error is a specific/expected one.     * It's written to stdout for backward compatibility (WebHCat consumes it). */
//  Sum all non-null double column values for avg; maintain isGroupResultNull; after last row of   last group batch compute the group avg when sum is non-null. 
//  Otherwise, it returns the expression that originated the column 
//  get task associated with this union 
//  Verify integerDigitCount given fastScale. 
//  Return value modulo n but always in the positive range (0..n-1).   Since n is prime, this gives good spread for numbers that are multiples   of one billion, which is important since timestamps internally   are stored as a number of nanoseconds, and the fractional seconds   part is often 0. 
/*  container reuse  */
//  Timestamp Scalar case becomes use long/double scalar class. 
/*      * If after classifying filters there is more than 1 joining predicate, we     * don't handle this. Return null.      */
//  creating new jars for classes that have already been packaged. 
//  D8. Unnormalize: Divide R to get result 
//  Find the new database id 
//  Get the internal map structure (MAP_KEY_VALUE) 
// txnid 3 was committed and thus not open 
//  unique the list 
//  Get our own instance of the transaction handler 
//  using such an aggregate fileId cache is not bulletproof and should be disable-able. 
//  Try with chunked stream. Here the chunked output didn't get a chance to write the end-of-data 
//  The pruner should not have completed. 
//  distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0] 
//  Perform decorrelation. 
//  If there is mismatch in bucketingVersion, then it should be set to   -1, that way SMB will be disabled. 
//  drop a stats parameter, which triggers recompute stats update automatically 
//  @@protoc_insertion_point(class_scope:QueryIdentifierProto) 
//  Select all with the is null and is not null as 2 child expressions, and then   expect the 3rd child to not be invoked. 
//  optional string operationId = 8; 
//  if current pos is larger than shrinkedLength which is calculated for   each split by table sampling, stop fetching any more (early exit) 
//  write this out to a file, and import it into hive 
// create 2 rows in a file 000000_0_copy1 and 2 rows in a file 000001_0_copy1 
//  http path should begin with "/" 
//  They aren't the same, but we may be able to do a cast 
//  Whether to show a link to the most failed task + debugging tips 
//  Remember the condition variables for EXPLAIN regardless of whether we specialize or not. 
//  reason we compute interim row count, where join type isn't considered, is because later 
//  Some of these tests require intercepting System.exit() using the SecurityManager.   It is safer to  register/unregister our SecurityManager during setup/teardown instead   of doing it within the individual test cases. 
// throw if file already exists as that should never happen 
//  that the other locker hasn't checked yet and he could lock as well. 
//  test long->double version 
//  Unsupported for the test case 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getObject(java.lang.String, java.util.Map)    */
//  CLear the output 
//  Enabling grouping on the payload. 
//  need to handle offset with single digital hour, see JDK-8066806 
//  array<null> is compatible with any other array<type> 
//  'and' nodes need to be intersected 
//  All zeroes. 
/*    * Debug.    */
//  Since we don't have an explicit AM end signal yet - we're going to create   and discard AMNodeInfo instances per query. 
//  If the UDF depends on any external resources, we can't fold because the   resources may not be available at compile time. 
//  if it is to create view, we do not use table alias 
//  Tokens cannot be used for the management protocol (for now). 
//  likely unsupported combination of params   https://bugs.openjdk.java.net/browse/CODETOOLS-7901296 is not available yet to skip benchmark cleanly 
//  only need to execute it to get db Lock 
//  We assume one request is only for one file. 
/*  Convert a skewed table to non-skewed table.  */
//  For calcite, isDeterministic just matters for within the query. 
//  Nothing to do, this is EOF 
//  If the input is already present, make sure the new parent is added to the input. 
//  This operator has not been removed, include it in the list of existing operators 
//  Set http path 
//  Submit the Spark job 
//  Now test with repeating flag 
/*    * Based on the Paper by Daniel Lemire: Streaming Max-Min filter using no more   * than 3 comparisons per elem.   *   * 1. His algorithm works on fixed size windows up to the current row. For row   * 'i' and window 'w' it computes the min/max for window (i-w, i). 2. The core   * idea is to keep a queue of (max, idx) tuples. A tuple in the queue   * represents the max value in the range (prev tuple.idx, idx). Using the   * queue data structure and following 2 operations it is easy to see that   * maxes can be computed: - on receiving the ith row; drain the queue from the   * back of any entries whose value is less than the ith entry; add the ith   * value as a tuple in the queue (i-val, i) - on the ith step, check if the   * element at the front of the queue has reached its max range of influence;   * i.e. frontTuple.idx + w > i. If yes we can remove it from the queue. - on   * the ith step o/p the front of the queue as the max for the ith entry.   *   * Here we modify the algorithm: 1. to handle window's that are of the form   * (i-p, i+f), where p is numPreceding,f = numFollowing - we start outputing   * rows only after receiving f rows. - the formula for 'influence range' of an   * idx accounts for the following rows. 2. optimize for the case when   * numPreceding is Unbounded. In this case only 1 max needs to be tarcked at   * any given time.    */
//  row 1 - results should be null 
//  Only set up the updater for insert.  For update and delete we don't know unitl we see   the row. 
/*  simplified from (position + (i - prefix) + sync.length) - SYNC_SIZE  */
//  try to instantiate the old replv1 task generation on every event produced. 
//  For each partition in each table, drop the partitions and get a list of 
//  Value count rows. 
//  loading of metastore stats is async; execute a simple to ensure they are loaded 
//  Since there was an allocation failure - don't try assigning tasks at the next priority. 
// execute a query 
//  Permissions for the metrics file 
//  remove Auth cookie 
//  Prefix for top level properties. 
//  Otherwise this is not a sampling predicate and we need to 
//  No Cookies for requested URI, authenticate user and add authentication header 
//  We need to use the expanded text for the materialized view, as it will contain 
/*  there are nulls in our column  */
//  The types for ROWS BETWEEN or RANGE BETWEEN windowing spec 
//          (* and $expr) 
//  Start of the split is before this slice.   Simple case - we will read cache from the split start offset. 
//  Someone else replaced/removed a stale value, try again. 
//  The ArgumentCompletor allows us to match multiple tokens 
//  Now create SparkTasks from the SparkWorks, also set up dependency 
//  Determines if we should cache a table (& its partitions, stats etc), 
//  the schema for intersect distinct is like this   R3 on all attributes + count(c) as cnt   finally add a project to project out the last column 
//  Otherwise, fall through and proceed with non-Decimal64 vector expression classes... 
//  generate the serialized keys of the batch. 
//  Indexes of those equivalent columns 
/*    * (non-Javadoc)   *    * @see   * org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator#processRow(java   * .lang.Object)   *    * - hand row to each Function, provided there are enough rows for Function's   * window. - call getNextObject on each Function. - output as many rows as   * possible, based on minimum sz of Output List    */
//  relations involved in JOIN 
//  If we have not added to this column before, we bail out 
//  precision is char length 
//  returns single row/column 
//  double 
//  Even if we are just setting the scale when newScale is greater than the current scale, 
//  Note: hypothetically, a generic WM-aware-session should not know about guaranteed tasks.         We should have another subclass for a WM-aware-session-implemented-using-ducks.         However, since this is the only type of WM for now, this can live here. 
// principal name, can be a user, group, or role 
// update clause 
//  Fast check, if the next day directory exists return it. 
/*  first_name = 'john'  */
//  make compile happy 
//  Check if the function can be short cut. 
//  Don't strip quotes. 
//  we start from sq2, end up with sq1. 
//  Reopen implies the use of the reopened session for the same query that we gave it out   for; so, as we would have failed an active query, fail the user before it's started. 
//  Set the session key token (Base64 encoded) in the headers 
//  Not much we can do about it honestly 
/*  TODO HIVE-18991    List<String> materializedViews = client.getMaterializedViewsForRewriting(dbName);    Assert.assertEquals(1, materializedViews.size());    Assert.assertEquals(tableNames[3], materializedViews.get(0));     */
//  proper index of a dummy. 
//  deal with dynamic partitions 
//  Store away the keystore. 
//  Verify that the correct methods are invoked on AccumuloInputFormat 
//  Alter unpartitioned table set table property 
//  Based on SecurityUtil. 
//  Setup InBloomFilter() UDF 
//  Set transportMode 
//  A thread is spun up to start these other threads.  That's because we can't start them   until after the TServer has started, but once TServer.serve is called we aren't given back   control. 
// both delete events land in corresponding buckets to the original row-ids 
//  First we extract the information that the query provides 
//  500ms   50ms 
/*      * Add these 3 values:     *     * mixedUp     * green     * NULL     * <4 char string with mult-byte chars>      */
//  A struct column can have a null child column 
//  Build OI with timestamp granularity column 
//  if active passive HA enabled, use default HA namespace 
//  Need to send the splits to multiple buckets 
//  Determine maximum of all non-null double column values; maintain isGroupResultNull. 
//  Create queryId based route 
//  Check that the bit at the given index is '1' or '0' 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getObject(java.lang.String)    */
//  nodeOfInterest is the query 
//  ignore files of 0 length 
//  TODO: make add asynchronous: add shouldn't block the higher level calls 
//  Adding name of the log file in an extra log line, so it is easier to find   the original if there is a test error 
//  if the schemaDestf is null, it means the destination is not in the local file system 
//  exit 
// since we may have both Update and Delete branches, Auth needs to know 
//  verify that partition rename succeded. 
//  match found! use it   update the tables. 
//  the constructor wasn't defined in the implementation class. Flag error 
//  Create the list record, copy first record value/key lengths there. 
/*    * This number is a safety limit for 32MB of writables.    */
//  Get the GenericUDFStructField to process the field of Struct type 
//  1 asc, 0 desc 
//  multi-line 
//  @@protoc_insertion_point(builder_scope:NotTezEvent) 
//  add dependencies for the jars 
//  Save the actual values from each row as opposed to the String representation. 
//  Note: Regardless of what the Input File Format returns, we have determined   with VectorAppendRow.initConversion that only currentDataColumnCount columns   have values we want.     Any extra columns needed by the table schema were set to repeating null   in the batch by setupPartitionContextVars. 
//  Make binary integer value in the bytearray 
//  Use table-dir as root-dir. 
//  No further checks if not a file split. Return equality. 
//  Convert PARENT -> RS -> SEL -> FS to PARENT -> FS 
//  only left input repeating 
//  Offset trims. 
//  Inject a behaviour where it repeats the INSERT event twice with different event IDs 
//  returns whether a record was forwarded 
//  Ordered columns are the source columns. 
//  Note that in theory, we are guaranteed to have a session waiting for us here, but   the expiration, failures, etc. may cause one to be missing pending restart. 
//  MY_ENUM_STRUCT_MAP 
//  verify NULL output in entry 0 is correct 
//  For a given table and its bucket full file path list,   only keep the base file name (remove file path etc).   And put the new list into the new mapping. 
//  Get the "transactional_properties" tblproperties value 
//  Insert overwrite on dynamic partition 
//  Make sure get on a table with no key returns empty list 
/*    * This operator is allowed after mapjoin. Eventually, mapjoin hint should be done away with.   * But, since bucketized mapjoin and sortmerge join depend on it completely. it is needed.   * Check the operators which are allowed after mapjoin.    */
//  tokens 
//  IDS 
//  construct dummy null row (indicating empty table) and   construct spill table serde which is used if input is too 
//  The request has succeeded but we failed to add these partitions. 
//  Don't move this code to the parent class. There's a binary   incompatibility between hadoop 1 and 2 wrt MiniDFSCluster and we   need to have two different shim classes even though they are 
//  process   a   sync entry   minus SYNC_ESCAPE's length   read syncCheck 
//  Add jars that are already in the tmpjars variable 
//  2 running. 
/*  2. doesn't have all skewed values within its data  */
//  We only accept a struct, which means that we're already nested one level deep 
//  treat all inputs as string, the return value will be converted to the appropriate type. 
//  10,000,000. 
//  script operator is a black-box to hive so no optimization here   assuming that nothing can be pushed above the script op   same with LIMIT op   create a filter with all children predicates 
//  DEVENAGARI LETTER KA U+0915 (3 bytes) 
//  handle null on both sizes (not repeating) 
//  Read paths from each symlink file. 
/*           * Change the current thread name to include parent thread Id if it is executed          * in thread pool. Useful to extract logs specific to a job request and helpful          * to debug job issues.           */
//  Get the total available memory from memory manager 
//  Pre-fill the pool halfway. 
//  ready to start execution on the cluster 
//  The DPP sink has no target, remove the subtree. 
//  first hash is used to locate start of the block (blockBaseOffset)   subsequent K hashes are used to generate K bits within a block of words   To avoid branches during probe, a separate masks array is used for each longs/words within a block. 
//  Populate list of exclusive splits for every sampled alias 
//  The expression can be any one of Double, Long and Integer. We   try to parse the expression in that order to ensure that the   most specific type is used for conversion. 
//  Close files 
//  Update the topOps appropriately 
//  Retrieve generator 
//  Make sure dbName and tblName are valid. 
//  if read is not direct, we do not need to check its autho. 
//  this means we are hitting nested subquery so don't   need to go further 
// so that we only allocate a writeId only if actually adding data   (vs. adding a partition w/o data) 
//  modify the options to reflect the event instead of the base row 
//  Remember to remove this when we're out of the loop,   we can't do it in the loop or we'll get a concurrent modification exception. 
//  Setup 
//  if scalar query has aggregate and no windowing and no gby avoid adding sq_count_check 
//  {Big Value Len} {Big Value Bytes} 
//  unprocessed role: get its parents, add it to processed, and call this   function recursively 
//  returns a map<bucketNum, list<record> > 
//  Swap to get reference on the left side 
//  There's no point adding a task with forceLocality set - since that will never exit the queue.   Add other tasks if they are not already in the queue. 
//  used to indicate the input is sorted, and so a BinarySearchRecordReader shoudl be used 
//  lastInputPath should be changed by the root of the operator tree ExecMapper.map() 
//  if there isn't already a session name, go ahead and create it. 
//  ASIA_INDIA 
// subqueries will need outer query's row resolver 
//  Test that 2 separate tables don't coalesce. 
/*  allowVoidProjection  */
//  only release the related resources ctx, driverContext as normal 
// This can happen as we are querying the getFunctions before we are getting the actual function  in between there can be a drop function by a user in which case our call will fail. 
//  Iterate over partition columns to figure out partition name 
//  Set two dummy classes as authorizatin managers. Two instances should get created. 
//  Invalid if table is partitioned, but endPoint's partitionVals is empty 
//  a reader schema was provided 
//  rename 
/* newline */
//  Empty new database name 
//  The caller remembers the small value length. 
//  set internal input format for all partition descriptors 
//  Comparison to null will always return false 
//  With multiple users concurrently issuing insert statements on the same partition has   a side effect that some queries may not see a partition at the time when they're issued,   but will realize the partition is actually there when it is trying to add such partition   to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).   For example, imagine such a table is created:    create table T (name char(50)) partitioned by (ds string);   and the following two queries are launched at the same time, from different sessions:    insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'    insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException   In that case, we want to retry with alterPartition. 
//  In replication, if source file does not exist, try cmroot 
//  Now we have a complete statement, process it   write the line to buffer 
//  UNDONE: more! 
//  create the MapJoinOperator 
//  IF conditional expression 
//  for a better insertion performance values are added to temporary unsorted   list which will be merged to sparse map after a threshold 
//  spot check 
//  JDBC says that 0 means return all, which is the default 
//  Validate the row values 
//  ID 5 has been committed, all others open 
//  Drop table, ignore error. 
//  create a mocked metastore client that returns 3 table objects every time it is called   will use same size for TableIterable batch fetch size 
//  rows forwarded will be received by ListSinkOperator, which is replacing FS 
//  1 Ensure columnNames are unique - CALCITE-411 
//  unregister functions from local system registry that are not in getAllFunctions() 
//  delete all the objects created 
// todo: side note on the above: LockRequestBuilder combines the both default@acidtblpart entries to 1 
//  Enums are one of two types we fudge for Hive. Enums go in, Strings come out. 
//  Struct 
//  Get the sizes from the key buffer and aggregate 
//  Use UpdateFragmentResponseProto.newBuilder() to construct. 
//  Update buffer 
// map-side join aliases 
//  Copy the text 
//  some small alias is not known or too big 
/*  * Adapts an Arrow batch reader to a row reader  */
//  1 ptf invocation may entail multiple PTF operators) 
//  option to bypass task cleanup task was introduced in hadoop-23 (MAPREDUCE-2206) 
//  output positions. 
//  This was the only predicate, set filter expression to null 
//  TCTLSeparatedProtocol is not done yet. 
//  Finally, verify the key bytes match. 
//  optional   optional   optional   optional   optional   optional 
//  For binary join, firstSmallTable is the only small table; it has reference to spilled big   table rows;   For n-way join, since we only spill once, when processing the first small table, so only the   firstSmallTable has reference to the spilled big table rows. 
//  PartitionPruner may create more folding opportunities, run ConstantPropagate again. 
//  Will swap in the Vectors from underlying row batch. 
//  ==== Hive command operation types starts here ==== // 
//  We need to know if it is CTE or not.   A CTE may have the same name as a table.   For example,   with select TAB1 [masking] as TAB2   select * from TAB2 [no masking] 
//     them as they come back from restarts. 
/*    * The abstract context for the 3 kinds of vectorized reading.    */
//  We need the size above to take effect. 
/*        * if the current table function has no partition info specified: inherit it from the PTF up       * the chain.        */
//  When split-update is enabled, we do not need to account for buckets that aren't covered.   This is a huge performance benefit of split-update. And the reason why we are able to   do so is because the 'deltas' here are actually only the delete_deltas. All the insert_deltas   with valid user payload data has already been considered as base for the covered buckets. 
//  SEMI_SHARED can share with SHARED but not with itself 
//  allow anything. 
//  attempt made to save partition values in non-partitioned table - throw error. 
//  2. We merge IN expressions 
//  If the user is explicitly importing a new external table, clear txn flags from the spec. 
//  ceil(numSplits / numPaths), so we can get at least numSplits splits. 
//  We do not vectorize MR Reduce. 
//  #4 - We apply the granularity function 
//  SOURCE_TABLE_NAME 
//  Check if number of distinct keys is greater than given max number of entries 
//  We can wrap inputs if the execution is vectorized, or if we use a wrapper. 
//  Ascending 
/*  Maximum value seen so far  */
//  all external file systems 
//  ANALYZE TABLE 
//  Rule cannot be applied if there are GroupingSets 
//  original read block 
// since set autocommit starts an implicit txn, close it 
//  Wipe out partition columns 
// make sure we get the right data back before/after compactions 
//  No ACID in code path -- set ROW__ID to NULL. 
//  Inadequate total resources - will never succeed / wait for new executors to become available 
//  Does the same thing as getFunctionInfo, except for getting the function info. 
//  FileMetadata 
//  When the same node goes away and comes back... the old entry will be lost - which means   we don't know how many fragments we have actually scheduled on this node. 
/*  @bgen(jjtree) Start  */
//  fallback to old mechanism which serves SMB Joins. 
//  Expected to fail due to old schema 
//  Run an hcat expression and return just the json outout.  No 
//  null means the method does not accept number of arguments passed. 
//  miss in locality request, try picking consistent location with fallback to random selection 
//  In case of success, trigger a scheduling run for pending tasks. 
//  Double the size of the array if needed 
//  combine splits only from same tables and same partitions. Do not combine splits from multiple   tables or multiple partitions. 
//  order in which the results should   be output 
//  Generic settings 
//  Now flush/forward all keys/rows, except the last (current) one 
//  Returns true if columns could be inferred, false otherwise 
//  slightly different depending on where the test is run, specifically due to file size estimation 
//  dynamic partitions 
//  Merge the files in the destination table/partitions by creating Map-only merge job   If underlying data is RCFile a RCFileBlockMerge task would be created. 
//  Shutdown Metrics 
//  flag for no scan during analyze ... compute statistics 
//  LLAP not enabled, no-op. 
//  This assumes that Grouping will always be used. 
//  Note that, hive does not support UDFToDouble etc in the query text. 
// ************************************************************************************************   Decimal Multiply. 
//  HOSTNAME 
//  Remove the resource plan - disable WM. All the queries die. 
//  we should have been able to reach the union from only one side. 
//  Call super VectorMapJoinOuterFilteredOperator, which calls super MapJoinOperator with 
//  Set the semijoin hints in parse context 
/*      * Strip off leading blanks and check for a sign.      */
//  Hits, misses tracked for a candidate node 
//  best effort 
//  Parse out the context and make sure it isn't empty 
//  create table like <tbl_name> 
//  Add the input 'newInput' to the set of inputs for the query.   The input may or may not be already present.   The ReadEntity also contains the parents from it is derived (only populated   in case of views). The equals method for ReadEntity does not compare the parents   so that the same input with different parents cannot be added twice. If the input   is already present, make sure the parents are added.   Consider the query:   select * from (select * from V2 union all select * from V3) subq;   where both V2 and V3 depend on V1 (eg V2 : select * from V1, V3: select * from V1),   addInput would be called twice for V1 (one with parent V2 and the other with parent V3).   When addInput is called for the first time for V1, V1 (parent V2) is added to inputs.   When addInput is called for the second time for V1, the input V1 from inputs is picked up,   and it's parents are enhanced to include V2 and V3   The inputs will contain: (V2, no parent), (V3, no parent), (V1, parents(V2, v3))     If the ReadEntity is already present and another ReadEntity with same name is   added, then the isDirect flag is updated to be the OR of values of both. 
//  bounded by max executors 
// for Insert Overwrite 
//  find first operator upstream with valid (non-null) column expression map 
//  execute session hooks 
//  If getNameToSplitSample is not empty, at least one of the source   tables is being sampled and we can not optimize. 
//  For the children, we populate the NewToOldExprMap to keep track of   the original condition before rewriting it for this operator 
//  cost must be positive, so nudge it 
//  BitSet for flagging aborted write ids. Bit is true if aborted, false if open  default value means there are no open write ids in the snapshot 
// exercise a broad range of timestamps close to the present. 
//  2. Is this distinct UDAF 
//  Not an analyze table column compute statistics statement - don't do any rewrites 
//  So make sure that argLists is of size one. 
/*    * Convert mapjoin to a bucketed mapjoin.   * The operator tree is not changed, but the mapjoin descriptor in the big table is   * enhanced to keep the big table bucket -> small table buckets mapping.    */
//  Factor includes scale. 
//  Roughly based on BigInteger code. 
//  What is the ColumnVector type of the aggregation result? 
// Need to make sure that we are using segment identifier 
// do not authorize temporary uris 
/*    * Common generate join results from hash maps used by Inner and Outer joins.    */
//  whose constructor would not have been called 
/*          * Get our Multi-Key hash set information for this specialized class.          */
//  Create the Select Operator 
//  'select count(key) from T' as far as the reducer is concerned. 
//  In the states where a background operation is in progress, wait for the callback.   Also, ignore any duplicate calls; also don't kill failed ones - handled elsewhere. 
//  No status change for active resource plan, first activate another plan. 
//  Return false if categories are not equal 
//  RCFile supports a configurable SerDe 
//  this tests the case where older data has an ambiguous list and is not   named indicating that the source considered the group significant 
//  The mapJoinTaskFileSinkOperator writes to a different directory 
//  NumHashFunctions (1 byte) + bitset array length (4 bytes) 
//  packing into a vertex, typically a table scan, union or join 
/*        * We will get "regular" single rows from the Input File Format reader that we will need       * to {vector|row} deserialize.        */
//  PARAMETERS 
//  CAN_EVOLVE 
//  clear parameters in last-invoke 
//  This should go fine, since header should be less than the configured header size 
//  since we are walking backwards, seek back a buffer width so that   we load the previous buffer of rows 
//  Test string "0" 
//  batch already in memory anyway so we will bypass the memory checks. 
//  Verify the content of subdirs 
//  True if we are running test and the extra test file should be used when the logs are 
//  and set the pattern and excludeMatches accordingly. 
//  Create ORC file with small stripe size so we can write multiple stripes. 
//  Without Data 
//  ABORTED 
//  This relies heavily on what method determineSplits ... calls and doesn't. 
//  Add to metastore 
//  Got an error, might be there anyway due to a   permissions problem. 
//  Get the first live service instance 
//  @formatter:on 
//  Convert input row to standard objects. 
//  If they are both types of strings, that should be fine 
//  Consider generating a column group equal value series? 
//  equivalent works must have dpp lists of same size 
//  Only the IO threads need this, so there'd be at most few dozen objects. 
// this should error at analyze scope 
//  Va Syllable MEE U+A521 (3 bytes) 
//  Figure out if there are any currently running compactions on the same table or partition. 
//  Nothing to close here. 
//  add the merge MR job 
//  Create Select Operator 
//  No nulls case, not repeating 
//  The reason we poll here is that a blocking queue causes the query thread to spend   non-trivial amount of time signaling when an element is added; we'd rather that the   time was wasted on this background thread. 
//  length of each value in the map. 
//  Writes the TimestampTZ's serialized value to the internal byte array. 
//  verify the config 
//  Last singleton or range? 
//  This table has not been modified since materialization was created,   nothing to do 
// In DbTxnManager.acquireLocks() we have   2 ReadEntity: [default@acidtblpart@p=p1, default@acidtblpart] 
//  Current buffer size should be larger than initial size 
//  The group by keys and distinct keys should be the same for all dests, so using the first 
//  can't get any info without a plan 
//  supposed dump path does not exist. 
/*  Returns true if it passes the test, false otherwise.  */
//  Get cost of the subset, best rel may have been chosen or not 
//  If the txnid is 0, then there are no transactions in this heartbeat 
//  Is this the first keyValueSeparator in this entry? 
//  For fields in descending order, do a bit flip first. 
//  returns non-null FetchTask instance when succeeded 
//  reported once, so break 
//  Make a Connection object that will throw an exception 
//  Bucket MapJoin in LLAP 
//  invariant: p > hll.p 
//  Sort-merge join 
//  We have evicted the entire list. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNCharacterStream(int, java.io.Reader)    */
//  Prepare data for Client Stat Publishers (if any present) and execute them 
//  extract the correlation out of the filter 
//  If multiple rules can be matched with same cost, last rule will be choosen as a processor 
//  localize hive-exec.jar as well. 
//  Make sure for existing destination we return false as per FileSystem api contract 
//  A runtime that launches runnable tasks as separate Threads through   TaskRunners   As soon as a task isRunnable, it is put in a queue   At any time, at most maxthreads tasks can be running   The main thread polls the TaskRunners to check if they have finished. 
//  for BETWEEN clause 
//  Read configuration parameters 
//  Update sum of the length of the values seen so far 
//  No need to add not null filter for a constant. 
//  Step1: rename tmp output folder to intermediate path. After this   point, updates from speculative tasks still writing to tmpPath   will not appear in finalPath. 
//  Some existing chunk, Find max 
// save some info for webUI for use after plan is freed 
// now run as if it's a major Compaction so we collapse events 
//  Now a duplicated field name.  Should fail 
//  (num of distinct vals for col in IN clause  / num of distinct vals for col ) 
/*      * Do careful maintenance of NULLs.      */
//  If grant option specified, only update the privilege, don't remove it.   Grant option has already been removed from the privileges in the section above 
//  via a hint. 
//  MAX_PARTS 
/*      * after a lead and lag call, allow Object associated with SerDe and writable associated with     * partition to be reset     * to the value for the current Index.      */
//  the element type is not a tuple - so no subschema 
//  Text 
//  First value is repeated for all batches. 
//  end of digits 
//  JobClose has already been performed on this operator 
//  Simulate renaming via another metastore Thrift server or another Hive CLI instance 
//  Data structures specific for vectorized operators. 
//  call. 
//  VARCHAR BETWEEN 
//  the client to direct all calls to a catalog that does not yet exist. 
//  Copy into the current union task plan if 
//  We could do a wrapper with only size() and get() methods instead of List, to be sure. 
//  Create function desc 
//  for non-singular args, count can include null, i.e. (,) is counted as 1 
//  get buffer size and stripe size for base writer 
//  start_date is Sat, 3 letters day name 
//  For now, keep the old logic for non-MM non-DP union case. Should probably be unified. 
//  check for number of created files 
//  these are from ColumnPrunerSelectProc 
//  DataNucleus wants us to auto-create, but we shall do no such thing. 
//  Caller will set signum. 
//  Reset the iter to start. 
//  columns, and originalRR is the original generated select 
//  TODO: Should be checked on server side. On Embedded metastore it throws MetaException,   on Remote metastore it throws TTransportException 
//  max ndv across all column references from both sides of table 
//  trailing spaces are not significant 
//  The leaves could be shared in the tree. Use Set to remove the duplicates. 
//  Source directory is specified, so treat the target as a directory  
//  top of the operator tree, this could also reduce the amount of data going to the reducer 
//  The children are input. 
//  done with this operator 
//  This is also used for MM table conversion. 
//  Tests for the Partition exchange_partition(Map<String, String> partitionSpecs, String   sourceDb, String sourceTable, String destdb, String destTableName) method 
//  Set some conf vars 
//  mapping of the fieldid to the field 
//  For views, the entities can be nested - by default, entities are at the top level 
//  multi-GBY single-RS (TODO) 
//  Bogus encoding. 
//  Initialize with 0 for non-ACID and non-MM tables. 
//  clone postJoinFilters 
// shouldn't be any others 
//  These are the filters which are common for every QTest. 
//  Tasks can exist in the delayed queue even after they have been scheduled.   Trigger scheduling only if the task is still in PENDING state. 
//  Starting from the startNodes, add the children whose parents have been   included in the list. 
//  PARTS_FOUND 
//  Test adding a constraint 
//  Print the value 
//  Execute "set" command and retrieve values for the conf & vars specified above 
//  Don't reset anything, we are reusing column vectors. 
//  Provide an instance of the code doesn't try to make a real Instance   We just want to test that we fail before trying to make a connector   with null username 
//     specify them by the rules in {@link effectiveWindowFrame} 
//  part of the big table portion of the join output result. 
// Passing query spec, column names and column types to be used as part of Hive Physical execution 
//  process reduce sink added by hive.enforce.bucketing or hive.enforce.sorting 
//  resultSchema will be null if   (1) cbo is disabled;   (2) or cbo is enabled with AST return path (whether succeeded or not,   resultSchema will be re-initialized)   It will only be not null if cbo is enabled with new return path and it   succeeds. 
//  After reading the batch, reset the pointer to beginning. 
//  Preserve existing return type behavior for division:   Non-decimal division should return double 
//  Clusters (Buckets) 
//  optional string am_host = 6; 
//  be false and hence if db Not found we should error out. 
//  DPHJ is disabled, only attempt BMJ or mapjoin 
//  test zero-divide to show it results in NULL 
//  When DAG specific cleanup happens, it'll be better to link this to a DAG though. 
//  update should take place, such as with replication. 
//  check IF/OF/Serde 
/*      * NOTE: We do not alter the projectedColumns / projectionSize of the batches to just be     * the included columns (+ partition columns).     *     * For now, we need to model the object inspector rows because there are still several     * vectorized operators that use them.     *     * We need to continue to model the Object[] as having null objects for not included columns     * until the following has been fixed:     *    o When we have to output a STRUCT for AVG we switch to row GroupBy operators.     *    o Some variations of VectorMapOperator, VectorReduceSinkOperator, VectorFileSinkOperator     *      use the row super class to process rows.      */
//  total size of each hash entry 
//  requestLine 
//  The current plan can be thrown away after being merged with the   original plan 
//  Ensure this explicitly since versions before 2.7 read doesn't do it. 
//  if external table and custom root specified, update the parent path 
//  Remove the DDL time so that it gets refreshed 
//  Base name (varchar vs fully qualified name such as varchar(200)). 
//  we are putting join keys at last part of the spilled table 
//  If the table is bucketed on a partition column, not valid for bucketing 
//  Granularity column 
//  hence need not allow if same event is applied twice. 
//  validate the first parameter, which is the expression to compute over 
//  Decide default directory selection. 
//  committed baseTxnId 
//  Close the underlying stream if we own it... 
//  Prior singleton or range? 
//  instead of multiple times 
//  Data columns. 
//  Parameter 1 was an array of arrays, so make sure that the inner arrays contain   primitive strings. 
//  Byte.MAX_VALUE 
//  We have selected a port as a client port.  Update clientPortList if necessary.   it is not in the list, add the port 
// Calendar.getInstance calculates the current-time needlessly, so cache an instance. 
//  Position of distinct column in aggregator list of map Gby before rewrite. 
//  This can happen for truncate table case for non-MM tables. 
//  Create a barrier task for dependency collection of import tasks 
/*      * Gather up big and small table output result information from the MapJoinDesc.      */
//  if bitpacking is disabled, all register values takes 8 bits and hence   we can be more flexible with the threshold. For p=14, 16K/5 = 3200   entries in sparse map can be allowed. 
// if no other SD references this CD, we can throw it out. 
//  Copy the files out of the archive into the temporary directory 
//  Unsupported types - error 
//  Save to connectionMap so it can be closed at user's convenience. 
//  TABLES 
//  if the default was decided by the serde 
/*   Here is the layout we expecttarget/tmp/org.apache.hadoop.hive.ql.TestTxnCommands-1521148657811/ export  _metadata  p=1      q=1       000002_0      q=2          000001_0 warehouse     acidtbl     acidtblpart     nonacidnonbucket     nonacidorctbl     nonacidorctbl2     t      p=1       q=1        delta_0000001_0000001_0000            _orc_acid_version            bucket_00000       q=2           delta_0000001_0000001_0000               _orc_acid_version               bucket_00000      p=2          q=2              delta_0000001_0000001_0000                  _orc_acid_version                  bucket_00000     timport         p=1             q=1              000002_0             q=2                 000001_023 directories, 11 files */
//  This assumes all struct cols immediately follow struct 
//  Analyze and create tbl properties object 
//  Not needed without semi-join reduction 
//  To know if we need to bail out 
//  Create a http client with a retry mechanism when the server returns a status code of 401. 
//  We only need to promote if comp.type is > existing.type.  For   efficiency we check if existing is exclusive (in which case we   need never promote) or if comp is exclusive or shared_write (in   which case we can promote even though they may both be shared   write).  If comp is shared_read there's never a need to promote. 
// Since we cannot know what columns will be needed by a PTF chain,  we do not prune columns on PTFOperator for PTF chains. 
//  Copy limit 
//  tolerance to check double equality 
// set where derby logs 
//  create a new table similar to previous one. 
// didn't find any lock with extLockId but at ReadCommitted there is a possibility that  it existed when above delete ran but it didn't have the expected state. 
//  set r.convertedParameters 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setCharacterStream(java.lang.String,   * java.io.Reader)    */
//  For cases where the table is external 
//  as long as we add them to a list, order is preserved from now on. 
//  Oracle) doesn't exhibit this problem. 
//  Compare timestamp to timestamp. 
//  The bucketingVersion is not relevant here as it is never used.   For SMB, we look at the parent tables' bucketing versions and for 
//  Week granularity 
//  Create the object inspector and the lazy binary struct object 
//  check if oozie has set up a hcat deleg. token - if so use it 
//  Move past separator. 
//  as indicated by selectedInUse and the sel array. 
//  will trigger cleanup 
//  Simplify vector by brute-force flattening noNulls and isRepeating   This can be used to reduce combinatorial explosion of code paths in VectorExpressions 
//  if we are on viewfs we don't want to use /tmp as tmp dir since rename from /tmp/..   to final /user/hive/warehouse/ will fail later, so instead pick tmp dir   on same namespace as tbl dir. 
//  Get the named url from user specific config file if present 
//  Only used for testing. 
//  cast(.... as string) 
//  We do not filter when PTF is in reducer. 
//  collection separator 
//  If it is a case operator, we need to rewrite it 
//  Execute query in Druid 
//  compute count only if the register values are updated else return the   cached count 
// 6 partitions 
//  Create a GSS context 
//  0 1   1 0 
//  check if table with the new name already exists 
//  invoked for test classes 
/*  * when adding support for new types, we should try to use classes of Hive value system to keep  * things more readable (though functionally it should not make a difference).   */
/*    * Maximum number of decimal digits in a decimal64 long.    */
//  single-row case, there's no next 
//  prefix for column names auto generated by hive 
//  Location is not set we utilize METASTOREWAREHOUSE together with database name 
//  -1 when negative; 0 when decimal is zero; 1 when positive. 
//  ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS;   The plan consists of a simple TezTask followed by a StatsTask.   The Tez task is just a simple TableScanOperator 
//  execute statement with the conf overlay 
//        that fractions or query parallelism add up, etc. 
//  whether we have to enforce sort anyway, e.g. in case of RS deduplication 
//  2 more variations of callbacks; increase + decrease and decrease + increase, the 2nd call coming   before the message is sent; no message should ever be sent. 
//  Using MetaStore running in an existing cluster 
//  Test normal drop, should drop unconditionally. 
//  then groups (this is arbitrary). 
//  Asserts the class invariant. (Same types.) 
//  Test andFilter operation. 
//  Where to write our key and value pairs. 
//  Long.MAX_VALUE 
//  will return back the original token (which we know is insufficient) 
// covers both streaming api and post compaction style. 
//  We will unset s on i=0, and t on i=1. Only these should be updated; and nothing for 2. 
//  If any destination partition is present then throw a Semantic Exception. 
//  Add only dynamic partition columns to the temp table (input data file). 
//  This should fail 
//  Sleep for 100ms 
//  set now so we can verify it changed 
//  Rare case of buffer boundary. Unfortunately we'd have to copy some bytes. 
//  this is effectively the same as the dense register impl. 
//  It sorts 
//  Before the ownerType exists in an old Hive schema, USER was the default type for owner.   Let's set the default to USER to keep backward compatibility. 
//  Test the InputFormat execution path 
//  Just fetch one blob if we've serialized thrift objects in final tasks 
//  llap-common   llap-tez   llap-server   hive-exec   hive-common (https deps)   Jetty rewrite class   ZK registry 
//  SELECT statement or dynamic SQL 
//  timeout in 5 minutes 
//  operator tree for processing row further (optional) 
//  Test adding multiple partitions in a single partition-set, atomically. 
/*    * By default, the list is empty - if an operator wants to add more counters,   * it should override this method and provide the new list. Counter names returned   * by this method should be wrapped counter names (i.e the strings should be passed   * through getWrappedCounterName).    */
//  add 10 partitions on the filesystem 
// upload archive file to hdfs 
//  Otherwise we'd create a NullWritable and that isn't what we want. 
//  We keep the hash multi-set result for its spill information. 
//  Check if giving invalid address causes retry in connection attempt 
//  tag 
//  To remain consistent, we need to set input and output formats both 
// Start Metrics for Standalone (Remote) Mode 
//  success only if all the commands were successful 
//  Update max length if new length is greater than the ones seen so   far 
//  Thread pool for actual execution of work. 
//  Create join RR: we need to check whether we need to update left RR in case 
//  parition column case.   partition filter will be evaluated by partition pruner so   we will not evaluate partition filter here. 
//  Give it a LOT of slack, since on low numbers consistent hashing is very imprecise. 
//  6. Run aggregate-join transpose (cost based)      If it failed because of missing stats, we continue with 
//  Concurrent revocation and increase - before the message is sent. 
//  LocalJobRunner does not work with mapreduce OutputCommitter. So need   to use MiniMRCluster. MAPREDUCE-2350 
// 2 distinct partitions created  txnid+1 because we want txn used by previous driver.run("insert....) 
/*      * STRING:     *   Range, values, empty strings.      */
//  Call rename_partition without an environment context. 
//  Run value expressions over original (whole) input batch. 
// / CREATE TABLE scenarios 
//  Nothing to do here. 
//  remember the original parent list before we start modifying it. 
//  Initialize the execution engine based on cluster type 
//  Prefer numeric type arguments over other method signatures 
//  Nearly C/P from OrcInputFormat; there are too many statics everywhere to sort this out. 
//  assume the index is bad and do a full scan 
//  batch size from input and decaying factor of 2 
//  Indicates a type was derived from the deserializer rather than Hive's metadata. 
//  ((R1.x=R2.x) and R1.z=10)) and rand(1) < 0.1 order by R1.x limit 10 
//  find 10^37 
//  only last 2 rows qualify 
//  Delayed to find a local match 
//  This check isn't absolutely mandatory, given the aborted check outside of the 
//  run a query in a loop so that we hit a 401 occasionally 
/*    * (non-Javadoc)   *   * @see java.sql.Wrapper#unwrap(java.lang.Class)    */
//  Find the bucket id, and switch buckets if need to 
//  just return, stats gathering should not block the main query. 
//  create session, test if the hook got fired by checking the expected property 
//  sets the env variable HIVE_JOB_CREDSTORE_PASSWORD to value defined by 
//  For Java serialization only 
//  UNDONE   Assert.assertTrue(vectorizer.validateMapWorkOperator(map, null, false)); 
//  Special case handling for Multi-OR and Multi-AND. 
//  setup dynamic partition pruning where possible 
//  need to add filter   create tableOp to be filterDesc and set as child to 'top' 
//  Throwing InvalidObjectException would be more appropriate, but we do not change the API 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#registerOutParameter(int, int)    */
//  Skip from block to block since we only need the header 
//  filter operator has the same output columns as its parent 
//  Calculate unselected ones in last evaluate. 
//  will fail 
//  Neither open nor opening. 
//  authorize the revoke, and get the set of privileges to be revoked 
//  Nothing to do. No registration involved. 
//  Serialize the keys and append the tag 
//  add all UDTF columns 
//  build the new list 
//  Optimize plan 
//  used for create mapJoinDesc, should be in order 
//  Handle NULL, we return the type of pcA 
// since we have no base, there must be at least 1 delta which must a result of acid write  so it must be immediate child of the partition 
//  dumpFile(INPUT_FILE_NAME); 
//  <Parent Ops>-SEL-GB1-RS1-GB2-RS2 
//  UNDONE: Don't reuse for now.   rowBytesContainer.resetWrite(); 
//  Otherwise value may be too long, convert to appropriate value based on params 
//  Reduce sink of group by operator 
//  add user privileges 
// check if 30 Julian Days between Jan 1, 2005 and Jan 31, 2005. 
//  First row determines isGroupResultNull and long firstValue; stream fill result as repeated. 
// to make insert into non-acid take shared lock 
//  0 NULL   1 NULL 
//  Handle NULL, we return the type of pcB 
//  Create a delete delta that has rowIds divisible by 2 but not by 3. This will produce 
/*  mergeCount  */
//  optional bytes initial_event_signature = 11; 
// props = new Properties();  props.setProperty("fs.default.name", cluster.getProperties().getProperty("fs.default.name")); 
//  DIRECT encoding 
//  begin + write + abort 
//  join with group-by, having, order-by 
//  Try to deserialize using DeserializeRead our Writable row objects created by SerializeWrite. 
//  must be called last 
//  Get the last valid row in the batch still available. 
//  type_name used to be tbl.getProperty(META_TABLE_NAME).   However, now the value is DBName.TableName. To make it backward compatible,   we take the TableName part as type_name.   
//  set up the java key provider for encrypted hdfs cluster 
//  Safeguard against potential issues in CBO RowResolver construction. Disable CBO for now. 
//  Input #2 is type date (epochDays). 
//  Execute 
//     ve = vc.getVectorExpression(exprDesc);      assertTrue(ve instanceof IfExprCharScalarCharScalar); 
//  Note: WmFragmentCounters are created before Tez counters are created. 
//  For import statement, require uri rwx+owner privileges on input uri, and   necessary privileges on the output table and database   NOTE : privileges are only checked if the object of that type is marked as part of ReadEntity or WriteEntity   So, if a table is present, Import will mark a table as a WriteEntity, and we'll authorize for that, and if not present, 
//  Rounding results in 10^N. 
//  VectorInBloomFilterColDynamicValue should have all of the necessary information to vectorize. 
//  has also been adjusted to point to these buffers instead of compressed data for the ranges. 
//  Thread reading the ATS GUID 
//  We need to iterate to detect original directories, that are supported in MM but not ACID. 
//  Partial partition spec supplied. Make sure this is allowed. 
//  if column name is not contained in needed column list then it   is a partition column. We do not need to evaluate partition columns 
//  IS_FORCE_DEACTIVATE 
// now generate insert statement 
//  Variable-length arguments 
/*    * {@inheritDoc}    */
//  in all hadoop versions. 
// this error should really be produced by Hive (DDLTask) 
//  reposition at the begining 
// gets X lock on T 
//  possible if all children have same expressions, but not likely. 
//  Use old timestamp writable hash code for backwards compatibility 
//  binary transport settings 
//  didn't set the last repl ID due to some failure. 
//  Adding these job properties will make them available to the OutputFormat in checkOutputSpecs 
//  we pass null for aliases here because mergeWithChildrenPred filters   aliases in the children node context and we need to filter them in   the current JoinOperator's context 
//  Create a Table object 
//  todo: constant op constant 
//  If the table is not external and it might not be in a subdirectory of the database 
/*  * Looks for a hive-site.xml from the classpath. If found this class parses the hive-site.xml * to return a set of connection properties which can be used to construct the connection url * for Beeline connection  */
//  Common case - the segment is in one buffer. 
// if here after commit()/abort() but before next beginNextTransaction(), currentTxnIndex still 
/*    * When supported, read a field by field number (i.e. random access).   *   * Currently, only LazySimpleDeserializeRead supports this.   *   * @return  Return true when the field was not null and data is put in the appropriate   *          current* member.   *          Otherwise, false when the field is null.    */
//  return the column numbers of the bucketed columns 
//  POOLS 
// ignore 
//  Inform the shuffle handler 
//  used to cleanup cache 
//  Past the timeout. 
//  Tracks requests executing per node 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getInfo(org.apache.hive.service.cli.SessionHandle, java.util.List)    */
//  1.3.2 process the actual join 
//  Verify if the configs are merged 
//  count characters forward 
/*  Referring to job tracker in 0.20 and resource manager in 0.23  */
//  5. Check if select involves UDTF 
//  Overwrite if the remote file already exists. Whether the file can be added   on executor is up to spark, i.e. spark.files.overwrite 
// Global config of vectorized input format is enabled; check if these inputformats are excluded 
//  This input rel does produce the cor var referenced.   Assume fieldAccess has the correct type info. 
//  Native vectorization supported. 
//  make sure miniHS2_2 is the new leader 
//  Allow implicit conversion from Byte -> Integer -> Long -> Float -> Double 
//  at start-up, we may be unable to get number of executors 
//  Based on actual timing. 
//  a reduce vertex 
//  checked in SemanticAnalyzer. Should not happen 
//  Write the first part of the array 
//  Test deprecated HCatAddPartitionsDesc API. 
//  All good. 
//  analyzeCreateTable uses this.ast, but doPhase1 doesn't, so only reset it   here. 
//  In case we're searching through an especially large set of data, send a heartbeat in   order to avoid timeout 
//  Case 4: column stats, hash aggregation, grouping sets 
//  check stripHiddenConfigurations removes the property 
//  the partition column we're interested in 
//  Table in non 'hive' catalog 
//  If no Run times present, then set -1, indicating no values 
//  We assume the existing vector is always valid. 
//  3. Update Update Join Key to List<JoinLeafPredicateInfo> to use 
//  First, find first record for the key. 
//  @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.serde2.proto.test.Complex) 
//  The join keys cannot be transformed in the sub-query currently.   TableAccessAnalyzer.genRootTableScan will only return the base table scan   if the join keys are constants or a column. Even a simple cast of the join keys   will result in a null table scan operator. In case of constant join keys, they would 
//  non-deterministic functions as well as runtime constants are not materializable. 
/*  * Specialized class for native vectorized reduce sink that is reducing on a Uniform Hash * single long key column.  */
//  All but decimal. 
//  in via the properties 
/*      * roles grants      */
//  Now only string, text, int, long, byte and boolean comparisons are   treated as special cases.   For other types, we reuse ObjectInspectorUtils.compare() 
//  It is NULL value with different type, we need to introduce a CAST   to keep it 
//  NOTE: this method MUST call distributeGuaranteedOnTaskCompletion before exiting. 
//  of any other queries running in the session 
//  All getXXX needs toLowerCase() because they are directly called from   SemanticAnalyzer   All setXXX does not need it because they are called from QB which already   lowercases   the aliases. 
// ************************************************************************************************   Decimal Debugging. 
//  Make sure it compiles with both Hadoop 2 and Hadoop 3. 
//  we just created this directory - it's not a case of pre-creation, so we nuke. 
//  Test EventUtils.restrictByMessageFormat - this restricts events generated to those   that match a provided message format 
/*  isTezOrSpark  */
//  make this client wait if job trcker is not behaving well. 
//  @@protoc_insertion_point(class_scope:SourceStateUpdatedResponseProto) 
//  Start is blocking, so run one of the tasks on the main thread. 
//  These are the columns in the big and small table that are ByteColumnVector columns. 
//  get both tableAlias and column name from columnOrigin 
//  Try with an extra base. 
//  Replication case: export table <tbl> to <location> for replication 
//  FileSinkOperator knows how to properly write to it. 
/*  no reason to retry if the challenge ticket is not valid.  */
//  No valid inputs - possible in MM case. 
//  NOTE: wh.getFileStatusesForUnpartitionedTable() can be REALLY slow 
//  Alter a table in the wrong catalog 
//  c7:map<string,string>   c8:struct<r:string,s:int,t:double>   c9:tinyint   c10:smallint   c11:float   c12:bigint 
//  4. Construct SortRel 
//  first stripe will satisfy the predicate and will be a single split, last stripe will be a 
/*    * Initialize string table in a lazy fashion.    */
//  They cancel each other. 
//  Hive doesn't support a currency type 
//  3. Walk through the Join Condition Building NDV for selectivity 
//  First, go through and set all our values for datanucleus and javax.jdo parameters.  This 
//  Store the mapping -> path, bucket number   This is needed since for the map-only job, any mapper can process any file.   For eg: if mapper 1 is processing the file corresponding to bucket 2, it should   also output the file corresponding to bucket 2 of the output. 
/*    * Left semi join (hash set).    */
//  Break the client 
// verify 
//  dirName uniquely identifies destination directory of a FileSinkOperator.   If more than one FileSinkOperator write to the same partition, this dirName   should be different. 
// ignored 
//  make sure credential provider path points to HIVE_SERVER2_JOB_CREDSTORE_LOCATION 
//  this method's main use is to help unit testing this class 
/*    * Common one time setup by native vectorized map join operator's processOp.    */
//  Test select root from root:struct<col1:struct<a:boolean,b:double>,col2:double> 
//  transaction batch size > 1 case 
//  The underlying database field is varchar, we need to compare numbers. 
//  this is the bit where we make sure we don't group across partition   schema boundaries 
//  The set of pruning sinks 
//  Node1 now has free capacity. task1 should be allocated to it. 
//  User can override value of sparkCloneConfiguration in Hive config to true 
//  use the existing TableInfo object. Else, create a new one. 
//  Immutable causes #copy(obj) to return the original object 
//  -(2^32-1) 
//  1 for comma 
//  Assume the high watermark can be used as maximum transaction ID. 
//  This case possible if CM path is not enabled. 
//  codes and messages. This should be fixed. 
//  Compile internal query to capture underlying table partition dependencies 
//  Create/Delete/Write/Admin to the authenticated user 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setCharacterStream(int, java.io.Reader,   * int)    */
//  Run a self-test query. If it doesn't work, we will self-disable. What a PITA... 
//  ImmutableList 
//  selectively used by fetch formatter 
//  revert configs to not affect other tests 
//  SubQuery was just one conjunct 
/*    * Test if single-threaded implementation checker throws HiveException when the there is a dummy   * directory present in the nested level    */
//  @@protoc_insertion_point(class_scope:EntityDescriptorProto) 
//  3. Let a transaction be aborted 
//  cancel given delegation token 
// H1 - no capacity if force, should allocate otherwise 
//  as done 
/*      * The callable shouldn't be null to execute. The thread pool also should be configured     * to execute requests.      */
//  LOG.info("Writing value at " + valueOffset + " length " + (tailOffset - valueOffset)); 
//  sans header row 
//  Check the output of FixAcidKeyIndex - it should indicate the index was valid. 
// table or partition's statistics and table or partition's column statistics are accurate or not. 
//  cannot be performed as a map-only job 
//  number of digits to mask from the end 
/*  Switch from a 1-based start offset (the Hive end user convention) to a 0-based start offset     * (the internal convention).      */
//  Try to allocate from target-sized free list, maybe we'll get lucky. 
//  Put shuffle version into http header 
//  vertex is a mergejoin 
//  This means the exception was caused by something other than a race condition   in creating the partition, since the partition still doesn't exist. 
//  Find the skew information corresponding to the table 
//  boundary. Length will span two compression buffers. 
//  requesting for the stats source will implicitly initialize it 
// simulate partition update 
/*  * Specialized class for doing a vectorized map join that is an left semi join on a Single-Column String * using a hash set.  */
//  MD-ONLY table alter 
//  We skip first child as is not involved (is the revert boolean)   The target type needs to account for all 3 operands 
//  Need to find the tables and data as drop is not part of this dump 
//  Handle the status change. 
//  to cause any problem, the cleaner thread will remove this when this jar expires. 
//  Determine the transactional_properties of the table from the job conf stored in context.   The table properties are copied to job conf at HiveInputFormat::addSplitsForGroup(),   & therefore we should be able to retrieve them here and determine appropriate behavior.   Note that this will be meaningless for non-acid tables & will be set to null. 
//  c11-c20   c21-c23 
//  Return true to retain an item, and false to filter it out. 
//  thought of creating template for each shims, but I couldn't generate proper mvn script 
/*  id <=> 30  */
//  Set the big table position. Both the reduce work and merge join operator   should be set with the same value. 
//  Create a minimalistic table 
//  silent overflow 
//  boolean invert (not)   expression   left expression   right expression 
//  Add the new paths to the znodes list. We'll try for their removal as well. 
//  If all the queries are map-only, anyway the query is most optimized 
//  inject properties from the main App that matches allowedPrefix 
//  The name should not be changed, so reload the db with the original name 
//  End SemiJoinRule.java 
// such base is created by 1st compaction in case of non-acid to acid table conversion  By definition there are no open txns with id < 1. 
//  verify that a multi byte LIKE expression doesn't match a non-matching string 
//  fail similarly when memory allocations fail 
//  this means we have just created a table and are specifying partition in the   load statement (without pre-creating the partition), in which case lets use   table input format class. inheritTableSpecs defaults to true so when a new   partition is created later it will automatically inherit input format   from table object 
//  remove count(distinct) in map-side gby 
//  Create scratch dirs for this session 
//  o depends on this 
//  order by or a sort by clause. 
//  Extract the actual row from row batch 
//   In 'nobucket' table we capture bucketid from streamedtable to workaround a hive bug that prevents joins two identically bucketed tables 
//  Move past pair separator. 
//  Float 
/*  @bgen(jjtree) TypeString  */
//  TODO: Clean up SessionState/Driver/TezSession on exit 
//  Check dead session get cleared 
/*    * See {@link #next(NullWritable, VectorizedRowBatch)} first and   * {@link OrcRawRecordMerger.OriginalReaderPair}.   * When reading a split of an "original" file and we need to decorate data with ROW__ID.   * This requires treating multiple files that are part of the same bucket (tranche for unbucketed   * tables) as a single logical file to number rowids consistently.   *   * todo: This logic is executed per split of every "original" file.  The computed result is the   * same for every split form the same file so this could be optimized by moving it to   * before/during split computation and passing the info in the split.  (HIVE-17917)    */
//  not backward compatible 
//  Handle remaining lower long word digits as integer digits. 
//  look for bean style accessors get_fieldName and is_fieldName 
//  HTTPS cannot be done with zero copy. 
//  subqueries. 
//  And, finally, save the VectorizationContext. 
//  For now, leave DECIMAL precision/scale in the name so DecimalColumnVector scratch columns   don't need their precision/scale adjusted... 
//  Go to task details, fetch task tracker url 
//  This should eventually hang in the delay code. 
//  Get http service port # 
//  Find the SourceInfo to put values in. 
//  (in this case a missing specification) of UTF String storage 
//  getGenericUDF() actually clones the UDF. Just call it once and reuse. 
//  New key. 
// return 0xD8000000 + lowSurrogate; 
//  User can override value for sparkCloneConfiguration in Hive config to false 
//  Ignore the exception and fall through the default currentStateId 
//  empty String args:  
//  Note: Hadoop metric reporter does not support tags. We create a single reporter for all metrics. 
//  if we autosave, then save 
/*    * cache of rows; guaranteed to contain precedingSpan rows before   * nextRowToProcess.    */
//  UDFArgumentTypeException is expected 
//  Stream variables. 
//  Calculation below is consistent with BloomFilter.optimalNumOfBits().   Also, we are capping the BloomFilter size below 100 MB (800000000/8) 
//  Across MR process boundary tz is normalized and stored in type   and is not carried in data for each row. 
//  2. If we need to generate limit 
//  leverage TEZ-3437: Improve synchronization and the progress report behavior. 
//  single threaded scheduler for tasks from wait queue to executor threads 
//  The SIMD optimized form of "a >= b" is "((a - b) >>> 63) ^ 1" 
//  scale down. does rounding 
//     to do with the sessions after we go thru all the concurrent user actions. 
//  Secure ZK is only set up by the registering service; anyone can read the registrations. 
//  Replacing it is the right thing to do though, since we expect the AM to kill all the fragments running on the node, via timeouts.   De-allocate messages coming in from the old node are sent to the NodeInfo instance for the old node. 
//  handled below 
//  Use the serialization scale and create a BigInteger with trailing zeroes (or   round the decimal) if necessary.     Since we are emulating old behavior and recommending the use of HiveDecimal.bigIntegerBytesScaled   instead just do it the slow way.  Get the BigDecimal.setScale value and return the   BigInteger.   
// add the columns in residual filters 
//  Make a copy since we intend to mutate sum. 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.newBuilder() 
//  Inherit the environment variables 
//  Eventually enough small writes should result in another buffer getting created 
/*  Get all locks  */
//  admin check -   allows when hadoop.security.instrumentation.requires.admin is set to false   when hadoop.security.instrumentation.requires.admin is set to true, checks if hadoop.security.authorization   is true and if the logged in user (via PAM or SPNEGO + kerberos) is in hive.users.in.admin.role list 
// where missing columns are NULL-filled 
//  without extra structs 
//  Parse out 'n' and 'k' if we haven't already done so, and while we're at it,   also parse out the precision factor 'pf' if the user has supplied one. 
//  LOG is not static to make debugging easier (being able to identify which sub-class 
//  Null path => unmanaged 
//  11. put accessed columns to readEntity 
//  Requesting more partitions than allowed should throw an exception 
/*    * Helper to setup default environment for a task in YARN.    */
//  add -v and --verbose to print verbose message 
//  timestamp 
//  3rd query's session has compile lock timeout of 100 secs, so it should 
//  first check if we will allow the user to create table. 
//  to a sort-merge join 
//  Number of variables and assignment expressions 
//  Use session registry - see Registry.isPermanentFunc() 
//  if we are collapsing, figure out if this is a new row 
//  events are processed. Otherwise, task metrics may get lost. See HIVE-13525. 
//  Spot check only. Non-standard cases are checked for the same template in another test. 
//  If we are currently performing a binary search on the input, don't forward the results   Currently this value is set when a query is optimized using a compact index.  The map reduce   job responsible for scanning and filtering the index sets this value.  It remains set   throughout the binary search executed by the HiveBinarySearchRecordResder until a starting 
//  Extracted from FunctionRegistry 
//  partial partition spec has null partHandle 
//  Use MapFieldEntry.newBuilder() to construct. 
//  This means that the lock is ready to be cleaned, hence it cannot 
//  CDs are reused; go thry partition SDs, detach all CDs from SDs, then remove unused CDs. 
//  This Oid for Kerberos GSS-API mechanism. 
//  advance the reader until we reach the minimum key 
//  expect readReader return same Key & Value objects (common case) 
//  as well as a Set (for all others) to ensure determinism. 
//  Not sequential with previous. 
//  add whether the row is filtered or not   this value does not matter for the dummyObj   because the join values are already null 
//  Logger attempts 
// doing a SELECT first is less efficient but makes it easier to debug things 
/* in UTs, there is no standalone HMS running to kick off compaction so it's done via runWorker()     but in normal usage 'concatenate' is blocking,  */
//  Table properties 
//  MetadataStore 
//  We need a input object inspector that is for the row we will extract out of the 
//  no-op for non-test mode for now 
//  a flag that helps to set the correct driver state in finally block by tracking if   the method has been returned by an error or not. 
//  Rewrite the load to launch an insert job. 
//  Hive will always require user to specify exact sizes for char, varchar;   Binary doesn't need any sizes; Decimal has the default of 10. 
//  Re-using the TokenRewriteStream map for views so we do not overwrite the current TokenRewriteStream 
//  most common scenario 
//  inside! 
//  find which column contains the raw data size (both partitioned and non partitioned 
/*      * VARCHAR: string length beyond max      */
// Allocate new Vectorization context to reset the intermediate columns. 
//  There should be 1 new directory: base_-9223372036854775808 
//  If the parent is same as the ts, then we have a cycle. 
//  We have to get mtable again because DataNucleus. 
//  this is for basic stats 
//  All key input columns are repeating.  Generate key once.  Lookup once. 
//  don't need to merge, add the move job 
/*        * Currently, VectorizedParquetRecordReader cannot handle nested complex types.        */
//  timestamp +/- interval_day_time   interval_day_time + timestamp 
//  Use SourceStateUpdatedResponseProto.newBuilder() to construct. 
//  HIVE-19588 removes listStatus from the code path so there should only be one read ops (open) after HIVE-19588 
//  Avro only allows maps with Strings for keys, so we only have to worry   about deserializing the values 
//  In case we need it for the other case. 
//  Segment Metadata query that retrieves all columns present in   the data source (dimensions and metrics). 
//  find all the indexes of the sub byte[] 
//  UNDONE: No for List and Map; Yes for Struct and Union when field count different... 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setSavepoint(java.lang.String)    */
//  a path for a split unqualified the split from being sampled if:   1. it serves more than one alias   2. the alias it serves is not sampled   3. it serves different alias than another path for the same split 
//  Update the partition col stats for a table in cache 
/*  * An single long value hash map based on the BytesBytesMultiHashMap. * * We serialize the long key into BinarySortable format into an output buffer accepted by * BytesBytesMultiHashMap.  */
//  executeUpdate() of Prepared statement 
// convert BytesWritable to byte[] 
//  if oldPath is destf or its subdir, its should definitely be deleted, otherwise its   existing content might result in incorrect (extra) data.   But not sure why we changed not to delete the oldPath in HIVE-8750 if it is   not the destf or its subdir? 
//  Set the context attribute to true which will be interpreted by the request   interceptor 
//  HS2 connections guard rails 
//  For now, we disable the test attempts. 
//  Gather output works operators 
//  Not currently supported. 
//  Assign values from the row to local variables 
//  expressions for project operator 
//  expressions. These comparisons are AND'ed together. 
//  Even in large (say VARCHAR(2000)) columns most strings are small 
//  optional .SubmissionStateProto submission_state = 1; 
/*    * (non-Javadoc)   *   * @see javax.sql.CommonDataSource#setLoginTimeout(int)    */
//  the last field is the union field, if any 
// the token file location comes after mainClass, as a -D prop=val 
//  Test integer 
//  if outPath does not exist, then it means all paths within combine split are skipped as   they are incompatible for merge (for example: files without stripe stats).   Those files will be added to incompatFileSet 
//  Stop on non-existent option. 
//  First try temp table 
//  Replication done, we need to check if new value is set for existing property 
//  Use HiveVarchar's internal Text member to read the value. 
//  remove them 
//  3.2 Add column info corresponding to partition columns 
//  create map local operators 
//  Counter for rows emitted 
// share the code with RecordReader. 
// for fullAcid we don't want to delete any files even for OVERWRITE see HIVE-14988/HIVE-17361 
//  setup whitelist 
//  MapJoin and SMBJoin not supported 
//  The plan consists of a StatsTask only. 
//  We may not own the table object, create a copy 
//  Now get from cache 
//  Find the table we will be working with. 
//  Wrap up the current query string since we can not add another "inList" element value. 
//  Also MIN_HISTORY_LEVEL will have 1 entry for the open txn. 
//  Proportion of extra space to provide when allocating more buffer space. 
//  Map from integer tag to distinct aggrs 
//  Initialize container to use for storing tuples before emitting them 
//  4. Otherwise, we create a new condition 
//  If source is already CM path, the checksum will be always matching 
//  we scale up sumNulls based on the number of partitions 
// check that partition keys have not changed, except for virtual views 
//  Use old value reference word.   LOG.debug("VectorMapJoinFastLongHashTable expandAndRehash key " + tableKey + " slot " + newSlot + " newPairIndex " + newPairIndex + " empty slot (i = " + i + ")"); 
//  In compatibility mode we need to hook to set, and use 
//  first check all (1) tables 
//  See serialization of decimal for explanation (below) 
//  and any databases other than the default database. 
//  update statistics based on column statistics.   OR conditions keeps adding the stats independently, this may   result in number of rows getting more than the input rows in 
//  Create a file system handle 
// and do a Load Data into the same table, which should now land in a delta/ 
//  Remove any parents from MapJoin again 
//  Determine the lock type to acquire 
//  Initial write (small value) 
//  input long[] is set as such without copying, so any modification to the source will affect bloom filter 
//   value might have been changed because of the normalization in conversion 
//  is set to bootstrap dump location used in C. 
//  Already exists. 
//  tests setting maxRows to 0 
//  First calculate the length of the output string 
//  Since metaVars are all of different types, use string for comparison 
/*      * Count input and output are LONG.     *     * Just modes (PARTIAL2, FINAL).      */
//  Check if there are column stats available for these columns 
//  derived classes can set this to different object if needed 
//  the plan file should always be in local directory 
//  We must iterate over all the delete records, until we find one record with 
//  Get the valid write id list for all the tables read by the current txn 
//  Don't user uber in "all" mode - everything can go into LLAP, which is better than uber. 
/*  isOriginalMapJoin  */
//  if it is not analyze command and not column stats, then do not gatherstats 
//  Converts negative byte to positive index 
//  Not possible to expand since we have more than one chunk with a single segment.   This is the case when user wants to append a segment with coarser granularity.   e.g If metadata storage already has segments for with granularity HOUR and segments to append have DAY granularity.   Druid shard specs does not support multiple partitions for same interval with different granularity. 
//  this is the last branch, and it is always false   We assume alwaysFalse filter will get pushed down to TS so this   branch so it won't read any data. 
//  repeated .IOSpecProto input_specs = 10; 
//  to get at least 10 splits 
//  Integer.MIN_VALUE 
//  finally make sure the file sink operators are set up right 
// now check that stats for partition we didn't modify did not change 
//  we need to edit the configuration to setup cmdline. clone it first 
//  ### NOTE: fix for sf.net bug 879425.   Working around an issue in jline-2.1.2, see https://github.com/jline/jline/issues/10   by appending a newline to the end of inputstream 
//  parameters of the form : KEY.colx:t.coly 
//  need to reset the monitor, as operation handle is not available down stream, Ideally the   monitor should be associated with the operation handle. 
//  Usually this means we've already created the tables, so clean them and then try again 
//  Use magic value to indicating we are writing the big value length. 
//  There will be 4 data nodes   There will be 4 task tracker nodes 
// For BC, ignore this for now, but leave a log message 
//  default implementation 
//  When fraction is exactly 0.5 and lowest new digit is odd, go towards even. 
// get override compression properties via "tblproperties" clause if it is set 
//  Use GroupInputSpecProto.newBuilder() to construct. 
//  Do not create predicate if the leaf is not on the passed schema. 
//  Clear value arrays. 
//  Read all values. 
//  If map type, contains schema of the value element. 
//  Postgres specific parser 
//  push the context on to the end of the serialized n-gram estimation 
//  For this variation, we serialize the key without caring if it single Long,   single String, multi-key, etc. 
//  if so. If that should ever change, this will need reworking. 
//   throw new RuntimeException("varchar type used without type params");  } 
/*      * How many data columns is the partition reader actually supplying?      */
//  no checks for non-secure hadoop installations 
//  Set up the JDBC connection pool 
//  It is assumed the caller have already allocated write id for adding/updating data to   the acid tables. However, DDL operatons won't allocate write id and hence this query   may return empty result sets.   Get the write id allocated by this txn for the given table writes 
//  Report success for all other cases. 
/*  (id between 23 and 45) and       first_name = 'alan' and       substr('xxxxx', 3) == first_name and       'smith' = last_name and       substr(first_name, 3) == 'yyy'  */
/*    * INTERVAL_DAY_TIME.    */
/*  Construct a string of column names based on the number of column types  */
// this is actually a ALTER TABLE DROP PARITITION statement 
//  if serde is null, the input doesn't need to be spilled out 
//  Ignore. 
/*        * Not able to find thread to execute the job request. Raise Busy exception and client       * can retry the operation.        */
//  check the mere mortals! 
//  We can clear the global error when we see that it was set in a   descendant node of a group by expression because   processGByExpr() returns a ExprNodeDesc that effectively ignores   its children. Although the error can be set multiple times by   descendant nodes, DFS traversal ensures that the error only needs to   be cleared once. Also, for a case like   SELECT concat(value, concat(value))... the logic still works as the   error is only set with the first 'value'; all node processors quit   early if the global error is set. 
// this does "Path.uri.compareTo(that.uri)" 
//  DROP_TABLE EVENT to partitioned table 
//  reached the end of the result file 
//  if the data is not escaped, simply copy the data. 
//  We cache the values 
//  else do the common code at the end. 
//  Various final services, configs, etc. 
/*            * We previously assigned *some* rows with non-NULL values. The batch indices of           * the unassigned row were tracked.            */
//  with HIVE-11304, hive.root.logger cannot have both logger name and log level.   if we still see it, split logger and level separately for hive.root.logger   and hive.log.level respectively 
//  Failed to dump the side-table, remove the partial file 
//  Merge the two into the lateral view join   The cols of the merged result will be the combination of both the   cols of the UDTF path and the cols of the all path. The internal   names have to be changed to avoid conflicts 
//  HADOOP_JOB_ID 
//  Used to support a.b where a is a list of struct that contains a field   called b.   a.b will return an array that contains field b of all elements of array a. 
// no rows match 
//  Timeout for nodes is larger than delay - immediate allocation 
// rename partition 
//  if we are working on a stripe, over the min stripe size, and   crossed a block boundary, cut the input split here. 
//  if one of the child conditions is true/false. 
//  Read the list 
// initialize mapwork with smbMapJoin information. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBinaryStream(java.lang.String,   * java.io.InputStream, long)    */
//  A pending update is not done.   The task has terminated, out of date heartbeat. 
/*    * Setup our left semi join specific members.    */
//  Methods summary 
//  Current nodes in the cache 
// find all Acid FileSinkOperatorS 
//  union is encountered for the first time 
//  if there is any partition column (in static partition or dynamic 
//  This is not a new key; we'll overwrite the key and hash bytes - not needed anymore. 
//  this should throw ClassCastException 
//  we need side file for this test, so we create 2 txn batch and test with only one 
//  returns Set<?> 
//  Do not support MM tables either at this point. We could do it with some extra logic. 
//  Ensure that it is a full qualified path (in most cases it will be since tbl.getPath() is full qualified) 
//  2) Get locks that are relevant:   - Exclusive for INSERT OVERWRITE. 
//  All other primitive types are simple 
//  Verify if HWM is properly set after REPL LOAD 
/*    * build:   *    ^(TOK FROM   *        ^(TOK_SUBQUERY   *            {the input SubQuery, with correlation removed}   *            subQueryAlias    *          )    *     )    */
//  Do semantic analysis and plan generation 
//  StandardList uses ArrayList to store the row. 
// T2 is an acid table so this should fail 
//  do row resolve once more because the ColumnInfo in row resolver is already removed 
//  Replicate only 2 INSERT INTO operations. 
/*  256 files x 1000 size for 99 splits  */
//  Where a List<Partitions is already provided   Where we want to fetch Partitions lazily when they're needed. 
//  Create source table. 
//  get current mapred work and its local work 
//  Test with remote metastore service 
//  Note: this tableExport is actually never used other than for auth, and another one is 
//  Not efficient, but we don't expect this to be called frequently. 
//  Propagate this value from HS2; don't allow users to set it.   In HS2, initConf will be set; it won't be set otherwise as noone calls setupPool. 
//  If we are doing an acid operation they will always all be true as RecordUpdaters always   collect stats 
//  Misc DDL 
//  We are waiting for next block. Either we will get it, or be told we are done. 
//  Validate input to ReduceWork. 
//  push not through between... 
// which is not tracked directly but available on /jobs/<id> node via "mtime" in Stat 
//  There is an extra dependency on MetricsRegistry for snapshot IF. 
//  There was a parallel deallocate; it didn't account for the memory. 
//  Check if table/partition in C doesn't have ckpt property 
// the alias 
//  set database specific parameters 
//  add to the map 
//  Irrelevant. 
//  also populate with StorageDescriptor->SerDe.Parameters 
//  print out the location of the log file for the user so 
//  Create the configuration hadoop-site.xml file 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNull(java.lang.String, int)    */
//  Wait for all invocations to complete. 
//  Gobble up the exception. Message delivery is best effort. 
//  Alias 
//  Now expand the view definition with extras such as explicit column   references; this expanded form is what we'll re-parse when the view is 
// the split is from something other than the 1st file of the logical bucket - compute offset 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.newBuilder() 
//  perform data operation 
// check it has expected version marker 
//  There should really only be one line with "Script failed..." 
//  unequal strings 
//  Bootstrap dump/load 
//  select first 100 and last 100 rows 
//  reduce side work 
//  executed on too small number of reducers. 
//  Use DFS to traverse all the branches until RS or DPP is hit. 
//  first write on a table will allocate write id and rest of the writes should re-use it. 
// ================ 
//  We do not need Zookeeper at the moment 
//  This has to be done synchronously to avoid the caller getting this session again.   Ideally we'd get rid of this thread-local nonsense. 
//  initialize stats publishing table 
//  last item 
//  TS operator 
//  For the case no implicit type conversion, e.g., varchar(5) and varchar(10),   pick the common type for all the keys since during run-time, same key type is assumed. 
//  let's say that passing null in will not do any filtering. 
//  Has the table changed since the query was cached?   For transactional tables, can compare the table writeIDs of the current/cached query. 
// (assume) not a temp table - Try underlying client 
//  Bloom Filter uses binary 
//  We should have some QUERY; and also its parent because by supposition we are in subq. 
//  Skip for tests if WM is not present. 
// driverRun("insert overwrite table rc5318 select * from inpy"); 
//  6.2. Ensure we have stripe metadata. We might have read it before for RG filtering. 
//  a column family become a MAP 
/*  @bgen(jjtree) Xception  */
//  should never come here 
// Plan is using DummyPartition, so can only lock the table... unfortunately 
//  This is a catch all state - when containers have not started yet, or LLAP has not started yet. 
//  SIGNED comparison to Long.MIN_VALUE decimal. 
//  Cache column-list from this.sd. 
//  if the hostname doesn't contain a port, add the configured port to hostname 
/*    * BINARY.    */
//  Null out final members. 
//  beware of any implementation whose hashcode is mutable by reference   inserting into a Map and then changing the hashcode can make it    disappear out of the Map during lookups 
// this simulates the completion of txnid:2 
//  Constructor useful making a projection vectorization context.  E.g. VectorSelectOperator.   Use with resetProjectionColumns and addProjectionColumn. 
//  class Iterator; 
//  If rootNotModified is false, then startIndx and endIndx will be stale. 
//  Missing class setting field 
//  The table should also be considered a part of inputs, even if the table is a   partitioned table and whether any partition is selected or not 
//  add a fake partition dir on fs 
//  3.2 Rank functions type is 'int'/'double' 
//  Add the path to alias mapping 
//  return array of 6 fields, where the last field has the actual data 
//  output column of the ReduceSink operator 
//  write totalSeconds, nanos to DataOutput 
//  Serialize context. 
//  @@protoc_insertion_point(class_scope:NotTezEvent) 
//  RequestManager will catch this and handle like any other error. 
/*  if client is requesting fetch-from-start and its not the first time reading from this operation       * then reset the fetch position to beginning        */
//  Necessary to compare against HiveConf defaults as hive-site.xml is not available on task nodes (like AM). 
//  precision 10 
//  add needed columns 
//  Kryo setter 
//  This is dealing with tasks from a different submission, and cause the kill   to go out before the previous submissions has completed. Handled in the AM 
//  In strict mode, in the presence of order by, limit must be specified. 
//  1. Extract join type 
//  a single call to get all column stats for all partitions 
//  ignore the predicate in case it is not a sampling predicate 
//  Duplicates logic in TextMetaDataFormatter 
//  precision 11 
//  Before any activity on the table, no open IDs 
/*  (non-Javadoc)   * This provides a LazyLong like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  The first group. 
//  Start all the Outputs. 
//  Open a txn with no writes. 
//  set hive-site.xml to default hive-site.xml that has embedded metastore 
//  NOTE: here we should use the new partition predicate pushdown API to get a list of pruned list, 
//  the max size of memory for buffering records before writes them out 
//  Verify schema 
//  decimalToTimestamp should be consistent with doubleToTimestamp for this level of   precision. 
//  has reached the end of the current batch. Let's fetch the next batch. 
//  Sets the sticky bit on stickyBitDir - now removing file kv1.txt from stickyBitDir by   unprivileged user will result in a DFS error. 
/* ACID tables have complex directory layout and require merging of delta files          * on read thus we should not try to read bucket files directly */
//  ExprNodeColumnDesc, ExprNodeConstantDesc, ExprNodeDynamicValueDesc, etc do not have   LEAD/LAG inside. 
//  If it is not an INSERT, we do not need to anything 
//  CAPABILITIES 
//  existsOrdering AND existsPartitioning should be false. 
//  LOG.debug(CLASS_NAME + " logical " + logical + " batchIndex " + batchIndex + " NULL"); 
//  \1 followed by each key and then each value 
//  Append task specific info to stagingPathName, instead of creating a sub-directory.   This way we don't have to worry about deleting the stagingPathName separately at   end of query execution. 
//  because divisor is negative, quotient is at most 1.   remainder must be dividend itself (quotient=0), or dividend -   divisor 
//  f(0) is always 1 
//  F | unknown | unknown 
//  Empty value, too. 
//  System.err.println(c.requesturi); 
/*  * Cache of delegation tokens.  When {@link TempletonControllerJob} submits a job that requires * metastore access and this access should be secure, TCJ will add a delegation token to the * submitted job.  When the job completes we need to cancel the token since by default the token * lives for 7 days and over time can cause OOM (if not cancelled).  Cancelling from  * TempletonControllerJob.LauchMapper mapper (via custom OutputCommitter for example) requires * the jar containing HiveMetastoreClient (and any dependent jars) to be available on the node * running LaunchMapper.  Specifying transitive closure of the necessary jars is  * configuration/maintenance headache for each release.  Caching the token means cancellation is  * done from WebHCat server and thus has Hive jars on the classpath. *  * While it's possible that WebHCat crashes and looses this in-memory state, but this would be an * exceptional condition and since tokens will automatically be cancelled after 7 days,  * the fact that this info is not persisted is OK.  (Persisting it also complicates things  * because that needs to be done securely) * @see TempletonControllerJob  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNClob(java.lang.String, java.sql.NClob)    */
//  search for the key 
//  RENAME_TABLE EVENT to unpartitioned table 
/*  @bgen(jjtree) Service  */
//  first find out if any of the jobs needs to run non-locally 
// sort for readability 
//  it matters only for permanent functions 
//  keep a mapping from tag to the fetch operator alias 
//  COL_NAMES 
/*  storage of table could be on any storage system: hbase, cassandra etc.  */
//  The results of this query execution might be cacheable.   Add a placeholder entry in the cache so other queries know this result is pending. 
/*    * If this task contains a join, it can be converted to a map-join task if this operator is   * present in the mapper. For eg. if a sort-merge join operator is present followed by a regular   * join, it cannot be converted to a auto map-join.    */
//  (and those from IN(...) follow it) 
//  UNDONE: Inner count 
//  No multi-parameter aggregations supported. 
//  Attempt to delete temp file, if this fails, not much can be done about it. 
//  check this because "123 << 32" will be 123. 
/*    * Inner join (hash map).    */
/*    * Allocate the source conversion related arrays (optional).    */
// Test regular outputformat 
//  5. Create Join rel 
//  merge should update registers and hence the count 
//  hive conf 
//  Do not check the state - this is coming from the updater under epic lock. 
//  Pre-allocated member for storing the (physical) batch index of matching row (single- or 
//  First, determine whether rounding is necessary based on rounding point, which is inside   integer part.  And, get rid of any fractional digits.  The result scale will be 0. 
//  Now check QB in more detail. canHandleQbForCbo returns null if query can 
//  an error occurred, re-try 
//  TODO: the only reason this is done this way is because we want unique Subject-s so that         the FS.get gives different FS objects to different fragments. 
// todo: make these like OperationType and remove above char constatns 
//  We recursively create the exprNodeDesc. Base cases: when we encounter   a column ref, we convert that into an exprNodeColumnDesc; when we   encounter   a constant, we convert that into an exprNodeConstantDesc. For others we   just   build the exprNodeFuncDesc with recursively built children. 
//  verify when second argument is repeating 
// no exception should be thrown 
//  We provide a faster way to write a hive interval year month without a HiveIntervalYearMonth object. 
//  strip off the STOP marker, which may be left if all the fields were in   the serialization 
//  have different settings from those of HiveServer2. 
//  Prepare prefix and suffix 
//  all table column names 
//  uri is added later 
//  We assume the caller will handle extra columns default with nulls, etc. 
//  this is only useful for the daemons to know themselves 
//  TODO: check defaults: maxTimeout, keepalive, maxBodySize,   bodyRecieveDuration, etc. 
//  Currently, we support LazySimple deserialization:        org.apache.hadoop.mapred.TextInputFormat      org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     AND        org.apache.hadoop.mapred.SequenceFileInputFormat      org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe 
//  set permissions for current user on DAG 
/*      * 1. On encountering a DOT, we attempt to resolve the leftmost name     *    to the Parent Query.     * 2. An unqualified name is assumed to be a SubQuery reference.     *    We don't attempt to resolve this to the Parent; because     *    we require all Parent column references to be qualified.     * 3. All other expressions have a Type based on their children.     *    An Expr w/o children is assumed to refer to neither.      */
//  + ")"; 
//  Update table stats. For partitioned table, we update stats in alterPartition() 
//  A: 1/1 running, 1 queued; B: 2/2 running, C: 1/2 running, D: 1/1 running, 1 queued. 
//  verify NULL output in entry 1 is correct 
/*    * The JDBC spec says when you have duplicate column names,   * the first one should be returned.    */
//  leftFast2 != 0 && leftFast1 == 0. 
//  try to find the file on the include path 
//  Find the buddy of the header at list level. We don't know what list it is actually in. 
//  We are in a stack trace 
//  Move all the partition columns at the end of table columns. 
//  no errors are tolerated 
/*  1 files x 1000 size for 111 splits  */
//  If there's a delegation token available then use token based connection 
//  Use that as the "current user" 
//  Start a third batch, abortTransaction everything, don't properly close it 
//  We call copyFromLocal below, so we basically assume src is a local file. 
//  We cannot push limit; bail out 
/*  *  */
/*    * This TableScanDesc flag is strictly set by the Vectorizer class for vectorized MapWork   * vertices.    */
//  most likely this value should not exist 
//  Clear away any residue from our optimizations. 
//  If inputs are not equal, we could zip up till here 
//  Void can go to anything 
//  #2 - UTC epoch for instant 
//  Configure getPassword() to fall back to conf if credential doesn't have entry  
//  Batch is full AND we have at least 1 more row... 
// change value of a metavar config param in new hive conf 
//  else create a new one 
//  read logs 
//  No valid inputs. 
//  Start delegation token manager 
//  For reasons that are completely incomprehensible to me the semantic   analyzers often ask for multiple locks on the same entity (for example   a shared_read and an exlcusive lock).  The db locking system gets confused   by this and dead locks on it.  To resolve that, we'll make sure in the   request that multiple locks are coalesced and promoted to the higher   level of locking.  To do this we put all locks components in trie based   on dbname, tablename, partition name and handle the promotion as new   requests come in.  This structure depends on the fact that null is a   valid key in a LinkedHashMap.  So a database lock will map to (dbname, null,   null). 
//  Non-empty java opts without -Xmx specified 
//  100 < x   start exclusive to infinity 
// in the clientUgi 
//  this command has terminated 
//  6. Iterate over all expression (after SELECT) 
//  Delete the data in the table 
//  Server thread pool   Start with minWorkerThreads, expand till maxWorkerThreads and reject 
// i.e. it's the 1st WHEN MATCHED 
// Tag of union field is the first byte to be parsed 
//  Update our counts for the last key. 
//  initialize the lazy object 
/*   */
//  that we will propagate to the inputs of the join 
//  In "select * from table" situations (non-MR), we can add things to the job   It's safe to add this to the job since it's not *actually* a mapred job. 
// Protect against a bad location being requested. 
/*  New method that distributes the Select query by creating splits containing   * information about different Druid nodes that have the data for the given   * query.  */
//  ARRAY_ENTRY 
//  step 2 (ANALYZE_STATE.ANALYZING), explain the query and provide the runtime #rows collected. 
//  Now, try to find the file based on SHA and name. Currently we require exact name match. 
//  dummy ops need to be updated to the cloned ones. 
//  tablescan and join operators. 
//  Any more left? 
//  MAP_ENTRY 
//  When overwriting, we just start with empty timeline, 
// Expand the array 
// when set to true use the overflow checked vector expressions 
//  Doesn't support creating VRBs. 
//  Generate result within big table batch itself. 
//  dummy alias: just use the input path 
//  We've found something that matches what we're trying to lock, 
//  if numPartitions could not be obtained from ORM filters, then get number partitions names, and count them 
//  -yyyyyyy-mm  : should be more than enough 
//  5. Gather GB Physical pipeline (based on user config & Grping Sets size) 
// read friendly string: ak[EXT]av[STX]bk[ETX]bv[STX]ck[ETX]cv[STX]dk[ETX]dv 
// Set HADOOP_USER_NAME env variable for child process, so that   it also runs with hadoop permissions for the user the job is running as 
//  add -r and --dry-run to generate list only 
/*    * (non-Javadoc) we should ideally not modify the tree we traverse. However,   * since we need to walk the tree at any time when we modify the operator, we   * might as well do it here.    */
//  This function should be overriden in every sub class   And the sub class should call super.init(m, parameters) to get mode set. 
//  we failed to submit after retrying. Destroy session and bail. 
//  Negative power with range -- adjust the scale. 
//  should copy properties first 
// https://commons.apache.org/proper/commons-pool/api-1.6/org/apache/commons/pool/impl/GenericObjectPool.html#setMaxActive(int) 
//  converted to sort-merge join 
// List<RolePrincipalGrant> roleGrantsList = getRolePrincipalGrants(roleMaps); 
//  Second incremental dump 
//  queryDirectory should not be null 
//  Go over all the destination structures and populate the related 
//  Append prefix 
//  exhausted the batch, no longer have to heartbeat for current txn batch 
//  involving constant true/false values. 
//  replace the "commar" to finish a 'IN' clause string. 
/*  there are nulls  */
//  Any redirect handlers need to be added first 
//  To avoid reading the footer twice, we will cache it first and then read from cache.   Parquet calls protobuf methods directly on the stream and we can't get bytes after the fact. 
// create failed compactions 
//  GBY without distinct keys is not prepared to process distinct key structured rows. 
//  it is a column i.e. a column-family with column-qualifier 
//  Evaluate the keys 
//  Replicate all the events except DROP 
//  TODO: Maybe throw AlreadyExistsException. 
//                c 
//  Otherwise, notify about Spark jobs after the state notification. 
/*    * Grouping sets members.    */
//  Do nothing 
// this code doesn't propagate      Assert.assertEquals("Wrong msg", ErrorMsg.CTAS_PARCOL_COEXISTENCE.getErrorCode(), cpr.getErrorCode()); 
//  Note: it's rather important that this (and other methods) catch Exception, not Throwable;   in combination with HiveSessionProxy.invoke code, perhaps unintentionally, it used   to also catch all errors; and now it allows OOMs only to propagate. 
//  not an event dump, not a table dump - thus, a db dump 
//  Last try: we try to parse it as date and transform 
//  walk. 
//  Re-setting the queue config is an old hack that we may remove in future. 
//  to trigger vectorForward 
//  non-verbose pattern is %-5p : %m%n. Look for " : " 
//  Make sure result precision/scale matches the input prec/scale 
//  optimize this newWork given the big table position 
//  Someone is allocating this arena. Wait a bit and recheck. 
//  to disk for the Hybrid Grace hash partitioning. 
//  will trigger 2 spills 
//  The child tasks may be null in case of a select 
//  Going through file list and make the retry list 
//  verifying that method is supported 
//  5. We create the new filter that might be pushed down 
//  dummy operator (for not increasing seqId) 
//  join keys dont match the bucketing keys 
//  get current input file name 
//  nothing needed here by default 
//  set yarn queue name 
//  go into the doAs below. 
//  V1 to V2 conversion. 
//  _hive.tmp_table_space, _hive.hdfs.session.path, and _hive.local.session.path are respectively   saved in hdfsTmpTableSpace, hdfsSessionPath and localSessionPath.  Saving them as conf   variables is useful to expose them to end users.  But, end users shouldn't change them. 
//  Test new HCatAddPartitionsDesc API. 
//  Chooses a representative alias and index to use as the String, the first is used because   it is set in the constructor 
//  This won't go into checkAndSend. 
//  A statement should be open even after ResultSet#close 
//  2. CPU cost = sorting cost 
//  Last batch can sometimes have less number of elements 
/*  Spot check correctness of decimal scalar multiply decimal column. The case for   * addition checks all the cases for the template, so don't do that redundantly here.    */
//  create conditional task and insert conditional task into task tree 
/*  trimBlanks  */
//  Add a mapping from the table scan operator to Table 
//  Relying on watchService.close to clean up all pending watches 
//  bypass for explain queries for now 
//  Exclude all standard table properties. 
//  Now the most important check - when we query this record for its schema, 
//  These schemata are used in other tests 
/*  Get a [NOT] BETWEEN filter expression. This is treated as a special case   * because the NOT is actually specified in the expression tree as the first argument,   * and we don't want any runtime cost for that. So creating the VectorExpression   * needs to be done differently than the standard way where all arguments are   * passed to the VectorExpression constructor.    */
//  When selectedInUse is set to false, everything in the batch is selected. 
//  This is effectively DAG completed, and can be used to reset statistics being tracked. 
//  get the object inspector for MyRow 
//  Note: Enhance showResourcePlan to display all the pools, triggers and mappings. 
//  Not HiveInputFormat, or a custom VertexManager will take care of grouping splits 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#execute(java.lang.String)    */
/*      * If the query was the result of analyze table column compute statistics rewrite, create     * a column stats task instead of a fetch task to persist stats to the metastore.     * As per HIVE-15903, we will also collect table stats when user computes column stats.     * That means, if isCStats || !pCtx.getColumnStatsAutoGatherContexts().isEmpty()     * We need to collect table stats     * if isCStats, we need to include a basic stats task     * else it is ColumnStatsAutoGather, which should have a move task with a stats task already.      */
//  that follows it. This is used for connecting them later. 
//  allow +/- 
//  verify cm.recycle(db, table, part) api moves file to cmroot dir 
//  this means no partition exists for the given partition   key value pairs - thrift cannot handle null return values, hence   getPartition() throws NoSuchObjectException to indicate null partition 
//  Try to read the default named url from the connection configuration file 
//  We already retrieved the incoming info, check without UGI. 
//  We need to track this as some listeners pass it through our config and we need to honor 
// Work 
//  Notification is generated for newly created partitions only. The subset of partitions 
//  DB-level REPL LOADs testing done, now moving on to table level repl loads.   In each of these cases, the table-level repl.last.id must move forward, but the   db-level last.repl.id must not. 
// may flood the log 
//  By hive 0.13 we should remove this code. 
// this creates an ORC data file with correct schema under table root 
//  End RelMdParallelism.java 
//  check aggOutputProject projects only one expression 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareStatement(java.lang.String, int, int, int)    */
//  Wait for all threads to be ready. 
//  log a warning incase no reporters were successfully added 
//  This is a true DROP TABLE 
//  is the left was at the left side of a right outer join? 
//  metastore schema only allows maximum 255 for constraint value column 
//  different, or duplicate some other function). 
//  LogicalProject maps a set of rows to a different set;   Without knowledge of the mapping function(whether it   preserves uniqueness), it is only safe to derive uniqueness   info from the child of a project when the mapping is f(a) => a.     Further more, the unique bitset coming from the child needs   to be mapped to match the output of the project. 
//  test third IF argument repeating 
//  At this point we don't have to do anything special. Just   run through the regular paces w/o creating a new task. 
//  if partition is not found   it is DESCRIBE table partition 
//  No customization of this API is done for most Authorization implementations. It is meant    to be used for special cases in Apache Sentry (incubating)   null is to be returned when no customization is needed for the translator   see javadoc in interface for details. 
//  create OperationLog object with above log file 
//  test long->string version 
//  this input rel does not produce the cor var needed 
//  Inject a behavior where REPL LOAD failed when try to load table "t2", it fails. 
//  for set role ALL, reset roles to default roles. 
//  num executors is less than max executors per query (which is not expected case), default executors will be 
//  make sure that they have the same type 
//  When selectedInUse is true, start with every bit set to false and selectively set   certain bits to true based on the selected[] vector. 
/*    * LIST.    */
//  Fits in one longword. 
//  38 decimal maximum - 32 digits in 2 lower longs (6 digits here). 
/*  isLastGroupBatch  */
//  precision 12 
//  We are going to serialize using the 4 basic types. 
//  If the first shot fails, then we log the waiting messages. 
//  Now filter. 
//  Now serialize 
//  Note that we cache each slice separately. We could cache them together at the end, but   then we won't be able to pass them to users without inc-refing explicitly. 
//  2. Walk through OB exprs and extract field collations and additional 
//  Start by only serializing primitives as-is 
//  If no join then there should only be either 1 TS or 1 SubQuery 
//  Various errors when creating Spark client 
//  its children's parents lists, also see childOperatorsTag in Operator) at here. 
// Expecting NOT to change the size of internal structures 
//  initialize destination table/partition 
//  set the bit to 1 if a key is not null 
// There is another batch to buffer 
//  CHECK_CONSTRAINTS 
//  If the first child is a TOK_TABLE_OR_COL, and nodeOutput[0] is NULL, 
//  Friday 30th August 1985 02:00:00 AM 
//   This optimizer will serialize all filters that made it to the    table scan operator to avoid having to do it multiple times on    the backend. If you have a physical optimization that changes    table scans or filters, you have to invoke it before this one. 
//  if it does not end with " then it is line continuation 
//  if tbl location is available use it   else derive the tbl location from database location 
//  precision 13 
//  MY_STRING_STRING_MAP 
//  Database name or pattern 
//  this shouldn't happen. The parser should have converted the union to be   contained in a subquery. Just in case, we keep the error as a fallback. 
//  column list 
//  Update cached aggregate stats for all partitions of a table and for all 
//  prefix for window functions, to discern LEAD/LAG UDFs from window functions with the same name 
//  its either a file or glob 
/*  wantWritable  */
//  these anyway. 
//  0 0   0 1 
//  Exclude constants. Aggregate({true}) occurs because Aggregate({})   would generate 1 row even when applied to an empty table. 
//  required   required 
//  Verify the writeId of this committed txn should be invalid for test txn. 
//  if data size is still 0 then get file size 
//  select from  the new table should pass 
//  For pfile, calculate the checksum for use in testing 
//  Optimize: whole decimal fits in two binary words. 
//  If semijoin keys and ts keys completely unrelated, the cardinality of both sets   could be obtained by adding both cardinalities. Would there be an average case? 
//  selPair.getKey() is the operator right before OB   selPair.getValue() is RR which only contains columns needed in result   set. Extra columns needed by order by will be absent from it. 
//  precision 14 
//  No validation. 
//  remember which mapjoin operator links with which work 
//  2. deal with static partition columns 
//  Check all the operators in the stack. Currently, only SELECTs and FILTERs 
//  Examine all digits being thrown away to determine if result is 0 or 1. 
//  5. Let Cleaner delete obsolete files/dirs 
//  generate the local work for the big table alias 
//  in the absence of column statistics, compute data size based on   based on average row size 
//  Project the columns of the GROUP BY plus the arguments   to the agg function. 
//  Calcite expects the grouping sets sorted and without duplicates 
/*      * group grants      */
//  LFU extreme, order of accesses should be ignored, only frequency matters.   We touch first elements later, but do it less times, so they will be evicted first. 
// since setStructFieldData and create return a list, getStructFieldData should be able to  handle list data. This is required when table serde is ParquetHiveSerDe and partition serde  is something else. 
//  Bootstrap Repl A -> B 
/*   we handle three types of scenarios with special case.  1. handling of db Level _metadata  2. handling of subsequent loadTask which will start running from the previous replicationState  3. other events : these can only be either table / function _metadata.    */
//  n-way join   It has been calculated in HashTableLoader earlier, so just need to retrieve that number 
//  partitioned input, not sorted. 
// --------------------------------------- 
//  We assume millisLocal is midnight of some date. What we are basically trying to do   here is go from local-midnight to UTC-midnight (or whatever time that happens to be). 
//  Read the configuration parameters 
//  precision 15 
//  grant 
//  1. If it is not an OR operator, we bail out. 
//  required   required   required   required   required   required   required   required   required   required   required   required   required   required   optional 
//  Use TerminateFragmentRequestProto.newBuilder() to construct. 
//  Re-map arguments. 
//  No static partition specified 
//  the bottom aggregate has converted the DISTINCT aggregate to a group by clause. 
//  mapping of bucket id to number of required tasks to run 
//  using different code blocks so that jdbc variables are not accidently re-used   between the actions. Different connection/statement object should be used for each action. 
//  future only takes final or seemingly final values. Make a final copy of taskId 
/*  * The equality is implemented fully, the implementation sorts the maps * by their keys to provide a transitive compare.   */
// from the map jobs) 
//  Grand-parent works - we need to set these to be the parents of the cloned works. 
//  Save last longword. 
//  precision 16 
//  Nothing to update if everything is the same 
//  Sequence of TableScan operators to be walked 
//  resFile   pCtx   RootTasks   FetchTask   analyzer  explainConfig   cboInfo 
// Update table schema to add the newly added columns 
//                   ----------------------------------------- 
//  Delete the data 
//  Add the setRCols to the input list 
//  Don't eat and wrap RuntimeExceptions because the ObjectBuffer.write...   handles SerializationException specifically (resizing the buffer)... 
//  that something is blocking it that would not block a read. 
//  Re-throw without losing original stack trace. 
//  Create the root of the operator tree 
//  flip the boolean variable 
//  copy the DP column values from the input row to dpVals 
//  Template expansion logic is the same for both column-scalar and scalar-column cases. 
//  precision 17 
//  SparkWork dependency graph - from a SparkWork with MJ operators to all 
//  Don't bother cleaning from the txns table.  A separate call will do that.  We don't   know here which txns still have components from other tables or partitions in the   table, so we don't know which ones we can and cannot clean. 
//  Record this change in the metastore 
//  if the TEST*.xml was not generated or was corrupt, let someone know 
// assert false; 
//  Get delegation token for user from filesystem and write the token along with   metastore tokens into a file 
//  Pass job to initialize metastore conf overrides 
//  precision 18 
//  We can process this batch immediately. 
/*  256 files x 100 size for 99 splits  */
//  These calls are to see how much data there is. The setFromBytes call below will do the same   readVInt reads but actually unpack the decimal. 
//  if const is first argument then evaluate the result 
//  Note: we could use RW lock to allow concurrent calls for different sessions, however all         those calls do is add elements to lists and maps; and we'd need to sync those separately         separately, plus have an object to notify because RW lock does not support conditions 
//  Do not merge if the MapredWork of MapJoin has multiple input aliases. 
//  the client requested that an extra map-reduce step be performed 
// The code inside the attribute getter threw an exception so log it, and   skip outputting the attribute 
/*          * Get our Multi-Key hash multi-set information for this specialized class.          */
//  trigger lazy read of metadata to make sure serialized data is not corrupted and readable 
//  Blindly add this as a integer list, should be sufficient for the test case.   Use the non-settable list object inspector. 
//  Thread is being interrupted. 
// Add hbase properties 
//  We want to wait for the iteration to finish and set the cluster fraction. 
//  End the root rp object. 
//  To be consistent with the behavior of listPartitionNames, if the   table or db does not exist, we return an empty list 
//  No char length available, copy whole string value here. 
//  after recovery there shouldn'table be any *_flush_length files 
/*    * Captures how the Input should be Ordered. This is captured as a list   * of ASTNodes that are the expressions in the Sort By clause in a   * PTF invocation.    */
//  operators with no 
//  Decimal 
//  initialize the array 
//  initialize aliasToWork 
//  Spot check decimal column-column subtract 
//  we did not get token set up by oozie, let's get them ourselves here.   we essentially get a token per unique Output HCatTableInfo - this is   done because through Pig, setOutput() method is called multiple times   We want to only get the token once per unique output HCatTableInfo -   we cannot just get one token since in multi-query case (> 1 store in 1 job)   or the case when a single pig script results in > 1 jobs, the single   token will get cancelled by the output committer and the subsequent   stores will fail - by tying the token with the concatenation of   dbname, tablename and partition keyvalues of the output   TableInfo, we can have as many tokens as there are stores and the TokenSelector   will correctly pick the right tokens which the committer will use and 
//  First check if the table dir exists (could have been deleted for some reason in pre-commit tests) 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getInfo(org.apache.hive.service.cli.SessionHandle, java.util.List)    */
//  Look for interfaces on both the class and all base classes. 
//  Table creation with a long table name causes ConnectionFailureException 
//  try recursive folding 
/*  This method inserts the right profiles into profiles CBO depending   * on the query characteristics.  */
/*    * Scratch arrays used in fastBigIntegerBytes calls for better performance.    */
// If kerberos security is enabled, and HS2 doAs is enabled,   then additional params need to be set so that the command is run as   intended user 
//  rewrite value index for mapjoin 
//  Initially, all deltas and rcs are 0; empty list starts at 0; there are no objects to take. 
//  If there are previous nodes, then AND the current node with the previous one 
//  SMALLINT 
//  input key is bigger than any of keys in hash 
//  run queries 
//  number of reducers. 
//  ADJACENCY_TYPE 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setObject(java.lang.String,   * java.lang.Object)    */
//  @@protoc_insertion_point(class_scope:SubmitWorkRequestProto) 
//  first see if there is a direct match 
/*    * A Boundary specifies how many rows back/forward a WindowFrame extends from the   * current row. A Boundary is specified as:   * - Range Boundary :: as the number of rows to go forward or back from                    the Current Row.   * - Current Row :: which implies the Boundary is at the current row.   * - Value Boundary :: which is specified as the amount the value of an                    Expression must decrease/increase    */
//  If the command has no schema, make sure nothing is printed 
// completion of txnid:idTxnUpdate2 
/*    * Return vector expression for a custom (i.e. not built-in) UDF.    */
//  Strip trailing carriage return on input   Ignore changes whose lines are all blank 
//  pRS-pJOIN-cRS-cGBY 
//  dynamic partition pruning pipeline doesn't have multiple children 
//  RQST 
//  we need to copy to standard object otherwise deserializer overwrites the values 
//  we have a storage specification for a map column type 
// because 1 txn may include different partitions/tables even in auto commit mode 
//  optional int64 dag_start_time = 4; 
/*      * Try to resolve a qualified name as a column reference on the Parent Query's RowResolver.     * Apply this logic on the leftmost(first) dot in an AST tree.      */
//  Recall that the sequence must be pRS-SEL*-cRS 
//  Set up the dynamic values in the childWork. 
//  KEY 
//  Finalize the last record. 
//  Return proper response 
//  setup DB 
/*      * Calculate the variance result when count > 1.  Public so vectorization code can use it, etc.      */
//  min() needed in the case that entire string is whitespace 
//  Finally, if we do not reduce the input size, we bail out 
//  Find TS operators with partition pruning enabled in plan   because these TS may potentially read different data for   different pipeline.   These can be:   1) TS with DPP.   2) TS with semijoin DPP. 
//  expand ALL privileges, if any 
//  8. convert SemiJoin + GBy to SemiJoin 
//  initialize the task and execute 
//  Set the leftmost header of the base and its buddy (that are now being merged). 
//  figure out if there is group by 
//  find out CPU msecs   In the case that we can't find out this number, we just skip the step to print 
/*    * The hash table slots.  For a bytes key hash table, each slot is 3 longs and the array is   * 3X sized.   *   * The slot triple is 1) a non-zero reference word to the key bytes, 2) the key hash code, and   * 3) a non-zero reference word to the first value bytes.    */
//  No capacity. Check if an element needs to be evicted. 
//  else - this means pig's optimizer never invoked the pushProjection   method - so we need all fields and hence we should not call the   setOutputSchema on HCatInputFormat 
//  External table should also check the underlying file size. 
//  We create the timeline for the existing and new segments 
// completion of txnid:idTxnUpdate4 
//  The pending query we were waiting on failed, but there might still be another   pending or completed entry in the cache that can satisfy this query. Lookup again. 
//  Find the PrivRequirements that match on IOType, ActionType, and HivePrivilegeObjectType add   the privilege required to reqPrivs 
//        ROW__ID 
//  TODO: we either have to kill HS2 or, as the non-actor model would implicitly,         hope for the best and continue on other threads. Do the latter for now. 
/*    * An always on bit to insure the key reference non-zero.    */
//  exponent=-7 
//  LlapDaemonMXBean methods. Will be exposed via JMX 
//  The size of deserialized partition shouldn't exceed half of memory limit 
//  Binary (TCP) mode 
//  The reason we do this guard is because when we do not have a good way of initializing   the config to the handler's thread local config until this call, so we do it then.   Once done, though, we need not repeat this linking, we simply call setMetaStoreHandler   and let the AuthorizationProvider and AuthenticationProvider do what they want. 
// When using -e, command is always a single line 
//  add the default SQL completions 
//  Configured warehouse FS is local, don't need to bother checking. 
//  Must be held by same thread 
//  if cascade=true, then we need to authorize the drop table action as well 
//  Ignore the exception because we are not comparing Long vs. String here.   There should never be an exception 
//  12.2.  Introduce exchange operators below join/multijoin operators 
//  start inclusive to end inclusive 
//  Trim off lower fractional digits but with NO ROUNDING. 
//  Use toString which will have exponents instead of toPlainString. 
//  In MR, mapreduce.task.attempt.id is same as mapred.task.id. Go figure. 
//  Test that when a transaction is aborted, the heartbeat fails 
//  Prevent hive configurations from being visible in Spark. 
//  The setupPartitionContextVars uses the prior read type to flush the prior deserializerBatch,   so set it here to none. 
//  it's non-deterministic 
//  Test that existing shared_write db with new exclusive coalesces to 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setCharacterStream(int, java.io.Reader)    */
//  make everything qualify and ensure selected is not in use 
//  Commit has succeeded (since no exceptions have been thrown.) 
//  SparkSubmit will take care of that for us. 
/*      * Get configuration parameters.      */
//  hence grouppingSetsPresent is true only at map side 
//  are columns used by this select operator. 
//  Set up client port - if we have already had a list of valid ports, use it. 
//  THEN NULL ELSE NULL: An unusual "case", but possible. 
//  since after various rules original relnode could have different   corref (or might not have at all) we need to traverse the new node   to figure out new cor refs and put that into map 
//  Test with table name which does not exists in the given database 
//  Will be called before closing the ORC file to stop writing any additional information   to the acid key index. 
//  Turn the tree set into an array so we can move back and forth easily 
//  For conditional expressions 
//  all partitions have been statically removed 
//  verify extension of values in the array 
//  special handling for set role r1 statement 
// make the table ACID 
//  Checking state per node for future failure handling scenarios, where an update 
//  Create a thread pool with #poolSize threads   Threads terminate when they are idle for more than the keepAliveTime   A bounded blocking queue is used to queue incoming operations, if #operations > poolSize 
//  Try with DECIMAL_64 input and DECIMAL_64 output. 
//  The row consists of some string columns, some Array<int> columns. 
//  ANALYZE command 
//  For MM table, we only want to delete delta dirs for aborted txns. 
//  ==== HiveServer2 metadata api types start here ==== //   these corresponds to various java.sql.DatabaseMetaData calls. 
//  4. Insert ReduceSide GB2 
/*  Largest possible base 10 exponent.  Any				 * exponent larger than this will already				 * produce underflow or overflow, so there's				 * no need to worry about additional digits.				  */
//        into LlapNodeId. We get node info from registry; that should (or can) include it. 
//  Float loses some precisions 
//  re-use old object, to prevent needless expr cloning 
//  to support names like _colx:1._coly 
/*    * The following tests spot-check that vectorized functions with signature   * DOUBLE func(DOUBLE) that came from template ColumnUnaryFunc.txt   * get the right result. Null propagation, isRepeating   * propagation will be checked once for a single expansion of the template   * (for FuncRoundDoubleToDouble).    */
//  but do a count on inner side before that to make sure it generates atmost 1 row. 
//  be a separate split 
//  to reverse this. 
/*      * Basic algorithm:     *     * 1. Determine if rounding digit is >= 5 for rounding.     * 2. Scale away fractional digits if present.     * 3. If rounding, clear integer rounding portion and add 1.     *      */
// txn (if there is one started) is not finished 
//  @@protoc_insertion_point(builder_scope:QueryIdentifierProto) 
// get Tokens for default FS.  Not all FSs support delegation tokens, e.g. WASB 
//  Convert the group by to a map-side group by 
//  silly little pager 
//  test backward scan 
//  serialization is the option selected 
//  Create the thread pool for the web server to handle HTTP requests 
//  bootstrap case 
//  if a table is inside view, we do not care about its authorization. 
// Druid Json timestamp column name 
//  required   required   optional   required   required   optional   optional   optional   optional   optional   optional   optional   optional 
//  Round without digits 
/*            * Equal key series checking.            */
//  result privilege 
/*  @bgen(jjtree) FieldType  */
/*  Dynamic partition pruning is enabled in some or all cases if either   * hive.spark.dynamic.partition.pruning is true or   * hive.spark.dynamic.partition.pruning.map.join.only is true    */
//  Auth specific confs 
//  Test not-equals operator for strings and integers. 
/*  Get total number of rows from all in memory partitions  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNCharacterStream(java.lang.String,   * java.io.Reader)    */
//  Run the last combined strategy, if any. 
/*  fastIntegerDigitCount  */
//  Because we use parentheses in addition to whitespace   as a keyword delimiter, we need to define a new ArgumentDelimiter 
// float 
//  slide the column names down by 6 for the name array 
/*        * Just in case we deserialize a decimal with trailing zeroes...        */
//  Build a map of Hive column Names (ExprNodeColumnDesc Name)   to the positions of those projections in the input 
// x= 
//  make sure it is a struct record 
// Test for publish with invalid partition key name 
//  Incomplete message in buffer. 
//  avoid double casting to preserve original string representation of constant. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getCatalog()    */
//  Should have printed out the header for the field schema 
//  for a table we explicitly try to load partitions as there is no separate partitions events. 
//  Handle COUNT/SUM/AVG function for the case of COUNT(*) and COUNT(DISTINCT) 
/*  Calculate the function result for row i of the batch and   * set the output column vector entry i to the result.    */
//  but that Set is immutable 
//  Note: as per our current constraints, the behavior of two parallel activates is         undefined; although only one will succeed and the other will receive exception.         We need proper (semi-)transactional modifications to support this without hacks. 
//  If we are asked to start from begining, clear the current fetched resultset 
//  Remove op from all its parents' child list. 
//  Break out and try executing. 
//  each newInput. 
//  Null qualifier would mean all qualifiers in that family, want an empty qualifier 
//  Have to use the length instead of the actual prefix because the prefix is location dependent   17 is 16 (16 byte MD5 hash) + 1 for the path separator   Can be less than 17 due to unicode characters 
//  Now abort 3.   Compact 4 and 5. 
//  need to do full scan 
//  JOIN, we need to update the state information accordingly 
//  Row offsets will be determined from the reader (we could set the first from last). 
//  [c,a,b,a,b,c] 
//  Check if list element and value are of same type 
//  Should have been replaced. 
//  Connecting to HS2 as foo. 
//  Generate the columns according to the column mapping provided 
//  this should never happen 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#isClosed()    */
/*    * A TableFunction may be able to accept its input as a stream.   * In this case the contract is:   * - startPartition must be invoked to give the PTF a chance to initialize stream processing.   * - each input row is passed in via a processRow(or processRows) invocation. processRow    *   can return 0 or more o/p rows.   * - finishPartition is invoked to give the PTF a chance to finish processing and return any    *   remaining o/p rows.    */
//  If we cannot lock, remove this from cache and continue. 
//  look at hivesubqueryremoverule to see how is this filter created 
//  a subscriber accept the feed and do something depending on the Task type 
//  Return remaining records (if any) from last processed input record. 
//  Duration is an estimate; if the size of the map changes, it can be very different. 
//  Add all. 
//  Tailing zeroes difference ok. 
//  connection. However we retry one more time on NoHttpResponseException 
//  1.3 process join 
//  Root Task cannot depend on any other task, therefore childTask cannot be 
//  virtual columns 
//  select query 
//  struct<col1:struct<a:boolean,b:double>,col2:double> 
//  MySQL returns 0 if the string is not a well-formed numeric value.   return LongWritable.valueOf(0);   But we decided to return NULL instead, which is more conservative. 
//  initialize buffer to read the entire stripe. 
//  group_b: user2 
//  keep the parent id correct 
//  GRANT_REQUEST 
/*      * Repeating IF expression?      */
//  remove all parameters that are tested.  if the parameter is tested it is part of 
//  Loop while you either have tasks running, or tasks queued up 
/*  Multiply by the same power of ten to shift the decimal point back to     * the original place. Places to the right of the decimal will be zero.      */
//  Test string column to string literal comparison 
// base has 10 rows, so 5 splits, 1 delta has 2 rows so 1 split, and 1 delta has 3 so 2 splits 
//  get partitionFilterString stored in the UDFContext - it would have   been stored there by an earlier call to setPartitionFilter   call setInput on HCatInputFormat only in the frontend because internally   it makes calls to the hcat server - we don't want these to happen in   the backend   in the hadoop front end mapred.task.id property will not be set in 
//  compute product of distinct values of grouping columns 
//  Someone else is also trying to append 
//  Columns 
//  add this dummy op to the dummp operator list 
//  Splice the section that we have evicted out of the list.   We have already updated the state above so no need to do that again. 
//  Unregister the functions as well 
// After processing subqueries and source tables, process   partitioned table functions 
//  Attempt to cleanup stack trace elements that vary by VM. 
// grape expects excludes key in args map 
/* It's imperative that {@code acquireLocks()} is called for all commands so that      HiveTxnManager can transition its state machine correctly */
//  Delegate updates over to the source state tracker. 
//  Bootstrap load in replica 
//  know if merge MR2 will be triggered at execution time 
//  before each test 
//  Create a znode under the rootNamespace parent for this instance of the server 
/*      * All ROW__IDs are unique on read after conversion to acid     * ROW__IDs are exactly the same before and after compaction     * Also check the file name (only) after compaction for completeness     * Note: order of rows in a file ends up being the reverse of order in values clause (why?!)      */
//  If the url passed to us is a valid url with a protocol, we use it as-is   Otherwise, we assume it is a name of parameter that we have to get the url from 
/*      * Create our vectorized copy row and deserialize row helper objects.      */
/*  * Specialized class for doing a vectorized map join that is an inner join on a Single-Column Long * and only big table columns appear in the join result so a hash multi-set is used.  */
// adjust counters and buffer limit 
//  in type system for this. 
//  Adding the reducers run time statistics for the job in the QueryPlan 
/*  Validate the operation of renaming a column name.  */
/*    * called if the SubQuery is Agg and Correlated.   * if SQ doesn't have a GroupBy, it is added to the SQ AST.    */
//  If an insert event is found, then return null hence no event is dumped. 
//  loop until the value is correct or we run out of tries 
//  Outer joins with post-filtering conditions cannot be merged 
//  Load each incremental dump from the list. Each dump have only one operation. 
//  coming from below. 
//  Create the table 
//  enable the hook check after the server startup, 
//  From the above checks, we know fast2 is zero. 
/*    * Get optional read variations for fields.    */
//      .5) 
//  c11-c20 
//  Underflow. 
//  Change lock manager to embedded mode 
//  if uncompressedOffset is in a middle of integer encoding runs (RLE, Delta etc.), consume 
// Handle table schema 
// this could be expensive when there are a lot of compactions.... 
//  expressions. 
// the current split should use the preceding split's footerbuffer in order to skip footer correctly. 
//  Make one partitioned 
//  fastScale + absPower > HiveDecimal.MAX_SCALE 
//  collect 
//  Select query 
//  list overhead + (configured number of element in list * size of element) 
//  short 
//  If it is a RIGHT / FULL OUTER JOIN, we need to iterate through the row container   that contains all the right records that did not produce results. Then, for each   of those records, we replace the left side with NULL values, and produce the   records.   Observe that we only enter this block when we have finished iterating through   all the left and right records (aliasNum == numAliases - 2), and thus, we have   tried to evaluate the post-filter condition on every possible combination.   NOTE: the left records that do not produce results (for LEFT / FULL OUTER JOIN)   will always be caught in the genObject method 
//  We can offer ECB even with some streams not discarded; reset() will clear the arrays. 
//  NOTE: this is to work around Hive Calcite Limitations w.r.t OB.   1. Calcite can not accept expressions in OB; instead it needs to be expressed   as VC in input Select.   2. Hive can not preserve ordering through select boundaries.   3. This map is used for outermost OB to migrate the VC corresponding OB   expressions from input select.   4. This is used by ASTConverter after we are done with Calcite Planning 
//  scratch cols are) 
/*  id <> 12  */
//  No updates before it's running. 
//  Make sure the referenced Schema exists 
//  Sort columns specified by table 
//  3. Construct new Row Resolver with everything from below. 
//  byte[] bytes = Arrays.copyOf(output.getData(), output.getLength()); 
//  This create and publish the segment to be overwritten 
// get the local path of downloaded jars. 
//  update key with assigned identifier 
//  useMinMax = minMaxEnabled; 
//  optional int32 dag_index = 2; 
//  Timestamp strings should parse ok 
//  Compactor should only schedule compaction for ttp2 (delta.num.threshold=4), not ttp1 
/*      * additions to the Group By Clause.      */
//  adding columns and limited integer type promotion is supported for ORC schema evolution 
//  CONSIDER: For now, recompute integerDigitCount... 
//  Temp HDFS path for Spark HashTable sink 
//  AM is responsive again (recovery?) 
//        somewhere like ZK? Try to randomize it a bit for now... 
//  Schedule task to cleanup dangling scratch dir periodically,   initial wait for a random time between 0-10 min to 
//  Use table descriptor for columns. 
/*    * same comment as OI applies here.    */
//  AND hash with mask to 0 out sign bit to make sure it's positive.   Then we know taking the result mod n is in the range (0..n-1). 
//  original toString takes too much space 
//  create new MapWork 
//  test that the values we added are there 
//  This "global" allows various validation methods to set the "not vectorized" reason. 
//  The table location already exists and may contain data. 
//  Search for any SparkPartitionPruningSinkOperator in the SparkTask 
//  ------------------------------------------------------------------------------- 
//  ////// 1. Generate ReduceSinkOperator 
/*  Rewrite only analyze table <> column <> compute statistics; Don't rewrite analyze table     * command - table stats are collected by the table scan operator and is not rewritten to     * an aggregation.      */
//  Add the task to the delayed task queue if it does not already exist. 
//  Check column type 
//  Now prepare partnames with 9 partitions: [tab1part1...tab1part8], which are contained in the 
//  Tez/LLAP requires RPC query plan 
// waits for SS lock on T8 from fifer 
//  all children expression should be resolved 
//  This call sets the default ssl params including the correct keystore in the server config 
//  current Key ObjectInspectors are standard ObjectInspectors 
//  check if the file exists 
//  make sure the schema mapping is right 
//  This will break the iterator. However, this is the last task we can add the way this currently   runs (only one duck is distributed when failedUpdate is present), so that should be ok. 
/*  1 files x 1000 size for 11 splits  */
//  Currently, only functions, columns, and scalars supported. 
//  pass the row rather than recordValue. 
//  Prefer methods with a closer signature based on the primitive grouping of each argument.   Score each method based on its similarity to the passed argument types. 
//  get-set methods 
/*  100 files x 100 size for 99 splits  */
// Test outputformat with compression 
//  This is a vectorized aware evaluator 
//  Single column unnamed primary key in default catalog and database 
//  We check whether merging the works would cause the size of   the data in memory grow too large. 
//  may get treated as base if split-update is enabled for ACID. (See HIVE-14035 for details) 
/*    * Called to transform tasks into local tasks where possible/desirable    */
//  Warning note : HMSHandler.getHiveConf() is not thread-unique, .getConf() is. 
/*    * Test the validation of incorrect NULL values in the tables   * @throws Exception    */
//  This method parses the custom dynamic path and replaces each occurrence 
//  Could not find an allowed path to a table scan operator,   hence we are done 
//    1. create the operator tree 
//  Must be deterministic order map for consistent q-test output across   Java versions 
//  Tests for dropPartition(String db_name, String tbl_name, String name,   boolean deleteData) method 
//  Validate the second parameter, which should be an integer 
//  Should never happen. 
//  scaling up, so o is definitely larger 
//  All the split strategies are done, so it must be safe to access splitFutures. 
//  if we do not force script execution, abort   when a failure occurs. 
//  Methods that create relational expressions 
//  input file name (big) to bucket number 
/*    * Use this constructor when there is NO output column.    */
//  this value should get over-written with correct value   ditto 
//  handles the case like line = show tables; --test comment 
//  there should be only one parent. 
// Make sure it itereated through all possible ConnParams 
/*  TODO HIVE-18991    CreationMetadata cm = client.getTable(dbName, tableNames[3]).getCreationMetadata();    cm.addToTablesUsed(dbName + "." + tableNames[1]);    client.updateCreationMetadata(dbName, tableNames[3], cm);     */
//  Set remaining fractional portion to nanos 
//  We shall have enough time to synchronize privileges during loading   information schema 
//  Create a FileSink operator 
//  Return directly if last value is null 
//  Must at least be able to return ti back. 
//  No conversion. 
//  helper methods 
//  Are we consuming too much memory 
//  backtrack can be null when input is script operator 
//  We want to use metricsDir in the same directory as the destination file to support atomic   move of temp file to the destination metrics file 
//  We need to compare partition name with requested name since some DBs   (like MySQL, Derby) considers 'a' = 'a ' whereas others like (Postgres, 
//  If copy fails, fall through the retry logic 
//  Lock the lowest priority buffer; try to evict - we'll evict some other buffer. 
//  Logger the callstack from which the error has been set. 
//  [ONLY] 
//  Generate MapredLocalWorks for MJ and HTS 
//  HIVE-14444 pending rename: afterClass 
//  3.2 if conjunctive predicate elements are more than one, then walk   through them one by one. Compute cross product of NDV. Cross product is   computed by multiplying the largest NDV of all of the conjunctive   predicate   elements with degraded NDV of rest of the conjunctive predicate   elements. NDV is   degraded using log function.Finally the ndvCrossProduct is fenced at   the join   cross product to ensure that NDV can not exceed worst case join   cardinality.<br>   NDV of a conjunctive predicate element is the max NDV of all arguments   to lhs, rhs expressions.   NDV(JoinCondition) = min (left cardinality * right cardinality,   ndvCrossProduct(JoinCondition))   ndvCrossProduct(JoinCondition) = ndv(pex)*log(ndv(pe1))*log(ndv(pe2))   where pex is the predicate element of join condition with max ndv.   ndv(pe) = max(NDV(left.Expr), NDV(right.Expr)) 
//  It's not likely if there is no bug. But in case it happens, we must   have found a wrong filter operator. We skip the optimization then. 
//  process join 
// run Major compaction 
//  constant, add them to colToConstants as half-deterministic columns. 
//  native vector map join hash table setup. 
//  in the cancel case where the driver state is INTERRUPTED, destroy will be deferred to   the query process 
//  MapReduce API. Catch the error, log a debug message and just keep going 
// todo: rename files case 
//  Reconnect is only supported for MR and Streaming jobs at this time 
//  Where 0 is the highest longword; 1 is middle longword, etc. 
//  fill in colstatus 
//  Errors are handled on the way over. FAIL/SUCCESS is informed via regular heartbeats. Killed   via a kill message when a task kill is requested by the daemon. 
// now make sure delete deltas are present 
//  for demo purposes) 
//  This hook verifies that the location of every partition in the inputs and outputs starts with   the location of the table.  It is a very simple check to make sure it is a subdirectory. 
/*      * There are 3 modes of reading for vectorization:     *     *   1) One for the Vectorized Input File Format which returns VectorizedRowBatch as the row.     *     *   2) One for using VectorDeserializeRow to deserialize each row into the VectorizedRowBatch.     *      Currently, these Input File Formats:     *        TEXTFILE     *        SEQUENCEFILE     *     *   3) And one using the regular partition deserializer to get the row object and assigning     *      the row object into the VectorizedRowBatch with VectorAssignRow.     *      This picks up Input File Format not supported by the other two.      */
/*  exponents into floating-point numbers.  */
//  IMPORT statement specified EXTERNAL 
//  a state that the driver enters after close() has been called to clean the query results   and release the resources after the query has been executed 
//  Periodically report progress on the Context object   to prevent TaskTracker from killing the Templeton   Controller task 
//  llap cluster info does not need admin privilege, since it is read only assigning privilege same as 
//  Get key columns 
// convert BytesWritable to byte][ 
//  LazySimpleSerDe can convert any types to String type using   JSON-format. However, we may add more operators.   Thus, we still keep the conversion. 
//  Add an interceptor to add in an XSRF header 
//  Come ride the API roller-coaster! 
//  add a map 
//  The source table now has 2 partitions, one in TEXTFILE, the other in ORC.   Test adding these partitions to the target-table *without* replicating the table-change. 
//  Must be deterministic order map for consistent test output across Java versions 
//  first we try to split the task 
//  this should never happen.   provide a good error message in case there's a bug 
//  SELECT * or SELECT TRANSFORM(*) 
//  If there is a failure from here to until when the metadata is changed,   the partition will be empty or throw errors on read. 
//  Expression splits of each part of the partition 
//  10^32 - 1 
//  make sure they are not public. 
//  Verify cleanup functionality.   Open a new session, since this case needs to close the session in the end. 
//  We found some old value but couldn't incRef it; remove it. 
//  if location specified, set in partition 
//  If no partition cols, just distribute the data uniformly   to provide better load balance. If the requirement is to have a single reducer, we should   set the number of reducers to 1. Use a constant seed to make the code deterministic. 
//  Link import tasks to the barrier task which will in-turn linked with repl state update tasks 
//  actualBatchSize 
//  SUM and COUNT are rolled up as SUM, hence SUM represents both here 
//  Use internal text member to read value 
//  Because TABLE_NO_AUTO_COMPACT was originally assumed to be NO_AUTO_COMPACT and then was moved 
//  Join or Filter does not change the old input ordering. All   input fields from newLeftInput(i.e. the original input to the old 
//  The unit of caching for ORC is (rg x column) (see OrcBatchKey). 
//  No nulls, not repeating 
//  Check interrupt at the last moment in case we get cancelled quickly. 
//  If jar file is in the hdfs, it should be downloaded first. 
//  reducers from the parent operators. 
//  Direct access interfaces. 
//  The syntax should not allow these fields to be null, but lets verify 
//  USERNAME 
//  first remove all the membership, the membership that this role has   been granted 
// this returns the source of corVar i.e. Rel which produces cor var 
//  Show Tracking URL for remotely running jobs. 
// make sure Driver returns all results   drop and recreate the necessary databases and tables 
//  DOUBLE_STATS 
// ---------------------------------------------------------------------------   Inner join specific members.   
//  Find table which name contains _to_find_ in the dummy database 
/*              * The RowResolver setup for Select drops Table associations. So             * setup ASTNode on unqualified name.              */
//  If the spark job finishes before this listener is called, the QUEUED status will not be set 
//  ACCUMULO-3015 Like the above, RangeInputSplit should have the table name 
//  static Pattern regexrid = Pattern.compile("x-id=([-0-9a-f]{36})");   static SimpleDateFormat dateparser = new   SimpleDateFormat("dd/MMM/yyyy:hh:mm:ss ZZZZZ"); 
//  set state as CLOSE as long as all parents are closed 
//  Try to readlock the candidateList; timeout after maxReaderWaitTime 
//  * implies all properties needs to be inherited 
//  Initialize 0.7.0 schema 
//  argument descriptors 
//  The input is sorted by alias, so if we are already in the last join   operand,   we can emit some results now.   Note this has to be done before adding the current row to the   storage,   to preserve the correctness for outer joins. 
// each items is a "key=value" format 
//  Get column names 
// 2. get the rewritten AST 
//  In test setup, we append '/next' to hive.repl.rootdir and use that as the dump location 
//  there were no exception. Batchsize doesn't change until there is an exception 
//  match or UNKNOWN 
//  Decimal -> String 
//  optional   optional   required   required   optional 
//  float types require no conversion, so use a no-op 
//  4. Build operator 
//  found files under currentPath add them to the queue if it is a directory 
// Full test 
//  Update location 
//  There could be several big table input files   mapping to the same small input file.   Find that one with the lowest bucket id. 
//  No need to evaluate, just forward it. 
/*    * This class captures the information about a   * conjunct in the where clause of the SubQuery.   * For a equality predicate it capture for each side:   * - the AST   * - the type of Expression (basically what columns are referenced)   * - for Expressions that refer the parent it captures the   *   parent's ColumnInfo. In case of outer Aggregation expressions   *   we need this to introduce a new mapping in the OuterQuery   *   RowResolver. A join condition must use qualified column references,   *   so we generate a new name for the aggr expression and use it in the   *   joining condition.   *   For e.g.   *   having exists ( select x from R2 where y = min(R1.z) )   *   where the expression 'min(R1.z)' is from the outer Query.   *   We give this expression a new name like 'R1._gby_sq_col_1'   *   and use the join condition: R1._gby_sq_col_1 = R2.y    */
//  Format is <table_name>:<hwm>:<minOpenWriteId>:<open_writeids>:<abort_writeids> 
//  in the form of T partition (ds="2010-03-03") 
//  If there are async requests, satisfy them first. 
//  Order 
//  Expect the correct OIs 
//  Assign repeated value (index 0) over and over. 
//  HAS_RESULT_SET 
//  Only copy data values if entry is not null. The string value   at position 0 is undefined if the position 0 value is null. 
//  clear out any parents as reducer is the root 
//  Turn off skew if an additional MR job is required anyway for grouping sets. 
// with HIVE-15032 this should use static parts and thus not need addDynamicPartitions 
//  The call to ATS appears to block indefinitely, blocking the ATS thread while   the hook continues to submit work to the ExecutorService with each query.   Over time the queued items can cause OOM as the HookContext seems to contain   some items which use a lot of memory.   Prevent this situation by creating executor with bounded capacity - 
//  only ask for the views. 
//  Go through each target column, generate the lineage edges. 
//  If necessary, copy the big table key into the overflow batch's small table   result "area". 
//  Always include headers since they contain non-vectorized objects, too. 
//  1. collect information about CTE if there is any.   The base table of CTE should be masked.   The CTE itself should not be masked in the references in the following main query. 
//  Set one of the partitions to be skipped, so that a command is created for every other one. 
/*            * We may have missed the start of the vertex due to the 3 seconds interval            */
//  We register it so we do not fire the rule on it again 
//  Each qfile may include at most one INCLUDE or EXCLUDE directive.     If a qfile contains an INCLUDE directive, and hadoopVer does   not appear in the list of versions to include, then the qfile   is skipped.     If a qfile contains an EXCLUDE directive, and hadoopVer is   listed in the list of versions to EXCLUDE, then the qfile is   skipped.     Otherwise, the qfile is included. 
//  Stub AccumuloConnectionParameters actions 
/*  The row matches skewed column names.  */
//  2.1 Translate Grouping set col bitset 
//  append the trailing path string, if any 
//  This function is not a deterministic function, but a runtime constant.   The return value is constant within a query but can be different between queries. 
//  9 is the length of "tblprops.". We only keep the rest 
//  aggregate operator 
/*      * Windows that are unbounded following don't benefit from Streaming.      */
//  struct<map1:map<string,string>,map2:map<string,string>>,string 
//  Estimation larger than max 
//  LINT_STRING 
//  All partitions with blurb="isLocatedInTablePath" should have 2 columns, 
//  only contain multi-sourced because multi-sourced cannot be hashed or direct readable 
//  if phase1Result false return 
//  the partition directory. e.g. .../hr=12-intermediate-archived 
//  Otherwise we have to wait until after the masking/filtering step. 
//  get map operator and initialize it 
//  Set appropriate Acid readers/writers based on the table properties. 
//  add a list 
//  If not, then create a set of hanging readers that do sort-merge to find the next smallest   delete event on-demand. Caps the memory consumption to (some_const * no. of readers). 
//  DateColumnArithmeticIntervalYearMonthColumn.txt   DateScalarArithmeticIntervalYearMonthColumn.txt   DateColumnArithmeticIntervalYearMonthScalar.txt     IntervalYearMonthColumnArithmeticDateColumn.txt   IntervalYearMonthScalarArithmeticDateColumn.txt   IntervalYearMonthColumnArithmeticDateScalar.txt     TimestampColumnArithmeticIntervalYearMonthColumn.txt   TimestampScalarArithmeticIntervalYearMonthColumn.txt   TimestampColumnArithmeticIntervalYearMonthScalar.txt     IntervalYearMonthColumnArithmeticTimestampColumn.txt   IntervalYearMonthScalarArithmeticTimestampColumn.txt   IntervalYearMonthColumnArithmeticTimestampScalar.txt 
//  TRIGGERS 
//  TODO HIVE-14042. In case of an abort request, throw an InterruptedException 
//  delete jar added using query1 
//  Aliases for Java Class Names 
//  The union is already initialized. However, the union is walked from   another input   initUnionPlan is idempotent 
//  FUTURE: We could check arg2ColVector.noNulls and optimize these loops. 
//  first violation for the session 
//  get custom path string 
//  value>) 
//  the basic idea is similar to unparseTranslator. 
//  Can be a ref cursor variable 
// when txnid is <> 0, the lock is 
//  If the task cannot finish and if no slots are available then don't schedule it.   Also don't wait if we have a task and we just killed something to schedule it. 
//  resolved task 
//  Ignore. Safe if it does not exist. 
//  check if the character array has the character 
//  5 is the head, 1<<p means the number of bytes for register 
//  Default executor, when no option is specified 
//  Step2: remove any tmp file or double-committed output files 
//  Event 22, 23, 24 
//  implement RelOptRule 
//  Invalidate cached aggregate stats 
//  Currently, partition spec can only be static partition. 
//  Load using same dump to a DB with table. It should fail as DB is not empty. 
//  If it is a CASE or WHEN, we need to check that children do not contain stateful functions   as they are not allowed 
//  Regular insert: export some MM deltas, then import into a new table. 
//  Note : ptnDesc can be null or empty for non-ptn table 
//  Construct a sorted Map of Partition Dir - Partition Descriptor; ordering is based on   patition dir (map key)   Assumption: there is a 1-1 mapping between partition dir and partition descriptor lists 
//  Iterate thru the cols and load the batch 
//  If there is no authorization, anybody has administrator access. 
//  This node was parsed while loading the definition of another view   being referenced by the one being created, and we don't want   to track any expansions for the underlying view. 
//  Remove expression node descriptor and children of it for a given predicate   from mapping if it's already on RS keys. 
//  no-op authentication 
//  now compact and see if compaction still preserves the data correctness 
//  Convert to millis 
//  Now look in the current Hive config value.  Again, avoiding getting defaults 
//  If the new numReducer is less than minReducer, we will not consider   ReduceSinkOperator with this newNumReducer as a correlated ReduceSinkOperator 
//  Add a dummy aggregate stats object for the above parts (part1...part9) of tab1 for col1 
//  5. Same as 3. Also emit extra records from a separate thread. 
//  if mode is all just run it 
//  Case 5: Test with originals, compacted_base, insert_deltas, delete_deltas (exhaustive test) 
//  check. 
//  Since we're passing the object output by the UDTF directly to the next 
//  which is the minimum non-0 value, 0.01 in this case. 
//  Set pending to false since scheduling is about to run. Any triggers up to this point   will be handled in the next run.   A new request may come in right after this is set to false, but before the actual scheduling.   This will be handled in this run, but will cause an immediate run after, which is harmless.   This is mainly to handle a trySchedue request while in the middle of a run - since the event   which triggered it may not be processed for all tasks in the run. 
//  2. Handle sessions that are being destroyed by users. Destroy implies return. 
//  unless already installed on all the cluster nodes, we'll have to 
//  Repeated NULL permutations. 
//  Will also read the last row. 
//  __HIVE_DEFAULT_NULL__ is the system default value for null and empty string.   TODO: we should allow user to specify default partition or HDFS file location. 
//  so wrap it in a big catch Throwable statement. 
/*    * for inner joins push a 'is not null predicate' to the join sources for   * every non nullSafe predicate.    */
//  The default equals provided by thrift compares the comments too for   equality, thus we need to compare the relevant fields here. 
//  Return node 
//  LOG.info("Read hash code for " + Utils.toStringBinary(key, 0, length) 
//  we need to set the merge work that has been created as part of the dummy store walk. If a   merge work already exists for this merge join operator, add the dummy store work to the 
// CombineHiveInputFormat may produce FileSplit that is not OrcSplit 
/*    * STRING.   *    * Can be used to write CHAR and VARCHAR when the caller takes responsibility for   * truncation/padding issues.    */
//  Lead on the whole partition not the iterator range 
// for fullAcid tables we don't delete files for commands with OVERWRITE - we create a new   base_x.  (there is Insert Overwrite and Load Data Overwrite) 
// add the privileges supported in authorization mode V1 
//  construct the list of columns that need to be projected 
//  this is guaranteed to be positive because types only have children   ids greater than their own id. 
//  properties file used to configure log4j2 
//  SR.SW.wait Lock we are examining is waiting.  In this case we keep   looking, as it's possible that something in front is blocking it or   that the other locker hasn't checked yet and he could lock as well or 
//  We've already locked the table as the input, don't relock it as the output. 
//  if the event is already replayed, then no need to replay it again. 
//  constant byte arrays 
//  The partition location already existed and may contain data. Lets try to   populate those statistics that don't require a full scan of the data. 
//  earlier implementation have quoted boolean values...so the new implementation should preserve this 
//  Translated includes may be a superset of writer includes due to cache. 
//  Create table in database in specific location 
//  this is the small table side. 
//  verify that partitioned table partition property set worked. 
//  Check an edge case where the DIRECT_SQL_MAX_QUERY_LENGTH does not allow one 'IN' clause with single value. 
//  if this parent does not contain a constant at this position, we   continue to look at other positions. 
//  Construct a KerberosToken -- relies on ProxyUser configuration. Will be the client making   the request on top of the HS2's user. Accumulo will require proper proxy-user auth configs. 
//  if orc table, restrict changing the file format as it can break schema evolution 
//  Null last 
//  it must be the server uri added by an older version HS2 
//  Add ColumnStatistics for tbl to metastore DB via ObjectStore 
/*      * Add a vector partition descriptor to partition descriptor, removing duplicate object.     *     * If the same vector partition descriptor has already been allocated, share that object.      */
//  avoid processing the same config multiple times, check marker 
/* For reads, whatever SARG maybe applicable to base it's not applicable to delete_delta since it has no      * user columns.  For Compaction there is never a SARG.      *  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setTime(java.lang.String, java.sql.Time,   * java.util.Calendar)    */
//  Single-Column Long specific imports. 
//  Find the location of the table 
//  Sin(double) 
//  Pretend we are in HS2. 
//  If logging is disabled, deny everything. 
//  We started with a single DRL, so we assume there will be no consecutive missing blocks   after the cache has inserted cache data. We also assume all the missing parts will   represent one or several column chunks, since we always cache on column chunk boundaries. 
// in order for this to work hive-site.xml must be on the classpath 
//  A branch is hit. 
/*      * set outputFromWdwFnProcessing      */
//  Go over childPullUpPredicates. If a predicate only contains columns in   'columnsMapped' construct a new predicate based on mapping. 
//  init keyFields 
//  We will iterate through the children: if it is an INSERT, we will traverse 
//  We use BytesWritable because it supports Comparable for our TreeMap. 
//  renew the metastore since the cluster type is unencrypted 
//  if MSB is set to 1 then next qPrime MSB bits contains the value of   number of zeroes.   if MSB is set to 0 then number of zeroes is contained within pPrime - p   bits. 
//  get the ndv 
//     If the patterns OR-AND-EqOp or OR-EqOp are not matched, we bail out 
//  TODO: shut down HS2? 
/*        * Why cannot we just use the ExprNodeEvaluator on the column?       * - because on the reduce-side it is initialized based on the rowOI of the HiveTable       *   and not the OI of the parent of this Operator on the reduce-side        */
//  shut down all the zk servers 
//  multiple udfs with the same max type. Unless we find a lower one   we'll give up. 
//  Type interval_year_month (LongColumnVector storing months). 
//  3: Create a partitioned table T2 => 1 event 
//  parse command line 
//  schema of the map-reduce 'value' object - this is heterogeneous 
//  That is guaranteed to fit any maximum allocation. 
// sort ascending by resource, nulls first 
//  n-gram estimator object 
//  Convert children to aggParameters 
//  corRel.getCondition was here, however Correlate was updated so it 
//  RESOURCE_TYPE 
//  LastAnalyzed is stored per column, but thrift object has it per multiple columns.   Luckily, nobody actually uses it, so we will set to lowest value of all columns for now. 
//  copy cloneToWork to ensure RDD cache still works 
// was false 
//  The output files of a FileSink can be merged if they are either not being written to a table   or are being written to a table which is not bucketed 
//  test if we need group-by shuffle 
//  0 Pending task which is not finishable 
//  dummy vertex is treated as a branch of a join operator 
// Try to parse ms where there is no millisecond part in input, expected to return .000 as ms 
//  checks if a resource has to be uploaded to HDFS for yarn-cluster mode 
//  Case 1- find rows which belong to write Ids that are not valid. 
//  total size of the inputs 
//  connection properties 
//  Creating a new connection is expensive, so we'll reuse this object 
//  test both inputs repeating 
//  Add back the non-expired session. No need to notify, we are the only ones waiting. 
//  The state has changed between this and previous check within this method.   The failed update was rendered irrelevant, so we just exit. 
//  Should ignore the failure 
/*            * Self-Describing Input Format will convert its data to the table schema. So, there           * will be no VectorMapOperator conversion needed.            */
//  Then, scale back. 
/*  * convert a RexNode to an ExprNodeDesc  */
//  use the object pool rather than creating a new object 
//  The response will have one entry per table and hence we get only one ValidWriteIdList 
//  Can only happens w/zcr for a single input buffer. 
/*    * These members have information for assigning a row column objects into the VectorizedRowBatch   * columns.   *   * We say "target" because when there is conversion the data type being converted is the source.    */
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.newBuilder() 
//  invalid inflation factor 
//  Compare the field names using ignore-case semantics 
//  Run with --check-index and save the output to file so it can be checked. 
//  TXN_TO_WRITE_IDS 
//  static class Iterator; 
//  For efficiency alpha is multiplied by m^2 
//  If there are more cores, use the number of cores 
//  Data needs deletion. Check if trash may be skipped.   Trash may be skipped iff:    1. deleteData == true, obviously.    2. tbl is external.    3. Either      3.1. User has specified PURGE from the commandline, and if not,      3.2. User has set the table to auto-purge. 
//  Determine who to run as 
//  upgrade from 2.0.0 schema and re-validate 
//  value based compare.. remove first 
// make sure both buckets are not empty 
//  a Field contains a FieldType which in turn contains a type 
//  Bootstrap dump shouldn't fail if the table is dropped/renamed while dumping it.   Just log a debug message and skip it. 
//  tables and tables inside view. Otherwise, Calcite will treat them as the same. 
//  output is bucketed 
// is present only in the ql/test directory 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.newBuilder() 
//  SQL is 1-indexed, Druid is 0-indexed. 
//  use the tez grouper to combine splits once per bucket 
//  no matter STATS_GENERATED is USER or TASK, all need to re-calculate the stats:   USER: alter table .. update statistics   TASK: from some sql operation which could collect and compute stats 
//  @@protoc_insertion_point(builder_scope:UpdateFragmentResponseProto) 
//  Missing fields? 
/*  Pushes a node on to the stack.  */
//  ensure metatore site.xml does not get to override this 
// create a delta directory 
//  heartbeats can only be sent for open transactions.   there is a race between committing/aborting a transaction and heartbeat.   Example: If a heartbeat is sent for committed txn, exception will be thrown.   Similarly if we don't send a heartbeat, metastore server might abort a txn   for missed heartbeat right before commit txn call. 
//  string representation of folding constant. 
//  The length of the scratch byte array that needs to be passed to bigIntegerBytes, etc. 
// do nothing as it's not a partitioned table 
//  Division by 0. 
//  Changes the owner to a user and verify the change 
//  START_ROW_OFFSET 
//  If the conf does not define any transactional properties, the parseInt() should receive   a value of 1, which will set AcidOperationalProperties to a default type and return that. 
//    construct object of above type   
//  Create the http/https url   JDBC driver will set up an https url if ssl is enabled, otherwise http 
//  The split ends within (and would read) the last row of this slice. Exact match. 
//   LOG.debug("spillSerializeRow spilled batchIndex " + batchIndex + ", length " + length); 
//  For whatever reason, reserve failed. 
//  An error, or couldn't find the task - lastSetGuaranteed does not change. The logic here   does not account for one special case - we have updated the task, but the response was   lost and we have received a network error. The state could be inconsistent, making 
//  output format string is not supported anymore, warn user of deprecation 
//  This is a test. The parameter hive.test.dummystats.publisher's value   denotes the method which needs to throw an error. 
// one call by init, one called here. 
//  c1:int   c2:boolean   c3:double   c4:string   c5:array<int> 
// do nothing to handle RU/D if we add another status 
//  Try fixing, this should result in new fixed file. 
//  For permanent functions, check for any resources from local filesystem. 
//  Check if ckpt property set in table/partition in B after bootstrap load. 
//  Call the different round flavor. 
//  Test the idempotent behavior of CREATE FUNCTION 
// setting success to false to make sure that if the listener fails, rollback happens. 
//  Only create a map-join if the user explicitly gave a join (without a mapjoin hint) 
//  context for list bucketing. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setCharacterStream(java.lang.String,   * java.io.Reader, int)    */
// this only responds to ^C 
//  verify the column name 
//  plumb the KryoMessageCodec instance through the constructors. 
// ========================== 30000 range starts here ========================// 
//  The lateral view forward operator has 2 children, a SELECT(*) and   a SELECT(cols) (for the UDTF operator) The child at index 0 is the   SELECT(*) because that's the way that the DAG was constructed. We   only want to get the predicates from the SELECT(*). 
//  MATERIALIZATION_TIME 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#cancel()    */
//  throw a HiveException for other than rcfile and orcfile. 
//  unparseTranslator. 
//  and return the ones which have a marked column 
//  This Project will be what the old input maps to, 
//  Return defaultName if selExpr is not a simple xx.yy.zz 
//  If table cache is not yet prewarmed, add this to a set which the prewarm thread can check   so that the prewarm thread does not add it back 
//  Joda parsing only supports up to millisecond precision 
//  Use exact byte array which might generate array out of bounds... 
//  total characters = 4; byte length = 10 
//  Check the other side of the join, using the DynamicListContext 
//  We assume this hashtable is loaded only when tez is enabled 
//  Do first comparison as UNSIGNED. 
//  Then, master commits if everything goes well. 
//  no join, no groupby, no distinct, no lateral view, no subq,   no CTAS or insert, not analyze command, and single sourced. 
//  Don't cache the filesystem object for now; Tez closes it and FS cache will fix all that 
//  Check mapred 
//  test basic case 
//  Make sure the FunctionInfo is listed as PERSISTENT (rather than TEMPORARY) 
//  Started ok; initialize context for new batch. 
//  operation 
//  we don't need to add this new entry since there's already an overlapping one 
//  There are several Hive RelNode types which do not have their own visit() method   defined in the HiveRelShuttle interface, which need to be handled appropriately here.   Per jcamachorodriguez we should not encounter HiveMultiJoin/HiveSortExchange   during these checks, so no need to add those here. 
//  TODO: Make expr traversal recursive. Extend to traverse inside   elements of DNF/CNF & extract more deterministic pieces out. 
//  Utility methods used to store pairs of ints as long. 
//  test-specific 
//  Instantiate the ValueProcessor based on the input type 
//  MY_ENUMSET 
//  Test is dependent on getting a new buffer within 1MB. 
//  Key is full table name string of format <db_name>.<table_name> 
//     the rest of optimizations 
//  only used for materialized views   only used for materialized views   only used for materialized views   only used for materialized views   only used for materialized views 
/*     With bucketed target table Union All is not removed    ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree  ~/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505510130462/warehouse/t/.hive-staging_hive_2017-09-15_14-16-32_422_4626314315862498838-1//Users/ekoifman/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505510130462/warehouse/t/.hive-staging_hive_2017-09-15_14-16-32_422_4626314315862498838-1/ -ext-10000     000000_0      _orc_acid_version      delta_0000001_0000001_0000          bucket_00000     000001_0         _orc_acid_version         delta_0000001_0000001_0000             bucket_000015 directories, 4 files */
//  interfere with the view creation). So skip the rest of this method. 
//  check all the arguments 
//  field to find record identifier in   field bucket is in in record id   OI for inspecting record id   OI for inspecting bucket id 
//  Only first or second operator contains DPP pruning 
//  Closing the chunked output stream early gives an error 
//  since we don't clone jobConf per alias 
//  TODO: Allocate work to remove the temporary files and make that   dependent on the redTask 
//  Evaluate 
//  Cannot have SCALAR, SCALAR. 
/*          * Common repeated join result processing.          */
//  may have been   created by   baseCommitter.commitJob() 
//  Check for this pattern.   The pattern matching could be simplified if rules can be applied   during decorrelation.     CorrelateRel(left correlation, condition = true)     LeftInputRel     Aggregate (groupby (0) single_value())       Project-A (may reference coVar) 
//  If not in test mode, then do no create the appender 
//  Always validate ACLs 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getQueryTimeout()    */
//  For backward compatibility: fieldNames can also be integer Strings. 
//  Replace VOID type with string when the output is a temp table or   local files.   A VOID type can be generated under the query:     select NULL from tt;   or   insert overwrite local directory "abc" select NULL from tt;     where there is no column type to which the NULL value should be   converted. 
//  a string of pathnames   the path separator   the file separator in a directory 
//  test null propagation 
/*  * DropTableDesc. * TODO: this is currently used for both drop table and drop partitions.  */
//  make sure REDUCE task environment points to HADOOP_CREDSTORE_PASSWORD 
//  We need some initial values in case user don't call initialize() 
//  Config settings 
/*    * If a QB is such that the aggregation expressions need to be handled by   * the Windowing PTF; we invoke this function to clear the AggExprs on the dest.    */
//  Add the DP path to the list of input paths 
//  Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs. 
//  see if we need to fetch default constraints from metastore 
//  the alias is modified to subq1:a and subq2:a from a, to identify the right sub-query. 
//  scale down/up the column statistics based on the changes in number of   rows from each parent. For ex: If there are 2 parents for JOIN operator   with 1st parent having 200 rows and 2nd parent having 2000 rows. Now if   the new number of rows after applying join rule is 10, then the column   stats for columns from 1st parent should be scaled down by 200/10 = 20x   and stats for columns from 2nd parent should be scaled down by 200x 
//  Check the stats 
//  If any table/partition is updated, then update repl state in db object 
//  make sure the schemas of both sides are the same 
//  Verify if no create table on t1. Only table t2 should  be created in retry. 
//  Signature for wrapped loader, see comments in LoadFuncBasedInputDriver.initialize 
// as --properties-file contains the spark.* keys that are meant for SparkConf object. 
//  check if this user has necessary privileges (reqPrivs) on this object 
//  We will requeue, and not kill, the queries that are not running yet. 
//  update the create table descriptor with the resulting schema. 
//  Optimize inner join keys of small table results. 
//  The middle and lowest longwords highest digit number is LONGWORD_DECIMAL_DIGITS. 
//  Note: this code would be invalid for transactional tables of any kind. 
//  Otherwise, fall through and process the what we saw before possible trailing blanks. 
//  Use hive type name. 
//  whether session is running in silent mode or not 
//  optional int64 purged_memory_bytes = 1; 
/*      * Partition and order by.      */
//  Mixed source (all types) 
//  if not boolean column return half the number of rows 
//  fill in coltype 
//  the column positions in the operator should be like this   <----non-partition columns---->|<--static partition columns-->|<--dynamic partition columns-->          ExprNodeColumnDesc      |      ExprNodeConstantDesc    |     ExprNodeColumnDesc             from input           |         generate itself      |        from input                                  | 
//  If Keep alive is enabled, do not close the connection. 
//  this is our row to test expressions on 
//  Buffer is at the top of the heap. 
//  make the new join rel 
//  3) We annotate the Aggregate operator with this info 
// this table needs to be converted to CRUD Acid 
//  Should make (100, +inf) 
//  Get or create Context object. If we create it we have to clean it later as well. 
//  disable feature 
//  Set to true by default.  Only actively set in the multiple key case to support Outer Join. 
//  Add new entry for this table 
//  Flush current group batch as last batch of group. 
//  infer if any column can be primary key based on column statistics 
//  Value. 
//  First, the cross join 
//  Keys 
//  Partition can't have this name 
// this covers backward compat cases where this prop may have been set already 
//  If this is the first time the table is being initialized to 'transactional=true',   any valid value can be set for the 'transactional_properties'. 
//  expand to all supported privileges 
//  a normal column is also a string 
// char(x),varchar(x) types 
//  Merge and sort result 
//  Check isShutdown opportunistically; it's never unset. 
//  query per mbean attribute 
//  so it doesn't make sense to have both and make sure one matches the other. 
//  Build regular expression for operator rule. 
//  First check - we should not have repeats in results 
//  finally connect the union work with work 
//  get compatible taskId for bucket-name 
//  If we have the optional fourth parameter, make sure it's also an integer 
//  Empty value. 
/*  base this on HiveOperation instead?  this and DDL_NO_LOCK is peppered all over the code...         Seems much cleaner if each stmt is identified as a particular HiveOperation (which I'd think         makes sense everywhere).  This however would be problematic for merge... */
//  Not a flattened struct, no need to unflatten 
//  Redo create-table/view analysis, because it's not part of   doPhase1. 
//  Explain type 
//  We want to make sure this runs at a low priority in the background 
//  set of input to the join that should be   omitted by the output 
//  For the tab.* case, add all the columns to the fieldList   from the input schema 
//  Note Hadoop 2.7.1 onwards includes a RestCsrfPreventionFilter class that is   usable as-is. However, since we have to work on a multitude of hadoop versions   including very old ones, we either duplicate their code here, or not support   an XSRFFilter on older versions of hadoop So, we duplicate to minimize evil(ugh).   See HADOOP-12691 for details of what this is doing.   This method should never be called if Hadoop 2.7+ is available. 
//  no more rows 
//  found dynamic partition pruning operator 
//  In light of results from union queries, we need to be aware that   sub-directories can exist in the partition directory. We want to   ignore these sub-directories and promote merged files to the   partition directory. 
//  production is: bool 
//  template, <ClassName>, <ValueType>, <VarianceFormula>, <DescriptionName>, 
//  time part of the timestamp should not be skipped 
//  OrcInputFormat will get a mock fs from FileSystem.get; add global files. 
//  2a=b 
//  The options --principal/--keypad do not work with --proxy-user in spark-submit.sh   (see HIVE-15485, SPARK-5493, SPARK-19143), so Hive could only support doAs or   delegation token renewal, but not both. Since doAs is a more common case, if both   are needed, we choose to favor doAs. So when doAs is enabled, we use kinit command,   otherwise, we pass the principal/keypad to spark to support the token renewal for 
//  Form a truncated boolean include array for our vector/row deserializers. 
//  if this is not a HASH groupby, return 
//  If the parents have already been created, create the last child only 
//  Check that the hcat result is valid and or has a valid json 
//  If lock response is ACQUIRED, we can create the heartbeater 
//  If op has parents it is guaranteed to be 1. 
//  randomUUID is slow, since its cryptographically secure, only first query will take time. 
//  Drop table after dump 
//  Get the sorted children expr strings 
//  Disallow update and delete on non-acid tables 
//  rest of the data is serialized long values for the bitset which are supposed to be bitwise-ORed. 
//  For a vertex group, all Outputs use the same Key-class, Val-class and partitioner.   Pick any one source vertex to figure out the Edge configuration. 
/*  Allow use of external byte[] for efficiency  */
//  The one join column for this specialized class. 
// use FsShell to change group, permissions, and extended ACL's recursively 
//  we need to convert the jobContext into a jobConf   0.18 jobConf (Hive) vs 0.20+ jobContext (HCat) 
//  No next partition.   No next partition.   Do nothing. 
//  Optimize the query: select count(distinct keys) from T, where   T is bucketized and sorted by T   Partial aggregation can be done by the mappers in this scenario 
//  check the properties expected in hive client without metastore 
//  Test with empty array 
//  unknown | F | F 
/*      * For now use Calcite' default formulas for propagating NDVs up the Query     * Tree.      */
//  The output stream of serialized objects 
//  This is to avoid getting notified of low memory too often and flushing too often. 
//  store the byte every eight elements or 
//  We create data buffers for these columns so we can copy strings into those columns by value. 
//  Make sure we pad the right amount of spaces; valLength is in terms of code points,   while StringUtils.rpad() is based on the number of java chars. 
//  nothing to do if there is no user HS2 connection configuration file   or beeline-site.xml in the path 
//  Also convert to/from binary-sortable representation. 
//  there are no nulls in either input vector 
//  NOTE: UDAF is not included in ExprColMap 
//  Pairwise: Column1HasNulls, Column1IsRepeating, Column2HasNulls, Column2IsRepeating 
//  This can happen for numbers less than 0.1   For 0.001234: bdPrecision=4, bdScale=6   In this case, we'll set the type to have the same precision as the scale. 
//  Same Query   Within dag priority - lower values indicate higher priority. 
//  We update twice to accurately detect if cache is dirty or not 
//  add constant struct field names references overhead 
//  Divide it by 2 so that we can have more reducers 
//  Parse until key separator (currentLevel + 1). 
//  rethrow the SQLException as is 
//  Create non-existent path for 0-row results 
//  introduce RS and EX before FS 
//  Sanity check to make sure there is no alias conflict after merge. 
//  Quote if the database requires it 
//  Left side 
//  Fail heartbeater, so that we can get a RuntimeException from the query.   More specifically, it's the original IOException thrown by either MR's or Tez's progress monitoring loop. 
//  Print only the errors, the operation log and the query results. 
/*        * Use the input RR of TableScanOperator in case there is no map-side       * reshape of input.       * If the parent of ReduceSinkOperator is PTFOperator, use it's       * output RR.        */
//  HIVE-13625 
//  the threshold should be less than 12K bytes for p = 14.   The reason to divide by 5 is, in sparse mode after serialization the   entriesin sparse map are compressed, and delta encoded as varints. The   worst case size of varints are 5 bytes. Hence, 12K/5 ~= 2400 entries in 
//  As this is called from replication task, the user is the user who has fired the repl command.   This is required for standalone metastore authentication. 
//  column/scalar IF 
//  Mapping from column name to default value 
//  We only expect -5 here because we'll get whichever of the partitions published its stats   last. 
//  Each ValidWriteIdList is separated with "$" and each one maps to one table 
// first set basic stats to true 
//  memory pressure. 
//  With uncompressed streams, we know we are done earlier. 
// return the current block's key length 
//  Always set the semijoin optimization as victim. 
//  Locations for each of the storage types 
//  Case when there are changes in multiple table properties. 
//  If the default pool is not disabled, override the size with the specified parallelism. 
//  Always keep transactional tables as managed tables. 
/*      * Basic algorithm:     *     * 1. Determine if rounding part meets banker's rounding rules for rounding.     * 2. Scale away fractional digits if present.     * 3. If rounding, clear integer rounding portion and add 1.     *      */
//  Test in http mode 
//  Add to self. 
//  write out the buffer into a file. Add beeline commands for autocommit and close 
//  @@protoc_insertion_point(builder_scope:LlapOutputSocketInitMessage) 
//  Set bitvector[index] := 1 
//  Reset the driver 
//  Use the defalut methods for next in the child class 
//  CREATION_METADATA 
//  Use its conversion ability. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setSQLXML(java.lang.String,   * java.sql.SQLXML)    */
//  add new token to shared store   need to persist expiration along with password 
//  Now generate the matchs.  Single small table values will be put into the big table   batch and come back in matchs.  Any multiple small table value results will go into 
//  The only time this condition should be false is in the case of dynamic partitioning 
//  test null on left 
//  sparse-sparse overload to dense 
//  FS 
//  Verify batch size 
//  the new table is rhs table 
//  restrict instantiation 
//  If the task hasn't started - inform about fragment completion immediately. It's possible for   the callable to never run. 
/*        * This is a column that we don't want (i.e. not included) -- we are done.        */
/*    * HiveHistory Object    */
//  we don't support using multiple chars as delimiters within complex types 
//  The c'tor should throw the error 
//  Find the positions/order of the sorted columns in the table corresponding 
//  Iterate through the line and invoke the addCmdPart method whenever the delimiter is seen that is not inside a 
//  The number of integer digits in the decimal.  When the integer portion is zero, this is 0. 
//  Cancel existing watches 
//  check if no compaction set for this table 
//  Add the value to the ArrayList 
//  Include state for cached columns 
//  NOTE: We keep the TypeInfo and dataTypePhysicalVariation arrays. 
//  -u <database url> 
//  But note that any sql error will also result in a return of false. 
//  The most correct behavior is to throw only if the request tries to enable the read-only mode. 
//  Start a third batch, but don't close it. 
//  Should generate ['q', +inf) 
//  To indicate whether the pages should be thrown away or not. 
/*  BLOBSTORE section  */
//  Passing char/varchar arguments should prefer the version of evaluate() with Text args. 
/*      * For a predicate check if it is a candidate for pushing down as limit optimization.     * The expression must be of the form rankFn <|<= constant.      */
//  When doing updates and deletes we always want to sort on the rowid because the ACID   reader will expect this sort order when doing reads.  So   ignore whatever comes from the table and enforce this sort order instead. 
//  stats from the record writer and store in the previous fsp that is cached 
// keep track of subqueries which are scalar, correlated and contains aggregate   subquery expression. This will later be special cased in Subquery remove rule   for correlated scalar queries with aggregate we have take care of the case where   inner aggregate happens on empty result 
//  We do not need to evaluate the input row for this parent.   So, we can just forward it to the child of this MuxOperator. 
//  In general case can have unlimited # of branches,   we currently only handle either 1 or 2 branch. 
//  check column order and types 
//  input.get(i+1) since input.get(0) is the schema; 
//  DynamicSerDe always writes out BytesWritable 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setInt(java.lang.String, int)    */
// Use destination table's db location. 
//  Other format pattern should also work 
//  Pass null to complete a batch 
//  partcol=... AND nonpartcol=...   is replaced with partcol=... AND TRUE   which will be folded to partcol=...   This cannot be done also for OR 
//  Maybe someone removed the field; probably ok to ignore. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setTimestamp(java.lang.String,   * java.sql.Timestamp)    */
//  optional parameter 
//  Read the template into a string; 
//  optional int64 timestamp = 3; 
//  get the input expression 
//  write ID with additional uncommitted IDs. Should match. 
//  join keys have difference sizes? 
//  Setup vectorized deserialization for the key and value. 
//  fourth group 
//  1) to partitioned table 
//  The Deadline Exception needs no retry and be thrown immediately. 
//  check if path conforms to Hive's file name convention. Hive expects filenames to be in specific format   like 000000_0, but "LOAD DATA" commands can let you add any files to any partitions/tables without   renaming. This can cause MoveTask to remove files in some cases where MoveTask assumes the files are   are generated by speculatively executed tasks.   Example: MoveTask thinks the following files are same   part-m-00000_1417075294718   part-m-00001_1417075294718   Assumes 1417075294718 as taskId and retains only large file supposedly generated by speculative execution.   This can result in data loss in case of CONCATENATE/merging. Filter out files that does not match Hive's   filename convention. 
//  Serde info 
//  If it was a merge task or a local map reduce task, nothing can be inferred 
//  Hang onto a byte array for holding smaller byte values 
//  This particular test doesn't care which of the lower pri tasks gets the duck. 
//  Have to do this in reverse order so that we drop the materialized view first. 
// hive.spark.* keys are passed down to the RemoteDriver via --conf, 
//  Private methods should never catch SQLException and then throw MetaException.  The public   methods depend on SQLException coming back so they can detect and handle deadlocks.  Private   methods should only throw MetaException when they explicitly know there's a logic error and   they want to throw past the public methods.     All public methods that write to the database have to check for deadlocks when a SQLException   comes back and handle it if they see one.  This has to be done with the connection pooling   in mind.  To do this they should call checkRetryable() AFTER rolling back the db transaction,   and then they should catch RetryException and call themselves recursively. See commitTxn for an example. 
//  override this for using extended FieldObject 
//  run given query and validate expecated result 
//  Only update someone waiting for info if we have the info. 
//  We're going to wait for the session to be abandoned. 
//  Inform the scheduler that this fragment has been killed.   If the kill failed - that means the task has already hit a final condition, 
// sd.setBucketCols(new ArrayList<String>(2));  sd.getBucketCols().add("name"); 
//  Found 
//  ^(TOK_DROPFUNCTION identifier ifExists? $temp?) 
//  Check if DB in B have ckpt property is set to bootstrap dump location used in B and missing for table/partition. 
//  set values by reference, copy the data out, and verify equality 
//  Find a free port 
//  Move the query results to the query cache directory. 
//  Partitioned table:   Need to get the old stats of the partition   and update the table stats based on the old and new stats. 
//  query column stats for column whose stats were updated in the previous call 
//  Move past key separator. 
//  2nd split is for delta_200_200 which is filtered out entirely by "txns" 
//  Jump to the end of current line. When a multiple line query is executed with -e parameter,   it is passed in as one line string separated with '\n' 
// nothing to be done 
//  Case of analyze command 
//  this should never happen. however we want to make sure we propagate the exception 
/*    * Verify table for Key: row and byte[] x Hash Table: HashMap    */
/*  List of partitions that are required - populated from processing each event  */
//  We buffer in a org.apache.hadoop.hive.serde2.ByteStream.Output since that is what   is used by VectorSerializeRow / SerializeWrite.  Periodically, we flush this buffer   to disk. 
//  LAST_EVENT 
//  A udf which sleeps for some number of ms to simulate a long running query 
//  There are 2 paths from the TS operator (or a previous LVJ operator)   to the same LateralViewJoinOperator.   TS -> SelectOperator(*) -> LateralViewJoinOperator   TS -> SelectOperator (gets cols for UDTF) -> UDTFOperator0   -> LateralViewJoinOperator   
//  source file system   list of source paths 
//  Nothing to prune for this MapWork 
//  No escaping happened, so we are already done. 
//  Small table values are set to null. 
/*    * Verify table for Key: Long x Hash Table: HashSet    */
//  Error in the script itself - likely caused by an incompatible change, or new functionality / states added. 
/*  n/a  */
//  Not setting a payload, since the MRInput payload is the same and can be accessed. 
//  if low memory canary is set and if records after set canary exceeds threshold, trigger a flush. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setMaxRows(int)    */
// initialize all the dummy ops 
//  Used by DatumWriter.  Applications should not call.  
//  in case of udtf, selectOutputRR may be null. 
//  ~ Constructors ----------------------------------------------------------- 
//  tiemestamp 
//  MAX_TABLES 
//  keep track of the principals on which privileges have been checked for   this object 
//  count(distinct x, y), count(distinct y, x), we find the correct mapping. 
//  query per mbean 
//  metaListeners in HiveMetaStore#cleanupRawStore 
//  Do not mess with Table instance 
//  Retrieve the tables from the metastore in batches. Some databases like 
//  Do not assign the input value object to the timestampValues array element.   Always copy value using set* methods. 
//  reduced by 1 
//  Map precision to the number bytes needed for binary conversion. 
//  Hive can't handle select rank() over(order by sum(c1)/sum(c2)) from t1 group by c3   but can handle    select rank() over (order by c4) from   (select sum(c1)/sum(c2)  as c4 from t1 group by c3) t2;   so introduce a project on top of this gby. 
//  Get the input table and make sure the keys match 
//  no harProcessor, regular operation 
/*    * Creates a job request object and sets up execution environment. Creates a thread pool   * to execute job requests.   *   * @param requestType   *          Job request type   *   * @param concurrentRequestsConfigName   *          Config name to be used to extract number of concurrent requests to be serviced.   *   * @param jobTimeoutConfigName   *          Config name to be used to extract maximum time a task can execute a request.   *   * @param enableCancelTask   *          A flag to indicate whether to cancel the task when exception TimeoutException   *          or InterruptedException or CancellationException raised.   *    */
//  LlapInputFormat needs to know the file schema to decide if schema evolution is supported. 
//  Is this required ? 
//  (specific to the multi group by optimization) 
//  not retrying when user explicitly stops the test 
//  event loads will behave similar to table loads, with one crucial difference   precursor order is strict, and each event must be processed after the previous one.   The way we handle this strict order is as follows:   First, we start with a taskChainTail which is a dummy noop task (a DependecyCollectionTask)   at the head of our event chain. For each event we process, we tell analyzeTableLoad to   create tasks that use the taskChainTail as a dependency. Then, we collect all those tasks   and introduce a new barrier task(also a DependencyCollectionTask) which depends on all   these tasks. Then, this barrier task becomes our new taskChainTail. Thus, we get a set of   tasks as follows:                     --->ev1.task1--                          --->ev2.task1--                  /               \                        /               \    evTaskRoot-->*---->ev1.task2---*--> ev1.barrierTask-->*---->ev2.task2---*->evTaskChainTail                  \               /                   --->ev1.task3--     Once this entire chain is generated, we add evTaskRoot to rootTasks, so as to execute the   entire chain 
//  For toString, the time does not matter 
/*  this is only used in the error code path  */
//  As open txn doesn't allocate writeid, the 2 entries for aborted and committed should be retained. 
//  Repair metadata in HMS 
// the time currentCompactions is generated and now 
// throw new IllegalStateException(msg); 
//  Instantiate the underlying output format  since there is no way to parametrize instance of Class 
//  If writing delta dirs, we need to make a clone of original options, to avoid polluting it for 
//  LATIN SMALL LETTER TURNED M U+026F (2 bytes) 
//  Connect using the JDBC URL and user/pass from conf 
//  convert logs to RowSet 
//  Trigger a scheduling run - in case there's some task which was waiting for this node to 
//  If the cookie based authentication is not enabled or the request does   not have a valid cookie, use the kerberos or password based authentication 
// excluded overrides included 
// The selected vector represents selected rows.  Clone the selected vector 
//  Not final since it may change later due to DECIMAL_64. 
//  Re-serialize the splits after grouping. 
//  Get all data out. 
//  CONCERN: We currently differentiate DECIMAL columns by their precision and scale..., 
//  we first use t.getParameters() to prune the stats 
//  In that case we just pick one and spill. 
//  this work is now moved to the parentWork, thus we should 
//  r--r--r-- 
//  Add udfData to UDF if necessary 
//  kids of reduce sink operator or mapjoin operators merged into root task 
// Set filter expression 
//  Catch throwables in a best-effort to report job status back to the client. It's   re-thrown so that the executor can destroy the affected thread (or the JVM can   die or whatever would happen if the throwable bubbled up). 
//  Ignore, this particular error is expected. 
//  NULL NULL 
//  merge it with children predicates 
//  DB opereations, none of them are enforced by Hive right now. 
//  Statistics to track allocations 
//  Trigger bootstrap replication 
//  KEY_TYPE_PTR 
/*    * fastSetFromBigIntegerBytes high word multiplier is 2^(56 + 56)   *   *    (2^56)*(2^56) =   *      5192296858534827628530496329220096 or   *     (1234567890123456789012345678901234)   *     (         1         2         3    )   *      5,192,296,858,534,827,628,530,496,329,220,096 or   *      51,9229685853482762,8530496329220096  (16 digit comma'd)    */
//  Normally, on import, trying to create a table or a partition in a db that does not yet exist   is a error condition. However, in the case of a REPL LOAD, it is possible that we are trying   to create tasks to create a table inside a db that as-of-now does not exist, but there is   a precursor Task waiting that will create it before this is encountered. Thus, we instantiate   defaults and do not error out in that case.   the above will change now since we are going to split replication load in multiple execution   tasks and hence we could have created the database earlier in which case the waitOnPrecursor will 
//  Encode 
//  Fix the non-printable chars 
//  Put the filter "skewed column = skewed keys" in op 
//  Pull the output schema out of the TaskAttemptContext 
//  2. Generate backtrack Select operator 
// verify if the filter is correct and returns 2 rows and contains 2 columns and the contents match 
/*    * Does the move task involve moving to a local file system    */
//  In LLAP only mode, grace hash join will be disabled later on by the LlapDispatcher anyway.   Since the presence of Grace Hash Join disables some "native" vectorization optimizations,   we will disable the grace hash join now, before vectorization is done. 
//  if compile is being called multiple times, clear the old shutdownhook 
//  \0 to terminate 
//  attach the original predicate to the table scan operator for index   optimizations that require the pushed predicate before pcr & later   optimizations are applied 
//  Retrieve *all* partitions from the table. 
// RecordReader rr = reader.rows(); 
//  reassign a new port, just in case if one of the MR services grabbed the last one 
//  Test select root.col1.a from root:struct<col1:struct<a:boolean,b:double>,col2:double> 
//  should have no more rows 
//  We do not call startGroup on operators below because we are batching rows in   an output batch and the semantics will not work.   super.startGroup(); 
//  listOfNeedFetchNext contains all tables that we have joined data in their   candidateStorage, and we need to clear candidate storage and promote their   nextGroupStorage to candidateStorage and fetch data until we reach a   new group. 
//  Set the newly-constructed ranges as the current state 
//  Make sure we don't get stuck at 0 time, however unlikely that is. 
//  However, if the above query contains dynamic partitions, subQ1 and   subQ2 have to write to directories: Parent/DynamicPartition/Child_1   and Parent/DynamicPartition/Child_1 respectively.   The movetask that follows subQ1 and subQ2 tasks still moves the directory   'Parent' 
//  Now, update this record as being worked on by this worker. 
//  The numerator of the TABLESAMPLE clause 
//  Also, if dynamic partitioning is being used, we want to   set appropriate list of columns for the columns to be dynamically specified.   These would be partition keys too, so would also need to be removed from   output schema and partcols 
//  NEW_FUNC 
// check 1st batch 
// SQL2REL_LOGGER.fine("There are no unique keys for " + left); 
// if acid is off post upgrade, you can't make any tables acid - will throw 
//  may be processed by a thread which ends up executing before a task. 
//  replicate the incremental drops 
//  Copy string value as-is 
//  if kerberos is not enabled 
//  constant char projection 
//  Only selects and filters are allowed 
//  We are skipping calling checkOutputSpecs() for each partition   As it can throw a FileAlreadyExistsException when more than one   mapper is writing to a partition.   See HCATALOG-490, also to avoid contacting the namenode for each new   FileOutputFormat instance.   In general this should be ok for most FileOutputFormat implementations   but may become an issue for cases when the method is used to perform   other setup tasks. 
//  Use a large capacity that doesn't require expansion, yet. 
// if destf is an existing directory:  if replace is true, delete followed by rename(mv) is equivalent to replace  if replace is false, rename (mv) actually move the src under dest dir  if destf is an existing file, rename is actually a replace, and do not need   to delete the file first 
//  classloader as parent can pollute the session. See HIVE-11878 
//  try reading table2 as user2 - should succeed 
/*  10 files x 1000 size for 1 splits  */
//  COLUMN_STATS_ACCURATE in params is set to correct value 
//  Exceeds MAX_VALUE 
//  ignore and use UUID instead 
//  If it's a non-deterministic UDF, set unknown to true 
//  ignore as this is expected 
//  no need to go further down for a select op with all file sink or script   child since all cols are needed for these ops   However, if one of the children is not file sink or script, we still go down. 
//  No need to run CBO (table ref or virtual table) or not supported 
//  Ignore the exception 
//  Revert the projected columns back, because batch can be re-used by our parent operators. 
//  Start the protocol server after properly authenticating with daemon keytab. 
// create an empty priv set 
//  Make sure we don't give out more than allowed due to double/rounding artifacts. 
//  The small table hash table for the native vectorized map join operator. 
/*              * Common left-semi join result processing.              */
//  don't need this anymore 
//  MODIFIER LETTER SMALL L U+02E1 (2 bytes) 
//  Retries will be done till decaying factor reduces to 0.  Decaying Factor is 2.   So, log to base 2 of batchSize plus 1 or Most Significant Bit   of batchsize plus 1 will give the number of expected calls 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#registerOutParameter(java.lang.String, int,   * java.lang.String)    */
//  no matter loc is the table location or part location, it must be a   directory. 
/*    * traverse a path of Filter, Projects to get to the TableScan.   * In case of Unique keys, stop if you reach a Project, it will be handled   * by the invocation on the Project.   * In case of getting the base rowCount of a Path, keep going past a Project.    */
//  For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations 
//  whole value is copied, including spaces 
//  The table is not yet loaded in cache 
//  If this is an insert, originalWriteId should be set to this transaction.  If not,   it will be reset by the following if anyway. 
//  VARCHAR tests 
//  memoize decorator for getOutputSizeInternal 
//  traverse the aborted txns list, starting at first aborted txn index 
/*        * Create a thread pool with no queue wait time to execute the operation. This will ensure       * that job requests are rejected if there are already maximum number of threads busy.        */
//  Some parts of the system can't handle rows with zero fields, so 
//  template, <ClassNamePrefix>, <ReturnType>, <OperandType>, <FuncName>, <OperandCast>, <ResultCast> 
//  if this is a delimiter directive, reset our delimiter 
//  preventing empty ctor from being callable 
//  An AccessControlException may be caused for other different errors,   but we take it as if our path is read-only 
//  All qfiles handled via this... 
//  We'd like to move this to HiveMetaStore for any non-native table, but   first we need to support storing NULL for location on a table 
//  We initialize the cache 
//  NOSASL 
//  fastIsLong returns false. 
//  do nothing here, we will throw an exception in the following block 
//  with many arguments. 
//  Try getObjectInspector 
//  the currAliasId and CurrTopOp is not valid any more 
//  3. See if the IN (STRUCT(EXP1, EXP2,..) has atleast one expression with partition   column with single table alias. If not bail out.   We might have expressions containing only partitioning columns, say, T1.A + T2.B   where T1.A and T2.B are both partitioning columns. 
// so with 2 FileSinks and 4 buckets, FS1 should see (0,1),(2,2),(0,3)(2,4) since data is sorted by ROW__ID where tnxid is the first component  FS2 should see (1,1),(3,2),(1,3),(3,4) 
//  String className = this.getClass().getName(); 
//  Binary search to find the closest bucket that v should go into.   'bin' should be interpreted as the bin to shift right in order to accomodate   v. As a result, bin is in the range [0,N], where N means that the value v is   greater than all the N bins currently in the histogram. It is also possible that 
//  Generate dummy pre-upgrade scripts with valid SQL 
//  groupingSets are known at map/reducer side; but have to do real processing 
//  Report size to the extent possible. 
//  Comparing on time is not sufficient since two may be created at the same time,   in which case inserting into a TreeSet/Map would break 
//  determine class 
//  Descending sort based on split size| Followed by file name. Followed by startPosition. 
//  optional string connected_vertex_name = 1; 
//  The following data should be changed 
// ship additional artifacts, for example for Tez 
//  query has run capture the time 
//  HIVE-14444 pending rename: after 
//  create a fetch operator 
//  This value should go into smallBuffer. 
//  TaskInfo instances for two different tasks will not be the same. Only a single instance should 
//  Test that write can acquire after read 
//  make both scaling up as follows   unscaledValue = significand * 5**(scale) *   2**(scale-twoScaleDown) 
//  2nd task requested host1, got host1 
//  3 Check data distribution in  buckets 
// This class provides and implementation for a case insensitive token checker  for the lexical analysis part of antlr. By converting the token stream into  upper case at the time when lexical rules are checked, this class ensures that the  lexical rules need to just match the token with upper case letters as opposed to  combination of upper case and lower case characteres. This is purely used for matching lexical  rules. The actual token text is stored in the same way as the user input without  actually converting it into an upper case. The token values are generated by the consume()  function of the super class ANTLRStringStream. The LA() function is the lookahead funtion  and is purely used for matching lexical rules. This also means that the grammar will only  accept capitalized tokens in case it is run from other tools like antlrworks which  do not have the ANTLRNoCaseStringStream implementation. 
//  TableSpec ts is got from the query (user specified),   which means the user didn't specify partitions in their query, 
//  this map should map columnInfo to ExprConstantNodeDesc 
//  If we couldn't find an asterisk, it's not a prefix 
//  state byte in the first record 
//  Encode a schema with v0, write out. 
//  Dont remove the operator for distincts 
//  ... and where the transaction has already been committed as per snapshot taken 
//  Now, create a delete delta that has rowIds divisible by 3 but not by 2. This will produce 
//  3/ write the null bytes 
//  For all other data types, use int conversion.  At some point, we should have all   conversions make sure the value fits. 
//  conversions 
//  remember the dummy ops we created 
//  This indicates we logged an inconsistency (from our point-of-view) and will not make this 
//  optional .UserPayloadProto user_payload = 2; 
//  CREATE_TIME 
//  no alias to stage.. no local task 
//  do nothing 
// H2 - should allocate 
//  Note that this method assumes you have already decided this is an Acid table.  It cannot 
//  Due to reflection, the jdo exception is wrapped in   invocationTargetException 
//  Test dryrun of schema upgrade 
//  be handled. 
//  This could be due to either URI syntax error or File constructor   illegal arg; we don't really care which one it is. 
//  This is fast path for query optimizations, if we can find this info   quickly using   directSql, do it. No point in failing back to slow path here. 
//  We currently evaluate the IN (..) constants in special ways. 
//  note - the row mapping is not relevant when aggregationBatchInfo::getDistinctBufferSetCount() == 1 
//  compare if they are the same constant. 
//  GroupBy query results as records (types defined by metastore) 
//  if the driver registered in the driver manager, get the connection via the driver manager 
//  prevent equals from being overridden in sub-classes   always use ExprNodeDescEqualityWrapper   if you need any other equality than Object.equals() 
// check empty DB 
//  Get the string value and convert to a Date value. 
//  verify that a multi byte LIKE expression matches a matching string 
//  ok even if the data is not deleted 
//  start index of -n means give the last n characters of the string 
//  PRIMARY_KEYS 
//  ELAPSED_MS 
/*  * This is a pluggable policy to chose the candidate map-join table for converting a join to a * sort merge join. The leftmost table is chosen as the join table.  */
//  4.1 Create structs 
// we can get the number of rows from the first vector 
//  timestamp/date is higher precedence than String_GROUP 
//  IN clauses need to be combined by keeping all elements 
/*  @bgen(jjtree) SenumDefList  */
//  If we have fired it already once, we return and the test will fail 
//  work.getTableSpecs() == null means it is not analyze command   and then if it is not followed by column stats, we should clean   column stats 
//  nice message 
// not making this configurable, best effort 
//  @@protoc_insertion_point(class_scope:IOSpecProto) 
//  Delete the data in the tables which have other locations 
// For some reason even with an MBeanException available to them Runtime exceptions  can still find their way through, so treat them the same as MBeanException 
//  4. Reopen is essentially just destroy + get a new session for a session in use. 
//  We're only considering the first element of the IN list for the type 
//  Use org.apache.hadoop.io.Text as our helper to go from byte[] to String. 
//  In some places (e.g. FileInputFormat) this BlockLocation is used to   figure out sizes/offsets and so a completely blank one will not work. 
//  don't fetch the footer, PPD happens in MS. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getResultSetMetadata(org.apache.hive.service.cli.OperationHandle)    */
//  Now only string, text, int, long, byte and boolean comparisons   are treated as special cases.   For other types, we reuse ObjectInspectorUtils.compare() 
//  to writing an instrumentation agent for object size estimation 
//  do nothing, fall through and verify the data 
//  Exclusives can never pass 
/*       This denotes listing of any directories where during replication we want to take care of      db level operations first, namely in our case its only during db creation on the replica      warehouse.    */
//  char to binary conversion: include trailing spaces? 
//  add the column only if the family has not already been added 
//  Make new client, since transport was closed for the last one. 
//  Create some ACID tables: T10, T11 - unpartitioned table, T12p, T13p - partitioned table 
//  Test various scenarios 
//  Ignoring and continuing to watch for additional elements in the dir. 
//  Leading spaces are significant 
//  recreate table as external, drop partition and it should 
//  Test HiveConf property variable substitution in hive-site.xml 
/*    * Computes the temporal run time statistics of the reducers   * for a specific JobId.    */
//  Stores the temporal statistics in milliseconds for reducers   specific to a Job 
// complete T2 txn 
//  bail on mux operator because currently the mux operator masks the emit keys   of the constituent reduce sinks. 
//  -i <init-query-file> 
//  TBL 
/*    * Initialize using target data type names.   * No projection -- the column range 0 .. types.size()-1    */
//  check if there is data in the resultset 
//  duplicate function with possibly replaced children 
//  Find context for current input file 
//  Yay! We short-circuited, skip everything remaining in the batch and return. 
//  Fetch the bucketing version from table scan operator 
/*      * track the input ColumnInfos that are added to the output.     * if a columnInfo has multiple mappings; then add the column only once,     * but carry the mappings forward.      */
//  There should be 2 original bucket files (000000_0 and 000001_0), plus a new delta directory. 
//  For dynamic partitioned hash join, ReduceSinkMapJoinProc rule may not get run for all   of the ReduceSink parents, because the parents of the MapJoin operator get   removed later on in this method. Keep track of the parent to mapjoin mapping 
//  return immediately if batch is empty 
//  INSERT EVENT to unpartitioned table 
//  If we are here it means, user is requesting a role he doesn't belong to. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getMoreResults(int)    */
//  Move data file to backup path 
//  The error codes are Hive-specific and partitioned into the following ranges:   10000 to 19999: Errors occurring during semantic analysis and compilation of the query.   20000 to 29999: Runtime errors where Hive believes that retries are unlikely to succeed.   30000 to 39999: Runtime errors which Hive thinks may be transient and retrying may succeed.   40000 to 49999: Errors where Hive is unable to advise about retries.   In addition to the error code, ErrorMsg also has a SQLState field.   SQLStates are taken from Section 22.1 of ISO-9075.   See http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt   Most will just rollup to the generic syntax error state of 42000, but   specific errors can override the that state.   See this page for how MySQL uses SQLState codes: 
//  when we start miniHS2_1 will be leader (sequential start) 
//  Repeated non-NULL permutations. 
//  AccumuloInputFormat expects 
//  anything else indicates failure 
//  Don't close the socket - the stream already does that if needed. 
//  Load all of the default config values from HiveConf. 
/*    * A finder finds the first index of its pattern in a given byte array.   * Its thread-safety depends on its implementation.    */
//  Mapping from constraint name to list of not null columns 
//  OutputFormat 
// executionResult will be ERRNO_OK only if all initFiles execute successfully 
//  find the base files (original or new style) 
//  Get the all path by making a select(*). 
//  Extrapolation is not needed for this column if   count(\"PARTITION_NAME\")==partNames.size()   Or, extrapolation is not possible for this column if   count(\"PARTITION_NAME\")<2 
//  Note: not actually used for pool sessions; verify some things like doAs are not set. 
//  create a dummy select - This select is needed by the walker to split the   mapJoin later on 
//  partitions are not added as write entries in drop partitions in Hive 
//  ignore all other chars outside the enclosure 
//  LazyObject can only be binary when it's not a string as well      return LazyFactory.createLazyObject(inspector,              ColumnEncoding.BINARY == rowIdMapping.getEncoding()); 
//  Build the supported formats list 
//  Mostly dup of genIncludedColumns 
// use bigint 
//  Require all the directories to be present with some values. 
//  The order of processing is as follows. We'd reclaim or kill all the sessions that we can   reclaim from various user actions and errors, then apply the new plan if any,   then give out all we can give out (restart, get and reopen callers) and rebalance the   resource allocations in all the affected pools.   For every session, we'd check all the concurrent things happening to it. 
//  (return all) 
//  Validate the value. 
//  10000500....00 
//  TODO fucntionCache 
//  then all output will be null 
// can't do much if outputTypeInfo is not set 
//  Let Driver strip comments using sql parser 
//  Working in the assumption that the user here will be the hive user if doAs = false, we'll make it past this false check. 
//  this method 
// first column exists 
//  Next user did specify perms. 
//  Transport mode 
//  Returns a list of the distinct exprs without duplicates for a given clause name 
//  Remove the znodes we've already tried from this list 
//  UTF-8 continuation bytes have 2 high bits equal to 0x80. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#setCatalog(java.lang.String)    */
//  pause so we don't get banned 
//  Lets use a timeout more than the socket lifetime to simulate a reconnect 
//  Delete any stale log files left around from previous failed tests 
// ---------------------------------------------------------------------------   Process Single-Column Long Inner Big-Only Join on a vectorized row batch.   
//  Get the query string from the conf file as the compileInternal() method might   hide sensitive information during query redaction. 
//  Check if any of the txns in the list is committed. 
//  Whether characters, such as 't/T', 'f/F', and '1/0' are interpreted as valid boolean literals. 
/*    * Test if the environment variables can be set. If this test fails   * all the other tests will also fail because environment is not getting setup    */
//  copy default aux classes (json/hbase) 
//  call the tests 
//  Format a Column for a create statement 
//  With trim=true, parsing can handle spaces 
/* OrcRecordUpdater is inconsistent about when it creates empty files and when it does not.      This creates an empty bucket. HIVE-17138 */
//  Not ok to run CBO, build error message. 
//  Remove the branches 
//  Normally, trailing fractional digits are removed.  But to emulate the   OldHiveDecimal setScale and OldHiveDecimalWritable internalStorage, we need to trailing zeroes   here.     NOTE: This can cause a decimal that has too many decimal digits (because of trailing zeroes)         for us to represent.  In that case, we punt and convert with a BigInteger alternate         code. 
//  count only on the keys 
//  Just examine the middle and lower words. 
//  get available map memory 
//  Can fail with NoSuchObjectException. 
//  Do not create counter if it does not exist - 
/*  Add list bucketing location mappings.  */
// streaming ingest dir - cannot have update/delete events 
//  Test that two different databases don't collide on their locks 
//  if counter name starts with VERTEX_ then we just return max value across all vertex since trigger   validation is only interested in violation that are greater than limit (*any* vertex violation).   Use case: If SHUFFLE_BYTES for any single vertex is > limit perform action 
//  get token info to check renew date 
//  If we cannot combine any of the children, we bail out 
//  We keep the hash set result for its spill information. 
//  Case 2: Test with originals and base => Single split strategy with two splits on compacted 
//  3. The result should have a project on top, otherwise we 
//  Set a watch on the znode 
//  DROP_PARTITION EVENT to partitioned table 
//  VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, taskName + ", " + getOperatorId() + " candidate " + CLASS_NAME + " batch"); 
//  At this point we may have parsed an integer. 
//  For a partition key type, this will be a primitive typeinfo. 
//  codes 
//  Data needs deletion. Check if trash may be skipped. 
//  The rule has been applied, we bail out 
//  using old table object, hence reset the owner to current user for new table. 
/* && srcs[0].getPath().getName().equals(EximUtil.DATA_PATH_NAME) -  still broken for partitions */
//  the createTable() above does not update the location in the 'tbl'   object when the client is a thrift client and the code below relies   on the location being present in the 'tbl' object - so get the table   from the metastore 
//  Num of total and completed tasks 
//  Check that the added partitions are as expected. 
//  Alter table ... partition column ( column newtype) only takes one column at a time. 
//  Hadoop Conf Var Names 
//  add count(KEY._col0) to replace distinct 
//  GBY,RS,GBY... (top to bottom) 
//  Supports random access. 
//  Get aggregate stats for all partitions of a table and for all but default 
//  Configuration and things set from it. 
//  Look at comments in DummyStoreOperator for additional explanation. 
//  We bypass the OR clause and select the second disjunct 
//  Force local cache if we have deltas. 
//  The child operators cleanup if input file has changed 
//  Attach the resources to the session cleanup. 
//  Special case for 0, because java doesn't strip zeros correctly on that number. 
//  ////// 4. Generate GroupbyOperator2 
//  non-null only for writing (server-side) 
/*   Set the umask in conf such that files/dirs get created with table-dir      * permissions. Following three assumptions are made:      * 1. Actual files/dirs creation is done by RecordWriter of underlying      * output format. It is assumed that they use default permissions while creation.      * 2. Default Permissions = FsPermission.getDefault() = 777.      * 3. UMask is honored by underlying filesystem.       */
//  registry for system functions 
//  Tracks new additions via add, while the loop is processing existing ones. 
//  required   required   optional 
//  2. Apply pre-join order optimizations 
//  Try to force-evict the fragments of the requisite size. 
//  register this comparator 
/*  10 files x 100 size for 9 splits  */
//  total 10 entries (2 valid + 8 fake) 
//  such as dropping a table or partition 
/*    * tracks number of exprs from correlated predicates added to SQ select list.    */
//  For left semi joins, we may apply the filter(s) now. 
//  Compute stats 
//     GROUP BY deptno                        // Aggregate A 
//  checkMetastore call will fill in result with partitions that are present in filesystem   and missing in metastore - accessed through getPartitionsNotInMs   And partitions that are not present in filesystem and metadata exists in metastore -   accessed through getPartitionNotOnFS 
//  No metadata => no ppd. 
//  Datanucleus throws NPE when we try to serialize a table object   retrieved from metastore. To workaround that we reset following objects 
//  Struct column, such as root? 
//  DatabaseMetaData.getCatalogs()  catalogs are actually not supported in 
//  MERGE statement 
//  Each listener called above might set a different parameter on the event.   This write permission is allowed on the listener side to avoid breaking compatibility if we change the API   method calls. 
/*  fill array with a pattern that will never match sync  */
//  now that URI and times are set correctly, set the original table's uri and times 
//  (54 % 4) << 62   54/4 
//  Don't exceed the range if we have one. 
//  Get the count of txns from the given list are in open state. If the returned count is same as   the input number of txns, then it means, all are in open state. 
// see HIVE-18154 
//  THEN CAST(NULL AS newInputTypeNullable) 
//  negate it 
// reducer.setGroupKeyObjectInspector(keyObjectInspector); 
//  Some other process is probably writing the file. Just sleep. 
/*     So Union All removal kicks in and we get 3 subdirs in staging.ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree /Users/ekoifman/dev/hiverwgit/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnNoBuckets-1505516390532/warehouse/t/.hive-staging_hive_2017-09-15_16-05-06_895_1123322677843388168-1/ -ext-10000     HIVE_UNION_SUBDIR_19      000000_0          _orc_acid_version          delta_0000016_0000016_0001     HIVE_UNION_SUBDIR_20      000000_0          _orc_acid_version          delta_0000016_0000016_0002     HIVE_UNION_SUBDIR_21         000000_0             _orc_acid_version             delta_0000016_0000016_0003 */
//  double scalar/scalar IF 
//  initialize mapredWork 
//  groupSetPosition includes all the positions 
//  Main path 2 - created a new file cache.   Someone created one in parallel.   Someone created one in parallel and then it went stale. 
// test minor compaction 
//  Reached end of file 
//  Unique rows. 
//  Does this hashcode belong to this reducer 
//  If a task failed, fetch its error code (if available).   Also keep track of the total number of failures for that 
//  merge should convert hll2 to DENSE 
// verify the data is the same 
//  to merge (only useful for extended merging as TS do not have inputs). 
//  Obtained from the HiveException thrown 
/*    * This number is carefully chosen to minimize overhead and typically allows   * one VectorizedRowBatch to fit in cache.    */
//  8. Merge, remove and reduce Project if possible 
//  to avoid concurrent modify the hashmap 
// this gives an easy way to get at compaction ID so we can only wait for those this  utility started 
//  check if all record writers implement statistics. if atleast one RW   doesn't implement stats interface we will fallback to conventional way 
//  print primary key containing parents 
//  TaskDisplay doesn't have a toString, using json 
/*      * Needed virtual columns are those used in the query.      */
// other alter operations are already supported by Hive 
//  static partition 
// Token is available, so replace the placeholder 
//  We get a semi join at here.   This map-side GroupByOperator needs to be removed 
//  No corresponding Writable classes for the following 3 in hadoop 0.17.0 
//  PARTITIONS 
//  rowFilterExpression is applied to the whole table, i.e., dbname.objectName   For example, rowFilterExpression can be "key % 2 = 0 and key < 10" and it 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getGeneratedKeys()    */
//  Operand | Not   T | F 
// we have WHEN MATCHED AND <boolean expr> THEN DELETE 
// prec 5, scale 2 
//  ID 
//  Update the tables in cache 
//  ------r--   rwxrwxrwx 
//  try again with left input also having no nulls 
//  Computes the skew for all the MapReduce irrespective   of Success or Failure 
//  check if there's at least some degree of stats available 
//  spill the current block to tmp file 
//  Sort order (+|-) 
//  Convert the n-gram list to a format suitable for Hive 
/*          * Get our Single-Column String hash multi-set information for this specialized class.          */
//  The TS rule for headers is - a header and buffer array element for some freeList   can only be modified if the corresponding freeList lock is held. 
//  compare register values and store the max register value 
//  load the schema version stored in metastore db 
//  compile and execute can get called from different threads in case of HS2 
//  ROLE 
//  2b-3a 
//  QueryId for the query in current transaction 
//  Generic http server error. 
// now check that stats were updated 
//  replace existing blanks with new blanks 
//  This value is always in seconds and includes an 's' suffix. 
//  Create the Group by Operator 
//  Create the corresponding client 
//  Contains explicit field "a" and partition "b". 
//  validate the DDL is a valid operation on the table. 
//  update the childOp of selectOp 
//  if we've read 63 bits, roll them into the result 
//  stream data into streaming table with N buckets, then copy the data into another bucketed table 
//  Keep connection open to hang on to associated resources (temp tables, locks). 
//  String including a '\u0000' style literal characters (\u732B is a cat in Kanji). 
//  in case jackson is able to deserialize...it may use a different implementation for the map - which may not preserve order 
//  Retrieve the additional HttpHeaders 
//  At least one partition per expression, if not ifExists 
/*    * Expr translation helper methods    */
//  Deserialize and test 
//  DETAILS:     PIG-1429 added support for boolean fields, which shipped in 0.10.0;   this version of Pig depends on antlr 3.4.     HCatalog depends heavily on Hive, which at this time uses antlr 3.0.1.     antlr 3.0.1 and 3.4 are incompatible, so Pig 0.10.0 and Hive cannot be depended on in the   same project. Pig 0.8.0 did not use antlr for its parser and can coexist with Hive,   so that Pig version is depended on by HCatalog at this time. 
//  rcfile 
//  repeat again 
//  <DescriptionValue> 
//  Map this key back to the ConfVars it is associated with. 
//  miniHS2_1 will become leader 
//  New array cannot contain the records w/the same key, so just advance, don't check. 
// try out some metastore operations 
//  now oldOrdinal is relative to oldInput 
//  OldHiveDecimal returns the hash code of its internal BigDecimal.  Our TestHiveDecimal   verifies the OldHiveDecimal.bigDecimalValue() matches (new) HiveDecimal.bigDecimalValue(). 
//  do not fold named_struct, only struct() 
//  column/scalar 
//  Three different cases 
//  count query 
//  taskKill() should also be received during a rejected submission,   we will let that logic handle retries. 
//  Cannot be 0. 
//  Create schema with no serde, then map it 
//  2. Construct JoinLeafPredicateInfo 
//  would only apply after the callback for the current message. 
//  Set up scratch directory 
//  Partitioned table - delete specific partitions a0, a2 
//  call-2: open to read - split 2 => mock:/mocktable1/0_1 
// create 1 delta file bucket_00000 
//  Note: the metadata cache may deallocate additional buffers, but not this one. 
//  get the path corresponding to the dynamic partition columns, 
/*          * Validate the column names that are present are the same.  Missing columns will be         * implicitly defaulted to null.          */
//  convert to HCatNotificationEvent, and then try to instantiate a ReplicationTask on it. 
//  into same work y. 
//  if any of join participants is from other MR, it has alias like '[pos:]$INTNAME'   which of size should be caculated for each resolver. 
// If the decendants contains LimitOperator,return false 
//  Create the folder and its parents if not there 
/*    * A TableFunction may be able to provide its Output as an Iterator.   * In case it can then for Map-side processing and for the last PTF in a Reduce-side chain   * we can forward rows one by one. This will save the time/space to populate and read an Output   * Partition.    */
//  create the create table grants with new config 
//  Parse the message field 
//  For PPD, we need a column to expression map so that during the walk,   the processor knows how to transform the internal col names.   Following steps are dependant on the fact that we called 
//  create dir for /mpart5 
//  init objectInspectors 
//  Executor to create the ATS events. 
//  swallow exception 
//  2. Simulate emitting records in processNextRecord() with small memory usage limit. 
//  return partition metadata 
//  Any (repeated) null key column is no match for whole batch. 
//  is updated the count will be computed again. 
//  Remaining batches. 
//  a map from each BaseWork to its cloned JobConf 
// if here we already saw current file and now found another file for the same bucket  so the current file is not the last file of the logical bucket 
//  Tag if we want to remain in trash after deletion.   If multiple files share the same content, then 
//  get list of events matching dbPattern & tblPattern 
//  There should be 1 delta dir per partition location 
//  if all chunks have been processed, nothing more to do. 
//  We copied the entire buffer.    else there's more data to process; will be handled in next call. 
//  5. Delete GBY - RS - GBY - SEL from the pipeline. 
// release the X lock on T7.p=1 
/*  Tests with queries which can be pushed down and executed with directSQL  */
//  This is select for update query which takes a lock if the table entry is already there in NEXT_WRITE_ID 
//  No inversion or escaping happened, so we are can reference directly. 
/*        * Single-Column String check for repeating.        */
// use sampled partitioning 
//  Record what type of write this is.  Default is non-ACID (ie old style). 
//  setting file length to Long.MAX_VALUE will let orc reader read file length from file system 
//  drop table is already enforced by Hive. We only check for table level location even if the   table is partitioned. 
//  max txn id does not change for a transaction batch 
//  TODO: add tez session reconnect after TEZ-3875 
/*      * alias      */
//  Set an explicit session name to control the download directory name 
//  check if table exists. 
// set the values to i+1 so they are different in the new stats 
//  Set foreign key name if null before sending to listener 
//  For intermediate sum field 
//  CREATE MV ... AS 
//  Create the ObjectInspectors for the fields. Note: Currently   ColumnarObject uses same ObjectInpector as LazyStruct 
//  If we're loading into a db, instead of into the warehouse, then the oldDbName and   newDbName must be the same 
//  colList will be null for FS operators. 
//  We have filled HiveDecimal.MAX_PRECISION digits and have no more room in our limit precision   fast decimal. 
//  set list work 
//  1.2. Adjust GroupingSet Position, GBKeys for GroupingSet Position if   needed. NOTE: GroupingID is added to map side GB only if we don't GrpSet 
/*      * Calculate the variance sample result when count > 1.  Public so vectorization code can     * use it, etc.      */
//  If we are using a shared database, then remove not known databases, tables, views. 
//  Empty list case 
//  test timestamp string 
//  Move past parent field separator. 
//  Add the test-null-appender to the default route 
//  we converted the expression to a search condition, so 
//  native or not would be decided by annotation. need to evaluate that first 
//  Not strictly necessary; do the whole queue check again. 
//  Don't create a new object if we are already out of memory 
//  Now add all the default handlers 
//  Skipping through comments 
//  1.a. Extract order for each column from collation 
//  for each sub-query. Also, these different filesinks need to be linked to each other 
/*  skewed column names.  */
//  no need to look at token info 
//  IF_NOT_EXISTS 
//  Neither input has nulls. Verify that this propagates to output. 
//  Check cases for arr[i].f and map[key].v   For these we should not generate paths like arr.f or map.v   Otherwise we would have a mismatch between type info and path 
// make sure to check format  is this right? 
//  Schema validation enforces that the Key is a String 
// Ignored the attribute was not found, which should never happen because the bean  just told us that it has this attribute, but if this happens just don't output  the attribute. 
//  sort both the lists 
//  Default column storage specification inherits from table level default 
/*    * Allocate overflow batch columns by hand.    */
//  Arithmetic operations rely on getting conf from SessionState, need to initialize here. 
//  spin until it resolves; extremely rare 
//  This may happen if ACID state is absent from config. 
//  Must be manually set with setMaxLength. 
//  Create NullAppender 
//  Only columns can be sorted/bucketed, in particular applying a function to a column   voids any assumptions 
//  user unsets queue name, will fallback to default session queue 
//  2. Reset the pointer. 
//  No valid HS2 generated cookies found, return null 
//  The set object containing the IN list. 
//  1.3.1 process hints 
//  S_INT_STRING 
/*  (non-Javadoc)   * This processor addresses the RS-MJ case that occurs in tez on the small/hash   * table side of things. The work that RS will be a part of must be connected   * to the MJ work via be a broadcast edge.   * We should not walk down the tree when we encounter this pattern because:   * the type of work (map work or reduce work) needs to be determined   * on the basis of the big table side because it may be a mapwork (no need for shuffle)   * or reduce work.    */
// bail here to make the operation idempotent 
// lets check that side files exist, etc 
//  Update NDV of joined columns to be min(V(R,y), V(S,y)) 
//  Add an alternative alias for the column this instance represents, and its index in the 
//  first batch is always based on batch size 
//  alphaMM value for 128 bits hash seems to perform better for default 64 hash bits 
//  so it won't be updated. 
// compare ports 
//  Lower this for big value testing. 
//  estimated number of reducers 
//  Since we did not remove reduce sink parents, keep the original value expressions 
//  RS for join, SEL(*) for lateral view   SEL for union does not count (should be copied to both sides) 
//  This should ideally happen in a separate thread 
//  To test the SortMergedDeleteEventRegistry, we need to explicitly set the   HIVE_TRANSACTIONAL_NUM_EVENTS_IN_MEMORY constant to a smaller value. 
//  if false, return false 
//  currOp now points to the top-most tablescan operator 
//  automatic conversion to double is done here 
//  data with the separator bytes before creating a "Put" object 
//  since the file is read by Pig, we need to make sure the values are in format that Pig   understands   otherwise it will turn the value to NULL on read 
//  Get the bucket positions for the table 
//  Add insert event twice with different event ID to allow apply of both events. 
//  if not match, copy again from cm 
//  convert any nulls present in map values to empty strings - this is done in the case   of backing dbs like oracle which persist empty strings as nulls. 
//  only one table, let's check all partitions 
//  Use NotTezEvent.newBuilder() to construct. 
//  setop 
//  that appear in the small table portion of the join output for outer joins. 
/*      * If there are deletes and reading original file, we must produce synthetic ROW_IDs in order     * to see if any deletes apply      */
//  no-op default 
//  Sign. 
//  Additional work for union operator, see union27.q 
//  This next section repeats the tests of testRightTrimWithOffset with a maxLength parameter that is   less than the number of current characters in the string and thus affects the trim. 
// mostly this indicates that the Initiator is paying attention to some table even though 
//  Prior none, singleton, or range? 
/*  n != batch.size when isRepeating  */
//  combine equivalent work into single one in SparkWork's work graph. 
//  Not included in equals. 
//  Create SelectDesc 
//  Try to allocate using brute force approach from each arena. 
//  USER 
//  Evaluate subsequent child expression over unselected ones only. 
//  authorization setup using SessionState should be revisited eventually, as   authorization and authentication are not session specific settings 
//  Grouping ID reference 
//  Verify the number of buckets equals the number of files   This will not hold for dynamic partitions where not every reducer produced a file for   those partitions.  In this case the table is not bucketed as Hive requires a files for 
//  ThriftStructObjectInspector will override and ignore __isset fields. 
//  all rows are filtered if repeating null, otherwise no rows are filtered 
/*  * An implementation of MetaStoreInitListener to verify onInit is called when * HMSHandler is initialized  */
//  Validate 
// the 'isOriginal' file is at the root of the partition (or table) thus it is  from a pre-acid conversion write and belongs to primordial writeid:0. 
//  This is not a cast; process the function. 
//  Fallback to picking up the value from environment. 
//  Move the record from txn_components into completed_txn_components so that the compactor 
//  ERROR_CODE 
//  'data' is created by export command/ 
//  This should also trigger meta listener notification via TServerEventHandler#deleteContext 
//  Cache if the client asks for it, else just return the value 
//  This assume no round up... 
//  ALLOC_FRACTION 
//  TODO: This should be making use of confDir to load configs setup for Tez, etc. 
//  An optional group containing a repeated anonymous group "bag", containing 
//  use HiveInputFormat so that we can control the number of map tasks 
//  otherwise, continue to get the group type until MAP_DEFINITION_LEVEL_MAX. 
/*      * 4. GBy      */
//  Mapping from constraint name to list of Check constraints 
/*    * At the Metadata level there are no restrictions on Column Names.    */
//  there was an authorization issue 
//  Subscriber can get notification about drop of a table in HCAT   by listening on a topic named "HCAT" and message selector string   as "HCAT_EVENT = HCAT_DROP_TABLE" 
// This read entity is a direct read entity and not an indirect read (that is when 
//  show table information 
//  First try without qualifiers - would resolve builtin/temp functions 
// if HiveConf has not changed, same object should be returned 
//  this method is called by the RCFile.Reader constructor, overwritten,   so we can access the opened file 
//  Finds all contextual n-grams in a sequence of words, and passes the n-grams to the 
//  slight hack to communicate to DynamicSerDe that the field ids are not   being set but things are ordered. 
//  and submit method of RemoteHiveSparkClient when the job config is created 
//  good 
// -----------------------------------------------------------------------------------------------   Create methods.  ----------------------------------------------------------------------------------------------- 
//  Database 
//  LOW_VALUE 
//  division by inverse multiplication. 
//  None of the group expressions are constant. Nothing to do. 
//    Type-specific implementations   
//  Output types. They will be the concatenation of the input refs types and 
//  Raise error if user has specified partition column for stats 
//  Create planner and copy context 
//  enable SSL support for HMS 
//  connect all small dir map work to the big dir map work 
//  add all the dependencies to a list 
//  find min NDV for joining columns 
//  add a filter count(c) = #branches 
//  in case of exception assume unknown type (256 bytes) 
//  Can't access metadata, carry on. 
//  Blindly add this as a integer list! Should be sufficient for the test case. 
//  batch statements? 
//  also not loaded. 
//  Look at everything in front of this lock to see if it should block 
//  add original entries 
//  Then we merge the operators of the works we are going to merge 
//  The FS can be closed from under us if the task is interrupted. Release cache buffers.   We are assuming here that toRelease will not be present in such cases. 
// NULL if any of the args are nulls 
//  at least 2 checkers always 
//  TODO Even out the batch sizes (i.e. 20/20/1 should be replaced by 14/14/13) 
//  nothing to do 
//  The original partition files are deleted after the metadata change   because the presence of those files are used to indicate whether   the original partition directory contains archived or unarchived files. 
//  need to set up output name for reduce sink now that we know the name   of the downstream work 
/*    * Allocate the target related arrays.    */
// make sure currently running txn is considered aborted by housekeeper 
//  Still the same column 
//  each column has height of 2: 
/*        * add list bucketing predicate to to the table scan operator        */
//  create fetch work 
//  If the file name to bucket number mapping is maintained, store the bucket number   in the execution context. This is needed for the following scenario:   insert overwrite table T1 select * from T2;   where T1 and T2 are sorted/bucketed by the same keys into the same number of buckets   Although one mapper per file is used (BucketizedInputHiveInput), it is possible that   any mapper can pick up any file (depending on the size of the files). The bucket number   corresponding to the input file is stored to name the output bucket file appropriately. 
//  provide us with a new URL to access the datastore. 
//  and \0 to terminate 
//  a sampling filter then we ignore the current filter 
//  Create a database with a table 
//  set default location if not specified and this is   a physical table partition (not a view) 
//  parse the struct using multi-char delimiter 
//  Drop dest4_sequencefile 
//  No timestamp format specified, just use default lazy inspector 
// no files found, for example empty table/partition 
//  Verify that the table was created successfully. 
//  Update the null counter 
//  All the partitions need to be updated; a single command can be used. 
//  connect using principal via Beeline with inputStream 
//  comment column is empty 
//  Remove any parallel edge between semijoin and mapjoin. 
//  Unlike RandomAccessFileAppender, do not append log when stopped. 
//  Will properly set string or binary serialization via createLazyField(...) 
//  Send some status periodically 
//  Stick back into result variables... 
//  Partition spec was already validated by caller when create TableSpec object. 
//  If this is a source table we do not copy it out 
//  set job name 
//  It is a Java class 
//  max # of rows can be put into one block 
//  string types get converted to double 
//  If the skew keys match the join keys, then add it to the list 
//  Either tableHandle isn't partitioned => null, or repl-export after ts becomes null => null.   or this is a noop-replication export, so we can skip looking at ptns. 
//  max length for the char/varchar, then the return type reverts to string. 
//  the OpParseContext of the parent SelectOperator. 
//  Key. 
//  indicator, so the chunked input does not know to stop reading. 
//  Compute the size of a query when the 'nextValue' is added to the current query. 
/*    * Test bad args to getXXX()   * @throws SQLException    */
//  Check whether the materialized view is invalidated 
//  Compare the value to each element of array until a match is found 
//  Set up task. 
//  Expectation here is not to run into a timeout 
//  any name, it does not   matter. 
/*    * Information on a field.  Made a class to allow readField to be agnostic to whether a top level   * or field within a complex type is being read    */
//  unfortunately we seem to get instances of varchar object inspectors without params   when an old-style UDF has an evaluate() method with varchar arguments.   If we disallow varchar in old-style UDFs and only allow GenericUDFs to be defined   with varchar arguments, then we might be able to enforce this properly. 
//  Note all maps and lists have to be absolutely sorted.  Otherwise we'll produce different   results for hashes based on the OS or JVM being used. 
/*  * The interface for a single byte array key hash multi-set contains method.  */
/*      * Guava versions <12.0 have stats collection enabled by default and do not expose a recordStats method.     * Check for newer versions of the library and ensure that stats collection is enabled by default.      */
//  Update to previous comment: there does seem to be one place that uses this   and that is to authorize "show databases" in hcat commandline, which is used   by webhcat. And user-level auth seems to be a reasonable default in this case.   The now deprecated HdfsAuthorizationProvider in hcatalog approached this in   another way, and that was to see if the user had said above appropriate requested   privileges for the hive root warehouse directory. That seems to be the best   mapping for user level privileges to storage. Using that strategy here. 
//  create map and fetch operators 
//  Trim down the total number of n-grams if we've exceeded the maximum amount of memory allowed     NOTE: Although 'k'*'pf' specifies the size of the estimation buffer, we don't want to keep         performing N.log(N) trim operations each time the maximum hashmap size is exceeded.         To handle this, we *actually* maintain an estimation buffer of size 2*'k'*'pf', and         trim down to 'k'*'pf' whenever the hashmap size exceeds 2*'k'*'pf'. This really has 
//  If the bucket is in the valid range, mark it as covered.   I wish Hive actually enforced bucketing all of the time. 
//  we used half the mem for small joins, now let's scale the rest 
//  BuddyAllocatorMXBean 
/*  * An single STRING key hash map optimized for vector map join. * * The key will be deserialized and just the bytes will be stored.  */
//  Use the current directory if it is not specified 
// this consistently works locally but never in ptest.... 
//  Test that 2 exclusive table locks coalesce to one 
//  Get tables, make sure the locations are correct 
//  Build VectorizedParquetColumnReader via Hive typeInfo and Parquet schema 
//  Get FieldSchema stuff if any. 
//  Set provider options 
//  test with and without specifying schema randomly 
// convert to the types needed for plugin api 
//  0 (lowest), 1 (middle), or 2 (high). 
//  Log summary every ~15 seconds. 
//  picks topN K:V pairs from input. 
/*    * Initialize one column's source deserializtion information.    */
//  we'd need to sleep once per round instead. 
//  the directory this move task is moving 
//  other operators or functions 
//  TS-1      TS-2    |          |   RS-1      RS-2      \      /        JOIN          |         FIL          |         RS-3     For the above complex operator tree,   selectivity(JOIN) = selectivity(RS-1) * selectivity(RS-2) and 
//  read the keys before the delta is flushed 
//  there are no elements in the map 
//  return the variable length from config 
//  Pattern to look for in the hive query and whether it matched 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.newBuilder() 
//  3. Perform a major compaction. 
//  ensure hiveserver2 site.xml does not get to override this 
/*    * We need the base (operator.java) implementation of start/endGroup.   * The parent class has functionality in those that map join can't use.   * Note: The mapjoin can be run in the reducer only on Tez.    */
//  Evaluate batch1 so that temporary arrays in the expression   have residual values to interfere in later computation 
//  100-200 byte string. Maybe replace with a local-dir-id, and construct on the fly.   3 longs + reference overheads. 
/*    * Union    */
//  Authorize individually. 
//  Fail the 2nd update too to get rid of the duck for the next test. 
//  if hash aggregation is not behaving properly, disable it 
//  if stripe offset is outside the split boundary then ignore the current   stripe as it will be handled by some other mapper. 
//  If still files remains to be copied due to failure/checksum mismatch after several attempts, then throw error 
//  directory does not exist, create it 
//  Find any constraints and drop them 
//  Verify that we have got correct set of delete_deltas also 
//  First generate all the opInfos for the elements in the from clause 
// fakePart path partition is added since the defined partition keys are valid 
//  OPERATION_HANDLE 
//  if this is a replication spec, then replace-mode semantics might apply. 
//  internal usage only   if length of variable length data type cannot be determined this length will be used. 
//  Create the column descriptors 
// return null only if the file system in schema is not recognized 
//  CREATE_TABLE - 5, TRUNCATE - 20(20 <= Id < 100), INSERT - 100 
//  we have to setup this again as the underlying PMF keeps getting reinitialized with original   reference closed 
//  Now set other column nullable too 
//  tezJsonParser 
//  Now the relevant TableScanOperators are known, find if there exists   a semijoin filter on any of them, if so, remove it. 
//  Right now, we do not handle the case that either of them is bucketed.   We should relax this constraint with a follow-up jira. 
//  the hashCode() and equals() methods of the key class. 
// No location - should allocate if force, no capacity otherwise 
//  This is the case when we've encountered a decimal separator. The fractional   part will not change the number, but we will verify that the fractional part 
//  Dryrun checks are meaningless for mutable table - we should always succeed   unless there is a runtime IOException. 
//  whether there's spilled data to be processed   used to hold restored 
// find dynamic partition columns  relies on consistent order via LinkedHashMap 
//  SessionState is not available in runtime and Hive.get().getConf() is not safe to call 
//  These are all values that we put here just for testing 
//  types get initialized in case they need to setup any   internal data structures - e.g., DynamicSerDeStructBase 
//  4. Keep track of colname-to-posmap && RR for new select 
//  If we can get them from HDFS, add group and permission 
//  1. Split leaf join predicate to expressions from left, right 
//  if project has any correlated reference, make sure they are also   provided by the current correlate. They will be projected out of the LHS 
//  a special property starting with mapreduce that we would also like to effect if it changes 
//  In that case, Executors.newFixedThreadPool will fail. 
//  Re-process spilled data 
//  Add Spark job handle id to the Hive History 
//  Prefix operator 
//  It is ok, ignore 
//  The user asked for stats to be collected.   Some stats like number of rows require a scan of the data   However, some other stats, like number of files, do not require a complete scan 
//  input paths. 
//  This ensures we don't create skew, e.g. with 8 ducks and 5 queries with simple rounding   we'd produce 2-2-2-2-0 as we round 1.6; whereas adding the last delta to the next query   we'd round 1.6-1.2-1.8-1.4-2.0 and thus give out 2-1-2-1-2, as intended.   Note that fractions don't have to all be the same like in this example. 
//  expect to fail since the time component is not 0 
//  verify syntax error 
//  fastIsInt returns false. 
// old partition does not exist 
//  use Kryo to serialize hashmap 
/*          * Caller is responsible for setting children and input type information.          */
//  "cause" is a root cause, and "e"/"first" is a useless   exception it's wrapped in. 
/*                * Multi-key specific lookup key.                */
//  storagehandler passed as table params 
//  Cast the input to decimal   If castType is decimal, try not to lose precision for numeric types. 
//  -- HIVE-4149 
//  this is only for the purpose of authorization, only the name matters. 
//  Specify an OrcSplit that starts beyond the offset of the last stripe. 
//  PROPERTIES 
// Transformation is to left join for correlated predicates and inner join otherwise, 
//  vertex's parent connections. 
//  Should generate [f,m] 
//  out of range probes 
//  set the key 
//  schema of the map-reduce 'key' object - this is homogeneous 
//  Crucial here that we don't reset the overflow batch, or we will loose the small table 
/*  * Convert tasks involving JOIN into MAPJOIN. * If hive.auto.convert.join is true, the tasks involving join are converted. * Consider the query: * select .... from T1 join T2 on T1.key = T2.key join T3 on T1.key = T3.key * * There is a map-reduce task which performs a 3-way join (T1, T2, T3). * The task would be converted to a conditional task which would have 4 children * a. Mapjoin considering T1 as the big table * b. Mapjoin considering T2 as the big table * c. Mapjoin considering T3 as the big table * d. Map-reduce join (the original task). * *  Note that the sizes of all the inputs may not be available at compile time. At runtime, it is *  determined which branch we want to pick up from the above. * * However, if hive.auto.convert.join.noconditionaltask is set to true, and * the sum of any n-1 tables is smaller than hive.auto.convert.join.noconditionaltask.size, * then a mapjoin is created instead of the conditional task. For the above, if the size of * T1 + T2 is less than the threshold, then the task is converted to a mapjoin task with T3 as * the big table. * * In this case, further optimization is performed by merging 2 consecutive map-only jobs. * Consider the query: * select ... from T1 join T2 on T1.key1 = T2.key1 join T3 on T1.key2 = T3.key2 * * Initially, the plan would consist of 2 Map-reduce jobs (1 to perform join for T1 and T2) * followed by another map-reduce job (to perform join of the result with T3). After the * optimization, both these tasks would be converted to map-only tasks. These 2 map-only jobs * are then merged into a single map-only job. As a followup (HIVE-3952), it would be possible to * merge a map-only task with a map-reduce task. * Consider the query: * select T1.key2, count(*) from T1 join T2 on T1.key1 = T2.key1 group by T1.key2; * Initially, the plan would consist of 2 Map-reduce jobs (1 to perform join for T1 and T2) * followed by another map-reduce job (to perform groupby of the result). After the * optimization, the join task would be converted to map-only tasks. After HIVE-3952, the map-only * task would be merged with the map-reduce task to create a single map-reduce task.  */
// @formatter:on 
//  then add any cor var from the left input. Do not need to change 
//  missing one partition on fs 
//  A hash map which stores job credentials. The key is a signature passed by Pig, which is 
//  create selection operator 
//  restore the reducer 
//  negative Unix time 
/*           This should be removed eventually. HIVE-17814 gives more detail          explanation of whats happening and HIVE-17815 as to why this is done.          Briefly for replication the graph is huge and so memory pressure is going to be huge if          we keep a lot of references around.         */
//  will cause underflow for result at position 0, must yield NULL 
//  can add .verboseLogging() to cause Mockito to log invocations 
/*  fast0  */
//  scale up/down 
//  Note: this may use additional inputs from the caller, e.g. maximum query   parallelism in the cluster based on physical constraints. 
// run Compaction 
//  Then, we need to set up the graph connection. Especially:   1, we need to connect this cloned parent work with all the grand-parent works. 
//  ORC writer reuses streams, so we need to clean them here and extract data. 
//  TODO: this makes many assumptions, e.g. on how generic args are done 
// nuke trailing " or " 
//  v1 is top level view, we should care about its access info. 
//  fetch table ablias 
/*    * build:   *         ^(TOK_SELECT   *             ^(TOK_SELECTEXPR {ast tree for count *}   *          )    */
//  trigger and action expressions are not validated here, since counters are not 
//  this might be a little bit too much...but in most cases this should be true 
//  the first time. 
//  use TextFile by default 
//  We will rewrite it to include the filters on transaction list   so we can produce partial rewritings 
//  Invalid character in new database name 
//  confirm the batch sizes were 5, 4 in the two calls to create partitions 
//  single predicate condition 
/*        * If the current table function has no order info specified;        */
//  miniHS2_1 will be leader 
//  We will try to merge this clause into one of the previously added ones. 
//  We have released the session by trying to reuse it and going back into queue, s3 can start. 
//  This bit should not be on for valid value references.  We use -1 for a no value marker. 
//  logRefreshError always throws. 
//  Do nothing but count the batch size. 
//  just remember it for later processing 
// http://www.postgresql.org/docs/8.1/static/errcodes-appendix.html 
//  check that the mapping schema is right;   check that the "column-family:" is mapped to  Map<key,?>   where key extends LazyPrimitive<?, ?> and thus has type Category.PRIMITIVE 
//  replace original STDDEV_SAMP(x) with     SQRT(       (SUM(x * x) - SUM(x) * SUM(x) / COUNT(x))       / CASE COUNT(x) WHEN 1 THEN NULL ELSE COUNT(x) - 1 END) 
//  SEL%SEL% rule. 
//  a signature string to associate with a HCatTableInfo - essentially 
//  if extended desc table then show the complete details of the table 
//  so that it produces multiple queries. For that we need at least 290. 
//  Insert overwrite on existing partition 
//  retValue is transient so store this separately. 
//  Sort only references field positions in collations field.   The collations field in the newRel now need to refer to the   new output positions in its input.   Its output does not change the input ordering, so there's no   need to call propagateExpr. 
//  Create the appender 
//  lock correctly. See the comment on the lock field - the locking needs to be reworked. 
//  to the small table result portion of the output for outer join. 
//  to prompt 
//  Not supported at all. 
//  Deserialize the HCatPartitionSpec using the target HCatClient instance. 
//  Otherwise, queue the session and make sure we update this pool. 
//  Scan through any remaining digits... 
//  If the original location exists here, then it must be the extracted files   because in the previous step, we moved the previous original location 
//  mapping of bucket id to size of all splits in bucket in bytes 
//  Tez requires us to use RPC for the query plan 
//  Compactor types 
//  When inserting into a new partition, the add partition event takes care of insert event 
//  If any events were queued by the responder, give them to the record reader now. 
//  In the end, update free list head. 
//  Configure export work 
//  the server sends 401 very frequently 
//  Read one field by one field 
//  This should be removed when authenticator and the 2-username mess is cleaned up. 
//  Release the unreleased buffers. See class comment about refcounts. 
//  Expiration queue is synchronized and notified upon when adding elements. Without jitter, we   wouldn't need this, and could simple look at the first element and sleep for the wait time.   However, when many things are added at once, it may happen that we will see the one that   expires later first, and will sleep past the earlier expiration times. When we wake up we   may kill many sessions at once. To avoid this, we will add to queue under lock and recheck   time before we wait. We don't have to worry about removals; at worst we'd wake up in vain.   Example: expirations of 1:03:00, 1:00:00, 1:02:00 are added (in this order due to jitter).   If the expiration threads sees that 1:03 first, it will sleep for 1:03, then wake up and   kill all 3 sessions at once because they all have expired, removing any effect from jitter.   Instead, expiration thread rechecks the first queue item and waits on the queue. If nothing   is added to the queue, the item examined is still the earliest to be expired. If someone   adds to the queue while it is waiting, it will notify the thread and it would wake up and   recheck the queue. 
//  Replication destination will not be external - override if set 
// create 2 rows in a file 00000_0 
//  There should only be a single split line... 
//  Filter files starts with ".". Note Hadoop consider files starts with   "." or "_" as hidden file. However, we need to replicate files starts   with "_". We find at least 2 use cases:   1. For har files, _index and _masterindex is required files 
//  Up to nanos 
//  Retrieve the acidOperationalProperties for the table, initialized in HiveInputFormat. 
//  Don't request any locks here, as the table has already been locked. 
//  -insert table as select- should not return a ResultSet 
/*      * Generate GB plan.     *     * @param qb     * @param srcRel     * @return TODO: 1. Grouping Sets (roll up..)     * @throws SemanticException      */
//  all keywrappers must be EmptyVectorHashKeyWrapper 
// do nothing to hanlde future RU/D where we may want to add new state types 
//  required   required   required   optional   optional   optional   optional   optional   optional 
//  If structs, recursively compare the fields 
//  Only altering the database property and owner is currently supported 
//  pRS-cGBYm-cRS-cGBYr (map aggregation) --> pRS-cGBYr(COMPLETE)   copies desc of cGBYm to cGBYr and remove cGBYm and cRS 
//  we do not use BytesWritable here to avoid the byte-copy from 
//  first_name = 'john' or    'greg' < first_name or    'alan' > first_name or    id > 12 or    13 < id or    id < 15 or    16 > id or    (id <=> 30 and first_name <=> 'owen') 
//  List bucketed table cannot be converted to transactional 
//  Create key/value structs and add the respective fields to each one 
//  6. Now apply a resource plan if any. This is expected to be pretty rare. 
//  verify that udf not in whitelist fails 
//  If this isn't equal, then bogus key values have been inserted, error out. 
//  for getPos() 
//  if schema size cannot be matched, then it could be because of constant folding   converting partition column expression to constant expression. The constant   expression will then get pruned by column pruner since it will not reference to   any columns. 
//  The situation here and in other readers is currently as such - setBuffers is never called   in SerDe reader case, and SerDe reader case is the only one that uses vector-s.   When the readers are created with vectors, streams are actually not created at all.   So, if we could have a set of vectors, then set of buffers, we'd be in trouble here;   we may need to implement that if this scenario is ever supported. 
//  Expected exception 
//  Column type 
//  for any exception in conversion to integer, produce NULL 
//  Set the DB_NOTIFICATION_EVENT_ID for future reference by other listeners. 
//  identd 
//  Can't assume JDK 1.8, so implementing this explicitly.   return Long.compare(x + Long.MIN_VALUE, y + Long.MIN_VALUE); 
//        That could be especially valuable given that this almost always the same set. 
//  TOK_QUERY above insert 
//  for each alias, add object inspector for short as the last element 
//  We need to send the state update again (the state has changed since the last one). 
//  Might not exist 
/*    * Ordinals for various reasons why an Error of this type can be thrown.    */
// When columns is 0, the union operator is empty. 
// check elements of the innermost struct 
//  param did not parse as a URL, so not a URL 
/*    * An extension to AcidOutputFormat that allows users to add additional   * options.   *   * todo: since this is only used for testing could we not control the writer some other way?   * to simplify {@link #OrcRecordUpdater(Path, AcidOutputFormat.Options)}    */
//  We have to initialize the thread pool before we start this one, as it uses it 
//  There are a couple of possibilities to consider here to see if we should recurse or not.   a) Path is a regex, and may match multiple entries - if so, this is likely a load and          we should listStatus for all relevant matches, and recurse-check each of those.          Simply passing the filestatus on as recurse=true makes sense for this.   b) Path is a singular directory/file and exists          recurse=true to check all its children if applicable   c) Path is a singular entity that does not exist          recurse=false to check its parent - this is likely a case of          needing to create a dir that does not exist yet. 
//  Is output type a BOOLEAN? 
//  If the record writer provides stats, get it from there instead of the serde 
//  Return join collations 
//  Only one send can be active at the same time. 
// Grouping sets expressions 
//  for all the other aggregations, we set the mode to PARTIAL2 
//  walk the other part of ast 
//  Wait for the first item to arrive at the queue and process it. 
//  Abandon the reuse attempt. 
//  Assert.assertEquals(1, resultDec.integerDigitCount()); 
// The FileSplit() constructor in hadoop 0.20 and 1.x is package private so can't use it.  This constructor is used to create the object and then call readFields()   so just pass nulls to this super constructor. 
/*      * See org.apache.hadoop.hive.ql.parse.TestMergeStatement for some examples of the merge AST      For example, given:      merge into acidTbl using nonAcidPart2 source ON acidTbl.a = source.a2      WHEN MATCHED THEN UPDATE set b = source.b2      WHEN NOT MATCHED THEN INSERT VALUES(source.a2, source.b2)      We get AST like this:      "(tok_merge " +        "(tok_tabname acidtbl) (tok_tabref (tok_tabname nonacidpart2) source) " +        "(= (. (tok_table_or_col acidtbl) a) (. (tok_table_or_col source) a2)) " +        "(tok_matched " +        "(tok_update " +        "(tok_set_columns_clause (= (tok_table_or_col b) (. (tok_table_or_col source) b2))))) " +        "(tok_not_matched " +        "tok_insert " +        "(tok_value_row (. (tok_table_or_col source) a2) (. (tok_table_or_col source) b2))))");        And need to produce a multi-insert like this to execute:        FROM acidTbl right outer join nonAcidPart2 ON acidTbl.a = source.a2        Insert into table acidTbl select nonAcidPart2.a2, nonAcidPart2.b2 where acidTbl.a is null        INSERT INTO TABLE acidTbl select target.ROW__ID, nonAcidPart2.a2, nonAcidPart2.b2 where nonAcidPart2.a2=acidTbl.a sort by acidTbl.ROW__ID     */
//  create a table with a unique name in testDb 
//  If the field corresponds to a column family in HBase 
//  Use Case 1.   amt == UNBOUNDED, is caught during translation 
//  year   month 
//  If newSortColList had a null value it means that at least one of the input sort   columns did not have a representative found in the output columns, so assume the data   is no longer sorted 
//  6. Return the new join as a replacement 
//  Not bothering with removing the entry. There's a limited number of hosts, and a good   chance that the entry will make it back in when the AM is used for a long duration. 
//  [A: 0, B: 2] 
//  Push the node in the stack 
//  @@protoc_insertion_point(builder_scope:PurgeCacheResponseProto) 
//  Query parallelism might be fubar. 
//  Create test-null-appender to drop events without queryId 
// set dest name mapping on new context; 1st chid is TOK_FROM 
/*  * With an aim to consolidate the join algorithms to either hash based joins (MapJoinOperator) or * sort-merge based joins, this operator is being introduced. This operator executes a sort-merge * based algorithm. It replaces both the JoinOperator and the SMBMapJoinOperator for the tez side of * things. It works in either the map phase or reduce phase. * * The basic algorithm is as follows: * * 1. The processOp receives a row from a "big" table. * 2. In order to process it, the operator does a fetch for rows from the other tables. * 3. Once we have a set of rows from the other tables (till we hit a new key), more rows are *    brought in from the big table and a join is performed.  */
//  Get the final state of the Spark job and parses its job info 
//  alias3 only can be selected 
// here we need to see if remaining columns are dynamic partition columns 
//  convert to Text and write it 
//  if join-key is null, process each row in different group. 
//  Third value. 
//  1. Build Column Names 
//  Finally write the hash code. 
/*        * If threads are not configured then they will be executed in current thread itself.        */
// conf.set("hadoop.job.history.location",new File(workDir).getAbsolutePath()+"/history"); 
//  Go through all bytes in the byte[] 
//  Create the LazyObject for storing the rows 
//  overflow 
//  for now, check only table & db 
//  since for LEFT join we are only interested in rows from LEFT we can get rid of right side 
// column stats will be inaccurate 
//  Move the hfiles file(s) from the task output directory to the   location specified by the user. 
//  all nulls are now explicit 
//  mysql, postgres, sql server 
/*    * (non-Javadoc)   *   * @see org.apache.hadoop.hive.ql.optimizer.Transform#transform(org.apache.hadoop.hive.ql.parse.   * ParseContext)    */
//  It might just be the default, in which case we can drop that one if it's empty 
//  set the arguments for GenericUDFStructField 
//  Set up the base class 
//  Process the bytes that can be escaped (the last one can't be). 
//  create a dummy select to select all columns 
//  Intentionally using the deprecated method to make sure it returns correct results. 
//  All fields have been parsed, or bytes have been parsed.   We need to set the startPositions of fields.length to ensure we   can use the same formula to calculate the length of each field.   For missing fields, their starting positions will all be the same,   which will make their lengths to be -1 and uncheckedGetField will   return these fields as NULLs. 
//  Run with --recover and save the output to a file so it can be checked. 
//  this scales down o.v[0] to 0 because 2^32 = 4.2E9. No   possibility of rounding. 
//  change the children of the original join operator to point to the map 
//  Wait for all threads to be ready.   Release them at the same time. 
/*     nonacidnonbucket/     000000_0     000000_0_copy_1     000000_0_copy_2     base_0000004      bucket_00000     delete_delta_0000002_0000002_0000      bucket_00000     delete_delta_0000004_0000004_0000      bucket_00000     delta_0000001_0000001_0000      bucket_00000     delta_0000002_0000002_0000      bucket_00000     delta_0000003_0000003_0000         bucket_00000    6 directories, 9 files     */
//  This is only needed if a new grouping set key is being created 
//  Check for the escaped colon to remove before doing the expensive regex replace 
//  Make one a materialized view 
//  The scale of the decimal. 
//  The offset in the destination array for the beginning of this missing range. 
//  Test EventUtils.getEventBoundaryFilter - this is supposed to only allow events 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#cancelOperation(org.apache.hive.service.cli.OperationHandle)    */
//  in the for loop below 
//  Note that for temp tables there is no need to rename directories 
//  they don't seem to work. The IPC timeout needs to be set instead. 
//  source: HiveEvents.proto 
//  If we have cases where we are running a query like count(key) or count(*),   in such cases, the readColIDs is either empty(for count(*)) or has just the   key column in it. In either case, nothing gets added to the scan. So if readAllColumns is   true, we are going to add all columns. Else we are just going to add a key filter to run a 
//  OBJECT_TYPE_PTR 
//  generate data file for test 
//  3. Input, Output Row Resolvers 
//  2. Generate the vertex/submit information for all events. 
//  partitioned table, we expect partition values   convert user specified map to have lower case key names 
//  Fallback to integer parsing 
//  Save so we can verify end of stream   We need mark support - wrap with BufferedInputStream. 
// we don't expect corr vars withing JOIN or UNION for now   we only expect cor vars in top level filter 
//  Get the maximum of the number of tasks in the stages of the job and cancel the job if it goes beyond the limit. 
/*      * Bail if having clause uses Select Expression aliases for Aggregation     * expressions. We could do what Hive does. But this is non standard     * behavior. Making sure this doesn't cause issues when translating through     * Calcite is not worth it.      */
//  We don't care about these. 
//  If the user hasn't been reading by row, use the fast path. 
//  we assert only if we expected to assert with this call. 
//  5/ update the list byte size 
//  Close the writer to finalize the metadata. 
//  since we may need to split the task, let's walk the graph bottom-up 
//  grouping happens in execution phase. The input payload should not enable grouping here, 
//  note: v[0] ^ v[1] ^ v[2] ^ v[3] would cause too many hash collisions 
//  Don't throw an exception if the target location only contains the staging-dirs 
//  suffix first 
// with split update, new version of the row is a new insert 
//  Junk after trailing blank padding. 
//  production is:   [this.fieldid :] Requiredness() FieldType() this.name FieldValue()   [CommaOrSemicolon()] 
/*  enable zero copy record reader  */
//  Consider a query like:   select a, b, count(distinct c) from T group by a,b with rollup;   Assume that hive.map.aggr is set to true and hive.groupby.skewindata is false,   in which case the group by would execute as a single map-reduce job.   For the group-by, the group by keys should be: a,b,groupingSet(for rollup), c 
/*      * If the form is Dim loj F or Fact roj Dim or Dim semij Fact then return     * null.      */
//  The job configuration is passed in so the configuration will be cloned   from the pig job configuration. This is necessary for overriding   metastore configuration arguments like the metastore jdbc connection string   and password, in the case of an embedded metastore, which you get when   hive.metastore.uris = "". 
//  Turn off metastore-side authorization 
//  masking and filtering should be created here 
//  Copy info that may be required in the new copy.   The SettableUDF calls below could be replaced using this mechanism as well. 
//  on the task generated in the first pass. 
//  This is only required to support the deprecated methods in HCatAddPartitionDesc.Builder. 
//  m = 2^p 
//     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_IN_TEST, hiveInTest);   turn on db notification listener on meta store 
//  Remove entries from completed_txn_components as well, so we don't start looking there   again but only up to the highest write ID include in this compaction job.  highestWriteId will be NULL in upgrade scenarios 
//  TODO: add interval and complex types 
//  A join is eligible for a sort-merge join, only if it is eligible for   a bucketized map join. So, we dont need to check for bucketized map   join here. We are guaranteed that the join keys contain all the 
//  still null 
//  type with padded spaces. 
//  or from expressions. 
//  the columns correspond. 
//  In case the query is served by HiveServer2, don't pad it with spaces, 
//  Set the security key provider so that the MiniDFS cluster is initialized   with encryption 
//  This ends up being set to "test" | mvn ${testCasePropertyName} for instance 
//  So let's not use them anywhere unless absolutely necessary. 
//  Parser has done some verification, so the order of tokens doesn't need to be verified here. 
//  Find the new delta file and make sure it has the right contents 
//  Overflow.  Use slower alternate. 
//  consistent with other APIs like makeExpressionTree, null is returned to indicate that   the filter could not pushed down due to parsing issue etc 
//  EXPR 
//  test both repeating 
//  NonZeroExitCodeExceptions can have long messages and should be   trimmable when published to the JIRA via the JiraService 
//  Append is not supported in the cluster, try to use create 
/*  @bgen(jjtree) ConstListContents  */
//  Use our specialized hash table loader. 
//  update config in Hive thread local as well and init the metastore client 
//  For last stripe we need to get the last trasactionId/bucket/rowId from the last row. 
//  Go through the set of partition columns, and find their representatives in the values 
//  Table is partitioned by single key 
//  double column/scalar IF 
//  Expected row count of the join query we'll run 
//  Finish the current 'IN'/'NOT IN' clause and start a new clause.   replace the "commar". 
//  SHA matches... 
//  First PK 
//  Try with an extra delta. 
//  of histogram bins to use in the percentile approximation. 
//  Re-order the wait queue. Note: we assume that noone will take our capacity based   on the fact that we are doing this under the epic lock. If the epic lock is removed,   we'd need to do the steps under the queue lock; we could pass in a f() to update state. 
//  Try to load the composite factory if one was provided 
//  check for the operators who will process rows coming to this Map Operator 
//  Call the real methods for these 
//  This can happen e.g. for a data stream when all the values are null. 
//  No complex type support for now. 
//  PersistentEphemeralNode will make sure the ephemeral node created on server will be present   even under connection or session interruption (will automatically handle retries) 
//  This must be called after all the explicit register calls. 
//  Try again with a value that won't fit in 5 digits, to make 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#openSession(java.lang.String, java.lang.String, java.util.Map)    */
/*    * 1. Find out if the operator is invoked at Map-Side or Reduce-side   * 2. Get the deserialized QueryDef   * 3. Reconstruct the transient variables in QueryDef   * 4. Create input partition to store rows coming from previous operator    */
//  verify both am get node heartbeat 
//  Test HCatContext 
//  Verify individual arguments 
//  Otherwise, throw FileAlreadyExistsException, which means the file owner is   dead 
//  Read the items from the input stream and confirm they match 
//  If bucket map join, only a split goes in memory 
//  Semijoin DPP work is considered a child because work needs 
//  2) The dimension columns 
//  Push each aggregate function down to each side that contains all of its   arguments. Note that COUNT(*), because it has no arguments, can go to 
// we do this for Update/Delete (incl Merge) because we introduce this column into the query  as part of rewrite 
//  A few situations where we need the default table path, without a DB object 
// Repeating non null 
//  end of else - i.e. could not allocate 
//  if the clause contains any expression. 
//  Make sure we actually go the table name 
//  This method will only return full resource plan when activating one,   to give the caller the result atomically with the activation. 
//  Wait for the server to bootup 
//  Create a Hadoop configuration without inheriting default settings. 
//        we need to set the global to null to do that, this "reuse" may be pointless. 
// noinspection ConstantConditions 
//  We do this because if function returns null,   the mapping for key is removed, i.e. the table is mutated. 
//  Note : This implements HttpRequestInterceptor rather than extending   HttpRequestInterceptorBase, because that class is an auth-specific   class and refactoring would kludge too many things that are potentially   public api.     At the base, though, what we do is a very simple thing to protect   against CSRF attacks, and that is to simply add another header. If   HS2 is running with an XSRF filter enabled, then it will reject all   requests that do not contain this. Thus, we add this in here on the   client-side. This simple check prevents random other websites from   redirecting a browser that has login credentials from making a   request to HS2 on their behalf. 
//  Scopes of execution (code blocks) with own local variables, parameters and exception handlers 
//  BIT_VECTOR_SIZE is 31, we can use 32 bits, i.e., 4 bytes to represent a 
//  SEL(no-compute)-SEL. never seen this condition   and also, removing parent is not safe in current graph walker 
//  we can't handle distinct 
// otherwise, it is the case of analyze table T compute statistics for columns; 
//  Create a TableScan operator 
//  MSB 64 - p bits 
//  serialize path, offset, length using FileSplit 
//  connect using token via Beeline using script 
//  Determine the keys for the current clause. 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getMoreResults()    */
//  ENABLE_CSTR 
//  getWritableObject() will convert LazyPrimitive to actual primitive   writable objects. 
//  Pack the output into the scratch longs. 
//  Run Cleaner. 
//  The dispatcher fires the processor corresponding to the closest matching rule and passes 
//  Need reset during re-open when needed 
//  We might generate other types that are not recognized, e.g., a field reference   if it is a nested field, but since this is just an additional optimization,   we bail out without introducing the Select + GroupBy below the right input   of the left semijoin 
//  Make sure nanos are preserved 
//  off so we don't sit and hammer the metastore in a tight loop 
//  Finish the scheduled compaction for ttp2, and manually compact ttp1, to make them comparable again 
//  If the current subExpression is pre-calculated, as in Group-By etc. 
//  parsing statement is now done, on to logic. 
// until change is made to use the admin option. Default to false with V2 authorization 
//  Corresponds to SemAnalyzer genGroupByPlan1MR   Corresponds to SemAnalyzer genGroupByPlan2MR   Corresponds to SemAnalyzer 
//  If table doesn't exist, allow creating a new one only if the database state is older than the update. 
//  New batch has been fetched. If it's not empty, we have more elements to process. 
//  Maximum number of paritions aggregated per cache node 
//  Bottom operator does not contain offset/fetch 
//  Heartbeat only for active tasks. Errors, etc will be reported directly. 
//  Tests for dropPartition(String db_name, String tbl_name, List<String> part_vals,   PartitionDropOptions options) method 
//  And, a mutable read position for thread safety when sharing a hash map. 
// result could be cached if this object were to be made immutable...  
//  the "has decimal or second VInt" flag is not set 
//  append the stripe buffer to the new ORC file 
//  for singular arg, count should not include null   e.g., count(case when i=1 and department_id is not null then 1 else null end) as c0,  
//  Put the mapping from part to pruner_pred 
//  HashMap 
//  If parent table is in the same database, change it to the actual db on destination   Otherwise, keep db name 
/*      * used for buffering appends before flush them out      */
//  IS_IN_UNMANAGED 
//  The index of the child which the last row was forwarded to in a key group. 
//  This means the reader has already been closed. 
//  0.5 added for rounding off 
//  Ceil on an integer argument is a noop, but it is less code to handle it this way. 
//  boolean + double + AtomicLong 
//  6.3 Get rid of TOK_SELEXPR 
/*      * Use a different batch for vectorized Input File Format readers so they can do their work     * overlapped with work of the row collection that vector/row deserialization does.  This allows     * the partitions to mix modes (e.g. for us to flush the previously batched rows on file change).      */
//  @@protoc_insertion_point(class_scope:LlapDaemonProtocol) 
//  This call is accessed from server side 
//  set these streams only if the stripe is different 
//  there is no CTE, walk the whole AST 
//  this optimizer is for replacing FS to temp+fetching from temp with   single direct fetching, which means FS is not needed any more when conversion completed. 
//  Selectivity: key cardinality of semijoin / domain cardinality 
//  locations if orig-loc becomes important 
//  1) Cancel the previous expansion, if any. 
//  Don't read enough data for the first message to be decoded. 
// test params 
/*    * for functions that don't support a Window, this provides the rows remaining to be    * added to output. Functions that return a Window can throw a UnsupportedException,   * this method shouldn't be called. For Ranking fns return 0; lead/lag fns return the   * lead/lag amt.    */
//  Retrieve creation metadata if needed 
//  there can be only one instance per path 
//  5. Build Rel for Select Clause 
/* if doAs user is different than logged in user, need to check that      that logged in user is authorized to run as 'doAs' */
//  3rd task requested unknown host, got host2 since host1 is full and only host2 is left in random pool 
//  Replacing an inactive plan. 
//  Update cacheUsage to reference the pending entry. 
//  remember it for additional processing later 
//  If we are not pulling constants, OR   we are pulling constants but this is not a constant 
//  Only change txnMgr if the setting has changed 
//  Verify the fetched log (from the beginning of log file) 
//  Since interval types not currently supported as table columns, need to create them   as expressions. 
//  Set the move task to be dependent on the current task 
//  Try to read the given named url from the connection configuration file 
//  if FieldCount == 2, get types for key & value, 
//  setup the completer for the database 
//  Let's try to populate those stats that don't require full scan. 
//  In the future, we can add checking for username, groupname, etc based on   HiveAuthenticationProvider. For example,   "hive_test_user".equals(context.getUserName()); 
//  this tests the case where older data has an ambiguous structure, but the   correct interpretation can be determined from the repeated name, "array" 
//  likely we found a table scan operator 
//  MY_BYTE 
//  exit the JVM if Ctrl+C is received   and no current statement is executing 
//  11. Finally, for all the pools that have changes, promote queued queries and rebalance. 
//  Interpret Ctrl+C as a request to cancel the currently   executing query. 
//  following SEL will do CP for columns from UDTF, not adding SEL in here 
//  Sort by operator ID so we get deterministic results 
//  Replace any \* that appear in the prefix with a regular * 
/*    * Inverse of 2^63 = 2^-63.  Please see comments for doDecimalToBinaryDivisionRemainder.   *   * Multiply by 1/2^63 = 1.08420217248550443400745280086994171142578125e-19 to divide by 2^63.   * As 16 digit comma'd 1084202172485,5044340074528008,6994171142578125   *   * Scale down: 63 = 44 fraction digits + 19 (negative exponent or number of zeros after dot).   *   * 3*16 (48) + 15 --> 63 down shift.    */
//  timestamp NOT BETWEEN 
//  number of columns   a vector for each column   number of rows that qualify (i.e. haven't been filtered out)   array of positions of selected values 
//  Test one random hi-precision decimal add. 
//  First aggregation calculation for group. 
//  write a base file in partition 0 
//  offset to some "offset" in "middle" of the slice (but see TODO for firstStart). 
//  Relocate all assigned slots from the old hash table. 
/*      * Count the number of digits in the mantissa (including the decimal     * point), and also locate the decimal point.      */
//  A delete/update generates a delete event for the original row. 
//  Was using Hadoop-internal API to get tasklogs, disable until  MAPREDUCE-5857 is fixed. 
//    Rewrite logic:     The original left input will be joined with the new right input that   has generated correlated variables propagated up. For any generated   cor vars that are not used in the join key, pass them along to be   joined later with the CorrelatorRels that produce them.   
//  debugDumpKeyProbe(keyOffset, keyLength, hashCode, slot); 
// run it with each split strategy - make sure there are differences 
//  By default, many existing @Explain classes/methods are NON_VECTORIZED.     Vectorized methods/classes have detail levels:       SUMMARY, OPERATOR, EXPRESSION, or DETAIL.   As you go to the right you get more detail and the information for the previous level(s) is   included.  The default is SUMMARY.     The "path" enumerations are used to mark methods/classes that lead to vectorization specific   ones so we can avoid displaying headers for things that have no vectorization information   below.     For example, the TezWork class is marked SUMMARY_PATH because it leads to both   SUMMARY and OPERATOR methods/classes. And, MapWork.getAllRootOperators is marked OPERATOR_PATH   because we only display operator information for OPERATOR.     EXPRESSION and DETAIL typically live inside SUMMARY or OPERATOR classes.   
//  Hash bits in ref don't match. 
/*  * Specialized class for doing a vectorized map join that is an inner join on a Single-Column String * using a hash map.  */
// gets SS lock on T8 
// No drop part listener events fired for public listeners historically, for drop table case.  Limiting to internal listeners for now, to avoid unexpected calls for public listeners. 
//  Shuffle read metrics. 
/*      * 2. setup resolve, make connections      */
//  JT timeout in msec. 
//  There are 2 cases where we increment CREATED_DYNAMIC_PARTITIONS counters   1) Insert overwrite (all partitions are newly created)   2) Insert into table which creates new partitions (some new partitions) 
/*      * ASCII 01-1F are HTTP control characters that need to be escaped.     * \u000A and \u000D are \n and \r, respectively.      */
// Thrift cannot return null result 
//  Set the bucketing Version 
//  the multiple parts to partition predicate are joined using and 
//  Used by readNextField/skipNextField and not by readField. 
//  Get "Script failed with code <some number>" 
//  Restart asynchronously, don't block the caller. 
//  easier case. take a quick path 
//  authorization checks passed. 
//  Default is false 
//  LRU. Could also implement LRU as a doubly linked list if CacheEntry keeps its node. 
//  replacing getAliasToWork, so should use that information instead. 
//  Short.MAX_VALUE 
// gets SS lock on T7 
/*    * Returns true if the join conditions execute over the same keys    */
//  Get configuration parameters 
//  Next arena is being allocated. 
//  Put the mapping from table scan operator to pruner_pred 
//  Add path components explicitly, because simply concatenating two path   string is not safe, for example:   "/" + "/foo" yields "//foo", which will be parsed as authority in Path 
//  The default ones are created in case of null; tests override this. 
//  Do not make a remote call under any circumstances - this is supposed to be async. 
//  we should now only have the unexpected folders left 
//  Flush the metastore cache.  This assures that we don't pick up objects from a previous   query running in this same thread.  This has to be done after we get our semantic   analyzer (this is when the connection to the metastore is made) but before we analyze, 
//  Note this includes any outer join keys that need to go into the small table "area". 
/*  first_name <=> 'owen'  */
//  LONG_STATS 
//  End PushFilterPastJoinRule.java 
//  No need to set type name, it should always be timestamplocaltz 
// update the credential provider location in the jobConf 
//  an external table 
//  The SIMD optimized form of "a == b" is "(((a - b) ^ (b - a)) >>> 63) ^ 1" 
//  keys 
//  map join work 
//  set it back so that column pruner in the optimizer will not do the 
//  No support for statistics. That seems to be a popular answer. 
//  Give up. 
//  Verify that we have got correct set of deltas. 
//  Test gt/lt/lte/gte for numbers. 
//  verify that flattening and unflattenting no-nulls works 
//  As current txn is aborted, this won't read any data from other txns. So, it is safe to unregister   the min_open_txnid from MIN_HISTORY_LEVEL for the aborted txns. Even if the txns in the list are   partially aborted, it is safe to delete from MIN_HISTORY_LEVEL as the remaining txns are either 
//  Reverse the value 
//  AMNodeInfo will only be cleared when a queryComplete is received for this query, or   when we detect a failure on the AM side (failure to heartbeat).   A single queueLookupCallable is added here. We have to make sure one instance stays   in the queue till the query completes. 
//  offer accepted and r2 gets evicted 
//  the root path is not useful anymore 
//  TOK_SKEWED_LOCATION_MAP 
//  the partitions that are not required 
// / if COUNT returns true since COUNT produces 0 on empty result set 
//  base env impl simply defers to System.getenv. 
/*  all 32 bit numbers qualify & multiply up to get nano-seconds  */
//  Carefully handle NULLs.. 
//  populate map task 
/*            * If the rhs references table sources and this QBJoinTree has a leftTree;           * hand it to the leftTree and let it recursively handle it.           * There are 3 cases of passing a condition down:           * 1. The leftSide && rightSide don't contains references to the leftTree's rightAlias           *    => pass the lists down as is.           * 2. The leftSide contains refs to the leftTree's rightAlias, the rightSide doesn't           *    => switch the leftCondAl1 and leftConAl2 lists and pass down.           * 3. The rightSide contains refs to the leftTree's rightAlias, the leftSide doesn't           *    => switch the rightCondAl1 and rightConAl2 lists and pass down.           * 4. In case both contain references to the leftTree's rightAlias           *   => we cannot push the condition down.           * 5. If either contain references to both left & right           *    => we cannot push forward.            */
// 1,2,3,4,5,6,7 
//  there are two cases - array<Type> and array<struct<...>>   in either case the element type of the array is represented in a   tuple field schema in the bag's field schema - the second case (struct)   more naturally translates to the tuple - in the first case (array<Type>)   we simulate the tuple by putting the single field in a tuple 
//  Returning null from this fn can serve as an err condition. 
//  crossing reduce sink or file sink means the pruning isn't for this parent. 
//  aggregation for semijoin 
//  Druid Returns 400 Bad Request when not found. 
//  ////// Generate GroupbyOperator for a partial aggregation 
//  merge only if the register length matches 
//  not currently handled 
/*  fall through - miss in locality or no locality-requested  */
//  We assume AM might be bad so we will not try to kill the query here; just scrap the AM. 
//  bloom-1 is known to have higher fpp, to make tests pass give room for another 3% 
//  Calculate tags individually since the schema can evolve and can have different tags. In worst case, both schemas are same   and we would end up doing calculations twice to get the same tag   Determine index of value from fileSchema   Determine index of value from recordSchema 
/*  13 < id or  */
//  it's essentially a MapJoinDesc 
//  return the url based on the aggregated connection properties 
//  Fill down null flags.   UNDONE 
//  2. Walk through leaf predicates building up JoinLeafPredicateInfo 
//  ip address 
// return "com.microsoft.sqlserver.jdbc.SQLServerDriver"; 
/*  scaleDown  */
//  ensure associated master key is available 
//  UNDONE: UNKNOWN OPTION? 
//  Release memory - simple deallocation. 
//  that it can be read by the input format. 
//  check privileges on input and output objects 
//  The external client handling umbilical responses and the connection to read the incoming   data are not coupled. Calling close() here to make sure an error in one will cause the   other to be closed as well. 
//  Sum all non-null long column values; maintain isGroupResultNull. 
//  Data is expected to be a series of data chunks in the form <chunk size><chunk bytes><chunk size><chunk bytes>   The final data chunk should be a 0-length chunk which will indicate end of input. 
//  From the jar file, the parent is /lib folder 
//  Verify if the data are intact even after applying an applied event once again on missing objects 
/* select all locks for this ext ID and see which ones are missing */
//  We don't write rowSeparatorByte because that should be handled by file   format. 
//  Set up cache directory 
//  There are no inputs, so rel does not need to be changed. 
//  Fail - No "transactional" property is specified 
//  There are different cases for Group By depending on map/reduce side, hash aggregation,   grouping sets and column stats. If we don't have column stats, we just assume hash   aggregation is disabled. Following are the possible cases and rule for cardinality   estimation 
//  [A: 0, B: 1] 
//  If doAs is set to true for HiveServer2, we will create a proxy object for the session impl. 
//  Stop Kafka Ingestion first 
/* real grant time added by metastore */
/*        * mix -- mix 3 32-bit values reversibly.       * This is reversible, so any information in (a,b,c) before mix() is       * still in (a,b,c) after mix().       *       * If four pairs of (a,b,c) inputs are run through mix(), or through       * mix() in reverse, there are at least 32 bits of the output that       * are sometimes the same for one pair and different for another pair.       *       * This was tested for:       * - pairs that differed by one bit, by two bits, in any combination       *   of top bits of (a,b,c), or in any combination of bottom bits of       *   (a,b,c).       * - "differ" is defined as +, -, ^, or ~^.  For + and -, I transformed       *   the output delta to a Gray code (a^(a>>1)) so a string of 1's (as       *    is commonly produced by subtraction) look like a single 1-bit       *    difference.       * - the base values were pseudorandom, all zero but one bit set, or       *   all zero plus a counter that starts at zero.       *       * Some k values for my "a-=c; a^=rot(c,k); c+=b;" arrangement that       * satisfy this are       *     4  6  8 16 19  4       *     9 15  3 18 27 15       *    14  9  3  7 17  3       * Well, "9 15 3 18 27 15" didn't quite get 32 bits diffing for       * "differ" defined as + with a one-bit base and a two-bit delta.  I       * used http://burtleburtle.net/bob/hash/avalanche.html to choose       * the operations, constants, and arrangements of the variables.       *       * This does not achieve avalanche.  There are input bits of (a,b,c)       * that fail to affect some output bits of (a,b,c), especially of a.       * The most thoroughly mixed value is c, but it doesn't really even       * achieve avalanche in c.       *       * This allows some parallelism.  Read-after-writes are good at doubling       * the number of bits affected, so the goal of mixing pulls in the       * opposite direction as the goal of parallelism.  I did what I could.       * Rotates seem to cost as much as shifts on every machine I could lay       * my hands on, and rotates are much kinder to the top and bottom bits,       * so I used rotates.       *       * #define mix(a,b,c) \       * { \       *   a -= c;  a ^= rot(c, 4);  c += b; \       *   b -= a;  b ^= rot(a, 6);  a += c; \       *   c -= b;  c ^= rot(b, 8);  b += a; \       *   a -= c;  a ^= rot(c,16);  c += b; \       *   b -= a;  b ^= rot(a,19);  a += c; \       *   c -= b;  c ^= rot(b, 4);  b += a; \       * }       *       * mix(a,b,c);        */
//  we have found a reduce sink 
// replace trailing ',' 
//  Parse integer portion. 
// actually only at most one loop 
//  previous call to projectNonColumnEquiConditions updated it 
//  Multiply by self. 
//  All key input columns are repeating.  Generate key once.  Lookup once.   Since the key is repeated, we must use entry 0 regardless of selectedInUse. 
// plan   Make sure we need locks.  It's possible there's nothing to lock in 
//  test if the group by needs partition level sort, if so, use the MR style shuffle   SHUFFLE_SORT shouldn't be used for this purpose, see HIVE-8542 
//  Nothing else is updated after the first update. 
//  in filter expression since it will be taken care by partition pruner 
/*      * Currently we are not handling dynamic sized windows implied by range     * based windows.      */
//  create a union above all the branches 
//  -ve dates are also valid Timestamps - dates are within 1959 to 2027 
//  should be called after session registry is checked 
//  test null values 
//  Try to deserialize using DeserializeRead our Writable row objects created by SerDe. 
//  for canceling the query (should be bound to session?) 
//  could get either query hint or select expr 
//  get old table 
/*  If every row qualified (newSize==n), then we can ignore the sel vector to streamline         * future operations. So selectedInUse will remain false.          */
//  initialized, which may cause it to drop events. 
//  Check whether it is a monotonic preserving cast, otherwise we cannot push 
//  sub fields 
//  load hiveserver2-site.xml if this is hiveserver2 and file exists   metastore can be embedded within hiveserver2, in such cases   the conf params in hiveserver2-site.xml will override whats defined 
//  Call set_ugi, only in unsecure mode. 
//  if the aggregation type is min/max, we extrapolate from the   left/right borders 
//  Output columns ok? 
//  make a list before opening the RPC attack surface 
//  first adjust count() expression if any 
//  Most of the stuff we can handle are generic function descriptions, so   handle the special cases. 
/*    * Context for reading a Vectorized Input File Format.    */
//  exponent part 
//  Must be deterministic order map for consistent q-test output across Java versions - see HIVE-9161 
//  3. If we could not transform anything, we bail out 
//  names[0] have the Db name and names[1] have the view name 
//  Do this check in case the ciphertext actually makes sense in some way. 
//  1st Txn 
//  For TOK_FUNCTION, the function name is stored in the first child,   unless it's in our   special dictionary. 
//  Arithmetic with a type timestamp (TimestampColumnVector) and type interval_year_month (LongColumnVector storing 
//  compute keys and values as StandardObjects 
//  If the table was already marked as 'transactional=true', then the new value of   'transactional_properties' must match the old value. Any attempt to alter the previous   value will throw an error. An exception will still be thrown if the previous value was   null and an attempt is made to set it. This behaviour can be changed in the future. 
//  If the reduce sink has not been introduced due to bucketing/sorting, ignore it 
//  Once NodeId includes fragmentId - this becomes a lot more reliable. 
//  need to fill in information about the key and value in the reducer 
//  Accumulo token information. 
//  @@protoc_insertion_point(class_scope:TerminateFragmentResponseProto) 
//  Create vectorized expr 
//  compute the total size per bucket 
//  If changing this file, make sure to make corresponding changes in llap-daemon-log4j2.properties 
//  Repeating non null 
//  If this is a constant boolean expression, return the value. 
//  this offer will be rejected 
//  Create a reduceSink operator followed by another limit 
//  Make a special case for "\\_" and "\\%" 
//  Create a new map join operator 
//  End ASTNodeOrigin.java 
//  We only check one file, so exit the loop when we have at least   one. 
//  RUNNING state 
//  Lookup of cache entries by table used in the query, for cache invalidation. 
//  create a test table with auto.purge = true 
//  Print a message if we reached at least 1000 rows for a join operand   We won't print a message for the last join operand since the size   will never goes to joinEmitInterval. 
//  https://issues.apache.org/jira/browse/HIVE-17627 
//  EUROPE_UK 
//  show partition information 
//  No group value. 
//  nothing further to test. 
//  Initialize the results 
//  Build the expression based on the partition predicate 
//  Also try other patterns 
/*  Create list bucketing sub-directory only if stored-as-directories is on.  */
//  Fill the column vector with nulls 
//  x events to insert, last repl ID: replDumpId+x 
// check that we start with default db 
//  On failure, the SASL handler will throw an exception indicating that the SASL   negotiation failed. 
//  The fact that stdev doesn't increase with increasing missCount is captured outside. 
//  we allow null values for map. 
//  If StatOptimization is not applied for any reason, the FetchTask should still not have been set 
//  this is the option which will make it a delete writer 
//  we need to do new hashmap, since stats object is reused across calls. 
//  Iterate backwards, from the destination table to the top of the tree   Based on the output column names, get the new columns. 
//  MAX_DECIMAL 9's WITH ROUND. 
//  Add data files to the partitioned table 
// sqlLine.getOpts().setEntireLineAsCommand(true); 
//  Iterate through the list 
//  of newProject, plus any aggregates that the oldAgg produces. 
//  assumes deserializer is not buffering itself   position over uncompressed stream. not sure what   effect this has on stats about job 
// check elements of the innermost union 
//  when ';' is not yet seen   number of lines to fetch in batch from remote hive server 
//  child process. so we add it here explicitly 
//  create the project before GB because we need a new project with extra column '1'. 
//  it's hadoop-1 
//  Columns statistics for complex datatypes are not supported yet 
//  TRIGGER_EXPRESSION 
//  Store ugi in transport if the rpc is set_ugi 
/*  first_name = last_name  */
//  This yields empty because starting idx is out of bounds. 
//  For debug tracing: the name of the map or reduce task. 
//  Need to differentiate between an unmatched pattern and a non-existent database 
// downstream code expects it to be set to a valid value 
//  materialized 
//  If cpartcols or ppartcols have constant node expressions avoid the merge. 
//  once called first, it will never be able to   write again. 
//  Base case:  we are eager if a child is stateful 
//  Match the given bytes with the like pattern 
//  For some reason even with an MBeanException available to them   Runtime exceptions can still find their way through, so treat them   the same as MBeanException 
//  Settable 
//  Instead of retrying with this task, we will try to pick a different suitable task. 
//  shared_write 
//  Hive 0.12 behavior where double / decimal -> decimal is gone. 
//  Check that the union has come out unscathed. No scathing of unions allowed. 
//  TODO: Should be checked on server side. On Embedded metastore it throws MetaException,   on Remote metastore it throws TProtocolException 
//  map that says which mapjoin belongs to which work item 
//  so we can later run the same logic that is run in ReduceSinkMapJoinProc. 
//  Same union, order reveresed 
//  SW.E: Lock we are examining is exclusive 
//  check the character numbers with the length 
//  user wants file store based configuration 
//  partition columns will always at the last 
//  Drop partition will clean the partition entry from the compaction queue and hence cleaner have no effect 
//  sequence file write 
//  List of input expressions. If a particular aggregate needs more, it   will add an expression to the end, and we will create an extra 
//  Pre-compute group-by keys and store in reduceKeys 
//  Throw an error if the user asked for sort merge bucketed mapjoin to be enforced   and sort merge bucketed mapjoin cannot be performed 
//  type mismatch when string col is filtered by a string that looks like date. 
//  SHOW LOCKS (no filter) 
//    } 
// Attempt extended Acl operations only if its enabled, but don't fail the operation regardless. 
//  Additional argument is needed, which is the outputcolumn. 
//  in this delta directory can be considered as a base. 
/*  setByValue  */
//  Figure out which stripes we need to read. 
//  skip padded values 
//  Add a projection column to a projection vectorization context. 
//  ArgumentCompletor always adds a space after a matched token.   This is undesirable for function names because a space after   the opening parenthesis is unnecessary (and uncommon) in Hive.   We stack a custom Completor on top of our ArgumentCompletor 
//  if column type is char and constant type is string, then convert the constant to char 
//  get result vector 
//  operator (top) 
// implicitConvertable = FunctionRegistry.implicitConvertable(ti1, ti2); 
// verify all scopes were recorded 
//  Already existing table 
//  fetch by name and type 
//  TODO: we could generate vector row batches so that vectorized execution may get triggered 
//  iterate for the second time to get all the dependency. 
//  we are guaranteed that we can get data here (since 'size' is not zero) 
//  -------------------------------- Post Pass ---------------------------------- // 
//  Date high/low value is stored as long in stats DB, but allow users to set high/low   value using either date format (yyyy-mm-dd) or numeric format (days since epoch) 
//  LOCKID 
//  to the HCatalog. 
//  Transformation: sq_count_check(count(*), true) FILTER is generated on top    of subquery which is then joined (LEFT or INNER) with outer query    This transformation is done to add run time check using sq_count_check to    throw an error if subquery is producing zero row, since with aggregate this    will produce wrong results (because we further rewrite such queries into JOIN) 
//  The first child should be the table we are deleting from 
//  The session is active but not found in the pool - internal error. 
//  CALCITE-1690 
//  complete path futures and schedule split generation 
//  If we want to handle counters 
//  could be mux/demux operators. Currently not supported 
//  We need to insert 'null' before processing first row for the case: X preceding and y preceding 
//  for analyze repl load, we walk through the dir structure available in the path,   looking at each db, and then each table, and then setting up the appropriate   import job in its place. 
//  (Don't insist NullStructSerDe produce correct column names). 
//  this sets up the map operator contexts correctly 
//  length of compression buffer (compressed or uncompressed length) 
//  verify that udf in black list fails even though it's included in whitelist 
//  2. CPU cost = HashTable  construction  cost  + 
//  do not fill tokenizer until user requests since filling it could read   in data   not meant for this instantiation. 
//  In case of order by, only 1 reducer is used, so no need of   another shuffle 
//  number of output columns   array of pathnames, each of which corresponds to a column   mapping from pathnames to enum PARTNAME   array of returned column values   object pool of non-null Text, avoid creating objects all the time   array of null column values   input ObjectInspectors 
//  ISO-8601 timestamps 
//  populated if column is Map type   @deprecated as of 0.13, slated for removal with 0.15 
//  Assume by default that we would find everything. 
//  Net transfer cost 
//  Remove parent reduce-sink operators 
//  Execute query 
//  We want to try these, whether they succeed or fail. 
//  ignore exception - we simply don't add this attribute   back in to the resultant set. 
//  Initialize workload management. 
//  Entire batch is filtered out. 
//  Generate umbilical token (applies to all splits) 
//  Array of String 
//  Smile mapper is used to read query results that are serialized as binary instead of json 
//  PARTS 
/*  * This is the central piece for Bucket Map Join and SMB join. It has the following * responsibilities: * 1. Group incoming splits based on bucketing. * 2. Generate new serialized events for the grouped splits. * 3. Create a routing table for the bucket map join and send a serialized version as payload * for the EdgeManager. * 4. For SMB join, generate a grouping according to bucketing for the "small" table side.  */
//  Case 3: Test with originals and deltas => Two split strategies with two splits for each. 
/*  there are NULLs in the inputColVector  */
//  a null object, we do not serialize it 
//  If original scale less than 6, use original scale value; otherwise preserve at least 6 fractional digits 
//  Project the subset of fields. 
//  anything else:  preserve original call 
//  hive server's session input stream is not used 
//  first 2 stripes will satisfy the predicate and merged to single split, last stripe will be a 
//  skip walking the children 
//  Register comes in before the unregister for the previous dag 
//  MM insert query, move itself is a no-op. 
//  Return the mocked StorageDescriptor 
//  Fractional part 
//  Find the base, created for IOW. 
//  Otherwise, build a timeline of existing segments in metadata storage 
// ---------PTF functions------------ 
//  Need to notify any queries waiting on the change from pending status. 
//  try these best effort 
//  append regexes that user wanted to add 
//  Hostname 
/*    * Specify the columns to deserialize into a range starting at a column number.    */
//  We only need to check conformance if alter table enabled acid.   INSERT_ONLY tables don't have to conform to ACID requirement like ORC or bucketing. 
//  Calculate collection. 
//  totalSize and the numFiles are set. 
//  if bucket columns are empty, then numbuckets must be set to -1. 
/*    * Tests with queries which cannot be executed with directSQL, because the contain like or in.   * After falling back to ORM the number of partitions cannot be fetched by the   * ObjectStore.getNumPartitionsViaOrmFilter method. They are fetched by the   * ObjectStore.getPartitionNamesPrunedByExprNoTxn method.    */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNClob(int, java.sql.NClob)    */
//  Retrieve HIVE_TXN_TIMEOUT in MILLISECONDS (it's defined as SECONDS),   then divide it by 2 to give us a safety factor. 
//  Add a maintenance thread that will attempt to trigger a cache clean continuously 
//  If we didn't try to launch a job it either means there was no work to do or we got   here as the result of a communication failure with the DB.  Either way we want to wait 
//  merge it with the downstream col list 
//  We store VARCHAR type stripped of pads. 
//  Then, actually do the compaction. 
//  sanity check - we should not receive keys with tags 
//  For unpartitioned table, partition values are specified 
//  this parameter is a constant 
/*  10 files x 100 size for 10 splits  */
//  We need to add a non-match row with nulls for small table values. 
//  3.3 Try obtaining UDAF evaluators to determine the ret type 
//  signed comparisons 
//  Not creating a view, so no need to track view expansions. 
//  Deserialize just the columns we a buffered batch, which has only the non-key inputs and   streamed column outputs. 
//  Operands converted to timestamp, result as interval day-time 
/*  @bgen(jjtree) Field  */
//  Clone the token as we'd need to set the service to the one we are talking to. 
//  Alternate connect string specification configuration 
//  Concurrent increase and revocation - before the message is sent. 
//  Add field separator 
//  If we are using a test specific database, then we just drop the database 
//  stats again 
/*    * for map-side invocation of PTFs, we cannot utilize the currentkeys null check   * to decide on invoking startPartition in streaming mode. Hence this extra flag.    */
//  Reserve 5 bytes for writeValueRecord to fill. There might be junk there so null them. 
//  if enforce bucketing/sorting is disabled numBuckets will not be set. 
//  copy over configs touched by above method 
//  Skewed info 
// set the configuration up such that proxyUser can act on  behalf of all users belonging to the real group(s) that the 
//  skip it , the for loop will skip its value 
//  @@protoc_insertion_point(class_scope:TerminateFragmentRequestProto) 
//  we are closing a file without writing any data in it 
//  Replicate the drop events and check if tables are getting dropped in target as well 
/*  'alan' > first_name  */
//  big table alias 
//  TEST - repeating non-NULL & no-selection 
//  Hive does not support ResultSetMetaData on PreparedStatement, and Hive DESCRIBE   does not support queries, so we have to execute the query with LIMIT 1 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getHoldability()    */
//  Retrieve from PK side 
//  Since SerDe reuses memory, we will need to make a copy. 
//  Done with this branch 
// assert that c3 has got added to table schema 
//  select operator to project these two columns 
//  repeated string lString = 4; 
//  The fixed size for the aggregation class is already known. Get the   variable portion of the size every NUMROWSESTIMATESIZE rows. 
//  If all the txns in the list have pre-allocated write ids for the given table, then just return.   This is for idempotent case. 
//  Find all configurations where the key contains any string from hiddenSet 
//  We provide this public method to help EXPLAIN VECTORIZATION show the evaluator classes. 
//  no privileges required, so don't need to check this object privileges 
//  Make sure the key ID mismatch causes error. 
//  Definitely not a long. 
//  Add all columns from lateral view 
//  Constructing the row object, etc, which will be reused for all rows. 
//  In general a filter cannot be pushed below a windowing calculation.   Applying the filter before the aggregation function changes   the results of the windowing invocation.     When the filter is on the PARTITION BY expression of the OVER clause   it can be pushed down. For now we don't support this. 
/*  Add list bucketing pruner.  */
// (z,x) 
/*  there are nulls in the inputColVector  */
//  walking through all active nodes, if they don't have potential capacity. 
//  Don't invoke from within a scheduler lock 
//  Instantiate BloomFilterCheck based on input column type 
//  SKEWED_COL_VALUE_LOCATION_MAPS 
//  Since bigTableKeyExpressions may do a calculation and produce a scratch column, we   need to map the right column. 
//  adds a null element 
//  Do the conversion by going through a 64 bit integer 
//  We assume the CHAR maximum length was enforced when the object was created. 
/*    * Job callable task for job submit operation. Overrides behavior of execute()   * to submit job. Also, overrides the behavior of cleanup() to kill the job in case   * job submission request is timed out or interrupted.    */
//  before the drop. 
//  We cannot to scaled up decimals that cannot be represented.   Instead, we use a BigInteger instead. 
//  For each source to read, get a shared lock 
//  deleteData, ignoreUnknownTable, ifPurge 
//      it's not worth adding the extra state. 
//  set the escaping related properties 
//  if exception happens during doCopyOnce, then need to call getFilesToRetry with copy error as true in retry. 
//  Validate the second parameter, which should be an array of strings 
//  finally add a project to project out the last 2 columns 
//  Handle implementation of Instance and invoke appropriate InputFormat method 
//  add_partitions(empty list) : ok, normal operation 
//  don't want the table dir 
//  used for LAZY_FETCH_PARTITIONS cases 
//  Definitely not a short. 
//  First incremental dump 
/*  @bgen(jjtree) TypeList  */
//  ROLE_NAME 
//  child JVM won't need to change debug parameters when creating it's own children 
//  All partitions with blurb="hasNewColumn" were added after the table schema changed, 
//  bad input 
//  To remove the primitive types 
//  Cast decimal input to returnType 
//  Keep cause as the original exception 
//  Reached the end of a field? 
//  Now start HS2 with low message size limit. This should prevent any connections 
//  Start the job 
//  user 
//  Use TBLPROPERTIES 
//  remember original string representation of constant. 
//  We bail out 
//  reduce might end up creating an expression with null type   e.g condition(null = null) is reduced to condition (null) with null type   since this is a condition which will always be boolean type we cast it to   boolean type 
//  Open a session and set up the test data 
//  Add default size for columns for which stats were not available 
//  add constant object overhead for struct 
// ************************************************************************************************   Decimal to Binary Conversion. 
// txn 3 is not empty txn, so we get a better msg 
//  Transient members initialized by transientInit method. 
//  It does not sort, memory footprint is zero 
//  If they are not equal, we could zip up till here 
//  Remove the container mapping 
//  Don't need to track anything for this task. No new notifications, etc. 
//  Overflow.  This is not expected. 
//  (with 000000_0, 000000_0_copy_1, 000000_0_copy_2) 
//  Check whether the specified BaseWork's operator tree contains a operator 
//  disable json file writing 
//  create proper table/column desc for spilled tables 
//  check the partitions in partSpec be the same as defined in table schema 
//  TODO HIVE-13454 Add additional information such as #executors, container size, etc 
//  normal close, when there are inserts. 
//  Overrides values from the hive/tez-site. 
//  Build Hive Table Scan Rel 
//  Get old stats object if present 
//  check if they are operating on the same table, if not, move on. 
//  Bootstrap done, now on to incremental. First, we test db-level REPL LOADs.   Both db-level and table-level repl.last.id must be updated. 
// the partition did not change, 
//  Re-create the remote client if not active any more 
//  optional int32 attempt_number = 4; 
//  Verify that there are no missing privileges 
//  @@protoc_insertion_point(class_scope:UpdateQueryRequestProto) 
//  rowId >= '2014-01-01' 
//  Always localize files from conf; duplicates are handled on FS level.   TODO: we could do the same thing as below and only localize if missing. 
//  get SSL socket 
// Reset buffers to store filter push down columns 
//  close() after *expiry time* and *a cache access* should  have tore down the client 
//  mapreduce.tez.input.initializer.serialize.event.payload should be set   to false when using this plug-in to avoid getting a serialized event at run-time. 
//  The reader doesn't support offsets. We adjust offsets to match future splits.   If cached split was starting at row start, that row would be skipped, so +1 byte. 
//  This is needed for serde of PagingSpec as it uses JacksonInject for injecting SelectQueryConfig 
//  Now create three types of delete deltas- first has rowIds divisible by 2 but not by 3,   second has rowIds divisible by 3 but not by 2, and the third has rowIds divisible by   both 2 and 3. This should produce delete deltas that will thoroughly test the sort-merge   logic when the delete events in the delete delta files interleave in the sort order. 
//  complex tree with multiple parents 
//  constant for now, will make it configurable later. 
//  Remaining fields are cells addressed by column name within row. 
// could not abort all txns in this batch - this may happen because in parallel with this  operation there was activity on one of the txns in this batch (commit/abort/heartbeat)  This is not likely but may happen if client experiences long pause between heartbeats or  unusually long/extreme pauses between heartbeat() calls and other logic in checkLock(),  lock(), etc. 
/*    * @return The multi-set count for the lookup key.    */
//  Then, find the leftmost logical sibling select, because that's what Hive uses for aliases.  
//  hiveConf, getConf and setConf are in this class because AlterHandler extends Configurable.   Always use the configuration from HMS Handler.  Making AlterHandler not extend Configurable   is not in the scope of the fix for HIVE-17942. 
//        a significant effect when 'k'*'pf' is very high. 
//  test min = 0, max = 0 generates each stripe 
//  Do not change the location, so it is tested that the location will be changed even if the   location is not set to null, just remain the same 
//  If top operator is not a pure limit, we bail out 
/*  use zero copy record reader  */
//  QueryInfo will only exist if more work came in, after this was scheduled. 
//  move over the separator for next search 
//  MERGEPARTIAL 
//  Deserialize the bloom filter 
//  may be drive this via configuration as well. 
//  Close the connection 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#fetchResults(org.apache.hive.service.cli.OperationHandle)    */
//  We set the signature for the view if it is a materialized view 
//  REPL_SRC_TXN_IDS 
//  set the thread name with the logging prefix. 
//  Derived. 
//  For negative testing purpose.. 
//  A helper object that efficiently copies the big table columns that are for the big table 
//  order in which the results should 
//  Update tags of reduce sinks 
//  Maximum number of open transactions that's allowed 
//  optional int32 myint = 1; 
//  OPERATION_TYPE 
//  Object Hash. 
//  make the new projRel to provide a null indicator 
//  Marker to track if there is starting double quote without an ending double quote 
//  because that is monotonically increasing to give new unique row ids. 
/*    * DOUBLE.    */
//  It is not an external table 
//  Optional vectorized value expressions that need to be run on each batch. 
//  If it a map-reduce job, create a temporary file 
/*  Make sure all the NULL entries in this long column output vector have their data vector   * element set to the correct value, as per the specification, to prevent later arithmetic   * errors (e.g. zero-divide).    */
//  STAGE_ID 
//  Aux values 
/*          * ExecutionException is raised if job execution gets an exception. Return to client         * with the exception.          */
//  if we died for any reason lets get a new set of hosts 
//  See the comment in TimestampStreamReader.setBuffers. 
//  VectorizedBatchUtil.debugDisplayBatch( batch, "VectorReduceSinkOperator processOp "); 
/*          * NOT Repeating.          */
//  should be a no-op for sparse 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setTimestamp(java.lang.String,   * java.sql.Timestamp, java.util.Calendar)    */
//  neg-infinity to start inclusive 
//  Oracle requires special treatment... as usual. 
//  scalar/column 
//  hide constructor 
//  when reading the hashtable, MapJoinObjectValue calculates alias filter and provide it to join 
//  we need to set this, because with HS2 and client side split   generation we end up not finding the map work. This is   because of thread local madness (tez split generation is   multi-threaded - HS2 plan cache uses thread locals). Setting   VECTOR_MODE/USE_VECTORIZED_INPUT_FILE_FORMAT causes the split gen code to use the conf instead   of the map work. 
//  The list of servers the database/partition/table can locate on 
//  Write the results into the file 
//  Read from the given Accumulo table 
//  Server ok. 
//  make 'm' multiple of 64 
// The keys used to store info into the job Configuration 
//  If no import tasks generated by the event or no table updated for table level load, then no   need to update the repl state to any object. 
/*    * Helper to determine the size of the container requested   * from yarn. Falls back to Map-reduce's map size if tez   * container size isn't set.    */
//  clear everything 
//  Parse fraction portion. 
//  This is based on Vectorizer code, minus the validation. 
//    Helpers   
//  If in some selected binary operators (=, is null, etc), one of the   expressions are 
//  in return path, we may have aggr($f0), aggr($f1) in GBY   and then select aggr($f1), aggr($f0) in SEL.   Thus we need to use colExp to find out which position is   corresponding to which position. 
//  Look for AppMasterEventOperator or ReduceSinkOperator 
//  See the comment inside updatePartitionBucketSortColumns. 
//  The child is a single Range 
//  rename needs change the data location and move the data to the new location corresponding   to the new name if:   1) the table is not a virtual view, and   2) the table is not an external table, and   3) the user didn't change the default location (or new location is empty), and 
//  Basic adding and removing operations, called only while holding lock 
//  First we delete the materialized views 
//  Null direction 
//  reuse existing perf logger. 
//  check if the new entry contains the existing 
//  Merge the sidefile into the newly created hash table 
//  column value lengths for each of the selected columns 
//  finally open the store 
//  inline map join operator 
//  SMBJoin not supported 
//  if the ast has 3 children, the second *has to* be partition spec 
//  CTAS case: the file output format and serde are defined by the create   table command rather than taking the default value 
//  I16_VAL 
//  this position in parent is a constant   reverse look up colExprMap to find the childColName 
//  The utility of this method is not certain. 
//  Setting the hidden list 
//  We assume that AMs and HS2 run under the same user. 
//  check for this pattern   The pattern matching could be simplified if rules can be applied   during decorrelation,     CorrelateRel(left correlation, condition = true)     LeftInputRel     Project-A (a RexNode)       Aggregate (groupby (0), agg0(), agg1()...)         Project-B (references coVar)           rightInputRel 
//  inserts are done. 
//  Transactions should be committed. 
//  code copied over from UDFWeekOfYear implementation 
//  Subscriber can get notification of newly add partition in a   particular table by listening on a topic named "dbName.tableName"   and message selector string as "HCAT_EVENT = HCAT_ADD_PARTITION" 
//  returns name of hashfile made by HASHTABLESINK which is read by MAPJOIN 
//  we'd increase the base limit, and adjust dynamically based on IO and processing perf delays. 
//  Skip the potential big table identified above 
//  load hivemetastore-site.xml if this is metastore and file exists 
//  this table_desc does not contain the partitioning columns 
//  The order in which the two paths are added is important. The   lateral view join operator depends on having the select operator   give it the row first. 
//  Owid with the given value found! Searching now for rowId...   Retrieve the actual CompressedOwid that matched.   Check if rowId is outside the range of all rowIds present for this owid. 
//  we first use getParameters() to prune the stats 
//  The following actions are authorized through SQLStdHiveAccessController, 
//  Grouping id should be pruned, which is the last of key columns   see ColumnPrunerGroupByProc 
//  By default just the Hive catalog should be cached. 
//  partition spec string to input file names (big) 
//  we are using PLAIN Sasl connection with user/password 
//  We can get the table definition from tbl. 
//  alterPartition/alterTable is happening via statsTask or via user. 
/*    * Right trim a slice of a byte array and return the new byte length.    */
//  Overriden to copy start index / end index, that is needed through optimization,   e.g., for masking/filtering 
//  nothing special - just use the DynamicSerDeFieldList's children methods to 
//  On error close the channel. 
//  Can't subtract NULL. 
//  For subclasses 
//  If the expression is a IS [NOT] NULL on a non-nullable   column, then we can either remove the filter or replace   it with an Empty. 
/*    * Submit job request. If maximum concurrent job submit requests are configured then submit   * request will be executed on a thread from thread pool. If job submit request time out is   * configured then request execution thread will be interrupted if thread times out. Also   * does best efforts to identify if job is submitted and kill it quietly.    */
// Input 
//  This query create a pending cache entry but it was never saved with real results, cleanup.   This step is required, as there may be queries waiting on this pending cache entry.   Removing/invalidating this entry will notify the waiters that this entry cannot be used. 
//  Alright fine, we'll use our defaults 
//  day   hour   minute   second 
//  Can this task be merged with the child task. This can happen if a big table is being 
//  impossible to throw any json exceptions. 
//  copy nulls from the non-repeating side 
//  doing a single_value() on the entire input 
// this code doesn't propagate 
//  Check if any of the partitions already exists in destTable. 
//  Test that existing shared_read table with new shared_write coalesces to 
/*    * (non-Javadoc)   *   * @see org.apache.hadoop.hive.ql.lib.Node#getName()    */
//  comment 
// Combine credentials and credentials from job takes precedence for freshness 
//  fetch 2 in same schema 
//  set up service and client 
//  Semijoin DPP work is considered a descendant because work needs 
//  Inner big-table only join specific. 
//  check the stats 
//  TypeInfoBasedObjectInspector typeInfoBasedObjectInspector =   (ObjectInspector)oi;   return typeInfoBasedObjectInspector.getTypeInfo();   } 
//  Finally add the partitioning columns 
//  NEED_RESULT 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#setPoolable(boolean)    */
//  No physical dir gets created. 
//  1) Create two bucketed tables 
//  used if the join   input is too large   to fit in memory 
//  This get should fail because its variance ((10-5)/5) is way past MAX_VARIANCE (0.5) 
/*  1 files x 100 size for 10 splits  */
/*  @bgen(jjtree) Typedef  */
//  assert bucket listing is as expected 
//  Set up the transaction/locking db in the derby metastore 
//  finally write out the pieces (sign, scale, digits) 
//  We only increase the targets here. 
//  calling from code that does not use filters as-is. 
//  make some noisy to avoid disk caches data. 
//  required   required   optional   optional   optional 
// multiple clients may initialize the hook at the same time 
//  and it is an insert overwrite or insert into table 
//  is distinct 
//  Send an actual heartbeat only if the task count is > 0 
/*    * Tests whether there is another List element or another Map key/value pair.    */
/*      * ensure that we throw out any exceptions above highWatermark to make     * {@link #isWriteIdValid(long)} faster      */
//  WHITE FLAG WITH HORIZONTAL MIDDLE BLACK STRIPE U+26FF (3 bytes) 
//  with data 
//  mark txn "2L" aborted   mark txn "8L" aborted 
/*   // TODO MS-SPLIT I'm 99% certain we don't need this, as MetastoreConf.newMetastoreConf already  adds this resource.  static {    conf.addResource("hive-site.xml");  }   */
//  Logger to the string appender 
//  It's not clear how filtering for e.g. "stringCol > 5" should work (which side is   to be coerced?). Let the expression evaluation sort this one out, not metastore. 
//  try dropping table as user2 - should fail 
//  250 seems to be reasonable upper limit for this 
//  We need to make sure that the key type and the value types are settable. 
/*      * 1. Create the PTFDesc from the Qspec attached to this QB.      */
//  if we aren't building a split, start a new one. 
//  The following datetime/interval arithmetic operations can be done using the vectorized values. 
//  To Unpartitioned table 
//  Write ID of committed txn should be valid. 
//  Try an invalid state transition on the handle. This ensures that the actual state   change we're interested in actually happened, since internally the handle serializes   state changes. 
//  as we are overwriting segments with new versions 
//  UNDONE: Multiple types... 
// create some data 
//  Insert HAVING plan here 
/*    * when a QBJoinTree is merged into this one, its left(pos =0) filters can   * refer to any of the srces in this QBJoinTree. If a particular filterForPushing refers   * to multiple srces in this QBJoinTree, we collect them into 'postJoinFilters'   * We then add a Filter Operator after the Join Operator for this QBJoinTree.    */
//  blank " " (1 byte)   LATIN LETTER SMALL CAPITAL L U+029F (2 bytes) 
//  Tests for int add_partitions(List<Partition> partitions) method 
//  is not special cased later in subquery remove rule 
//  If cluster info changes, qam should be called with the same fractions. 
//  Preempt on specific host 
/*      * Override this for concrete initialization.      */
//  NOT USED 
//  This is map of which vectorized row batch columns are the big table key columns.  Since   we may have key expressions that produce new scratch columns, we need a mapping. 
//  DB 
//  final row computation will consider join type 
//  TABLENAME 
// here there is only 1 "split" since we only have data for 1 bucket 
//  The following data might be changed 
//  index is disabled 
//  Drop partition will clean the partition entry from the compaction queue and hence worker have no effect 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNClob(java.lang.String, java.io.Reader)    */
//       } 
//  Generate lineage info for create view statements   if LineageLogger hook is configured. 
//  we always have to find at least one location, otw the test is useless 
//  In the future, if we support arbitrary characters in   identifiers, then we'll need to escape any backticks   in identifier by doubling them up. 
//  Add materializations to planner 
//  There could be multiple keyValuePairs separated by comma 
/*                * Single-Column String specific lookup key.                */
//  Decrease refcount. 
//  For now, bail out on decimal constants with larger scale than column scale. 
//  The thread should stop after this.  
//  create TS to read intermediate data 
//  we need the mapping and type information. 
//  Pick the maximum # reducers across all parents as the # of reduce tasks. 
/*    * Use this constructor when only ascending sort order is used.    */
//  default prefix is delta_prefix 
//  Test-only counter. 
//  Dynamic partition keys should be added to field schemas. 
//  Invariant that Avro's tag ordering must match Hive's. 
//  The buffer has lived in the heap all along. Restore heap property. 
//  Configure PREEXECHOOKS with DisallowTransformHook to disallow transform queries 
//  If the main thread throws an exception for some reason, propagate the exception to the   client and initiate a safe shutdown 
//  DP columns starts with tableFields.size() 
//  If any child is null, set unknown to true 
//  Get the children expr strings 
// -----------------------------------------------------------------------------------------------   Precision/scale enforcement methods.  ----------------------------------------------------------------------------------------------- 
//  Cannot handle NULL scalar parameter. 
//  We only support Decimal64 on 2 columns when the have the same scale. 
//  add the remaining fields 
//  unknown | unknown | unknown 
/*  (non-Javadoc)     * Serializes this value into the format used by @{link #java.math.BigInteger}     * This is used for fast assignment of a Decimal128 to a HiveDecimalWritable internal storage.     * See OpenJDK BigInteger.toByteArray for a reference implementation.      * @param scratch     * @param signum     * @return      */
//  Struct. 
//  If there's a failure from here to when the metadata is updated,   there will be no data in the partition, or an error while trying to read   the partition (if the archive files have been moved to the original   partition directory.) But re-running the archive command will allow   recovery 
//  Since this is a dynamic partitioned hash join, the work for this join should be a ReduceWork 
//  IGNORE_PROTECTION 
//  Test getter for configuration object. 
//  Note that we do not need a lock for this entity.  This is used by operations like alter   table ... partition where its actually the partition that needs locked even though the table 
//  replace ReduceSinkOp with HashTableSinkOp for the RSops which are parents of MJop 
//  This the is Object Hash class variation. 
//  Augment 
//  Boolean values are stores a 1's and 0's, so convert and compare 
//  This column is not included. 
//  Add an initial column to a vectorization context when 
//  if getMacroName is null, we always treat it different from others. 
//  update the seed 
//  transactional is found, but the value is not in expected range 
//  Use PurgeCacheRequestProto.newBuilder() to construct. 
//  We have data until the end of current block if we had it until the beginning. 
//  No record on disk, more data in write buffer 
//  Path has writing permissions 
/*  Singleton  */
//  Format a list of Columns for a create statement 
//  set the skipped field to null 
//  check whether HiveConf initialize log4j correctly 
//  partition name to value 
//  Events. 
//  Task has been queued for execution by the driver 
// thrown, for example, if there is no such user on the system 
//  This method tries to merge the join with its left child. The left 
//  non-qualified types should simply return the TypeInfo associated with that type 
// Output will also be repeating 
//  confirm the batch sizes were 5, 5 in the two calls to create partitions 
// avoid NPE below if for some reason -e argument has multi-line command 
//  Check partition exists if it exists skip the overwrite 
//  Check if the query results were cacheable, and created a pending cache entry.   If we successfully saved the results, the usage would have changed to QUERY_USING_CACHE. 
//  can prevent updates from being sent out to the new node. 
/*        * Finally write out the pieces (sign, power, digits)        */
//  different paths. 
//  Make sure that the port is unused. 
//  Now that we have the big table index, get real numReducers value based on big table RS 
//  Then UKs 
//  with the releases probably running in the other closing thread. 
//  Do not check for ACID; it does not create new parts and this is expensive as hell.   TODO: add an API to get table name list for archived parts with a single call;         nobody uses this so we could skip the whole thing. 
//  number of dynamic partition columns   involved partitions in TableScanOperator/FileSinkOperator 
//  Put in the beginning; relies on the knowledge of internal implementation. Pave? 
// this would usually be at block boundary  this would usually be at block boundary 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBinaryStream(java.lang.String,   * java.io.InputStream)    */
//  this must be a hadoop 2.4 version or earlier.   Resorting to the earlier method of getting the properties, which uses SASL_PROPS field 
//  if source exists, rename. Otherwise, create a empty directory 
// no more original files to read 
//  Re-verify directory layout and query result by using the same logic as above 
//  matches FIL-PROJ-TS 
//  If there are no columns (projection only join?) just assume no weight. 
//  Use GetTokenRequestProto.newBuilder() to construct. 
// Read with partition filter 
//  Fraction below 0.5 is implicitly 0. 
//  Filter the name list. Removing elements one by one can be slow on e.g. ArrayList, 
/*    * If HIVE_SERVER2_JOB_CREDSTORE_LOCATION is set check HIVE_SERVER2_JOB_CREDSTORE_PASSWORD_ENVVAR before   * checking HADOOP_CREDENTIAL_PASSWORD_ENVVAR    */
// assert this.nextGroupStorage[alias].size() == 0; 
//  Not cleaning the interrupt status. 
//  RCFile specific parameter 
//  Is this a list type? 
//  handle overloaded methods first 
//  Hide constructor, for make benefit glorious Singleton. 
//  Nothing to trim (ASCII). 
//  Check Configuration for any user-provided Authorization definition 
//  LOG.info("Writing value at " + valueOffset + " length " + valueLength);   In an unlikely case of 0-length key and value for the very first entry, we want to tell   this apart from an empty value. We'll just advance one byte; this byte will be lost. 
//  may need to strip away the STOP marker when in thrift mode 
//  public static MetadataListStructObjectInspector getInstance(int fields) {   return getInstance(ObjectInspectorUtils.getIntegerArray(fields));   } 
//  If we are currently searching the data for a place to begin, do not return data yet 
// Map<?,?> c14Value = (Map<?,?>) rowValues[13];  assertEquals(0, c14Value.size()); 
// https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2/src/packages/templates/conf/hdfs-site.xml 
// 1) Start from a clean slate (metastore) 
//  reduceSink is only available during compile 
// get partition, this partition should not have the newly added column since cascade option 
//  remove from current root task and add conditional task to root tasks 
//  Derby script format is RUN '<file>' 
//  without data 
//  get the avgLen 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getCrossReference(org.apache.hive.service.cli.SessionHandle)    */
//  Try to deserialize 
//  Have at least 1 non-blank; Skip trailing blank characters. 
//  This check is for controlling the correctness of the current state 
//  could be changed in local execution optimization 
//  set other table properties 
// small optimization - delete delta can't be in raw format 
//  HTTP mode 
//  Retrieve the user name, do the final validation step. 
//  if not see if the mesg follows type of format, which is typically the   case: 
//  And, we have their type infos. 
//  generate the map join operator 
//  Verify escaped partition names don't return partitions 
//  UDAF is assumed to be deterministic 
//  Verify we added an entry for each output. 
//  Convert the bucket map-join operator to a sort-merge map join operator 
//  Not supported for MM tables - sampler breaks separate MM dirs into splits, resulting in   mismatch when the downstream task looks at them again assuming they are MM table roots.   We could somehow unset the MM flag for the main job when the sampler succeeds, since the   sampler will limit the input to the the correct directories, but we don't care about MR. 
//  The context table name can be null if repl load is done on a full db.   But we need table name for alloc write id and that is received from source. 
//  the result is the last 1 character, which occupies 4 bytes 
//  update based on the final value of the counters 
//  simply dispatch the call to the right method for the actual (sub-) type of   BaseWork. 
//  Create a mapping from the group by columns to the table columns 
//  1. Generate the statement of analyze table [tablename] compute statistics for columns   In non-partitioned table case, it will generate TS-SEL-GBY-RS-GBY-SEL-FS operator   In static-partitioned table case, it will generate TS-FIL(partitionKey)-SEL-GBY(partitionKey)-RS-GBY-SEL-FS operator   In dynamic-partitioned table case, it will generate TS-SEL-GBY(partitionKey)-RS-GBY-SEL-FS operator   However, we do not need to specify the partition-spec because (1) the data is going to be inserted to that specific partition   (2) we can compose the static/dynamic partition using a select operator in replaceSelectOperatorProcess.. 
//  timestamp scalar/column 
//  We just had leading zeroes (and possibly a dot and trailing blanks).   Value is 0. 
//  Get the next "inList" value element if needed. 
//  Do formal conversion... 
//  Create a new union operator 
//  UNDONE: Also remember virtualColumnCount... 
//  The input timestamps are stored as long values 
//  partitioned insert 
//  Write id has changed, it is not valid anymore,   we need to recompile 
//  Do not remove the parameter yet, because we have separate initialization routine   that will use it down below. 
//  4. Extract join key expressions from HiveSortExchange 
//  requested. 
//  in HiveSparkClientFactory 
//  Check that we don't find unexpected columns 
//  JDBC URL: jdbc:hive2://<host>:<port>/dbName;sess_var_list?hive_conf_list#hive_var_list   each list: <key1>=<val1>;<key2>=<val2> and so on   sess_var_list -> sessConfMap   hive_conf_list -> hiveConfMap   hive_var_list -> hiveVarMap 
//  check if the partition exists (it shouldn't) 
//  CATALOG 
// this simulates the completion of "delete from tab1" txn 
//  We copied the entire buffer. 
//  We will be using this for each RG while also sending RGs to processing.   To avoid buffers being unlocked, run refcount one ahead; so each RG  
//  Check that all the outputs have been processed; if not, insert them into queue   before the current vertex and try again. It's possible e.g. in a structure like this:     _1    / 2   3  4 where 1 may be added to the queue before 2 
//  Otherwise, we can reuse the session. Either the kill has failed but the user managed to   return early (in fact, can it fail because the query has completed earlier?), or the user 
//  no files at all 
//  CAT_NAME 
//  1. Build map of column name to col index of original schema   Assumption: Hive Table can not contain duplicate column names 
//  The map we're building. 
//  Determine maximum of all non-null decimal column values; maintain isGroupResultNull. 
//  Ignore errors in SSL tests where the connection is misconfigured. 
//  Add filters that apply to more than one input 
//  New metadata always have two parameters. 
//  If the bucket id set component of this data structure proves to be too large there is the   option of moving it to Trove or HPPC in an effort to reduce size. 
//  Copy the padding 
//  The caller could re-check the location, but would probably find it locked. 
//  For SEL-SEL(compute) case, move column exprs/names of child to parent. 
//  found a DP, but there exists ST as subpartition 
//  Timer will be null we aren't using the metrics 
// Found child mapjoin operator.  Its size should already reflect any mapjoins connected to it, so stop processing. 
//  TODO Implement to remove all watches for the specified pathString and it's sub-tree 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.operation.Operation#getNextRowSet(org.apache.hive.service.cli.FetchOrientation, long)    */
//  No input data. 
//  Default serialization format. 
//  which case, user the latter match. 
//  Fill up host1 with p2 tasks.   Leave host2 empty   Try running p1 task on host1 - should preempt 
//  aggregates in group-by. 
//  Unpartitioned table, writing to the scratch dir directly is good enough. 
//  Now try to pick another task to update - or potentially the same task. 
//  2.2 Convert ExprNode to RexNode 
// since conflicting txn rolled back, commit succeeds 
//  If the directory needs to be changed, send the new directory 
//  sizes 
//  If the table is not partitioned, return empty list. 
//  Turn on mocked authorization 
//       LOG.info("Args to har : "+ arg);      } 
//  Return zz for "xx.zz" and "xx.yy.zz" 
//  3. Get Calcite Return type for Agg Fn 
//  Handle in next round. 
//  The sorting columns of the child RS are more specific than   those of the parent RS. Assign sorting columns of the child RS   to the parent RS. 
//  this relates to db level event tracked via databaseEventProcessed 
//  all done parsing, let's run stuff! 
//  Build the versions list. 
//  File is successfully copied, just skip this file from retry. 
// All must be selected otherwise size would be zero  Repeating property will not change. 
//  will make sure the task's place in the wait queue is held until it gets scheduled. 
//  Just spot check because we already checked the logic for long.   The code is from the same template file. 
//  Copy in remainder digits... which start at the top of remainder2. 
/*      * We want to resolve the leftmost name to the Parent Query's RR.     * Hence we do a left walk down the AST, until we reach the bottom most DOT.      */
//  no begin + write 
//  repeated int32 lint = 3; 
// since there is txn open, we are heartbeating the txn not individual locks 
//  for persistent function   if the function is dropped, all functions registered to sessions are needed to be reloaded 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNCharacterStream(int, java.io.Reader,   * long)    */
// Need shut down background thread gracefully, driver.close will inform background thread  a cancel request is sent. 
//  NOTE: Calcite considers tbls to be equal if their names are the same. Hence   we need to provide Calcite the fully qualified table name (dbname.tblname)   and not the user provided aliases.   However in HIVE DB name can not appear in select list; in case of join   where table names differ only in DB name, Hive would require user 
// re-test the locks 
//  Configure http client for cookie based authentication 
//  build the resultset from response 
//  no worker identity 
//  input rel. 
//  in this case we need to get the working directory   and this requires a FileSystem handle. So revert to   original method. 
//  verify table not fuond error 
//  If the given Task is a SparkTask then search its Work DAG for SparkPartitionPruningSinkOperator 
//  Try underlying client 
//  2. Obtain Col Stats for Non Partition Cols 
//  Either we got the tablename from the IMPORT statement (first priority)   or from the export dump. 
//  add ".*" to the regex to match anything else afterwards the partial spec. 
//  Spark configurations are updated close the existing session   In case of async queries or confOverlay is not empty,   sessionConf and conf are different objects 
//  2.2.1 Maintain join keys (in child & Join Schema)   2.2.2 Update Join Key to JoinLeafPredicateInfo map with keys 
//  1. Find our bearings in the stream. 
/*  (non-Javadoc)   * This processor addresses the RS-MJ case that occurs in spark on the small/hash   * table side of things. The work that RS will be a part of must be connected   * to the MJ work via be a broadcast edge.   * We should not walk down the tree when we encounter this pattern because:   * the type of work (map work or reduce work) needs to be determined   * on the basis of the big table side because it may be a mapwork (no need for shuffle)   * or reduce work.    */
//  TODO: Handle replication of changes to Table-STATS. 
//  Check LLAP-aware split (e.g. OrcSplit) to make sure it's compatible. 
//  n-way: all later small tables 
// this doesn't create a key index presumably because writerOptions are not set on 'options' 
//  get column 
//  We are the last of the concurrent operations to finish. Commit. 
//  Consolidation since all leaves are required. 
//  should set "-foo bar"   should set "-blah"   should be ignored. 
/*    * a subclass must provide the {@link TableFunctionEvaluator} instance.    */
//  add partition keys to table schema   NOTE : this assumes that we do not ever have ptn keys as columns   inside the table schema as well! 
//  unix_timestamp is polymorphic (ignore class annotations) 
//  10^18 - 1 
//  TEMPORARY Until Native Vector Map Join with Hybrid passes tests...   HiveConf.setBoolVar(physicalContext.getConf(),      HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN, false); 
//  Get the databases 
//  write three files in partition 1 
//  If columns is null, then we need to create the leaf 
//  Functions like NVL, COALESCE, CASE can change a    NULL introduced by a nonpart column removal into a non-null   and cause overaggressive prunning, missing data (incorrect result) 
//  input pruning is enough; add the filter for the optimizer to use it   later 
//  for SMB join, replaced with number part of task-id , making output file name   if big alias is not partitioned table, it's bucket number 
// If HADOOP_PROXY_USER is set in env or property, 
//  Worth waiting for the timeout. 
//  [-version|--version] 
//  submit to accept dag (if session is closed, this will include re-opening of session time, 
/*            * No realativeOffsetWord in last value.  (This was the first value written.)            */
//  should be final but Writable 
// col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT] 
//  http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/cache/CacheBuilder.html 
//  Count the tasks in intermediate state as waiting. 
//  Case 1: NO column stats, NO hash aggregation, NO grouping sets 
//  REWRITE_ENABLED 
//  Check that the data still exist 
//  Meh. 
//  leftTree != null 
//  if partition and bucket columns are sorted in ascending order, by default 
//  ////// Generate ReduceSinkOperator2 
//  Submit spark job through local spark context while spark master is local mode, otherwise submit   spark job through remote spark context. 
//  Connect this TableScanOperator to child. 
//  DataOutputStream to BytesWritable 
//  1. Test with doAs=false 
/*  first_name in      ('john', 'sue')  */
//  WEIGHT 
/*       * this allows doAs (proxy user) to be passed along across process boundary where      * delegation tokens are not supported.  For example, a DDL stmt via WebHCat with      * a doAs parameter, forks to 'hcat' which needs to start a Session that      * proxies the end user       */
//  Otherwise just leave it up to Tez to decide how much memory to allocate 
//  Clone readerOptions for deleteEvents. 
/*        * For submit operation, tasks are not cancelled. Verify that new job request       * should fail with TooManyRequestsException.        */
// No partitions match the specified partition filter 
/*              * Common inner join result processing.              */
//  Nothing to do here... 
//  versioning (adding/deleting fields). 
//  Remove nested DPPs 
//  The dispatcher fires the processor corresponding to the closest matching rule and passes the context along 
/*  Object Inspectors corresponding to the struct returned by TerminatePartial and the     * fields within the struct - "maxLength", "sumLength", "count", "countNulls", "ndv"      */
//  compute groupby columns from groupby keys 
/*  helper function to allow Set()/Collection() operations with ExprNodeDesc  */
//  make sure that file does not exist 
//  The retainAll method does set intersection. 
//  Test for user "neo" 
//  Add constraints if necessary 
//  Initialize footer buffer. 
// Output 
/*      * This method is to check if the new column list includes all the old columns with same name and     * type. The column comment does not count.      */
/* Case we are partitioning the segments based on time and max row per segment maxPartitionSize */
//  Pull out the first table from the "show extended" json. 
//  For all the other column groups, generate new values down. 
//  1. Setup TableScan Desc 
//  but we assume it's extremely rare for individual partitions. 
//  while and should be done when we start up. 
//  Get the single TableScanOperator.  Vectorization only supports one input tree. 
// passing "creds" prevents duplicate tokens from being added 
//  Make sure we don't compact if we don't need to compact; but do if we do. 
//  FULL_TABLE_NAME 
//  Due to HIVE-6404, define our own constant 
//  session.open will unset the queue name from conf but Mockito intercepts the open call 
//  long/double/decimal 
// if row is null, it means there are no more rows (closeOp()). another case can be that the buffer is full. 
//  Give the outThread a chance to finish before marking the operator as done 
/*    * Tests whether credential provider is updated when HIVE_JOB_CREDSTORE_PASSWORD is set and when   * hiveConf sets HiveConf.ConfVars.HIVE_SERVER2_JOB_CREDSTORE_LOCATION   *   * JobConf should contain the mapred env variable equal to ${HIVE_JOB_CREDSTORE_PASSWORD} and the   * hadoop.security.credential.provider.path property should be equal to value of   * HiveConf.ConfVars.HIVE_SERVER2_JOB_CREDSTORE_LOCATION    */
/*              * Feed current full batch to operator tree.              */
//  If any table/partition is updated, then update repl state in table object 
//  fetch remaining logs 
//  . After the join, only selects and filters are allowed. 
//  This might be a deadlock, if so, let's retry 
//  delete jars added using query2 
//  Setup client side split generation. 
//  If type for column and constant are different, we currently do not support pushing them 
//  Now add the key wrapper arrays 
//  Have a non-NULL value on hand. 
//  Lock should be freed up now. 
//  Run Compaction Worker to do compaction.   But we do not compact a MM table but only transit the compaction request to 
//  For a list, the value and key lengths of 1st record were overwritten with the   relative offset to a new list record. 
//  NOT selectedInUse 
//  FROM_EVENT_ID 
//  We've switched to Joda/Java Calendar which has a more limited time range.... 
//  This assumes the distribution of variable size keys/aggregates in the input   is the same as the distribution of variable sizes in the hash entries 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getClientInfo(java.lang.String)    */
/*     Sets up a TaskSpec which has vertex1 as it's input, and tasks belonging to vertex2      */
//  Do another loop if table is bucketed 
//  We succeeded in adding all the values, so 
//  return type should have same length as the input. 
/*    * Only for testing    */
//  This is our use case of not having passwords stored in in the clear in hive conf files. 
//  priority = 20 / (3000 - 100) = 0.0069 
//  for hive udtf operator 
//  We were going to kill some queries and reuse the sessions, or maybe restart and put the new   ones back into the AM pool. However, the AM pool has shrunk, so we will close them instead. 
//  We will test the reconfiguration of the header size by changing the password length. 
//  The lower bits are the absolute value offset. 
//  All are null so none are selected 
//  need to remove by hive .13. Also, do not change default (see SMB operator) 
//  the system properties. 
//       LOG.info("Nuking " + dir); 
//  Compute the values 
//  Original bucket files, delta directory and delete_delta should have been cleaned up. 
//  Modify partition column type, and comment 
// what I want is order by cc_end desc, cc_start asc (but derby has a bug https://issues.apache.org/jira/browse/DERBY-6013)  to sort so that currently running jobs are at the end of the list (bottom of screen)  and currently running ones are in sorted by start time 
//  Hive doesn't have the concept of not-null 
//  Check if we start to forward rows to a new child.   If so, in the current key group, rows will not be forwarded   to those children which have an index less than the currentChildIndex.   We can call flush the buffer of children from lastChildIndex (inclusive)   to currentChildIndex (exclusive) and propagate processGroup to those children. 
//  Split into 16 digit middle and lowest longwords remainder / division. 
//  scrutinize escape pair, specifically, replace \' to ' 
// would be useful to have enum for Type: insert/delete/load data 
//  denseColIx is index in ORC writer with includes. We -1 to skip the root column; get the   original text file index; then add the root column again. This makes many assumptions.   Also this only works for primitive types; vectordeserializer only supports these anyway.   The mapping for complex types with sub-cols in ORC would be much more difficult to build. 
/*      * Restriction.8.m :: We allow only 1 SubQuery expression per Query.      */
//  3. Outside   we need to create a new limit 0 
//  Set generic options 
/*    * Hive syntax allows to define CASE expressions in two ways:   * - CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END (translated into the   *   "case" function, ELSE clause is optional)   * - CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END (translated into the   *   "when" function, ELSE clause is optional)   * However, Calcite only has the equivalent to the "when" Hive function. Thus,   * we need to transform the "case" function into "when". Further, ELSE clause is   * not optional in Calcite.   *   * Example. Consider the following statement:   * CASE x + y WHEN 1 THEN 'fee' WHEN 2 THEN 'fie' END   * It will be transformed into:   * CASE WHEN =(x + y, 1) THEN 'fee' WHEN =(x + y, 2) THEN 'fie' ELSE null END    */
/*          * Multi-Key specific repeated lookup.          */
//  shamelessly copied from Path in hadoop-2 
//  * ambiguous case, which should be assumed is 3-level according to spec 
//  scalar/scalar IF 
//  pretend that one field is used. 
//  Buffer is at the leaf node. 
//  just drop transactional=false.  For backward compatibility in case someone has scripts   with transactional=false 
//  2. We create the join aux structures 
//  operator (below) 
/* Some HiveExceptions (e.g. SemanticException) don't set          canonical ErrorMsg explicitly, but there is logic          (e.g. #compile()) to find an appropriate canonical error and          return its code as error code. In this case we want to          preserve it for downstream code to interpret */
//  the update statement (remember split-update U=D+I)! 
//  Free up the HTable connections 
//  Only a single double was passed as parameter 2, a single quantile is being requested 
//  col > -1 
/*    * build:   *          ^(TOK_WHERE   *             {is null check for joining column}    *           )    */
//  A type date (LongColumnVector storing epoch days) minus a type date produces a   type interval_day_time (IntervalDayTimeColumnVector storing nanosecond interval in 2 longs). 
/*    * Initialize using an ObjectInspector array and a column projection array.    */
//  All columns: data, partition, and virtual are added. 
//  unlimited lifetime 
//  Parse out the kerberos principal, host, realm. 
//  Copy credentials and any new config added back to JobContext 
//  This type of VectorizedOrcAcidRowBatchReader can only be created when split-update is 
//  obtain delegation token for the give user from metastore 
//  list of paths that don't need to merge but need to move to the dest location 
//  Exception expected 
// Negative tests 
//  Sort itself should not reference cor vars. 
//  additional bookkeeping info for the stored stats 
//  call-1: open to read - split 1 => mock:/mocktable1/0_0 
//  7. Return result 
//  add the bits to the bottom of the current word 
//  then remove all the grants 
//  fractional part has, starting with zeros 
//  create table as select 
//  Ensure the correct task was preempted. 
//  Note : The reason this method exists outside the no-arg getDeserializer method is in   case there is a user-implemented MessageFactory that's used, and some the messages   are in an older format and the rest in another. Then, what MessageFactory is default   is irrelevant, we should always use the one that was used to create it to deserialize.     There exist only 2 implementations of this - json and jms     Additional note : rather than as a config parameter, does it make sense to have   this use jdbc-like semantics that each MessageFactory made available register   itself for discoverability? Might be worth pursuing. 
//  Set bit in NULL byte when a field is NOT NULL. 
//  1.5% tolerance for long range bias (when no bias enabled) and 5% when (no   bias is disabled) and   0.5% for short range bias 
//  extract the raw data size, and update the stats for the current partition 
//  Don't evaluate nondeterministic function since the value can only calculate during runtime. 
//  Since the output of the UDTF is a struct, we can just forward that 
//  Get the standard ObjectInspector of the row 
//  Test a single, high-precision divide of random inputs. 
//  Add it to result in order we are processing. 
//  Get col object out 
//  testcase.testWithColumnNumber(count, 2, checkCorrect, codec);   testcase.testWithColumnNumber(count, 10, checkCorrect, codec); 
//  If the child is also decimal, no cast is needed (we hope - can target type be narrower?). 
//  replace-overwrite introduces no new files 
// In DbTxnManager.acquireLocks() we have   1 ReadEntity: default@values__tmp__table__1 
//  check if the sample columns are the same as the table bucket columns 
//    throw new HiveException("allMatchs is not in sort order and unique");   } 
//  Inform the routing purgePolicy.   Send out a fake log message at the ERROR level with the MDC for this query setup. With an   LLAP custom appender this message will not be logged. 
//  desired parallelism of the reduce task. 
/*    * (non-Javadoc)   *    * @see   * org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator#canAcceptInputAsStream   * ()   *    * WindowTableFunction supports streaming if all functions meet one of these   * conditions: 1. The Function implements ISupportStreamingModeForWindowing 2.   * Or returns a non null Object for the getWindowingEvaluator, that implements   * ISupportStreamingModeForWindowing. 3. Is an invocation on a 'fixed' window.   * So no Unbounded Preceding or Following.    */
//  Test that write blocks write but read can still acquire 
/*              * Single-Column Long outer get key.              */
//  Since right has a longer digit tail and it doesn't move; we will shift the left digits   as we do our addition into the result. 
//  If old table is *not* in the cache but the new table can be cached 
//  interval types can use long version 
//  2. Convert NONACIDORCTBL to ACID table 
//  Don't set dagClient to null here - execute will only clean up operators if it's set. 
//  Should only be called for testing. 
//  make one entry produce false in result 
//  Add INT values 
//  Empty string 
//  Timestamps are stored as long, so convert and compare 
//  If we get past this, then the column name did match the hive pattern for an internal   column name, such as _col0, etc, so it *MUST* match the schema for the appropriate column.   This means people can't use arbitrary column names such as _col0, and expect us to ignore it 
//  return worst case if unknown 
//  internal names. 
//  Remove unnecessary information from target 
//  auto-determine local mode if allowed 
//  Try the basic test with non-chunked stream 
//  REPL DUMP 
// done - output does not need to be committed as hive does not use outputcommitter 
//  Transaction states 
//  list the current connections 
//  This map is used for set the stats flag for the cloned FileSinkOperators in later process 
//  convert the partition filter expression into a string expected by   hcat and pass it in setLocation() 
//  Type doesn't require any qualifiers. 
// start reading role names from next position 
//  We do not know what it is, we bail out for safety 
//  not a column 
//  Make sure I can add it back 
//  Disable it to avoid verbose app state report in yarn-cluster mode 
//  Case 4: Test with originals and deltas but now with only one bucket covered, i.e. we will   have originals & insert_deltas for only one bucket, but the delete_deltas will be for two   buckets => Two strategies with one split for each.   When split-update is enabled, we do not need to account for buckets that aren't covered.   The reason why we are able to do so is because the valid user data has already been considered   as base for the covered buckets. Hence, the uncovered buckets do not have any relevant 
// check first, otherwise webhcat.log is full of stack traces from FileSystem when  clients check for status ('exitValue', 'completed', etc.) 
//  Remove the previously peeked element. 
//  ignore if the parent already exists 
//  construct valueTableDescs and valueFilteredTableDescs 
//  Offset before which this RG is guaranteed to end. Can only be estimated. 
//  Return greater than because of left's digits below right's scale. 
//  Creation time will be set by server and not us. 
//  This flow is usually taken for IMPORT command 
//  CONSIDER: Cleaning up this code and eliminating the arrays.  Vectorization only handles   one operator tree. 
//  Ignore starting quote 
//  MySQL returns 0 if the string is not a well-formed numeric value.   return IntWritable.valueOf(0);   But we decided to return NULL instead, which is more conservative. 
//  Convert the work containing to sort-merge join into a work, as if it had a regular join.   Note that the operator tree is not changed - is still contains the SMB join, but the   plan is changed (aliasToWork etc.) to contain all the paths as if it was a regular join.   This is used to convert the plan to a map-join, and then the original SMB join plan is used 
//  singleton 
//  9. Rerun PPD through Project as column pruning would have introduced   DT above scans; By pushing filter just above TS, Hive can push it into   storage (incase there are filters on non partition cols). This only 
//  entry 2 is null due to zero-divide 
//  For Spark,TEZ we rely on the generated SelectOperator to do the type casting.   Consider:      SEL_1 (int)   SEL_2 (int)    SEL_3 (double)   If we first merge SEL_1 and SEL_2 into a UNION_1, and then merge UNION_1   with SEL_3 to get UNION_2, then no SelectOperator will be inserted. Hence error   will happen afterwards. The solution here is to insert one after UNION_1, which 
//  elements of queue (Integer) are index to FetchOperator[] (segments) 
//  create partition specs 
//  Check if all the input txns are in open state. Write ID should be allocated only for open transactions. 
//  Do not exceed the configured max reducers. 
//  No dependency, check depth 
/*  All AST nodes must implement this interface.  It provides basic machinery for constructing the parent and child relationships between nodes.  */
//  Check that the files are removed 
//  Write lock for add, evict and clean operation 
/*  1 files x 100 size for 9 splits  */
//                    12346678.901234667890123466789012346678 
// strip '(' and ')' 
/*      * row resolver of the SubQuery.     * Set by the SemanticAnalyzer after the Plan for the SubQuery is genned.     * This is needed in case the SubQuery select list contains a TOK_ALLCOLREF      */
// no Worker so it stays in initiated state  w/o AND WAIT the above alter table retunrs almost immediately, so the test here to check that  > 2 seconds pass, i.e. that the command in Driver actually blocks before cancel is fired 
//  The column number and type information for this one column long reduce key. 
//  No lock required here 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setAsciiStream(int, java.io.InputStream,   * int)    */
//  if unspecified, default one or [\r\n] will be used for line break 
//  Prefixed with incrementalLoadFailAndRetry to avoid finding entry in cmpath 
//  indicates that the recorded value is null 
//  Entire query can be run locally.   Save the current tracker value and restore it when done. 
//  shallow copy ASTNode 
//  recursive types. 
//        This computes stats and should be in stats (de-duplicated too). 
//  OPERATION_STATE 
//  Most general case, where the left and right keys might have nulls, and   caller requires 3-valued logic return.     select e.deptno, e.deptno in (select deptno from emp)     becomes     select e.deptno,     case     when ct.c = 0 then false     when dt.i is not null then true     when e.deptno is null then null     when ct.ck < ct.c then null     else false     end   from e   left join (     (select count(*) as c, count(deptno) as ck from emp) as ct     cross join (select distinct deptno, true as i from emp)) as dt     on e.deptno = dt.deptno     If keys are not null we can remove "ct" and simplify to     select e.deptno,     case     when dt.i is not null then true     else false     end   from e   left join (select distinct deptno, true as i from emp) as dt     on e.deptno = dt.deptno     We could further simplify to     select e.deptno,     dt.i is not null   from e   left join (select distinct deptno, true as i from emp) as dt     on e.deptno = dt.deptno     but have not yet.     If the logic is TRUE we can just kill the record if the condition   evaluates to FALSE or UNKNOWN. Thus the query simplifies to an inner   join:     select e.deptno,     true   from e   inner join (select distinct deptno from emp) as dt     on e.deptno = dt.deptno   
/*  * Encapsulates statistics about the duration of all reduce tasks * corresponding to a specific JobId. * The stats are computed in the HadoopJobExecHelper when the * job completes and then populated inside the QueryPlan for * each job, from where it can be later on accessed. * The reducer statistics consist of minimum/maximum/mean/stdv of the * run times of all the reduce tasks for a job. All the Run times are * in Milliseconds.  */
//  Values 
//  get the last colName for the reduce KEY 
//        2) Some parts of session state, like mrStats and vars, need proper synchronization. 
//  First drop any databases in catalog 
/*  input data     *     * col0       col1     * ===============     * blue       red     * green      green     * red        blue     * NULL       red            col0 data is empty string if we un-set NULL property      */
//  Both inputs non-negative 
//  The real work-horse. Spend time and energy in this method if there is   need to keep HCatStorer lean and go fast. 
//  expected error 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setSQLXML(int, java.sql.SQLXML)    */
//  Get next batch 
//  Note, we print to System.err instead of ss.err, because if we can't parse our   commandline, we haven't even begun, and therefore cannot be expected to have   reasonably constructed or started the SessionState. 
// AddNotNullConstraintEvent addDefaultConstraintEvent = new AddNotNullConstraintEvent(defaultConstraintCols, true, this);  listener.onAddDefaultConstraint(addDefaultConstraintEvent); 
//  An item for new partition is queued now. 
//  ConversionHelper can be called without method parameter length   checkings   for terminatePartial() and merge() calls. 
//  First known state was COMPLETED. Wait for the app launch to start. 
//  Mapping from the regex to lines in the log file where find() == true 
//  2) Copy INSERT branch and duplicate it, the first branch will be the UPDATE   for the MERGE statement while the new branch will be the INSERT for the 
//  No isNull copying necessary. 
//  not use map join in case of cross product 
//  we have reached RUNNING state, now check if running nodes threshold is met 
//  We will wait for 30 seconds for the task to be cancelled.   If it's still not cancelled (unlikely), we will just move on. 
//  sit in the cache, while not in use. 
/*  @bgen(jjtree) Include  */
//  Set up a timeout to undo everything. 
//  Create table 
//  Don't register with deleteOnExit 
//  Non-empty java opts with -Xmx specified in B 
//  add partition column stats 
//  This can use significant resources and should not be done on the main query thread. 
// as this is corr scalar subquery with agg we expect one aggregate 
//  spark-llap always wraps query under a subquery, until that is removed from spark-llap 
//  Make sure the default value expression type is exactly same as column's type. 
//  3. dynamic partition columns 
//  gbInputRel's schema is like this 
//  The connection should fail since it the dry run 
//  Always set these so EXPLAIN can see. 
//  If a table is partitioned and immutable, then the presence   of the partition alone is enough to throw an error - we do   not need to check for emptiness to decide to throw an error 
//  test for boolean type 
//  marker annotations for functions that Reflector should ignore / pretend it does not exist 
//  Drop occured as part of replicating a drop, but the destination   table was newer than the event being replicated. Ignore, but drop   any partitions inside that are older. 
//  Column Type 
// sqlLine.getOpts().setAllowMultiLineCommand(false);  System.setProperty("sqlline.isolation","TRANSACTION_READ_COMMITTED");   We can be pretty sure that an entire line can be processed as a single command since   we always add a line separator at the end while calling dbCommandParser.buildCommand. 
/*        * If the input to the GBy has a tab alias for the column, then add an entry       * based on that tab_alias.       * For e.g. this query:       * select b.x, count(*) from t1 b group by x       * needs (tab_alias=b, col_alias=x) in the GBy RR.       * tab_alias=b comes from looking at the RowResolver that is the ancestor       * before any GBy/ReduceSinks added for the GBY operation.        */
//  inputs 
//  Good performance for common case where small table has no complex objects. 
/*  * Specialized class for doing a vectorized map join that is an inner join on a Multi-Key * using a hash map.  */
//  Create post-filtering evaluators if needed 
//  We want no lock here, as the database lock will cover the tables,   and putting a lock will actually cause us to deadlock on ourselves. 
//  before readBatch, initial the size of offsets & lengths as the default value, 
//  CLIENT_PROTOCOL 
//  write the value out 
//  Handler multi-line sql 
//  Test setting fetch size below max 
//  Candidate for preemption. 
//  remove this task from its children tasks 
//  nulls possible on left, right 
//  INTEGER_FLAG 
//  Used Memory = totalMemory() - freeMemory(); 
//  start would throw if it already existed here 
/*                the tableTracker here should be a new instance and not an existing one as this can               only happen when we break in between loading partitions.            */
//  then load the SparkClientImpl config 
// Peel off the n-1 levels to get to the underlying array 
//  to this task in the callback instead. 
//  Find functions which name contains _to_find_ in the default database 
//  MIN 
//  isset id assignments 
//  For integers, we have optional min/max filtering. 
//  -----------------------------------------------------------------------------------------------     Compare timestamp against timestamp, long (seconds), and double (seconds with fractional   nanoseconds).      TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn    TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   {Long|Double}Column  * {Long|Double}Col     {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn      TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampScalar    TimestampCol         {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   {Long|Double}Scalar  * {Long|Double}Col     {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampScalar      TimestampScalar      {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn    TimestampScalar      {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   {Long|Double}Column  * {Long|Double}Scalar  {Equal|Greater|GreaterEqual|Less|LessEqual|NotEqual}   TimestampColumn     ----------------------------------------------------------------------------------------------- 
//  Specifies a DB 
//  tracks the number of elements with the same rank at the current time 
//  if table is partitioned,add partDir and partitionDesc 
//  At this point we have a number.  Save it in fastResult.  Round it.  If we have an exponent,   we will do a power 10 operation on fastResult. 
//  set true for columns that needed to skip loading into memory. 
//  Simulate the unknown source 
//  blank " " (1 byte)   blank " " (1 byte)   blank " " (1 byte)   blank " " (1 byte)   blank " " (1 byte)   WHITE START U+2606 (3 bytes) 
//  The only way to get the return object inspector (and its return type) is to   initialize it... 
//  From the moment that we have two destination clauses,   we know that this is a multi-insert query.   Thus, set property to right value.   Using qbp.getClauseNamesForDest().size() >= 2 would be   equivalent, but we use == to avoid setting the property   multiple times 
//  expressions 
//  max value stored in registered is cached to determine the bit width for 
//  common code 
//  val 
//  workersPath is the directory path where all the worker znodes are located. 
//  if the variable was on the right, we need to swap things around 
//  test third IF argument with nulls 
//  Entry either expired, or was invalidated due to table updates 
//  Because the length of ListColumnVector.child can't be known now, 
//  static partition spec ends with a '/' 
//  Use the earlier match. 
//  Return true if the table is bucketed/sorted by the specified positions   The number of buckets, the sort order should also match along with the 
//  skip empty lines 
//  Base with delete deltas 
//  remember this rel so we don't fire rule on it again   REVIEW jhyde 29-Oct-2007: rules should not save state; rule   should recognize patterns where it does or does not need to do 
//  It is possible that the background init thread has finished in parallel, queued   the message for us but also returned the session to the user. 
//  let's use the table from the cache. 
//  Create the list of children 
//  WARNING NOTE : at this point, createDbExportDump lives only in a world where ReplicationSpec is in replication scope   If we later make this work for non-repl cases, analysis of this logic might become necessary. Also, this is using   Replv2 semantics, i.e. with listFiles laziness (no copy at export time) 
//  Would be nice if there was a way to determine if quotes are needed 
//  One input table 
//  2. Get Left Table Alias 
//  3.3.1 Get UDAF Evaluator 
//  Find the last matching -Xmx following word boundaries   Format: -Xmx<size>[g|G|m|M|k|K] 
//  Add custom cookies if passed to the jdbc driver 
//  We are doing work here we'd normally do in VectorGroupByCommonOperator's constructor.   So if we later decide not to specialize, we'll just waste any scratch columns allocated... 
//  set some info for the query 
//  remember min value and ignore it from the denominator 
/*        * Interrupt all thread and verify we get InterruptedException and expected Message.        */
//  returns false if index already exists in map 
//  Update condition 
/*    * Reads the the next field.   *   * Afterwards, reading is positioned to the next field.   *   * @return  Return true when the field was not null and data is put in the appropriate   *          current* member.   *          Otherwise, false when the field is null.   *    */
//  otherwise discard the escape char 
//  due to data freshness) 
//  Create top Project fixing nullability of fields 
// Test is failing due to Guava dependency, Druid 0.13.0 should have less dependency on Guava 
// allow operation in a txn 
//  we have tez installed 
//  Extract the bits of num into value[] from right to left 
//  Need to deep copy here since doing something like lastFrom = from instead, will make 
/*    * Serialize the row into a ByteStream.   *   * @param obj           The object for the current field.   * @param objInspector  The ObjectInspector for the current Object.   * @param level         The current level of separator.   * @param writeBinary   Whether to write a primitive object as an UTF8 variable length string or   *                      as a fixed width byte array onto the byte stream.   * @throws IOException  On error in writing to the serialization stream.   * @return true         On serializing a non-null object, otherwise false.    */
//  we expect json file to be updated 
//  Replace unparsable synonyms. 
//  Examine the buddy block and its sub-blocks in detail. 
//  Test drop_partition_by_name 
//  Hive DESCRIBE outputs "empty_string NULL" row before partition information 
//  this is to keep track if a subquery is correlated and contains aggregate 
//  We flip the bits because Calcite considers that '1'   means that the column participates in the GroupBy   and '0' does not, as opposed to grouping_id. 
//  Must obtain vectorized equivalents for filter and value expressions 
//  CREATE DB currently replicated as Noop. 
//  2. We will ask WM for a preliminary mapping. This allows us to escape to the unmanaged path      quickly in the common case. It's still possible that resource plan will be updated and 
//  if there are actual accumulo index columns defined then build   the comma separated list of accumulo columns 
//  NOTE: All dist cols have single output col name; 
//  We should prepare the valid write ids list based on validTxnList of current txn.   If no txn exists in the caller, then they would pass null for validTxnList and so it is   required to get the current state of txns to make validTxnList 
//  extra fields, but we don't.) 
//  create base OutputFormat 
//  PARTITION_VALS 
//  gives progress over uncompressed stream   assumes deserializer is not buffering itself 
// create and drop some additional metadata, to test drop counts. 
//  STRING_VALUE 
//  ACCUMULO-4670 RangeInputSplit doesn't preserve useSasl on the ClientConfiguration/ZooKeeperInstance 
//  Send a 401 to the client 
//  See addToExpirationQueue for why we re-check the queue. 
// -----------------------------------------------------------------------------------------------   Rounding / setScale methods.  ----------------------------------------------------------------------------------------------- 
// in case of Compaction, this is the 1st file of the current bucket 
//  Column expression of the table being filtered by the semijoin optimization. 
/*    * This method is overridden in each Task. TODO execute should return a TaskHandle.   *   * @return status of executing the task    */
//  double column/column IF 
//  dummy value for use in tests 
/*    * Called after the tasks have been generated to run another round of optimization    */
//  If it is a map-only job, the task needs to be processed 
//  get metastore/thrift privilege object for this principal and object, not looking at 
//  Null should be smaller than any other value, so put a null at the front   end 
//  We need to shift everything 8 bits left and then shift back to populate the sign field. 
/*    * An expression is either the left/right side of an Equality predicate in the SubQuery where   * clause; or it is the entire conjunct. For e.g. if the Where Clause for a SubQuery is:   * where R1.X = R2.Y and R2.Z > 7   * Then the expressions analyzed are R1.X, R2.X ( the left and right sides of the Equality   * predicate); and R2.Z > 7.   *   * The ExprType tracks whether the expr:   * - has a reference to a SubQuery table source   * - has a reference to Outer(parent) Query table source    */
//  role names are case-insensitive 
//  PARTITION_KEYS 
//  initialize the user's process only when you receive the first row 
//  mapjoin should not be affected by join reordering 
//  Unexpected. 
//  File handle 
//  Specify the external warehouse root 
//  RS-GB-RS 
//  Indicates whether a node is disabled - for whatever reason - commFailure, busy, etc. 
//  Note: the following code (removing folded constants in exprs) is deeply coupled with      ColumnPruner optimizer.   Assuming ColumnPrunner will remove constant columns so we don't deal with output columns.      Except one case that the join operator is followed by a redistribution (RS operator). 
//  Make sure we skip backward-compat checking for those tests that don't generate events 
//  convert to hcatschema and pass to HCatInputFormat 
//  Add those that are not part of the final set to residual 
//  parse a value 
//  Options for the python script that are here because our option parser cannot ignore the unknown ones 
//  Handle structs composed of partition columns, 
//  Read and decode dictionary ids. 
//  SR.SR: Lock we are examining is shared read 
//  that's what this this alter is, and if so swallow it. 
//  The output of the key expression is the input column. 
//  Unregister may come in after the new dag has started running. The methods are expected to   be synchronized, hence the following check is sufficient. 
/*      * used to capture view to SQ conversions. This is used to check for     * recursive CTE invocations.      */
/*    * Calculate the variance family {VARIANCE, VARIANCE_SAMPLE, STANDARD_DEVIATION, or   * STANDARD_DEVIATION_STAMPLE) result when count > 1.  Public so vectorization code can   * use it, etc.    */
//  Optimize for most common case -- primitive. 
//  Check if the pipeout files are removed 
// Volatile ensures that static access returns Metrics instance in fully-initialized state.  Alternative is to synchronize static access, which has performance penalties. 
//  fields, but do it for uniformity 
//  EVENT_TIME 
//  The sort-merge join creates the output sorted and bucketized by the same columns. 
//  (none) 
/*  Have to be able to peek ahead one byte  */
//  Note: Not currently part of the HiveRelNode interface 
//  add one more record and close 
//  Add any redirects 
/*      * get from conf to pick up changes; make sure not to set too low and kill the metastore     * MAX_SLEEP is the max time each backoff() will wait for, thus the total time to wait for     * successful lock acquisition is approximately (see backoff()) maxNumWaits * MAX_SLEEP.      */
// in case schema is not a file system 
//  Wait before launching the next round of connection retries. 
//  We create new view 
//  whether we need to do transformation for each parent 
//  Found UDF in metastore - now add it to the function registry 
//  Remove operator 
// all currently open txns (if any) have txnid >= than commitHighWaterMark 
//  move works following the current reduce work into a new spark work 
//  add sub-directory to the work queue if maxDepth is not yet reached 
//  Protection against construction. 
//  4. Update the existing row, and insert another row to newly-converted ACID table 
/*    * These parameters controls the maximum time job submit/status/list operation is   * executed in templeton service. On time out, the execution is interrupted and   * TimeoutException is returned to client. On time out   *   For list and status operation, there is no action needed as they are read requests.   *   For submit operation, we do best effort to kill the job if its generated. Enabling   *     this parameter may have following side effects   *     1) There is a possibility for having active job for some time when the client gets   *        response for submit operation and a list operation from client could potential   *        show the newly created job which may eventually be killed with no guarantees.   *     2) If submit operation retried by client then there is a possibility of duplicate   *        jobs triggered.   *   * Time out configs should be configured in seconds.   *    */
//  inputObjectInspectors 
//  -Xmx specified in KB 
//  Test various combinations. 
//  For non-views, we need to do some extra fixes 
// test owner 
//  total characters = 4; byte length = 9 
//  add owner privilege if user is owner of the object 
// remove WRITE_SET info for current txn since it's about to abort 
//  there is no backup servers; 
/*  Spot check correctness of decimal column multiply decimal scalar. The case for   * addition checks all the cases for the template, so don't do that redundantly here.    */
//  invariant: rightLength = leftLength   rightOffset is within the buffers 
//  get names of these roles and its ancestors 
/*  Dynamic partition pruning is enabled in some or all cases    */
//  Get the skewed values in all the tables 
//  skip the test if Java Cryptography Extension (JCE) Unlimited Strength   Jurisdiction Policy Files not installed 
//  Should return "tbl" and "tbl2" 
//  TestHS2ConnectionConfigFileManager 
//  Grouping sets are not allowed 
//  @@protoc_insertion_point(class_scope:FragmentRuntimeInfo) 
/*  * This hook is used for verifying the column access information * that is generated and maintained in the QueryPlan object by the * ColumnAccessAnalyzer. All the hook does is print out the columns * accessed from each table as recorded in the ColumnAccessInfo * in the QueryPlan.  */
//  STATS_DATA 
//  Get the exprNodeDesc corresponding to the first start node; 
//  This is the first time we have realized we are in a stack trace.  In this case,   the previous line was the error message, add that to the stack trace as well 
//  map built during translation 
/*      * Process the input columns to find a non-NULL value for each row.     *     * We track the unassigned batchIndex of the rows that have not received     * a non-NULL value yet.  Similar to a selected array.      */
//  GenericUDTF is stateful too, copy 
//  We could not get stats, we cannot convert 
//  Put into the map that this task was in before we decided to update it. 
//  We don't currently support the BETWEEN ends being columns.  They must be scalars. 
//  verify that the new version is added to schema 
//  The same object. 
//  For testing 
//  Turn escape off. 
//  drop table over10k; 
//  This is a rebuild, there's nothing to do here 
//  Default value is true, however, if an optimization deems this edge   important, it should set this to false. This does not guarantee that   the edge will stay, however, it increases the chances. 
//  and test dropping this specific table 
//  joinKeys/joinKeysOI are initialized after making merge queue, so setup lazily at runtime 
// tries to get X lock on T7.p=1 and gets Waiting state 
//  write an intermediate file to the specified path   the format of the path is: tmpPath/targetWorkId/sourceWorkId/randInt 
//  that it's easy to find reason for local mode execution failures 
// "delete from tab1" 
/*     Sets up a TaskSpec with no inputs, and tasks belonging to vertex1      */
//  Verify r1 was preempted. Also verify that it finished (single executor), otherwise   r2 could have run anyway. 
//  We refer to grouping_id column 
/*        * HIVE-6356       * The following code change is only needed for hbase-0.96.0 due to HBASE-9165, and       * will not be required once Hive bumps up its hbase version). At that time , we will       * only need TableMapReduceUtil.addDependencyJars(jobConf) here.        */
//  This constructor is used to momentarily create the object so match can be called. 
//  Caller will return the batch. 
//  check for required fields   check for sub-struct validity 
// @Test 
//  Use the same logic as ReduceSinkOperator.toHiveKey.   
//  SerDe Properties 
//  If a parameter is added to the restricted list, add a test in TestRestrictedList.Java 
//  INSERT DATA 
//  Thread pool for callbacks on completion of execution of a work unit. 
//  Only the big table input source should be vectorized (if applicable) 
//  Key to be used to save the partition to be dropped in partSpecs 
//  plain acid table   acid table with customized tblproperties 
//  Optional vectorized key expressions that need to be run on each batch. 
//  initialize with estimated element size 10   Record initial buffer size 
//  by JAR spec, if there is a manifest, it must be the first entry in   the   ZIP. 
//  Exclude the newly-generated select columns from */etc. resolution. 
//  the "-foo bar" and "-blah" params order is not guaranteed 
//  This should be a cost based decision, but till we enable the extended cost   model, we will use the given value for the variable 
//  todo authorization 
/*  10 files x 100 size for 99 splits  */
/*    * Returns the root node of the AST. It only makes sense to call this after a   * successful parse.    */
//  Input and Output Serdes 
//  non-cbo path retries to execute create view and   we believe it will throw the same error message 
//  11. Apply Druid transformation rules 
//  Here we know nd represents a group by expression. 
//  5 seconds 
//  Add identity 
// there is one repeated field for mapCol, the field name is "map" and its original Type is MAP_KEY_VALUE; 
//  Pause for a just a bit for retrying to avoid immediately jumping back into the deadlock. 
/*    * Tries to get the job result if job request is completed. Otherwise it sets job status   * to FAILED such that execute thread can do necessary clean up based on FAILED state.    */
//  Use synchronized map since even read actions cause the lru to get updated. 
//  If the columns of the old column descriptor != the columns of the new one,   then change the old storage descriptor's column descriptor.   Convert the MFieldSchema's to their thrift object counterparts, because we maintain   datastore identity (i.e., identity of the model objects are managed by JDO,   not the application). 
//  logger can be a resource stream or a real file (cannot use copy) 
//  Replace filter 
//  the base writer 
//  can directly add positions into corDefOutputs since join 
// create column info with new tableAlias 
//  In LRR case, if we just store 2 boundaries (which could be split boundaries or reader   positions), we wouldn't be able to account for torn rows correctly because the semantics of   our "exact" reader positions, and inexact split boundaries, are different. We cannot even   tell LRR to use exact boundaries, as there can be a mismatch in an original mid-file split   wrt first row when caching - we may produce incorrect result if we adjust the split   boundary, and also if we don't adjust it, depending where it falls. At best, we'd end up   with spurious disk reads if we cache on row boundaries but splits include torn rows.   This structure implies that when reading a split, we skip the first torn row but fully   read the last torn row (as LineRecordReader does). If we want to support a different scheme,   we'd need to store more offsets and make logic account for that. 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNClob(int, java.io.Reader)    */
//  hive.mapjoin.bucket.cache.size has been replaced by hive.smbjoin.cache.row, 
//  {Small Value Bytes}   (use small length from valueWordRef) 
//  If hint is true, shouldRemove is redundant anyway 
//  TABLE 
//  rowSetLog should contain execution as well as performance logs 
//  There are 2 paths under which the instances get registered   1) Standard path used by ZkRegistryBase where all instances register themselves (also stores metadata)   Secure: /hs2ActivePassiveHA-sasl/instances/instance-0000000000   Unsecure: /hs2ActivePassiveHA-unsecure/instances/instance-0000000000   2) Leader latch path used for HS2 HA Active/Passive configuration where all instances register under _LEADER      path but only one among them is the leader   Secure: /hs2ActivePassiveHA-sasl/_LEADER/xxxx-latch-0000000000   Unsecure: /hs2ActivePassiveHA-unsecure/_LEADER/xxxx-latch-0000000000 
//  The Tokens have no distinction between Identifiers and QuotedIdentifiers.   Ugly solution is just to surround all identifiers with quotes. 
//  if there is any aggregate function this group by is not un-necessary 
//  The condition fetched here can reference a udf that is not deterministic, but defined   as part of the select list when a view is in play.  But the condition after the pushdown   will resolve to using the udf from select list.  The check here for deterministic filters   should be based on the resolved expression.  Refer to test case cbo_ppd_non_deterministic.q. 
//  and must have 3 columns. Also, the partition locations must lie within the table directory. 
//  O1 
//  unable to get the database, set the dbName empty 
//   UDFToString we need the following mappings 
/*  * An bytes key hash multi-set optimized for vector map join. * * This is the abstract base for the multi-key and string bytes key hash multi-set implementations.  */
//  We need to update rootToWorkMap in case the op is a key, since even 
//  Also compute the correct cf:cq pairs so we can assert the right argument was passed 
//  value needs to be converted to match the type params (length, etc). 
//  Serialize 
//  then need to truncate the exceptions list accordingly. 
//  What can I do about it? 
//  4. Build new TS 
//  Verify if the data are intact even after applying an applied event once again on existing objects 
//  alter partitioned table set table property 
//  character, then reverse the whole string. 
// db.table -> return table 
//  convert integer to string 
//  test that underflow produces NULL 
//  index 1 set by child 
//  Map from integer tag to non-distinct aggrs with key parameters. 
//  Copy ColumnVectors to overflowBatch.  We remember buffered columns compactly in the   buffered VRBs without other columns or scratch columns. 
//  instantiate default values if not specified 
//  function calls from the query plan. 
//  add the partition again so that drop table with a partition can be 
//  Getters 
// -----------------------------------------------------------------------------------------------   Attribute methods.  ----------------------------------------------------------------------------------------------- 
//  [url] [host] [port] [db] 
//  If any input has not been rewritten, do not rewrite this rel. 
//  Mock BeeLine 
//  Set conf to use LLAP user rather than current user for LLAP Zk registry. 
//  Currently only print the first port to be consistent with old behavior 
//  O3 
//  project everything from the LHS and then those from the original 
//  do not need update stats in alter table/partition operations 
//  Number of rows that match the regex but have missing groups. 
//  Create partial Select query 
//  and a preemption should be attempted on host1, despite host2 having available capacity 
//  Session has expired and will be returned to us later. 
//  generate the cmd line to run in the child jvm 
//  3. Apply SARG if needed, and otherwise determine what RGs to read. 
//  Found a best match during this processing, use it. 
//  compare with old cacheEnd 
//  can be mux operator 
//  null for tables, VIRTUAL_VIEW for views, MATERIALIZED_VIEW for MVs 
//  Now that the properties are in, we can instantiate SessionState. 
//  O2 
//  repeated string tablesWritten = 9; 
//  Generic UDTF's 
// Output will also be repeating and null 
//  Add the table spec for the destination table. 
//  a>0 
//  Same session object is expected. 
//  If partitioning columns of the parent RS are not assigned,   assign partitioning columns of the child RS to the parent RS. 
//  Create 5 dbs 
//  matches 
//  this dependency is removed for HBase 1.0 
// ensures that FS object is cached so that everyone uses the same instance 
//  try to create RCFile.Reader 
//  And, their types. 
// The tests here are heavily based on some timing, so there is some chance to fail. 
//  perform incremental normalization 
//  Set the inferred sort columns for the file this FileSink produces 
//  round up 
//  LlapIoImpl.LOG.debug("diskData " + diskData); 
//  O5 
//  Mock BeeLineOpts 
//  Intentionally set this high so that it will not trigger major compaction for ttp1. 
// make sure to check for side file in case streaming ingest died 
//  Rewrite the delete or update into an insert.  Crazy, but it works as deletes and update   actually are inserts into the delta file in Hive.  A delete   DELETE FROM _tablename_ [WHERE ...]   will be rewritten as   INSERT INTO TABLE _tablename_ [PARTITION (_partcols_)] SELECT ROW__ID[,   _partcols_] from _tablename_ SORT BY ROW__ID   An update   UPDATE _tablename_ SET x = _expr_ [WHERE...]   will be rewritten as   INSERT INTO TABLE _tablename_ [PARTITION (_partcols_)] SELECT _all_,   _partcols_from _tablename_ SORT BY ROW__ID   where _all_ is all the non-partition columns.  The expressions from the set clause will be   re-attached later.   The where clause will also be re-attached later.   The sort by clause is put in there so that records come out in the right order to enable   merge on read. 
//  We consider this an enable issue, not a not vectorized issue. 
//  Our outputs are the transitive outputs of our inputs. 
//  we need to know if aggregate is COUNT since IN corr subq with count aggregate 
//  do this only when not initialized, but we may need to find a way to   tell the caller how to initialize the valid size 
//  Pass unparsed db name here 
//  add expr to the list of predicates rejected from further pushing   so that we know to add it in createFilter() 
//  This is a partition 
//  Exception is expected only if filter is enabled and injection is disabled 
//  O4 
//  catch up with the big table. 
//  Get service ticket from the authorization header 
/*    * Call this method may be called after all the all fields have been read to check   * for unread fields.   *   * Note that when optimizing reading to stop reading unneeded include columns, worrying   * about whether all data is consumed is not appropriate (often we aren't reading it all by   * design).   *   * Since LazySimpleDeserializeRead parses the line through the last desired column it does   * support this function.    */
//  Bad format. 
// process join filters 
//  Event 2, 3, 4 
//  COMPACTS 
//  test with a batch size of 30 and decaying factor of 2 
//  load set 
//  Position to beginning. 
//  Input to the script 
//  Set global member indicating which virtual columns are possible to be used by 
//  removes the threadlocal variables, closes underlying HMS connection 
/*    * Tests the case when tblPath/p1=a/p2=b/p3=c/file for a table with partition (p1, p2)   * does not throw HiveException    */
//  exceeds this value 
//  Set the table write id in all of the acid file sinks 
//  The get_splits call should have resulted in a lock on ACIDTBL 
/*   * (non-Javadoc)  *  * @see  * org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider  * #init(org.apache.hadoop.conf.Configuration)   */
// do this for all complex types and binary 
//  In case of KeepAlive, ensure that timeout handler does not close connection until entire 
//  some tests expected to pass invalid schema 
//  max 
//  Replace the reducer with our fully vectorized reduce operator tree. 
//  No longer relevant for WM. 
/*    * Set of functions to create the Null Check Query for Not-In SubQuery predicates.   * For a SubQuery predicate like:   *   a not in (select b from R2 where R2.y > 5)   * The Not In null check query is:   *   (select count(*) as c from R2 where R2.y > 5 and b is null)   * This Subquery is joined with the Outer Query plan on the join condition 'c = 0'.   * The join condition ensures that in case there are null values in the joining column   * the Query returns no rows.   *    * The AST tree for this is:   *    * ^(TOK_QUERY   *    ^(TOK FROM   *        ^(TOK_SUBQUERY   *            {the input SubQuery, with correlation removed}   *            subQueryAlias    *          )    *     )   *     ^(TOK_INSERT   *         ^(TOK_DESTINATION...)   *         ^(TOK_SELECT   *             ^(TOK_SELECTEXPR {ast tree for count *}   *          )   *          ^(TOK_WHERE   *             {is null check for joining column}    *           )   *      )   * )    */
//  Supports keeping a HiveIntervalDayTimeWritable object without having to import   that definition... 
//  Preven subsequent runs until a new trigger is set. 
// since HIVE-17089 if here, then it's not an acid table so there should never be any deltas 
//  Then, try the brute force search for something to throw away. 
//  O6 
//  Get the Job Handle id associated with the Spark job 
//  just a safe check to ensure that we are not reading empty delete files. 
//  Since we are creating with scale 0, no fraction digits to zero trim. 
//           LOG.info("Getting parent of "+ptnRoot.getName()); 
//  synthetic predicate with dynamic values 
//  To get around hbase failure on single node, see BUG-4383 
//  terminate the old task and make current task dependent on it 
//  after concatenating them with AND operator 
//  Start the Shuffle service before the listener - until it's a service as well. 
// e.g. map z->ColumnInfo for a 
//  Replicate table definition. 
//  Make sure that the number of column aliases in the AS clause matches 
//  If it contains a reducer, the optimization is always on.   Since there exists a reducer, the sorting/bucketing properties due to the   sort-merge join operator are lost anyway. So, the plan cannot be wrong by 
/*    * Runs the templeton controller job with 'args'. Utilizes ToolRunner to run   * the actual job.    */
// intentional fall through 
//  HiveConf hive.stats.ndv.error default produces 16 vectors 
/*        * Raise custom exception like IOException and verify expected Message.       * This should not invoke cancel operation.        */
//  if we're performing a binary search, we need to restart it 
//  Close the client connection with ZooKeeper 
//  do loop once again with the new cause of "current" 
// why is this checking for deltas.isEmpty() - HIVE-18110 
//  Retrieve attempt log into logDir 
// verify data 
//  No type promotion. Everything goes to decimal. 
//  The field bits (i.e. which fields to include) or "id" for each grouping set. 
//  the size of the biggest small table 
//  test forward scan 
//  Small case: Just write the value bytes only. 
//  Generate the kerberos ticket under the following scenarios:   1. Cookie Authentication is disabled OR   2. The first time when the request is sent OR   3. The server returns a 401, which sometimes means the cookie has expired 
//  we tried .. 
/*    * If a CTE is referenced in a QueryBlock:   * - add it as a SubQuery for now.   *   - SQ.alias is the alias used in QB. (if no alias is specified,   *     it used the CTE name. Works just like table references)   *   - Adding SQ done by:   *     - copying AST of CTE   *     - setting ASTOrigin on cloned AST.   *   - trigger phase 1 on new QBExpr.   *   - update QB data structs: remove this as a table reference, move it to a SQ invocation.    */
//  number of rows -1 means that statistics from metastore is not reliable 
//  Repeated non-NULL fill down column. 
//  2.1 Handle the case for unpartitioned table. 
//  try a zero-divide to show a repeating NULL is produced 
//  END pattern 
//  Setup hashcode 
//  Test setter for map object. 
//  Choose array size. We have two hash tables to hold entries, so the sum   of the two should have a bit more than twice as much space as the 
//  Added conf member to set the REPL command specific config entries without affecting the configs 
//  The number of reducers of the child RS is more specific than   that of the parent RS. Assign the number of reducers of the child RS   to the parent RS. 
//  won't happen 
//  Wrap the transport exception in an RTE, since Subject.doAs() then goes   and unwraps this for us out of the doAs block. We then unwrap one   more time in our catch clause to get back the TTE. (ugh) 
//  3) We propagate 
//  Do nothing in case proc is null 
//  The output precision is 10 greater than the input which should cover at least   10b rows. The scale is the same as the input. 
//  1. Convert inputs 
//  tables that were serialized with columnsetSerDe doesn't have metadata   so this hack applies to all such tables 
//  Cancel currently executing tasks 
//  the list element object inspector 
//  Skip rest of checks if user is admin 
//  standard error allowed for ndv estimates for FM-sketch. A lower value indicates higher accuracy and a 
//  replace existing view 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setArray(int, java.sql.Array)    */
//  ////// 1. Generate ReduceSinkOperator   There is a special case when we want the rows to be randomly distributed   to   reducers for load balancing problem. That happens when there is no   DISTINCT   operator. We set the numPartitionColumns to -1 for this purpose. This is 
//  Drop any left over catalogs 
//  TreeSet anyway uses TreeMap; so use plain TreeMap to be able to get value in collisions. 
// this is idempotent 
//  Adding column names used later by org.apache.hadoop.hive.druid.serde.DruidSerDe 
//  We expect cache requests from the middle here 
/*      * Count *any* input except null which is for COUNT(*) and output is LONG.     *     * Just modes (PARTIAL1, COMPLETE).      */
//  Some filters may have been specified in the SHOW LOCKS statement. Add them to the query. 
//  2. Gen GB-RS-GB-RS-GB pipeline 
//  it will be considered ENABLE and NOVALIDATE and RELY=false 
//  if the driver is not already available in the URL add the one provided 
//  check that src and dest are on the same file system 
//  of dynamic partition list to the StatsTask 
/* if this method ends with anything except a retry signal, the caller should fail the operation      and propagate the error up to the its caller (Metastore client); thus must reset retry counters */
/*    * Job Request type.    */
//  Start the scheduled poll task 
//  load data 
//  All the vertices belong to the same DAG, so we just use numbers. 
//  Make sure the table in the target database didn't get clobbered 
//  Fall through... 
//  this input rel is not rewritten 
//  Add a single child and restart the loop 
// write credential with token to file 
//  Use UpdateQueryResponseProto.newBuilder() to construct. 
//  read the schema version stored in metastore db 
//  It is possible that read and write entities contain a old version   of the object, before it was modified by StatsTask.   Get the latest versions of the object 
//  The buffer is not in the (full) heap. Demote the top item of the heap into the list. 
//  make sure all leaves are visited at least once 
//  successfully convert to bucket map join 
//  Evaluate the HashCode 
/*    * Represents a Select Expression in the context of Windowing. These can   * refer to the output of Windowing Functions and can navigate the   * Partition using Lead/Lag functions.    */
//  If filter condition is TRUE, we ignore it 
//        actually run (which is a different under doAs=false). This seems to be intended. 
//        reducers don't produce enough files; we'll do the same for MM tables for now. 
//  jar then this would be needed 
//  Don't retry immediately - use delay with exponential backoff 
//  LRFU cache policy doesn't store locked blocks. When we cache, the block is locked, so   we simply do nothing here. The fact that it was never updated will allow us to add it   properly on the first notifyUnlock.   We'll do is set priority, to account for the inbound one. No lock - not in heap. 
/*      * Extract information.      */
//  fold after replacing, if possible 
//  Should have also been thrown out. 
//  Add the newly generated IN clause to subExpr. 
//  \x05 
//  test that we don't drop the unnecessary tuple if the table has the corresponding Struct 
//  get children of key node 
// if this wasn't an empty txn, we'd get a better msg 
//  will be set to the larger of the parents 
//  If so, mark that branch as the big table branch. 
//  this will create a project which will project out the column in positions 
//  return the next back zk server's port 
//  merge should convert hll3 to DENSE 
//  In case the expression is TABLE.COL (col can be regex).   This can only happen without AS clause   We don't allow this for ExprResolver - the Group By case 
// Schema only 
//  calculate filter propagation directions for each alias   L<->R for inner/semi join, L<-R for left outer join, R<-L for right outer 
//  non-native and non-managed tables are not supported as MoveTask requires filenames to be in specific format, 
//    Extract each entry from the pathenv   
//    return true;   } 
//  as possible 
//  value needs to be converted to match type params 
//  Dump metrics to string as JSON 
//  Dump all the events except DROP 
//  Gained it again. 
//  not using FileInputFormat.setInputPaths() here because it forces a connection to the 
//  by default, we aggregate counters across the entire DAG. Example: SHUFFLE_BYTES would mean SHUFFLE_BYTES   of each vertex aggregated together to create DAG level SHUFFLE_BYTES.   Use case: If SHUFFLE_BYTES across the entire DAG is > limit perform action 
//  Skewed Info 
//  Make sure the updates are not sent to ZK out of order compared to how we apply them in AM. 
/*  Number of mantissa digits BEFORE decimal				 * point.  */
//  3, 5, 8 
//  6. Let Cleaner delete obsolete files/dirs 
//  Release all the previous buffers that we may not have been able to release due to reuse, 
//  Get the actual length first 
//  Ensure this is set in the config so that the AM can read it. 
//  drop test db and its tables and views 
//  Change the engine to tez 
//  Test for only colNames being empty 
//  This is a guard for special Druid types e.g. hyperUnique   (http://druid.io/docs/0.9.1.1/querying/aggregations.html#hyperunique-aggregator).   Currently, we do not support doing anything special with them in Hive.   However, those columns are there, and they can be actually read as normal   dimensions e.g. with a select query. Thus, we print the warning and just read them   as String. 
//  operations that have objects of type COMMAND_PARAMS, FUNCTION are authorized   solely on the type 
//  Note: this doesn't maintain proper newStream semantics (if any).          We could either clone this instead or enforce that this is only called once. 
//  str 
/*  Move logic to PrunerUtils.walkOperatorTree() so that it can be reused.  */
//  separator for open write ids   separator for aborted write ids 
//  Process the position alias in GROUPBY and ORDERBY 
//  There are none or they're not readable. 
//  Get the character/byte at the offset in the string equal to the fieldID 
// default column name 
//  Definitely a int; most ints fall here 
/*    * Right trim a slice of a byte array and place the result into element i of a vector.    */
//  though id is given as a Short by hcat, the map will emit it as an 
//  0 NULL 
//  columns have been added. 
// needed by initArgs for certain execution paths 
/*          * Single-Column String specific variables.          */
//  Multiple concurrent local mode job submissions can cause collisions in   working dirs and system dirs   Workaround is to rename map red working dir to a temp dir in such cases 
//  1 + 2 + 4 + 8 + 4 + 8 + 3 + 4 + 3 + 4 + 4 + 4 + 3 + 4 + 2 + 4 + 3 + 5 + 4 + 5 + 7 + 4 + 7 = 
//  get forwarded hosts address 
/*      * validate and setup patternStr      */
//  no duplicate column names   currently, it is a simple n*n algorithm - this can be optimized later if   need be   but it should not be a major bottleneck as the number of columns are   anyway not so big 
//  Should only need to insert the token the first time. 
//  Deserialize key into vector row columns. 
//  That Union[T, NULL] is converted to just T. 
//  BitSet for flagging aborted transactions. Bit is true if aborted, false if open  default value means there are no open txn in the snapshot 
/*  Test DatabaseMetaData queries which do not have a parent Statement  */
//  Looks like some pools were removed; kill running queries, re-queue the queued ones. 
//     top. 
//       Hive3 probably won't support MR so do we really care?  
//  Copy log file 
/*      * For fully specified ptn, follow strict checks for existence of partitions in metadata     * For unpartitioned tables, follow filechecks     * For partially specified tables:     *    This would then need filechecks at the start of a ptn write,     *    Doing metadata checks can get potentially very expensive (fat conf) if     *    there are a large number of partitions that match the partial specifications      */
//  Get partitions by name - ascending or descending 
//  Start the process to add MV to the cache 
//  Make sure all partitioning columns referenced actually   exist and are in the correct order at the end   of the list of columns produced by the view. Also move the field   schema descriptors from derivedSchema to the partitioning key 
//  We assume each SD has an unique serde. 
// Set the key, check if this is a new group or same group 
// Short Running updated nothing, so we expect 0 rows in WRITE_SET 
//  PART_VALUES 
/*  * The interface for a single long key hash map lookup method.  */
/*        * If the thread is still active and needs to be cancelled then cancel it. This may       * happen in case task got interrupted, or timed out.        */
//  replace the cRS to SEL operator 
//  let's remember the join operators we have processed 
//  Transaction and locking methods 
//  Remove the semijoin optimization branch along with ALL the mappings   The parent GB2 has all the branches. Collect them and remove them. 
//  Race with queryComplete 
//  Updated when we add this to the queue. 
//  Wait for the current future. 
//  STAGES: 03/04            [==================>>-----] 86%  ELAPSED TIME: 1.71 s 
//  Time after which metastore cache is updated from metastore DB by the background update thread 
//  Don't push a sampling predicate since createFilter() always creates filter   with isSamplePred = false. Also, the filterop with sampling pred is always 
//  bunch of things get setup in the context based on conf but we need only the MR tmp directory 
//  Web port 
//  If we did not add any factor or there are no common factors, we can 
// will succeed and transition to Initiated->Working->Ready for Cleaning 
//  Overlay the values of any system properties whose names appear in the list of ConfVars 
//  this conditions need to be pushed into semi-join since this condition   corresponds to IN 
//  Create db2/t1/part1                /part2                /part3   Test: recycle single file (part1)         recycle table t1 
//  create the temp directories 
//  Remove the paths which are not part of aliasToPartitionInfo 
//  Both neededColumnIDs and neededColumns should never be null.   When neededColumnIDs is an empty list,   it means no needed column (e.g. we do not need any column to evaluate 
//  store types and tables   separately because one cannot use a table (ie service.method) as a Struct 
/*    * Used by Struct and Union complex type readers to indicate the (final) field has been fully   * read and the current complex type is finished.    */
/*      * OPTIMIZATION     * The ConditionalTask avoids linking 2 MoveTask that are expensive on blobstorage systems. Instead of     * linking, it creates one MoveTask where the source is the first MoveTask source, and target is the     * second MoveTask target.      */
// p=10 needs major compaction 
//  We only can have a single partition spec 
//  All others from the remote service cause the task to FAIL. 
//  This is a file or something we don't hold locks for. 
//  v[10] -- since left integer #5 is always 0, some products here are not included. 
/*      * We have a field and are positioned to it.  Read it.      */
//  by default for all other objects this is false 
/*    * STRING, CHAR, VARCHAR, and BINARY.   *   * For CHAR and VARCHAR when the caller takes responsibility for   * truncation/padding issues.   *   * When currentExternalBufferNeeded is true, conversion is needed into an external buffer of   * at least currentExternalBufferNeededLen bytes.  Use copyToExternalBuffer to get the result.   *   * Otherwise, currentBytes, currentBytesStart, and currentBytesLength are the result.    */
//  group path alias according to work 
//  - There cannot exist any sampling predicate. 
//  all rows from right side will be present in resultset 
//  This will work with the new support of rewriting load into IAS. 
//  By assumption, ACID columns are currently always in the beginning of the arrays. 
/*  Authorization Errors 3000 - 3999  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getCharacterStream(java.lang.String)    */
//  Combine/sort temp and normal table results 
//  We have estimation, lowerbound and higherbound. We use estimation if   it is between lowerbound and higherbound. 
//  only do split pruning if HIVE-8732 has been fixed in the writer 
//  Set temp file containing error output to be sent to client 
/*    * (non-Javadoc)   *    * @see   * org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator#startPartition()    */
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.CLIServiceClient#getColumns(org.apache.hive.service.cli.SessionHandle, java.lang.String, java.lang.String, java.lang.String, java.lang.String)    */
//  Create tmp dir for MergeFileWork 
/*    * The small key length.   *   * If the key is big (i.e. length >= allBitsOn), then the key length is stored in the   * WriteBuffers.    */
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getSchemas(org.apache.hive.service.cli.SessionHandle, java.lang.String, java.lang.String)    */
//  DataNucleus objects get detached all over the place for no (real) reason. 
//  now let's take a look at input sizes 
//  2. We extract possible candidates to be pushed down 
//  remove restrictions on the variables that can be set using set command 
//  we can generate ranges from e.g. rowid > (4 + 5) 
//  ArrayListMultimap is important here to retain the ordering for the splits. 
//  Lookup nDVs on TS side. 
//  If the RS has other RS siblings, we will add it to be considered in next iteration 
//  Cannot wrap a reader for non-vectorized pipeline. 
//  Example: "000126" => 126 => "126" 
/*          * this is the small table side. In case of SMB join, we need to send each split to the         * corresponding bucket-based task on the other side. In case a split needs to go to         * multiple downstream tasks, we need to clone the event and send it to the right         * destination.          */
//  Keep order by name, consistent with JDO. 
/*  A delta may be present from a previous failed task attempt.  */
//  MIN_HISTORY_LEVEL will have 1 entry for the open txn. 
//  nested complex types cannot be folded cleanly 
//  the same process. This will still only get the functions from the first metastore. 
//  add only if the corresponding family has not already been added 
//  write the byte to stream every 4 key-value pairs 
//  base  = JAVA64_OBJECT + PRIMITIVES1 * 2 + JAVA64_FIELDREF;   entry = JAVA64_OBJECT + JAVA64_FIELDREF * 2 
//  start writing array contents 
//  Skip past blank characters. 
//  vectorized row batch, not for example, an original inspector for an ORC table, etc. 
//  Stats part 
//  If all of them were false, return false 
//  12 hours 
//  Get the sort by aliases - these are aliased to the entries in the   select list 
//  Grab the oldest in-memory buffered batch and dump it to disk. 
//  statics for when the mock fs is created via FileSystem.get 
//  this must be hadoop 2.4 , where getDefaultProperties was protected 
//  map table name to the correct ColumnStatsTask 
//  Use system zone when converting from timestamp to timestamptz 
//  This tests checks that appropriate delta and delete_deltas are included when minor   compactions specifies a valid open txn range. 
//  only case is full outer join with SMB enabled which is not possible. Convert to regular   join. 
//  print next vertex 
//  to dbname-matching alone. 
//  ATTRIBUTES 
// TODO: Even listener for default  AddDefaultConstraintEvent addDefaultConstraintEvent = new AddDefaultConstraintEvent(defaultConstraintCols, true, this); 
/*    * join current union task to old task    */
//  Verify the eventID was passed to the non-transactional listener 
//  2. We check whether there is a column needed by the      windowing operation that is missing in the      project expressions. For instance, if the windowing      operation is over an aggregation column, Hive expects      that column to be in the Select clause of the query.      The idea is that if there is a column missing, we will      replace the old project operator by two new project      operators:      - a project operator containing the original columns        of the project operator plus all the columns that were        missing      - a project on top of the previous one, that will take        out the columns that were missing and were added by the        previous project 
//  2. Insert the current constant value into exprNodeStructs list.   If there is no struct corresponding to the current element, create a new one, insert 
//  notify listeners 
//  Comparing paths multiple times creates lots of objects &   creates GC pressure for tables having large number of partitions.   In such cases, use pre-computed paths for comparison 
//  Skip duplicated grouping keys, it happens when define column alias. 
//  link SEL to FS 
//  try repeating on both sides 
//  miniHS2_2 will become leader 
//  Update the last access time for this node 
//  check input object's length, if it doesn't match   then output new writable with correct params. 
//  We have an IOException other than a BlockMissingException. 
//  expectedRestrictedMap 
/*            * Vectorizer does not vectorize in row deserialize mode if the input format has           * VectorizedInputFormat so input formats will be clear if the isVectorized flag           * is on, they are doing VRB work.            */
//  byte 
//  Values outside the column type bounds will fail at runtime. 
//  process join keys 
//  It must have a column name followed with type. 
//  Subtraction with a type date (LongColumnVector storing days) and type timestamp produces a 
//  Process the combine splits 
//  spilled small tables 
//  The first directory becomes the base for combining. 
//  Use the Tez hash table loader. 
//  field expression should be resolved 
//  This is SQL standard - max_n of zero items should be null. 
//  Look for functions without pattern 
/*  Default list bucketing directory name. internal use only not for client.  */
//  test acid with vectorization, no combine 
//  node already exists 
//  Get all items into an array and sort them 
//  Wait for the child process to finish 
//  create new union and sort 
//  finally we can create the grouped edge 
//  We do not need to do anything 
//  @@protoc_insertion_point(class_scope:UpdateQueryResponseProto) 
// ************************************************************************************************   Decimal Addition / Subtraction. 
//  5. Setup Expr Col Map 
// since raw data was (possibly) escaped to make split work,  now need to remove escape chars so they don't interfere with downstream processing 
//  get the map for posToVertex 
//  Compactor states (Should really be enum) 
// check privileges 
//  Copy files with retry logic on failure or source file is dropped or changed. 
//  We only consider tables for which we hold either an exclusive   or a shared write lock 
//  Create a new SparkWork for all the small tables of this work 
//  leadership status change happens inside synchronized methods LeaderLatch.setLeadership().   Also we use single threaded executor service for handling notifications which guarantees ordering for   notification handling. if a leadership status change happens when tez sessions are getting created,   the notLeader notification will get queued in executor service. 
//  The next row will require another call to increaseBufferSpace() since this new buffer should be used up. 
//  only one of them 
//  Iterate over the selects search for aggregation Trees.   Use String as keys to eliminate duplicate trees. 
//  The denominator of the TABLESAMPLE clause 
//  un-partitioned table 
//  Now that we have exited read lock it is safe to remove any invalid entries. 
//  Authentication only.   Authentication and integrity checking by using signatures.   Authentication, integrity and confidentiality checking 
//  Following is special cases for different type of subqueries which have aggregate and no implicit group by   and are correlatd   * EXISTS/NOT EXISTS - NOT allowed, throw an error for now. We plan to allow this later   * SCALAR - This should return true since later in subquery remove                rule we need to know about this case.   * IN - always allowed, BUT returns true for cases with aggregate other than COUNT since later in subquery remove          rule we need to know about this case. 
//  Initialize common server configs needed in both binary & http modes 
//  Partitioned table - delete all 
//  required   required   required   required   optional   optional   optional   optional   optional 
//  Test that existing shared_write table with new shared_read coalesces to 
//  The serialized all null key and its hash code. 
//  if the evaluate yields true then pass all rows else pass 0 rows 
//  configuration for the application master 
//  1. HDFS scratch dir 
//  Event 18 
//  Move the specified work from the sparkWork to the targetWork   Note that, in order not to break the graph (since we need it for the edges), 
//  print current state before exiting 
//  Retrieve results 
//  Other integer types not supported yet. 
//  file locations to be searched in the correct order 
//  Add tracking information. Check if source state already known and send out an update if it is. 
//  Finishable state is checked on the task, via an explicit query to the TaskRunnerCallable 
//  with the RS parent based on its position in the list of parents. 
//  the objects that have been printed. 
/*      we have to  add this one manually as for tests the db is initialized via the metastoreDiretSQL     and we don't run the schema creation sql that includes the an insert for notification_sequence     which can be locked. the entry in notification_sequence happens via notification_event insertion.     */
//  error in storage specification 
//  add input path 
//  F | F | F 
//  Construct the EdgeManager descriptor to be used by all edges which need   the routing table. 
// parser only allows foo(a,b), not foo(foo.a, foo.b) 
/*    * Assign a row from a list of standard objects up to a count    */
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setDate(int, java.sql.Date,   * java.util.Calendar)    */
//  when there are no live nodes in the cluster and this timeout elapses the query is failed 
//  required   optional   optional   optional   optional   optional   optional   optional   optional   optional 
//  GCE firewall is set through instance tags 
// First handle special cases.  If one of the special case methods cannot handle it,   it returns null. 
//  This case happens only when pRS key is empty in which case we can use   number of distribution keys and key serialization info from cRS 
//  Do nothing in this case. 
//  path of the FileSinkOperator table is a blobstore path. 
//  aggregate size from aggregation buffers 
//  the union operators from the operator tree later. 
//  since SQL is case insenstive just to make sure that the comparison b/w column names   and check expression's column reference work convert the key to lower case 
//  We can get away with the use of varname here because varname == hiveName for PWD 
//  The assumption here is the path is a file. Only case this is different is ACID deltas.   The isFile check is avoided here for performance reasons. 
//  Integer.MAX_VALUE 
//  these 5 delims passed as serde params   comment passed as table params 
//  The values from HiveIntervalDayTime.getNanos(). 
//  max # of rows = rows from left side 
//  There should only be column in sourceOperator 
//  To avoid denominator getting larger and aggressively reducing   number of rows, we will ease out denominator. 
// ---------------------------------------------------------------------------   Process Single-Column String Left-Semi Join on a vectorized row batch.   
//  get the table from the client again, verify the name has been updated 
// TxnManagerFactory is a singleton, so if the default is true, it has already been  created and won't throw 
//  If hint is provided use that size. 
//  In case the dynamic value resolves to a null value 
//  clean 
//  We will transform using clause and make it look like an on-clause.   So, lets generate a valid on-clause AST from using. 
//  2/ serialize the union - tag/value 
//  for shell commands, use unstripped command 
//  is used as an alias) 
//  try nulls on both sides 
//  to know if the job has finished is to check the futures here ourselves. 
//  alterPartition is only for changing the partition location in the table rename 
//  Create the final Group By Operator 
// check whether the username in the token is what we expect 
//  the task is no longer required, and asks for a de-allocation. 
//  Check the last node 
//  4M   the conf string for COLUMNS_BUFFER_SIZE 
//  keep this within 80 chars width. If more columns needs to be added then update min terminal   width requirement and SEPARATOR width accordingly 
//  Should only be managed tables passed in here.   Check if table is in the default table location based on the old warehouse root.   If so then change the table location to the default based on the current warehouse root.   The existing table directory will also be moved to the new default database directory. 
//  track of which small tables haven't been processed yet. 
//  10^8 - 1 
//  HIVE-6672: In HiveServer2 the JARs for this UDF may have been loaded by a different thread,   and the current thread may not be able to resolve the UDF. Test for this condition 
//  add_partitions(1,2,3) : ok, normal operation 
// create type with nestingLevel levels of nesting 
//  Write final 0-length chunk 
//  We might have multiple ranges coming from children 
//  Serialize the row component using the RowIdFactory. In the normal case, this will just 
//  build a map which tracks the name of column in input's signature to corresponding table column name   this will be used to replace column references in CHECK expression AST with corresponding column name   in input 
//  Specifying the right type info length tells LazyBinaryDeserializeRead which is the last   column. 
//  Pull out Deterministic exprs from non-deterministic and push down   deterministic expressions as a separate filter 
//  convert the metastore thrift objects to result objects 
//  running the MoveTask and MR task in parallel may   cause the mvTask write to /ds=1 and MR task write   to /ds=1_1 for the same partition. 
//  Currently not used in hive code-base, but intended to authorize actions   that are directly user-level. As there's no storage based aspect to this,   we can follow one of two routes:   a) We can allow by default - that way, this call stays out of the way   b) We can deny by default - that way, no privileges are authorized that   is not understood and explicitly allowed.   Both approaches have merit, but given that things like grants and revokes   that are user-level do not make sense from the context of storage-permission   based auth, denying seems to be more canonical here. 
//  Now go the correct way, through objectinspectors 
//  YARN Service has started llap application, now if for some reason   state changes to COMPLETE then fail fast 
//  the parent ops for hashTableSinkOp 
//  simply need to remember that we've seen a union. 
//  If there are no skewed values, nothing needs to be done 
// different locks from same txn should not conflict with each other 
//  Temporary selected vector 
//  Transform CASE WHEN with just a THEN/ELSE into an IF statement. 
//  currently metastore does not store column stats for   partition column, so we calculate the NDV from partition list 
/*       * Sleep until all threads with clean up tasks are completed.       */
//  get_table checks whether database exists, it should be moved here 
//  return the serialized bytes 
//  read the keys and values 
//  check if list elements are primitive or Objects 
//  Create three catalogs 
//  Accurate short value cannot be obtained. 
// export command uses _metadata.... 
//  Don't call builder.setWorkSpecSignature() - Tez doesn't sign fragments 
//  Because of the implementation of the JsonParserFactory, we are sure   that we can get a TezJsonParser. 
/*      * Algorithm:     * 1) Convert decimal to three 56-bit words (three is enough for the decimal since we     *    represent the decimal with trailing zeroes trimmed).     * 2) Skip leading zeroes in the words.     * 3) Once we find real data (i.e. a non-zero byte), add a sign byte to buffer if necessary.     * 4) Add bytes from the (rest of) 56-bit words.     * 5) Return byte count.      */
//  Create temp table directory 
/*    * TIMESTAMP.    */
//  Try one sorted. 
//  Input row resolver 
//  and pass it to setTaskPlan as the last parameter 
//  no child, no need for pruning 
/*  Validate location string.  */
//  Ignore this exception, if there is a problem it'll fail when trying to read or write. 
//  If the drop has to fail on non-existent partitions, we cannot batch expressions.   That is because we actually have to check each separate expression for existence.   We could do a small optimization for the case where expr has all columns and all   operators are equality, if we assume those would always match one partition (which   may not be true with legacy, non-normalized column values). This is probably a   popular case but that's kinda hacky. Let's not do it for now. 
//  tokenSig could be null 
//  Return true if this data type is handled in the output vector as an integer. 
//  otherwise, we don't know what to do so make it a maybe 
//  No SerDes here. 
//  This method is used to validate check expression since check expression isn't allowed to have subquery 
//  Try running a priority 1 task 
//  0 length files cannot be ORC files, not valid for MR. 
//  By default, we can always use the multi-key class. 
//  There was a parallel cache eviction - the evictor is accounting for the memory. 
//  partition or mixed case) 
//  But for now we will just retry. We will evict more each time. 
//  test default table types returned in 
//  Set metastoreOverlay parameters 
//  retrieve the stats obj that was just written 
//  Start of the split falls somewhere within or before this slice.   Note the ">=" - LineRecordReader will skip the first row even if we start   directly at its start, because it cannot know if it's the start or not.   Unless it's 0; note that we DO give 0 special treatment here, unlike the EOF below, 
/*   * (non-Javadoc)  *  * @see  * org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider  * #authorize(org.apache.hadoop.hive.ql.security.authorization.Privilege[],  * org.apache.hadoop.hive.ql.security.authorization.Privilege[])   */
/*          * for the Virtual columns:         * - the internalName is UPPER Case and the alias is lower case         * - since we put them in an OI, the fieldnames became lower cased.         * - so we look in the inputRR for the fieldName as an alias.          */
//  if multiple MOVE happens, only first move will be chosen 
//  If making this public, note that its ordering is undefined. 
//  add any partition key values provided as part of job info 
//  for remote JDBC client, try to set the conf var using 'set foo=bar' 
//  Wait for all the events to be written off, the order of service is important 
// Test for duplicate publish 
//  also initialize a paritioned table to test against. 
//  --------------------------------------------------------------   DATA SHOULD GET SORTED BY YOUR ETL/MERGE PROCESS HERE     Group the data by (partitionValues, ROW__ID.bucketId)   Order the groups by (ROW__ID.writeId, ROW__ID.rowId)   -------------------------------------------------------------- 
//  Throw if the tx wasn't rolled back. 
//  Hour granularity 
// Map<?,?> c6Value = (Map<?,?>) rowValues[5];  assertEquals(0, c6Value.size()); 
//  ELSE expression 
// Tstage is just a simple way to generate test data 
//  use a list for easy cumtomize 
//  Pairwise: InitOuputColHasNulls, InitOuputColIsRepeating, ColumnHasNulls, ColumnIsRepeating 
//  then try to get it from all 
//  conditions for being partition column 
//  this method could go beyond the integer ranges until we scale back   so, we need twice more variables. 
//  Now release a single session from A. 
//  could generate different error messages 
// Check if the record record is already encoded once. If it does  reuse the encoder. 
//  This method takes Object, so it accepts whatever types that are   passed in. 
//  Call this first, then send in an interrupt to the thread. 
//  DELETE_RULE 
//  ROWS 
//  Finally we remove the rest of the expression from the tree 
//  NOTE! It is necessary merge task is the parent of the move task, and not   the other way around, for the proper execution of the execute method of 
//  Verify the union has been hidden and just the main type has been returned. 
// build new environmentContext with ifPurge; 
//  not anti-symmetric 
//  ensure that we are consistent when comparing to the base class 
//  has its ReduceSink parent removed. 
//  deleted, then it is good.  So, the last parameter ifexists is set to true 
//  test repeating case 
//  retrieve delegation token for the given user 
//  check to see that the vertices are correct 
//  Escape the escape, and escape the asterisk 
//  add the tables as well to outputs 
//   /* in case of grouping sets; groupby1 will output values for every setgroup; this is the index of the column that information will be sent */ 
//  Set parameter to be false in connection 1.  int to smallint allowed 
//  all IFace APIs throw TException 
//  TarArchiveOutputStream seemed not to close files properly in error situation 
//  legacy handling 
/*  Release all locks - including PERSISTENT locks  */
//  Should make ['f', 'm\0') 
//  within a range specified. 
//  Check highest digit for rounding. 
//  start removing LRU nodes 
//  suppress here; if it is a real issue will get caught in where clause handling. 
//  The included columns of the reader / file schema that   include ACID columns if present. 
//  LOG.debug(CLASS_NAME + " logical " + logical + " batchIndex " + batchIndex + " Key Continues " + saveKey + " " + saveJoinResult.name()); 
// {2} should be lockid 
//  query will get cancelled before creating 57 partitions 
//  Copy order 
//  Some non-zero offsets. 
//  F | T | T 
//  add shutdown hook to flush the history to history file 
//  store partition key expr in map-targetWork 
// Cancel HCat and JobTracker tokens 
//  We have estimation, lowerbound and higherbound. We use estimation   if it is between lowerbound and higherbound. 
//  Aggregate mode - it should be followed by union   that we need to analyze 
//  Need to preserve currentDate 
//  newer overrides the older. 
//  4. continue analyzing from the child ASTNode. 
//  Cannot create NullWritable instances 
//  Warm up couple times 
//  TODO : simple wrap & rethrow for now, clean up with error 
//  explain analyze is composed of two steps   step 1 (ANALYZE_STATE.RUNNING), run the query and collect the runtime #rows 
//  the the validate input method 
//  Shouldn't happen in getAll. 
//  end of string 
//  Read all the fields and create partitions, SDs and serdes. 
//  merge histograms 
//  Do not allow users to override zero-copy setting. The rest can be taken from user config. 
//  a local temp dir specific to this driver 
//  print inline vertex 
//  overflow is not an error here. it just means "this" is   smaller 
//  Allow undecorated CHAR and VARCHAR to support scratch column type names. 
//  with all input columns repeating 
//  Make sure "this" has enough integer room to accomodate other's integer digits. 
//  DateWritableV2 is mutable, DateStatsAgg needs its own copy 
//  print information about calls that took longer time at INFO level 
/*          * validate input - Both new and old URI should contain valid host names and valid schemes.         * port is optional in both the URIs since HDFS HA NN URI doesn't have a port.          */
//  Enable metric collection for HiveServer2 
/*  Must check that x is not blank because otherwise you could     * get a false positive if the blank value was a value you     * were legitimately testing to see if it was in the set.      */
//  DAG specific counters 
//  could be exponential notations 
//  first search the classpath 
//  If many fileSinkDescs are linked to each other, it is a good idea to keep track of   tasks for first fileSinkDesc. others can use it 
//  positive number, flip the first bit 
//  Do nothing by default 
//  Virtual 
//  PathChildrenCache tried to mkdir when the znode wasn't there, and failed. 
//  production is: double 
//  allow partial partition specification for nonscan since noscan is fast. 
//  Create semijoin optimizations ONLY for hinted columns 
//  Ignore temporary tables 
//  use 3 as the row buffer size to force lots of re-buffering. 
//  not from a sub-query. 
//  both parts are scaling up. easy. Just check overflow. 
//  An exception from runtime that will show the full stack to client 
//  Revert hive.server2.restrict_information_schema to false 
//  TypeCheckProcFactor expects typecheckctx to have unparse translator 
//  3. Finally, try WM. 
//  If the number of joins < number of input tables-1, this is not a star join. 
// run Minor compaction 
//  This is how many bytes we need to store those additonal bits as a VInt. 
//  The index where the current char starts 
/*      * setup SymbolFunction chain.      */
//  Only if DAG is FAILED or KILLED the vertex status is fetched from AM. 
//  Get the RS and TS for this branch 
//  Clean up trash 
//  Map to keep track of which SMB Join operators and their information to annotate their MapWork with. 
//  needed for type parity 
//  Create the Parquet FilterPredicate without including columns that do not exist   on the schema (such as partition columns). 
// 2 from txnid:1, 1 from txnid:2, 1 from txnid:3 
//  SR.SR.acquired Lock we are examining is acquired;  We can acquire   because two shared reads can acquire together and there must be 
//  insert filter operator between target(child) and input(parent) 
// if here it's a non-acid schema file - check if from before table was marked transactional  or in base_x/delta_x_x from Load Data 
//  found a sub-directory at a depth less than number of partition keys   validate if the partition directory name matches with the corresponding   partition colName at currentDepth 
//  full outer join 
//  the resulting privileges need to be filtered on privilege type and   username 
//  authorized to perform action 
//  Check optimized-only hash table restrictions. 
//  Note: the definitions of what ODBC and JDBC keywords exclude are different in different         places. For now, just return the ODBC version here; that excludes Hive keywords          that are also ODBC reserved keywords. We could also exclude SQL:2003. 
//  Determine row schema for TSOP. 
// Configure the output key and value classes. 
//  VALIDATION_LEVEL 
// Test for setting the maximum partition count 
//  6.2 EXPR AS (ALIAS,...) parses, but is only allowed for UDTF's   This check is not needed and invalid when there is a transform b/c   the 
// test random scan 
//  For partitioned table, partitionVals are specified 
//  Oid for SPNego GSS-API mechanism. 
//  The aggregation batch vector needs to know when we start a new batch 
//  sort 
//  1 integer digit; 2 fraction digits.   Trailing zeroes are suppressed. 
// init lock manager 
//  Retrieve from FK side 
//  Dummy registry does not cache information and forwards all requests to metastore 
//  we are only trying to convert a BucketMapJoin to sort-BucketMapJoin. 
// use int as outputTypeInfo 
//  Calculate all the arguments 
//  note: explicit format to use Throwable instead of var-args 
//  Handle synthetic row IDs for the original files. 
//  Use task attempt number from conf if provided 
//  GrpSet Col needs to be constructed 
//  Just insert the record in the usual way, i.e., default to the simple behavior. 
//  Find the argument to the operator which is a constant 
//  if inverse is word-shifted for accuracy, shift it back here. 
//  required   required   required   optional   optional 
//  Keeps track of completed DAGS. QueryIdentifiers need to be unique across applications. 
//  We'll treat this as the aggregate col stats for part1...part9 of tab1, col1 
//  Setters 
//  verifyTable.verify(map); 
//  that would block us. 
//  validate connection 
//  Pick the correct parent, only one of the parents is not   ReduceSink, that is what we are looking for. 
// Check if different encryption zones 
//  3. Get Table Logical Schema (Row Type)   NOTE: Table logical schema = Non Partition Cols + Partition Cols +   Virtual Cols 
//  We've already obtained a lock on the table, don't lock the partition too 
//  Create list of one. 
/*  Unregister a task from the known and running structures  */
//  clean out previous contents 
// note that "update" uses dynamic partitioning thus lock is on the table not partition 
//  Default timestamp format still works? 
//  Read data from the znode for this server node   This data could be either config string (new releases) or server end 
//  get mapping of tables to columns used 
//  We need to store this record (if it is not done yet) in case   we should produce a result 
//  TODO: We don't want some random jars of unknown provenance sitting around. Or do we care?         Ideally, we should try to reuse jars and verify using some checksum. 
//  handle password 
//  We will not try partial rewriting for non-rebuild if incremental rewriting is disabled 
//  In HS2, the client should have been cached already for the common case.   Otherwise, this may actually introduce delay to compilation for the first query. 
//  Job vars 
//  lets take a look at the operator memory requirements. 
/*    * Since the operator tree is a DAG, nodes with mutliple parents will be   * visited more than once. This can be made configurable.    */
// Always inc the batch buffer index 
//  COL_TYPE 
//  We failed to do something that was rendered irrelevant while we were failing. 
//  Confirm the file is really fixed, and replace the old file. 
/*    * StringExpr uses Boyer Moore Horspool algorithm to find faster.   * It is thread-safe, because it holds final member instances only.   * See https://en.wikipedia.org/wiki/BoyerMooreHorspool_algorithm .    */
/*      * add any input columns referenced in WindowFn args or expressions.      */
//  The values from Timestamp.getTime(). 
// txn X write to b0 + b1   txn X + 1 write to b2 + b3  txn X + 2 write to b0 + b1  txn X + 3 write to b2 + b3 
//  Form result from lower and middle words. 
//  if input is not rewritten, or if it produces correlated   variables, terminate rewrite 
/*  id between 23 and 45  */
//  Filter timestamp against long (seconds) or double (seconds with fractional   nanoseconds). 
//  test second IF argument with nulls and repeating 
//  Add nothing more. 
// Out of allocated columns 
//  checking var exists and its value is right 
// HIVE-12631 
// Table name will be lower case unless specified by hbase.table.name property 
//  Open the next file 
//  Maximum tolerable variance in number of partitions between cached node and our request 
//  Drops partitions in batches.  partNotInFs is split into batches based on batchSize   and dropped.  The dropping will be through RetryUtilities which will retry when there is a   failure after reducing the batchSize by decayingFactor.  Retrying will cease when maxRetries 
//  cellValueTransformers is corresponding to the columns.   Its size should be the same as columns.   For example, if a table has two columns, "key" and "value"   we may mask "value" as "reverse(value)". Then cellValueTransformers 
//  ObjectRegistry is available via the Input/Output/ProcessorContext.   This is setup as part of the Tez Processor construction, so that it is available whenever an   instance of the ObjectCache is created. The assumption is that Tez will initialize the Processor   before anything else. 
//  Create SelectOp with granularity column 
//  so directly invoke ExecDriver 
//  Adjust negative result, again doing what SerializationUtils.readBigInteger does. 
//  F | T | F 
//  Need to check if there are overflow digits in the high word. 
/*  List of registered applications  */
/*    * Write a VInt using our temporary byte buffer instead of paying the thread local performance   * cost of LazyBinaryUtils.writeVInt    */
//  For Varchar or char type, return the max length of the type 
//  for the grouping set (corresponding to the rollup). 
/*  @bgen(jjtree) SenumDef  */
//  set the related attributes according to the keys and values 
//  handles cases where the query has a predicate "constant=column-name" 
//  NOTE: We are overwriting the constant vector value... 
//  do not do impersonation in CLI mode 
//  no compression 
//  2.1.1 Record Column Names that we needed stats for but couldn't 
//  complete split futures 
//  4) We check whether one of the operators is part of a work that is an input for   the work of the other operator.       Work1            (merge TS in W1 & W3)        Work1       |                        ->                   |        X     Work2                                         Work2       |                                             |     Work3                                         Work1   
//  Currently, the unions are not merged - each union has only 2 parents. So,   a n-way union will lead to (n-1) union operators. 
//  Original bucket files, delta directories and previous base directory should have been cleaned up. 
//  Return the fully-qualified path of path f resolving the path   through any symlinks or mount point 
//  Create database and table 
//  Simulate different filesystems by returning a different URI 
//  ArrayList 
//  Note : No DDL way to alter a partition, so we use the MSC api directly. 
//  TODO JDK 1.7 
//  the following code is used to collect column stats when   hive.stats.autogather=true 
/* e.g. ON source.t=1        * this is not strictly speaking invlaid but it does ensure that all columns from target        * table are all NULL for every row.  This would make any WHEN MATCHED clause invalid since        * we don't have a ROW__ID.  The WHEN NOT MATCHED could be meaningful but it's just data from        * source satisfying source.t=1...  not worth the effort to support this */
//  Nodes go stale after this 
//  Here only register the whole table for post-exec hook if no DP present   in the case of DP, we will register WriteEntity in MoveTask when the   list of dynamically created partitions are known. 
//  Move the result of getColumns() forward to match the columns of the query   c13   c14   c15   c16   c17 
//  do this to verify that Utilities.removeTempOrDuplicateFiles does not revert to default scheme information 
//  The background operation thread was cancelled 
//  NOTE: distinct expr can be part of of GB key 
//  Note, although HiveProxy has a method that allows us to check if we're being   called from the metastore or from the client, we don't have an initialized HiveProxy   till we explicitly initialize it as being from the client side. So, we have a   chicken-and-egg problem. So, we now track whether or not we're running from client-side   in the SBAP itself. 
//  Split by "/" to identify partition parts 
//  First, throw away digits below round digit. 
//  Add the lb path to the list of input paths 
//  stop the appenders for the operation log 
//  redact sensitive information before logging 
//  40 and 50 
//  Sharing this state assumes splits will succeed or fail to get it together (same FS).   We also start with null and only set it to true on the first call, so we would only do   the global-disable thing on the first failure w/the API error, not any random failure. 
//  load properties from hive configurations, including both spark.* properties, 
//  mark the original as abandoned. Don't need it anymore. 
//  Write record to byte buffer 
//  denom = Product of all NDVs except the least of all 
//  In order to expedite things in a general case, we are not actually going to reopen   anything. Instead, we will try to give out an existing session from the pool, and restart   the problematic one in background. 
//  Iterate through each day of the year, make sure Date/DateWritableV2 match 
// Serialize the output info into the configuration 
//  map b/w table alias   to RowContainer 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#getTypeMap()    */
//  Not async, or wasn't submitted for some reason (failure, etc.) 
//  INT can happen in cases where grouping() is used without grouping sets, in all other cases it should be LONG. 
//  Files created on Windows machines have different line endings   than files created on Unix/Linux. Windows uses carriage return and line feed   ("\r\n") as a line ending, whereas Unix uses just line feed ("\n"). 
/*     LOG.info("Modifying config values for ACID write");    conf.setBoolVar(ConfVars.HIVEOPTREDUCEDEDUPLICATION, true);    conf.setIntVar(ConfVars.HIVEOPTREDUCEDEDUPLICATIONMINREDUCER, 1);    These props are now enabled elsewhere (see commit diffs).  It would be better instead to throw    if they are not set.  For exmaple, if user has set hive.optimize.reducededuplication=false for    some reason, we'll run a query contrary to what they wanted...  But throwing now would be    backwards incompatible.     */
//  on it 
//  new transactions should be allowed to open 
// txns overlap; could replace ws_txnid   with txnid, though any decent DB should infer this  make sure RHS of join only has rows we just inserted as   part of this commitTxn() op  and LHS only has committed txns  U+U and U+D is a conflict but D+D is not and we don't currently track I in WRITE_SET at all 
//  Event 5, 6 (alter: stats update event) 
//  whether to cache current RDD. 
//  Null first 
//  The table was dropped before we got around to cleaning it. 
//  Indicate if the read buffer has data, for example,   when in reading, data on disk could be pull in 
//  tests a multimap structure for PARQUET-26 
//  Some other key (collision) - keep probing. 
//  Safe to cancel delegation tokens now. 
//  Update column expression map 
//  To make (almost) sure we get definite order, touch blocks in order large number of times. 
//  class Builder; 
//  Event 15, 16, 17 
//  if any of bigTableCandidates is from multi-sourced, bigTableCandidates should 
//  VIEW/MATERIALIZED_VIEW 
//  Scan the "output" directory for existing files, and add watches 
//  Note: it's not clear that we need to track this - unlike PoolManager we don't have non-pool         sessions, so the pool itself could internally track the ses  sions it gave out, since 
//  mix functions 
//  create tables and load data 
//  We are calling this here because we expect the method to be completely async. We also don't   want this call itself to go on a thread because we want the percent-to-physics conversion   logic to be consistent between all the separate calls in one master thread processing round.   Note: If allocation manager does not have cluster state, it won't update anything. When the 
/*  @bgen(jjtree) FieldList  */
//  Default value set to 100 milliseconds for test purpose 
//  The auto-commit mode is always enabled for this connection. Per JDBC spec, 
//  Constant propagation optimizer 
// augment source with a col which has 1 if it will cause an update in target, 0 otherwise 
//  QUERY_PARALLELISM 
//  if it is only one digit, add a leading 0. 
//  TODO: we can support date, int, .. any types which would have a fixed length value 
//  Obtain relevant object inspector for this typeinfo 
/*    * If the final PTF in a PTFChain can stream its output, then set the OI of its OutputShape   * to the OI returned by the TableFunctionEvaluator.    */
//  remember rhs table for semijoin 
// note that (X % 0) is illegal and (X % -1) = 0   -1 is a common default when the value is missing 
//  If a time zone is found in file metadata (property name: writer.time.zone), convert the   timestamp to that (writer) time zone in order to emulate time zone agnostic behavior.   If not, then the file was written by an older version of hive, so we convert the timestamp   to the server's (reader) time zone for backwards compatibility reasons - unless the   session level configuration hive.avro.timestamp.skip.conversion is set to true, in which   case we assume it was written by a time zone agnostic writer, so we don't convert it. 
//  Memory manager uses cache policy to trigger evictions, so create the policy first. 
//  find whether exists a local driver to accept the url 
//  shrinked size for this split. counter part of this in normal mode is   InputSplitShim.shrinkedLength.   what's different is that this is evaluated by unit of row using RecordReader.getPos()   and that is evaluated by unit of split using InputSplit.getLength(). 
//  methods need not be called many times. 
//  String including invalid '\000' style literal characters. 
//  Run Spark Dynamic Partition Pruning 
//  And, mark group keys as repeating. 
//  Return true in case one of the children is column expr. 
//  invalid partition exception 
//  Note: by closing stmt object, we are also reverting any session specific config changes. 
//  Query that should return nothing 
//  If a Spark installation is provided, use the spark-submit script. Otherwise, call the   SparkSubmit class directly, which has some caveats (like having to provide a proper 
//  First try our cast method that will handle a few special cases. 
//  getBytes function says: pos the ordinal position of the first byte in   the BLOB value to be extracted; the first byte is at position 1 
//  Save original projection. 
//  StatsTask require table to already exist 
//  TODO error handling; distinguish IO/connection failures,        attribute to appropriate spill output 
//  Convert just the decimal digits (no dot, sign, etc) into bytes.     This is much faster than converting the BigInteger value from unscaledValue() which is no 
//  The table names match, so check on the partitions 
//  verify it 
//  postpone the join processing for this pair by also spilling this big table row. 
//  ... and one without 
/*      * Look for normal match.      */
//  Check if we're operating on the same database, if not, move on 
//  Get the tables in the default database 
//  compareTo() 
//  since we can rely on approx lastAccessTime but don't want a performance hit 
//  only 32bits, but long to behave as unsigned 
//  Cache size 
//  be done only for non-views. 
//  Create a planner with a hook to update the mapping tables when a   node is copied when it is registered. 
//  remove the location of container tokens 
//  At this point, transport must contain client ugi, if it doesn't then its an old client. 
//  Get the error details from the underlying exception 
//  compare sorted strings, rather than comparing exact strings. 
//  do no filtering in old authorizer 
// Change the selected vector 
//  This string constant is used by AlterHandler to figure out that it should not attempt to   update stats. It is set by any client-side task which wishes to signal that no stats 
//  sum size of all files in the partition is smaller than size required 
//  a copy is required to allow incremental replication to work correctly. 
/*  @bgen(jjtree) TypeDouble  */
//  getColumn(), instead of using colValLenBufferReadIn directly. 
//  parent op can understand this expression 
//  If child keys are null or empty, we bail out 
//  No-op when authType is NOSASL 
//  Vectorized implementation of BROUND(Col, N) function 
//  if stats are same, no need to update 
// now convert T to acid 
//  Ignore and hope for the best. 
//  Prepare StringBuilder-s for "in (...)" lists to use in one-to-many queries. 
//  Get the base values w/o cache. 
// 1 databases created 
//  We have just obtained all we needed by splitting some block; now we need   to put the space remaining from that block into lower free lists.   We'll put at most one block into each list, since 2 blocks can always be combined   to make a larger-level block. Each bit in the remaining target-sized blocks count   is one block in a list offset from target-sized list by bit index.   We do the merges here too, since the block we just allocated could immediately be   moved out, then the resulting free space abandoned. 
//  If the stack has been explored already till that level,   obtained cached String 
//  get current bucket file name 
//  Combine NOT operator with the child operator. Otherwise, the following optimization   from bottom up could lead to incorrect result, such as not(x > 3 and x is not null),   should not be optimized to not(x > 3), but (x <=3 or x is null). 
//  re-use existing text member in char writable 
//  considered using URLEncoder, but it seemed too much 
//  max. This table is either the the big table or we cannot convert. 
//  Set port 
//  setValue() should be able to handle null input 
//  NEW_PARTS 
// hl_txnid <> 0 means it's associated with a transaction 
//  create/drop functions are marked as ADMIN functions   Usage of available functions in query are not restricted by sql   standard authorization. 
//  Hash bits don't match. 
//  Need to make sure that the this HiveServer2's session's SessionState is   stored in the thread local for the handler thread. 
//  Try appending to non extendable shard spec 
//  The import specification asked for only a particular partition to be loaded   We load only that, and ignore all the others. 
//  no stripe will satisfy the predicate 
//  for use in DDL statements that require an exclusive lock, 
//  ideally we would like to do this check based on the number of splits   in the absence of an easy way to get the number of splits - do this   based on the total number of files (pessimistically assumming that 
/* On Tez, below (T is transactional), we get the following layoutekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree  ~/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505500035574/warehouse/t/.hive-staging_hive_2017-09-15_11-28-33_960_9111484239090506828-1//Users/ekoifman/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505500035574/warehouse/t/.hive-staging_hive_2017-09-15_11-28-33_960_9111484239090506828-1/ -ext-10000     HIVE_UNION_SUBDIR_1      000000_0          _orc_acid_version          delta_0000001_0000001_0001              bucket_00000     HIVE_UNION_SUBDIR_2      000000_0          _orc_acid_version          delta_0000001_0000001_0002              bucket_00000     HIVE_UNION_SUBDIR_3         000000_0             _orc_acid_version             delta_0000001_0000001_0003                 bucket_0000010 directories, 6 files      */
//  Check if results need to be emitted.   Results only need to be emitted if there is a non-null entry in a table   that is preserved or if there are no non-null entries 
//  If we do a rename for a non-local file, we will be transfering the original   file permissions from source to the destination. Else, in case of mvFile() where we   copy from source to destination, we will inherit the destination's parent group ownership. 
//  Parse out sentences using Java's text-handling API 
//  Authorize all dropped-partitions in one shot. 
//  class HCatPartitionIterator; 
//        if possible 
//  JAVA32_OBJECT + PRIMITIVES1 * 2 + JAVA32_ARRAY; 
//  Read parameters 
//  if we have issues with stats, just scale linearily 
//  mgr == true 
//  user asked for map-side join 
//  Add 1 to counter default rounding 
//  High word gets integer rounding; middle and lower longwords are cleared. 
//  get the destination and check if it is TABLE 
//  don't check version if its a dry run 
//  4.3) Finally, we add SORT clause, this is needed for the UPDATE.         TOK_SORTBY           TOK_TABSORTCOLNAMEASC              TOK_NULLS_FIRST                 .                    TOK_TABLE_OR_COL                       cmv_basetable_2 
//  allowing fallback to default timestamp parsing if custom patterns fail. 
//  metastore returns object type such as global GLOBAL   when no object is specified.   such privileges are not applicable to this authorization mode, so   ignore them 
//  Use SimpleEntry to save the offset and rowcount of limit clause   KEY of SimpleEntry: offset 
//  MergeFileWork is sub-class of MapWork, we don't need to distinguish here 
//  4. Walk through Window Expressions & Construct RexNodes for those, 
//  nothing to unregister 
//  bootstrap, we skip current table update. 
//  first two stripes will satisfy condition and hence single split 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getTables(org.apache.hive.service.cli.SessionHandle, java.lang.String, java.lang.String, java.lang.String, java.util.List)    */
//  use version of existing max segment to generate new shard spec 
//  We assume a DAG is a DAG, and that it's connected. Add direct dependencies. 
//  create the aggregate 
//  No move pending, the allocator can release. 
//  Joda DateTime only has precision to millis, cut off any fractional portion 
//  Add files to compare to the arguments list 
//  Calculate window size 
/*    * Initialize one column's array entries.    */
//  estimate the size of each entry -   a datatype with unknown size (String/Struct etc. - is assumed to be 256   bytes for now). 
//  See if we can arrive at a smaller number using distinct stats from key columns. 
//  Logger to console 
//  3.1 Add Column info for non partion cols (Object Inspector fields) 
//  Get the tag value. 
// when minor compaction runs, we collapse per statement delta files inside a single  transaction so we no longer need a statementId in the file name 
//  NOTE: fetchOne doesn't throw new SQLFeatureNotSupportedException("Method not supported"). 
//  SOURCE_TABLES_UPDATE_DELETE_MODIFIED 
//  Spot check decimal scalar-column modulo 
/*    * Test whether a precision will fit within a decimal64 (64-bit signed long with <= 18 decimal   * digits).    */
//  instantiate empty list so that we don't error out on iterator fetching.   If we're here, then the next check of pos will show our caller that   that we've exhausted our event supply 
//  Use QueryIdentifierProto.newBuilder() to construct. 
// LIB_JARS should only be set if Sqoop is auto-shipped 
// verify data and layout 
//  get the sign of the big decimal 
//  Rather, unexpected usage exception. 
//  No matter whether it has acquired or not, we cannot pass an exclusive. 
//  4. Push down limit through outer join   NOTE: We run this after PPD to support old style join syntax.   Ex: select * from R1 left outer join R2 where ((R1.x=R2.x) and R1.y<10) or 
// save work to be initialized later with SMB information. 
//  SKEWED_COL_VALUES 
//  check if we should use delegation tokens to authenticate   the call below gets hold of the tokens if they are set up by hadoop   this should happen on the map/reduce tasks if the client added the   tokens into hadoop's credential store in the front end during job   submission. 
//  This is default implementation. Locking only works for incremental maintenance   which only works for DB transactional manager, thus we cannot acquire a lock. 
//  if this ast has only one child, then no partition spec specified. 
//  add into conditional task 
//  hashMap += JAVA32_OBJECT 
//  5% tolerance for long range bias and 2.5% for short range bias 
//  HIVE-18977 
//  dump information if call took more than 1 sec (1000ms) 
//  number of rows -1 means that statistics from metastore is not reliable   and 0 means statistics gathering is disabled   estimate only if num rows is -1 since 0 could be actual number of rows 
/*    * PTF variables   *  */
//  We start iterating through the foreign keys. This list might contain more than a single   foreign key, and each foreign key might contain multiple columns. The outer loop retrieves   the information that is common for a single key (table information) while the inner loop   checks / adds information about each column. 
//  Just go from the back and throw away everything we think is wrong; skip last item, the file. 
//  Compare the results 
//  Create the RecordReader 
//  remaining size needed after putting files in the return path list 
//  if the currently read token is a beginning of an array or object, move stream forward   skipping any child tokens till we're at the corresponding END_ARRAY or END_OBJECT token 
// fetch the row inserted after schema is altered and verify 
//  Set the current UGI to a fake user 
//  1. First, start the queries from the queue. 
//  Ignore 
//  now try to invalid alter table 
//  copy a value 
//  Properties passed by the client, to be used in execution hooks. 
//  The datastructure doing the actual storage during mapjoins has some per row overhead 
//  Split around the 'tab' character 
//  of it for a possible series of equal keys. 
//  map-reduce jobs will be run locally based on data size 
//  check isRepeating handling 
//  PartitionList is not evaluated until the run phase. 
//  The original exception is lost.   Not changing the interface to maintain backward compatibility 
//  For simplicity, to always have parents while storing pools in a flat structure, we'll   first distribute them by levels, then add level by level. 
//  Acquire 2nd Txn Batch 
//  All fields have been parsed, or bytes have been parsed.   We need to set the startPosition of fields.length to ensure we   can use the same formula to calculate the length of each field.   For missing fields, their starting positions will all be the same,   which will make their lengths to be -1 and uncheckedGetField will   return these fields as NULLs. 
//  All ReduceSinkOperators in this sub-tree. This set is used when we start to remove unnecessary 
//  the operator stack. The dispatcher generates the plan from the operator tree 
/*      * the rewritten where Clause      */
/*    * construct the ASTNode for the SQ column that will join with the OuterQuery Expression.   * So for 'select ... from R1 where A in (select B from R2...)'   * this will build (= outerQueryExpr 'ast returned by call to buildSQJoinExpr')    */
//  contains aliases from sub-query 
/*  (non-Javadoc)   * This provides a LazyDouble like class which can be initialized from data stored in a   * binary format.   *   * @see org.apache.hadoop.hive.serde2.lazy.LazyObject#init   *        (org.apache.hadoop.hive.serde2.lazy.ByteArrayRef, int, int)    */
//  4. Apply join order optimizations: reordering MST algorithm      If join optimizations failed because of missing stats, we continue with 
//  No SDs, probably a view. 
//  RELY_CSTR 
//  or the same day of the month 
//  E: Lock we are trying to acquire is exclusive 
// assuming this means we are not doing Auth 
//  Unsupported in-memory structure. 
/*  @bgen(jjtree) Typei32  */
//  STRING_VAL 
//  DDLSemanticAnalyzer has already checked if partial partition specs are allowed,   thus we should not need to check it here. 
//  create the dest directory if not exist 
//  Since addition is commutative, we can add in any order. 
//  If it's marked as too many aborted, we already know we need to compact 
//  Add the parameter here if it cannot change at runtime 
//  Now that we reordered QBJoinTrees, update leftaliases of all 
//  If the target table exists and is newer or same as current update based on repl.last.id, then just noop it. 
//  multiple distincts is not supported with skew in data 
//  part is also a virtual column, but part col should not in this   list. 
//  Create parent if it does not exist, recreation is not an error 
//  Now, we need to de-scratchify this location - i.e., get rid of any   _SCRATCH[\d].?[\d]+ from the location. 
//  read the entire data back and see if did everything right 
//  get table 
//  Iterate through all the elements in Pig Schema and do validations as   dictated by semantics, consult HCatSchema of table when need be.  helps with debug messages 
//  add this partition to post-execution hook 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNCharacterStream(java.lang.String,   * java.io.Reader, long)    */
//  Skip method finding if the method name didn't change, and class name didn't change. 
//  create expression tree with type cast from string to timestamp 
//  Just copy the information since there is nothing so far 
//  Iterate over all the fields picking up the nested structs within them 
//  Additional combinations of (long,double)X(column,scalar) for each of the second   and third arguments are omitted. We have coverage of all the source templates   already. 
//  Date and time parts 
//  We store the position of the argument for the function in the input. 
//  When a URI instance is initialized, it creates a bunch of private String   fields, never bothering about their possible duplication. It would be   best if we could tell URI constructor to intern these strings right away.   Without this option, we can only use reflection to "fix" strings in these   fields after a URI has been created. 
//  The grouping set key is present after the grouping keys, before the distinct keys 
//  See javadoc - no need to clean up the cache data anymore. 
// we have "insert into foo(a,b)..."; parser will enforce that 1+ columns are listed if TOK_TABCOLNAME is present 
//  Move on with our counts. 
//  Need to initialize the lock manager 
//  Verify the fetched logs from the beginning of the log file 
// final String query = tabMetaData.getProperty("hive.sql.query"); 
//  enable/disable bitpacking 
//  All must be selected otherwise size would be zero Repeating property will not change. 
//  verify the scratch directories has been cleaned up 
//  BitSet::wordsInUse is transient, so force dumping into a lower form 
// Subset of counters that should be of interest for hive.client.stats.publishers (when one wants to limit their publishing). Non-display names should be used". 
//  UNSCALED 
// clear state from previous txn 
/*  alternate1 = useLazySimpleEscapes  */
//           analyzeDatabaseLoad(dbNameOrPattern, fs, dir);          } 
//  Turn on metastore-side authorization 
//  Marked where the projected expr is coming from so that the types will   become nullable for the original projections which are now coming out 
//  Set the system properties needed by Pig 
//  Interrupt the current thread after 1 second 
//  null gets stored into column g which is a binary field. 
//  since it is a sort-merge join, only follow the big table 
//  1. Fully contained   topOffset + topLimit <= bottomLimit 
//  Backtrack bucket columns of cRS to pRS (if any) 
//  Note: never instantiate a task without TaskFactory.get() if you're not   okay with .equals() breaking. Doing it via TaskFactory.get makes sure   that an id is generated, and two tasks of the same type don't show   up as "equal", which is important for things like iterating over an   array. Without this, DTa, DTb, and DTc would show up as one item in   the list of children. Thus, we're instantiating via a helper method   that instantiates via TaskFactory.get() 
//  Authorization header must have a payload 
//  Retain only valid intersections (discard disjoint ranges) 
//  param had no scheme, so not a URL 
//  Whether the native vectorized map join operator has performed its 
//  whether current batch has any forwarded keys   mapping of index (lined up w/keys) to index in the batch   mapping of index in the batch (linear) to hash result   Size of the current batch. 
//  interface if so they aggregate the size of the aggregation buffer 
//  The unassigned batchIndex for the rows that have not received a non-NULL value yet.   A temporary work array. 
//  Cross with outer join: currently we do not merge 
//  8. Build Calcite Rel 
// non-acid to transactional conversion (property itself) must be mutexed to prevent concurrent writes.   See HIVE-16688 for use cases. 
//  As of writing this, there is no case where this could be false, this is just protection   from possible future changes 
//  This flow is usually taken for REPL LOAD   Our input is the result of a _files listing, we should expand out _files. 
//  For sub-queries, the id. and alias should be appended since same aliases can be re-used   within different sub-queries.   For a query like:   select ...     (select * from T1 a where ...) subq1    join     (select * from T2 a where ...) subq2   .. 
//  For a kerberos setup 
/*  * Interface for a vector map join hash table (which could be a hash map, hash multi-set, or * hash set) for a single byte array key.  */
//  after "."? 
// create this in doAs() so that it gets a security context based passed in 'ugi' 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getDate(java.lang.String,   * java.util.Calendar)    */
//  Why is this even in metastore? 
//  Remove DPP predicates 
//  such as "%abc" 
//  Found an expression that we can try to reduce 
//  Set to 2 so insert and update don't set it off but delete does 
//  Or the existing table is newer than our update. So, don't allow the update. 
//  Precondition check: verify whether the table is created and data is fetched correctly. 
//  Meta-conf cleanup should trigger an event to listener 
//  Bitwise OR the bitvector with the bitvector in the agg buffer 
//  When a new buffer is fetched ResultSet.next() should be called "incrementalBufferRows" more times 
//  if key doesn't contain CF, it's an encoded value from a previous iterator. 
//  one. 
//  This method gets the basic stats from metastore for table/partitions. This will make use of the statistics from   AnnotateWithStatistics optimizer when available. If execution engine is tez or spark, AnnotateWithStatistics   optimization is applied only during physical compilation because of DPP changing the stats. In such case, we   we will get the basic stats from metastore. When statistics is absent in metastore we will use the fallback of 
//  Pre-allocated member for storing index into the hashMapResults for each spilled row. 
// remove this alias from the alias list 
//  print out nth partition key for debugging 
//  1. If the schema is the same then bail out 
/*  * Test that the server code exists, and responds to basic requests.  */
//  This statement will attempt to move kv1.txt out of stickyBitDir as user foo.  HS2 is   expected to return 20009. 
//  DPP. Now look up nDVs on both sides to see the selectivity.   <Parent Ops>-SEL-GB1-RS1-GB2-RS2 
//  additional bits (beyond 31 bits) of the seconds-since-epoch part of timestamp. 
//  Equals 
//  Hive doesn't have an auto-increment concept 
//  Get a list of joins which cannot be converted to a sort merge join   Only selects and filters operators are allowed between the table scan and   join currently. More operators can be added - the method supportAutomaticSortMergeJoin 
//  We have filled HiveDecimal.MAX_PRECISION digits and have no more room in our limit precision   fast decimal.  However, since we are processing fractional digits, we do rounding.   away. 
//  The summary query returns only one row         
//  I couldn't think of a good way to reuse the keys and value objects   without even more allocations, so take the easy and safe approach. 
//  Compute distribution 
//  Arena cannot change after we have marked it as released. 
//  1.3. Set Partition cols in TSDesc 
// the token file location should be first argument of pig 
//  Extraction can be a subset of columns, so this is the projection --   the batch column numbers. 
//  ignore, shutting down anyway 
/*    * Simulate the join by driving the test big table data by our test small table HashMap and   * create the expected output as a multi-set of TestRow (i.e. TestRow and occurrence count).    */
//  No type name difference or adornment. 
//  We have a stream for included column, but in future it might have no data streams.   It's more like "has at least one column included that has an index stream". 
//  dynamic partitioning with custom path; resolve the custom path   using partition column values 
//  The first call to markFailed() should have removed the record from   COMPACTION_QUEUE, so a repeated call should fail 
//  This doesn't get used, but it's still necessary, see 
//  First Segment Granularity has to be here. 
// Is this a filter that should perform a comparison for sorted searches 
//  List of partitions 
/*  base_0000002  bucket_00000  bucket_00001 delete_delta_0000002_0000002_0000  bucket_00000|    bucket_00001 delta_0000001_0000001_0000  bucket_00000  bucket_00001 delta_0000002_0000002_0000     bucket_00000     */
//  These data structures are needed to create the new project 
//  Add the value to the vector 
// ************************************************************************************************   Decimal Validation. 
//  Verify if no create table/function calls. Only add foreign key constraints on table t2. 
//  Case with nulls 
//  change session's default queue to tezq1 and rerun test sequence 
//  after constant folding 
//  separator for open txns   separator for aborted txns 
//  for top constraining Sel 
//  Pick the first host always. Weak attempt at cache affinity. 
//  The key given by user is ignored - in case of Parquet we need to supply null 
//  check table params 
//  Test that adding a file to the remote context makes it available to executors. 
// if set, determines that task is complete. 
//  Assume this is the table we are at now. 
//  Generate a new task 
//  Read big value length we wrote with the value. 
//  Compute the number of values we want to read in this page. 
//  pattern to identify errors related to the client closing the socket early   idea borrowed from Netty SslHandler 
//  SQL standard - return null for zero elements 
//  all columns need to be at least a subset of the parentOfParent's bucket cols 
//  Replace table scan operator 
//  added by the multi group by optimization) 
//  The test table has 500 rows, so total query time should be ~ 500*500ms 
//  We assume the latter is pretty high, so we don't check for now. 
//  Generate the beeline args per hive conf and execute the given script 
//       LOG.info("System.getenv(\"HADOOP_TOKEN_FILE_LOCATION\") =["+  System.getenv("HADOOP_TOKEN_FILE_LOCATION")+"]"); 
//  TSimpleJSONProtocol does not support deserialization.   protocols.add(org.apache.thrift.protocol.TSimpleJSONProtocol.class.getName());   isBinaries.add(false);   additionalParams.add(null); 
//  It was not present in the cache (maybe because it was added by another HS2) 
//  prefer left, cause right might be missing 
//  prettier error messages to frontend 
//  -ddddddddd hh:mm:ss.nnnnnnnnn 
//  If there is a sort-merge join followed by a regular join, the SMBJoinOperator may not   get initialized at all. Consider the following query:   A SMB B JOIN C   For the mapper processing C, The SMJ is not initialized, no need to close it either. 
/*  * Simple one long key map join benchmarks. * * Build with "mvn clean install -DskipTests -Pdist,itests" at main hive directory. * * From itests/hive-jmh directory, run: *     java -jar target/benchmarks.jar org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneLongKeyBench * *  {INNER, INNER_BIG_ONLY, LEFT_SEMI, OUTER} *    X *  {ROW_MODE_HASH_MAP, ROW_MODE_OPTIMIZED, VECTOR_PASS_THROUGH, NATIVE_VECTOR_OPTIMIZED, NATIVE_VECTOR_FAST} *  */
//  recursively remove non-parent task from its children 
//  write out serialized plan with counters to log file 
/*  Object Inspector corresponding to the input parameter.      */
//  Already existing database 
//  Now all txns are removed from MIN_HISTORY_LEVEL. So, all entries from TXN_TO_WRITE_ID would be cleaned. 
//  delta writer 
//  default to origin in given time zone when aligning multi-period granularities 
//  Destroy before returning to the pool. 
//  Set the inferred bucket columns for the file this FileSink produces 
//     ii) if the project is trivial, a raw join 
//  If all of them were true, return true 
//  TODO: also account for Tez-internal session restarts; 
//  JDK 1.7 
//  Test partition listing with a partial spec - ds is specified but hr is not 
//  Rename fails if the file with same name already exist. 
//  stats from metastore only once. 
//  bd is greater than or equal to 1 
//  no dictionary 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#isPoolable()    */
//  this should fail 
//  check rightInputRel contains no correlation 
//  extra capacity in case we overrun, to avoid resizing 
//  Try another table. 
//  set the lock object with a dummy data, and then do a set if needed. 
// Evaluate the predicate expression 
//  sum(VCol*c) 
// INT64 is not yet supported 
//  6. Read data. 
//  timeout after reset 
//  future. 
//  Call the process function 
//  2.2 All the other FKs. 
//  use them. 
// complete T1 transaction (simulate writing to 2 partitions) 
//  We never resize the pool, so assume this is initialization.   If that changes, we might have to make the factory interface more complicated. 
//  Get the user/groups for checking permissions based on the current UGI. 
//  Middle word gets integer rounding. 
/*      * We check UDFs against the supportedGenericUDFs when     * hive.vectorized.adaptor.usage.mode=chosen or none.     *     * We allow all UDFs for hive.vectorized.adaptor.usage.mode=all.      */
/*    * True only for operators which produce atmost 1 output row per input   * row to it. This will allow the output column names to be directly   * translated to input column names.    */
//  To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting. 
// We need to factor this in to prevent overwhelming Spark executor-memory. 
//  1.3. Add all distinct params   NOTE: distinct expr can not be part of of GB key (we assume plan 
//  The canAccept part of this log message does not account for this allocation. 
//  Another task at a higher priority may have come in during the wait. Lookup the   queue again to pick up the task at the highest priority. 
//  For outer joins, since the small table key can be null when there is no match,   we must have a physical (scratch) column for those keys.  We cannot use the   projection optimization used by inner joins above. 
//  BFS 
//  TODO:   1) Handle compound partition keys (partition by k1+k2)   2) When multiple window clauses are present in same select Even if   Predicate can not pushed past all of them, we might still able to   push   it below some of them.   Ex: select * from (select key, value, avg(c_int) over (partition by   key), sum(c_float) over(partition by value) from t1)t1 where value <   10   --> select * from (select key, value, avg(c_int) over (partition by   key) from (select key, value, sum(c_float) over(partition by value)   from t1 where value < 10)t1)t2 
//  Note: we only run "for columns" command and assume no basic stats means no col stats. 
//  5% tolerance for estimated count 
//  they all modify primordial rows 
//  there are no fields in the struct 
//  test path; SQL is enabled and broken. 
//  5s timeout 
//  Once the eventId reaches 5-20-100, then just increment it sequentially. This is to avoid longer values. 
//  If the output column is of type string, initialize the buffer to receive data. 
//  A jump table to figure out whether to wait, acquire,   or keep looking .  Since   java doesn't have function pointers (grumble grumble) we store a   character that we'll use to determine which function to call.   The table maps the lock type of the lock we are looking to acquire to   the lock type of the lock we are checking to the lock state of the lock 
// PTF declarations 
//  load upgrade order for the given dbType 
//  The optimizer will automatically convert it to a map-only job. 
//  construct a path pattern (e.g., /*/*) to find all dynamically generated paths 
//  in the case of proxy users, the getCurrentUser will return the   real user (for e.g. oozie) due to the doAs that happened just before the   server started executing the method getDelegationToken in the MetaStore 
//  UNDONE: Used by tests... 
//  no authority - use default one if it applies 
//  hardcoded for reproducibility. 
// ensure txn timesout 
// unique to the store func and out file name (table, in our case). 
//  path format of segmentOutputPath -- > .../dataSource/interval/version/partitionNum/ 
//  create alias to work which contains the merge operator 
//  Template, <ClassName>, <ValueType> 
//  Output keys and aggregates into the output batch. 
//  Check permisssion on partition dirs and files created 
//  If Druid table is not an external table store the schema in metadata store. 
//  Inserting hive variables 
//  Set ours. 
//  2. Now, read all of the ranges from cache or disk. 
//  ==== HiveServer2 metadata api types ends here ==== // 
//  '-' sign and '.' 
//  that we can only process records 
//  Read from the stream using the protocol for each column in final schema 
//  Get key columns from inputs. Those are the columns on which we will distribute on.   It is also the columns we will sort on. 
// owner = "testOwner1" 
//  Don't enforce during test driver setup or shutdown. 
//  A Statement#cancel after ResultSet#close should be a no-op 
//  For every pattern 
//  MRv2 job tag used to identify Templeton launcher child jobs. Each child job   will be tagged with the parent jobid so that on launcher task restart, all   previously running child jobs can be killed before the child job is launched   again. 
//  throw an error if default value isn't what hive allows 
// start with an empty priv set; 
//  If the CTAS query does specify a location, use the table location, else use the db location 
// http://www.oratable.com/oracle-insert-all/  https://livesql.oracle.com/apex/livesql/file/content_BM1LJQ87M5CNIOKPOWPV6ZGR3.html 
//  Verify that the schemaTool ran pre-upgrade scripts and ignored errors 
//  Use any non-NULL values found; remember the remaining unassigned. 
//  restrict with any ranges found from WHERE predicates. 
//  First INSERT round. 
//  Go over the tables and populate the related structures.   We have to materialize the table alias list since we might 
//  arrays never change. So we will just do a shallow assignment here instead of copy. 
//  fastIsByte returns false. 
//  Come ride the API roller-coaster #2! The best part is that ctx has TezTaskAttemptID inside. 
//  having seen the root operator before means there was a branch in the   operator graph. There's typically two reasons for that: a) mux/demux   b) multi insert. Mux/Demux will hit the same leaf again, multi insert   will result into a vertex with multiple FS or RS operators. 
//  Incremental Repl B -> C with alters on db/table/partition 
//  generate the meta data for key   index for key is -1 
//  Nothing for the zkCreate models 
//  timestamp column/column IF 
//  generate output column names 
//  cleanup 
//  Assignment is the last thing in the try, so if it happen we assume success. 
//  in order to determine the sum field type (precision/scale) for Mode.PARTIAL2 and Mode.FINAL. 
//  the base table for the group by matches the skewed keys 
// case substring from index with length 
//  Swallow the exception and let the call determine what to do. 
/*  * Abstract class for a hash multi-set result.  */
//       hcatDriver.run("drop table "+TBL_NAME);      hcatDriver.run("create table junit_sem_analysis (a int) partitioned by (b string) stored as RCFILE");      hcatDriver.run("alter table "+TBL_NAME+" add partition (b='2010-10-10')");        List<String> partVals = new ArrayList<String>(1);      partVals.add("2010-10-10");        Map<String,String> map = client.getPartition(MetaStoreUtils.DEFAULT_DATABASE_NAME, TBL_NAME, partVals).getParameters();      assertEquals(map.get(InitializeInput.HOWL_ISD_CLASS), RCFileInputStorageDriver.class.getName());      assertEquals(map.get(InitializeInput.HOWL_OSD_CLASS), RCFileOutputStorageDriver.class.getName());    } 
//  create a standard settable struct object inspector. 
/* we acquire all locks for a given query atomically; if 1 blocks, all go into (remain) in                * Waiting state.  wait() will undo any 'acquire()' which may have happened as part of                * this (metastore db) transaction and then we record which lock blocked the lock                * we were testing ('info'). */
//  Validate query materialization (materialized views, query results caching.   This check needs to occur before constant folding, which may remove some 
//  create a new InputFormat instance if this is the first time to see this class 
//  HDFS scratch dir 
//  Dynamic partitioning usecase 
//  val == sign * significand * 2**exponent.   this == sign * unscaledValue / 10**scale.   so, to make val==this, we need to scale it up/down such that:   unscaledValue = significand * 2**exponent * 10**scale   Notice that we must do the scaling carefully to check overflow and 
//  Drop every table in the default database 
//  No parallel edge was found for the given mapjoin op,   no need to go down further, skip this TS operator pipeline. 
//  Move the clock forward 2000ms, and check the delayed queue 
//  Whether number of open transactions reaches the threshold 
//  subscribe feeds from the MoveTask so that MoveTask can forward the list 
//  we merge those that can be merged 
//  Reducers do not benefit from LLAP IO so no point in printing 
// to test initial metadata count metrics. 
//  test null on both sides 
//  DROP 
/*   @Override  public com.esotericsoftware.kryo.io.Output getHybridBigTableSpillOutput(int partitionId) {    throw new RuntimeException("Not applicable");  }   */
//  D3. Calculate Q hat - estimation of the next digit 
//  We currently only give the initial event to the task on the first heartbeat. Given   that the split is ready, it seems pointless to wait, but that's how Tez works. 
// if we got here, it means it's ok to acquire 'info' lock 
// we want to preserve 'columnName' as it was in original input query so that rewrite 
//  the persistent function is discarded. try reload 
//  Record repeating and no nulls state to be restored later. 
//  The session will be restarted and return to us. 
//  this doesn't create partition. 
//  Test long 
//  Column numbers of batch corresponding to expression result arguments 
//  if serialization fails we will throw incompatible metastore error to the client. 
//  If the regex is changed, make sure we compile the regex again. 
//  Reprocess the spilled data 
//  key and value objects are created once in initialize() and then reused   for every getCurrentKey() and getCurrentValue() call. This is important   since RCFile makes an assumption of this fact. 
//  Non-null ConfVar only defined in ConfVars 
//  Expected. 
//  Fail the retry. 
// ************************************************************************************************   Decimal Precision / Trailing Zeroes. 
//  Note, if metrics have not been initialized this will return null, which means we aren't 
//  If no writeIds allocated by txns under txnHwm, then find writeHwm from NEXT_WRITE_ID. 
//  renaming test to make test framework skip it 
//  optional string key = 1; 
//  We will try to reuse this, but session3 is queued before us. 
//  hm.getTable result will not have privileges set (it does not retrieve   that part from metastore), so unset privileges to null before comparing 
//  to handle the different Map definition in Parquet, eg:   definition has 1 group:     repeated group map (MAP_KEY_VALUE)       {required binary key (UTF8); optional binary value (UTF8);}   definition has 2 groups:     optional group m1 (MAP) {       repeated group map (MAP_KEY_VALUE)         {required binary key (UTF8); optional binary value (UTF8);} 
//       LOG.info("Searching for "+dynPathSpec); 
/*  ignore  */
// Inserts are not tracked by WRITE_SET 
//  FileSink cannot be simply cloned - it requires some special processing.   Sub-queries for the union will be processed as independent map-reduce jobs   possibly running in parallel. Those sub-queries cannot write to the same   directory. Clone the filesink, but create a sub-directory in the final path 
//  allow DP 
//  This is a dfs file 
//  This config contains all the configuration that master node wants to provide   to the HCatalog. 
//  If the command has an associated schema, make sure it gets printed to use 
// the Group By args are passed to cardinality_violation to add the violating value to the error msg 
//  Test equals operator for strings and integers. 
//  Code borrowed from VectorReduceSinkOperator 
//  Add the expression into the BloomFilter 
//  Get to the SelectOperator ancestor 
//  Rewrite the above plan:     CorrelateRel(left correlation, condition = true)     LeftInputRel     Project-A (a RexNode)       Aggregate (groupby(0), agg0(),agg1()...)         Project-B (may reference coVar)           Filter (references corVar)             RightInputRel (no correlated reference)   
//  is compressed? 
//  The SecurityContext set by AuthFilter 
//  We check the sizes of neededColumns and partNames here. If either   size is 0, aggrStats is null after several retries. Thus, we can 
//  default to Monday as beginning of the week 
//  if nLines <= 0, read all lines in log file. 
//  same for char 
//  Using a shared instance of the umbilical server. 
//  try again with some different data values 
//  By default we should set sparkCloneConfiguration to true in the Spark config 
//  insert into should skip and increment partition number to 3 
//  add non-null parameters to the schema 
//  static partitions 
/*      * Skim off the exponent.      */
/*    * (non-Javadoc)   *   * @see java.sql.Connection#createStatement(int, int, int)    */
// using old config value tests backwards compatibility 
//  OPERATION_COMPLETED 
/*  Whether the cache has been initialized or not.  */
//  One of these runs at the output of each reducer 
//  Column was not found in table schema. Its a new column 
//  user does not specify queue so use session default 
//  Set "global" member indicating where to store "not vectorized" information if necessary. 
//  Run our value expressions over whole batch. 
//  This is called once per AM, so we don't get the starting duck count here. 
// Authorization checks are performed by the storageHandler.getAuthorizationProvider(), if  StorageDelegationAuthorizationProvider is used. 
//  -> ^(TOK_REPLICATION $replId $isMetadataOnly) 
//  Only used by spillBigTableRow? 
//  AMReporter after the server so that it gets the correct address. It knows how to deal with 
//  We just logged an exception with (in case of JDO) a humongous callstack. Make a new one. 
//  Merge the two closest bins into their average x location, weighted by their heights.   The height of the new bin is the sum of the heights of the old bins.   double d = bins[smallestdiffloc].y + bins[smallestdiffloc+1].y;   bins[smallestdiffloc].x *= bins[smallestdiffloc].y / d;   bins[smallestdiffloc].x += bins[smallestdiffloc+1].x / d *     bins[smallestdiffloc+1].y;   bins[smallestdiffloc].y = d; 
//  Nothing new added to the queue while analyze runs. 
//  if we are running in local mode - then the amount of memory used   by the child jvm can no longer default to the memory used by the   parent jvm 
//  empty out the file 
//  Methods to set/reset getTable modifier 
//  ORIENTATION 
//  get session   update session allocation   kill query   destroy session   restart session   return session back to pool   move session to different pool 
//  Prefix partition with something to avoid it being a hidden file. 
//  Note: all the fields are only modified by master thread. 
//  Create the walker, the rules dispatcher and the context.   create a walker which walks the tree in a DFS manner while maintaining   the operator stack. The dispatcher 
//  If we're inside a replication scope, then the table not existing is not an error. 
//  Must be deterministic order map - see HIVE-8707     => we use Maps.newLinkedHashMap instead of Maps.newHashMap 
//  find the min/max based on the offset and length (and more for 'original') 
//  parameter value not changed to false in connection 2.  int to smallint throws exception 
//  ColumnInfos for table alias "". 
//  32767 % 256 = 255 
//  get all partitions that matches with the partition spec 
//  NULL does equal NULL here. 
//  #reducer is already 1 
//  Try with erroneously generated VOID 
//  Here we create a project for the following reasons:   (1) GBy only accepts arg as a position of the input, however, we need to sum on VCol*c   (2) This can better reuse the function createSingleArgAggCall. 
//  We have extracted the existence from the hash set result, so we don't keep it. 
//  this method should be called by MoveTask when there are dynamic   partitions generated 
// try not to leave any files open 
//  String value = (String)en.getValue(); // does not apply variable   expansion   does variable expansion 
//  Add negative self. 
//  Table location should not be printed with hbase backed tables 
//  it seems that it is not used by anything. 
//  repl metadata export, has repl.last.id and repl.scope=metadata   import repl metadata dump, table metadata changed, allows override, has repl.last.id 
//  May need to setup localDir for re-localization, which is usually setup as Environment.PWD.   Used for re-localization, to add the user specified configuration (conf_pb_binary_stream) 
//  We set the thread local username, in ThriftHttpServlet. 
/*    * HdfsUtils.setFullFileStatus(..) is called from multiple parallel threads. If AclEntries   * is modifiable the method will not be thread safe and could cause random concurrency issues   * This test case checks if the aclEntries returned from HadoopFileStatus is thread-safe or not    */
/*  * Directly deserialize with the caller reading field-by-field a serialization format. * * The caller is responsible for calling the read method for the right type of each field * (after calling readNextField). * * Reading some fields require a results object to receive value information.  A separate * results object is created by the caller at initialization per different field even for the same * type. * * Some type values are by reference to either bytes in the deserialization buffer or to * other type specific buffers.  So, those references are only valid until the next time set is * called.  */
//  Commenting as part of HIVE-12274 != and <> are not supported for CLOBs   tableNames = client.listTableNamesByFilter(dbName, filter, (short) 2);   assertEquals(2, tableNames.size()); 
//  get nanos since [epoch at fromZone] 
/* cloneToWork.containsKey(mapWork) */
//  Change the plan to this structure.     Project-A' (replace corvar to input ref from Join)     Join (left, condition = true)       LeftInputRel       Aggregate(groupby(0), single_value(0), s_v(1)....)         Project-B (everything from input plus literal true)           ProjInputRel 
//  SR: Lock we are trying to acquire is shared read 
//  Find all of the agg expressions. We use a List (for all count(distinct)) 
//  check for partition 
//  Index of where the buffer is; in minAllocation units (headers array). 
// sort the list to get sorted (deterministic) output (for ease of testing) 
//  capture stderr 
// constant, just return 
//  because zero is zero. Need to mention it in Javadoc. 
//  First, just allocate just the output columns we will be using. 
//  in this case we've determined that there's too much data   to prune dynamically. 
//  The return type will be the concatenation of input type and original values type 
//  INSERT OVERWRITE 
//  if call isn't EQUAL type and it has been determined that value generate might be   required we should rather generate value generator 
//  Convert the column to the correct type when needed and set in row obj 
//  This is a valid error message. 
//  For nested sub-queries, the alias mapping is not maintained in QB currently. 
//  Since DemuxOperator may appear multiple times in MuxOperator's parents list.   We use newChildIndexTag instead of childOperatorsTag.   Example:           JOIN             |            MUX           / | \          /  |  \         /   |   \         |  GBY  |         \   |   /          \  |  /           DEMUX   In this case, the parent list of MUX is [DEMUX, GBY, DEMUX],   so we need to have two childOperatorsTags (the index of this DemuxOperator in 
//  Testing no nulls and no repeating 
//  Non aggregate mode - analyze union operator 
//  t1 is inside v1, we should not care about its access info. 
//  PROGRESS_UPDATE_RESPONSE 
//  It's important to read the correct nulls! (in truth, the path is needed for SplitGrouper). 
//  prefix, extend start 
//  Debug only 
//  We run constant propagation twice because after predicate pushdown, filter expressions   are combined and may become eligible for reduction (like is not null filter). 
//  UNIQUE_CONSTRAINTS 
//  We assume this always comes from a user operation that took the lock. 
//  For password based authentication 
//  Build Druid query 
//  Lock ids are unique across the system. 
//  allow this form 
//  we start at index 1, since at 0 is the variable from table column 
//  -------------------------------------------------------------------------------   VERTICES: 03/04            [=================>>-----] 86%  ELAPSED TIME: 1.71 s 
//  Restored the renamed tables 
//  We can only flush after the updateAggregations is done, or the   potentially new entry "aggs"   can be flushed out of the hash table. 
//  Whatever. 
// 012345678901234567890123456789 
// Assert.assertEquals(4, stat.length); 
//  validate the create view statement at this point, the createVwDesc gets   all the information for semanticcheck 
//  First field is the row key. 
//  Move marker according to delta, change delta to 0. 
//  Update stats for transactional tables (MM, or full ACID with overwrite), even   though we are marking stats as not being accurate. 
//  Export is trivially retriable (after clearing out the staging dir provided.) 
// prevent instantiation 
/*      * Now a having clause can contain a SubQuery predicate;     * so we invoke genFilterPlan to handle SubQuery algebraic transformation,     * just as is done for SubQuery predicates appearing in the Where Clause.      */
//  UDAF in filter condition, group-by caluse, param of funtion, etc. 
//  IN clauses need to be combined by keeping only common elements 
//  format: partition=p_val   Add only when table partition colName matches 
//  Create a lock, but send the heartbeat with a long delay. The lock will get expired. 
//  all of the integer types   float and double   string, char, varchar 
// We currently commit after selecting the TXNS to abort.  So whether SERIALIZABLE  READ_COMMITTED, the effect is the same.  We could use FOR UPDATE on Select from TXNS  and do the whole performTimeOuts() in a single huge transaction, but the only benefit  would be to make sure someone cannot heartbeat one of these txns at the same time.  The attempt to heartbeat would block and fail immediately after it's unblocked.  With current (RC + multiple txns) implementation it is possible for someone to send  heartbeat at the very end of the expire interval, and just after the Select from TXNS  is made, in which case heartbeat will succeed but txn will still be Aborted.  Solving this corner case is not worth the perf penalty.  The client should heartbeat in a  timely way. 
//  Check for column encoding specification 
//  Set temp location. 
//  Outer key copying is only used when we are using the input BigTable batch as the output. 
//  Generate the temporary file name 
//  there may be multi distinct clauses for one column 
//  for debugging 
//  check if this table is sampled and needs more than input pruning 
//  set the session configuration 
//  optional bytes token = 2; 
/*    * Serializes a distinctValueEstimator object to Text for transport.   *   * <b>4 byte header</b> is encoded like below 2 bytes - FM magic string to   * identify serialized stream 2 bytes - numbitvectors because   * BIT_VECTOR_SIZE=31, 4 bytes are enough to hold positions of 0-31    */
//  null filters are supported to simplify client code 
//  A step function to increase the polling timeout by 500 ms every 10 sec,    starting from 500 ms up to HIVE_SERVER2_LONG_POLLING_TIMEOUT 
//  bytes. The higher-order bits come from the second VInt that follows the nanos field. 
//  METADATA 
//  the case that return type of the GenericUDF is not boolean, and if not all partition   agree on result, we make the node UNKNOWN. If they all agree, we replace the node 
//  The entry relevant to aborted txns shouldn't be removed from TXN_TO_WRITE_ID as   aborted txn would be removed from TXNS only after the compaction. Also, committed txn > open txn is retained. 
//  Merge the other estimation into the current one 
//  Provide an instance of the code doesn't try to make a real Instance   We just want to test that we fail before trying to make a connector   with null password 
//  For example,      SELECT deptno, COUNT(*), SUM(bonus), MIN(DISTINCT sal)      FROM emp      GROUP BY deptno     becomes        SELECT deptno, SUM(cnt), SUM(bonus), MIN(sal)      FROM (            SELECT deptno, COUNT(*) as cnt, SUM(bonus), sal            FROM EMP            GROUP BY deptno, sal)            // Aggregate B 
//  Positive number 
//  Extract the delegation Token from the UGI and add it to the job 
// Unpartitioned table: 1 row for Delete; Inserts are not tracked in WRITE_SET 
/*        * We need a total of poolThreadCount + 1 threads to start at same. There are       * poolThreadCount threads in thread pool and another one which has started them.       * The thread which sees atomic counter as poolThreadCount+1 is the last thread`       * to join and wake up all threads to start all at once.        */
//  SESSION_ID 
//  Shutdown HiveServer2 if it has been deregistered from ZooKeeper and has no active sessions 
//  There's some bogus code that can modify the queue name. Force-set it for pool sessions. 
//  for avro type, the serialization class parameter is optional 
//  Prepare 
//  if our dbName is null, we're interested in all wh events 
//  One and only one. 
//  init exec and set parameters, included 
//  2nd Txn Batch 
//  Add 'n' rows corresponding to the grouping sets. For each row, create 'n' rows,   one for each grouping set key. Since map-side aggregation has already been performed,   the number of rows would have been reduced. Moreover, the rows corresponding to the   grouping keys come together, so there is a higher chance of finding the rows in the hash   table. 
// outerRR belongs to outer query and is required to resolve correlated references 
//  set up the operator plan 
//  top is distinct, we can always merge whether bottom is distinct or not   top is all, we can only merge if bottom is also all   that is to say, we should bail out if top is all and bottom is distinct 
//  7. HDFS temp table space 
//  in the big table to bucket file names in small tables. 
//  This is serious black magic, as the following 2 lines do nothing AFAICT but without them   the subsequent call to listPartitionValues fails. 
//  Since getBucketHashCode uses this, HiveDecimal return the old (much slower) but   compatible hash code. 
//  operator that handles the output of these, e.g.: JoinOperator). 
//  Remove from the running list. 
/*    * In the non explain code path, we don't need to track Query rewrites.   * All add fns during Plan generation are Noops.   * If the get Rewrite methods are called, an UnsupportedOperationException is thrown.    */
//  This is a mapping of which big table columns (input and key/value expressions) will be 
//  Large cross product: generate the vector optimization using repeating vectorized   row batch optimization in the overflow batch. 
//  Not public since we must have column information. 
// Create the parameter declaration string 
//  map may not contain all sources, since input list may have been   optimized out   or non-existent tho such sources may still be referenced by the   TableScanOperator   if it's null then the partition probably doesn't exist so let's use   table permission 
//  The file is still cached. 
//  Set recursive traversal in case the cached query was UNION generated by Tez. 
//  zero check 
// make it look like streaming API use case 
//  For vectorized reduce-side operators getting inputs from a reduce sink,   the row object inspector will get a flattened version of the object inspector   where the nested key/value structs are replaced with a single struct:   Example: { key: { reducesinkkey0:int }, value: { _col0:int, _col1:int, .. } }   Would get converted to the following for a vectorized input:     { 'key.reducesinkkey0':int, 'value._col0':int, 'value._col1':int, .. }   The ExprNodeEvaluator initialzation below gets broken with the flattened   object inpsectors, so convert it back to the a form that contains the   nested key/value structs. 
//  Keep-alive information. The client should be informed and will have to take care of re-submitting the work.   Some parts of fault tolerance go here. 
//  Object that can take a set of columns in row in a vectorized row batch and serialized it. 
//  Don't change the table object returned by the metastore, as we'll mess with it's caches. 
//  SKEWED_INFO 
// create 1 more delta_x_x so that compactor has > dir file to compact 
//  write key element 
//  We decided to treat this map as regular object. 
//  NEED_MERGE 
//  We return the key itself, since no mapping was available/returned 
// nothing can do here 
/*    * Propagate null values for a two-input operator and set isRepeating and noNulls appropriately.    */
//  Since local jobs are run sequentially, all relevant information is already available   Therefore, no need to fetch job debug info asynchronously 
//  there are nulls, so null array entries are already initialized 
//  This is a combination of the jar stuff from conf, and not from conf. 
//  Don't allow for public. 
//  Boundary case: require at least one non-partitioned column 
//  Generate the new queryId if needed 
//  Fail - trying to set "transactional" to "true" but doesn't satisfy bucketing and Input/OutputFormat requirement 
//  Consider a query like:   insert overwrite table T3 select ... from T1 join T2 on T1.key = T2.key;   where T1, T2 and T3 are sorted and bucketed by key into the same number of buckets,   We dont need a reducer to enforce bucketing and sorting for T3.   The field below captures the fact that the reducer introduced to enforce sorting/   bucketing of T3 has been removed.   In this case, a sort-merge join is needed, and so the sort-merge join between T1 and T2 
//  op.outputVertexName may be null 
// should not happen as we take care of all existing types 
//  Sign - whether interval is positive or negative 
// depending on FileSystem implementation flush() may or may not do anything  
//  Sleep-time in milliseconds, between batches of delegation tokens dropped. 
//  later we can extend this to the union all case as well 
//  We implement this logic using replaceChildren instead of replacing   the root node itself because windowing logic stores multiple   pointers to the AST, and replacing root might lead to some pointers   leading to non-rewritten version 
//  We cannot send the ecb to consumer. Discard whatever is already there. 
//  set create time 
//  Compare start Position 
/*    * DbNotificationListener keys reserved for updating ListenerEvent parameters.   *   * DB_NOTIFICATION_EVENT_ID_KEY_NAME This key will have the event identifier that DbNotificationListener   *                                   processed during an event. This event identifier might be shared   *                                   across other MetaStoreEventListener implementations.    */
//  Make sure the broken signature doesn't work. 
//  Invalid cases 
//  This time, it completes by adding remaining partitions. 
//  queryId of the command   time at which lock was acquired   mode of the lock: EXPLICIT(lock command)/IMPLICIT(query) 
//  shift one 
//  rebuild that is more efficient than the full rebuild. 
/*  !inputColVector1.noNulls && !inputColVector2.noNulls  */
//  Methods to set/reset listPartitionNames modifier 
//  Must send on to VectorPTFOperator... 
//  if its false, we need to close recordReader. 
//  A scratch batch that will be used to play back big table rows that were spilled 
/*    * INT.    */
//  INSERT EVENT to partitioned table with dynamic ADD_PARTITION 
//  Arbitrary... we don't expect caller to hang out for 7+ mins. 
//  Replace with the milliseconds conversion 
//  child 1 is the type of the column 
//  Distribute the available memory between the tasks. 
//  If we are using a test specific database, then we just drop the database, and recreate 
//  Write back the final NULL byte before the last fields. 
//  Need to close the dummyOps as well. The operator pipeline   is not considered "closed/done" unless all operators are   done. For broadcast joins that includes the dummy parents. 
// this simulates that cleaning thread will error out while cleaning the notifications 
//  TODO:pc validate that there are no types that refer to this 
//  test after recovery 
//  Sign, zero, dot, 2 * digits (to support toFormatString which can add a lot of trailing zeroes). 
// if no sort keys are specified, use an edge that does not sort 
//  conversion functions take a single parameter 
//  We will break the uncompressed data in the cache in the chunks that are the size   of the prevalent ORC compression buffer (the default), or maximum allocation (since we   cannot allocate bigger chunks), whichever is less. 
//  Resets the aggregation calculation variable(s). 
// CHARARRAY is unbounded so Hive->Pig is lossless 
//  We could be scheduling a guaranteed task when a higher priority task cannot be   scheduled. Try to take a duck away from a lower priority task here. 
//  close is called by UDTFOperator 
//  Get serializable details of the destination tables 
//  If it is a function, we try to fold it 
/*      * Check.4.h :: For Exists and Not Exists, the Sub Query must     * have 1 or more correlated predicates.      */
//  String should have been truncated. 
/*    * @param currentKey   *          The current key.   * @param currentValue   *          The current value.    */
// tries to get X lock on T6 and gets Waiting state 
//  0.09765625BD * 0.09765625BD * 0.0125BD * 578992BD 
//  same jobs running side-by-side 
//  Finally, evaluate the aggregators 
//  sort bucket names for the big table 
//  Special date formatting functions 
//  see the indexes for colstats in IExtrapolatePartStatus 
//  Now we read the big-endian compacted two's complement int parts   Compacted means they are stripped of leading 0x00s and 0xFFs   This is why we do the intLength/pos tricks bellow   'length' is all the bytes we have to read, after we skip 'skip'   'pos' is where to start reading the current int   'intLength' is how many bytes we read for the current int 
//  Remove this entry from the table usage mappings. 
//  The group spans a VectorizedRowBatch.  Swap the relevant columns into our batch buffers,   or write the batch to temporary storage. 
//  No location should be created for views 
//  This can only happen once at decompress time. 
//  we should update it. Currently it refers to the source database name. 
//  Part or virtual 
//  Verify that getNextNotification() returns all events 
//  update startIndex 
//  If the from and to strings haven't changed, we don't need to preprocess again to regenerate   the mappings of code points that need to replaced or deleted 
//  Get the internal array structure 
/*   Schema provided by user and the schema computed by Pig    * at the time of calling store must match.     */
//  No exception for type checking for simplicity   Constructing the row ObjectInspector: 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getUpdateCount()    */
// populate target 
//  if not from cache, we still need to hook up the plans. 
//  Done with this part. 
//  Give it a value. 
//  We explicitly create immutable lists here as it forces the guava lib to run the transformations   and not do them lazily. The reason being the function class used for transformations additionally   also creates the corresponding replCopyTasks, which cannot be evaluated lazily. since the query 
//  Example from HiveDecimal.add header comments. 
//  We rebuild in-place the selected array with rows destine to be forwarded. 
//  The number of spark tasks executed by the HiveServer2 since the last restart 
//  Test more than one lock can be handled in a lock request 
//  repeated .org.apache.hadoop.hive.serde2.proto.test.IntString lintString = 5; 
//    throw new HiveException("sortMerged is not in sort order and unique");   } 
//  Batch allocation should always happen atomically. Either write ids for all txns is allocated or none. 
//  the probe failed, we must allocate a set of aggregation buffers   and push the (keywrapper,buffers) pair into the hash.   is very important to clone the keywrapper, the one we have from our   keyWrappersBatch is going to be reset/reused on next batch. 
//  DPP work is considered a child because work needs   to finish for it to execute 
//  If we evict a key that is not from this batch, initial i = (-1) + 1 = 0, as intended. 
// newTable has to exist at this point to compile 
//  Using endpoint identification algorithm as HTTPS enables us to do 
//  reset the buffer 
//  Need to update the keys? 
//  Since compilation is always a blocking RPC call, and schema is ready after compilation,   we can return when are in the RUNNING state. 
//  Theoretically the key prefix could be any unique string shared   between TableScanOperator (when publishing) and StatsTask (when aggregating).   Here we use   db_name.table_name + partitionSec   as the prefix for easy of read during explain and debugging. 
// this is to break a tie if insert + delete of a given row is done within the same  txn (so that currentWriteId is the same for both events) and we want the  delete event to sort 1st since it needs to be sent up so that   OrcInputFormat.getReader(InputSplit inputSplit, Options options) can skip it. 
//  Add the entry to the cache structures while under write lock. 
//  Remove any cached results from the previous test. 
/*      * In case of a select, use a fetch task instead of a move task.     * If the select is from analyze table column rewrite, don't create a fetch task. Instead create     * a column stats task later.      */
//  if the task has started, all operators within the task have   started 
//  nothing can be done 
//  Read the altered partition via CachedStore 
//  Read the tag 
//  No partitions need update. 
//  should be "key" and "reverse(value)" 
/*  * Abstract class for a hash set result.  */
//  As columns go down the DAG, the LVJ will transform internal column   names from something like 'key' to '_col0'. Because of this, we need   to undo this transformation using the column expression map as the   column names propagate up the DAG. 
/*  * Common hash code routines.  */
/*     * todo: parse    * convertToAcid_1527286288784.sql make sure it has    * ALTER TABLE default.tflat SET TBLPROPERTIES ('transactional'='true');    * convertToMM_1527286288784.sql make sure it has    * ALTER TABLE default.tflattext SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only');    *  */
//  if o is zero, easy. 
//  Reuse the same type for all. Only Ivy can return more than one, probably all jars. 
//  assert mapJoinPos == 0; 
//  Fail some inserts, so that we have records in TXN_COMPONENTS 
//  rcfile read 
//  Test string column to CHAR literal comparison 
/*      * @return The multi-set count for the lookup key.      */
//  Credentials can change across DAGs. Ideally construct only once per DAG. 
//  initialize load path 
//  NEW_PART 
//  Some Hive features depends on several MR configuration legacy, build and add   these configuration to JobConf here. 
//  Note: for collapse == false, this just sets keysSame. 
//   int length = output.getLength() - offset; 
//  This column is not included 
//  Fail compaction, so that we have failed records in COMPLETED_COMPACTIONS 
//  Note: for now we don't have to setError here, caller will setError if we throw. 
//  resultDec = dec.scaleByPowerOfTen(2);   Assert.assertEquals( 
//  Set up the hook that will disallow creating non-whitelisted UDFs anywhere in the plan.   We are not using a specific hook for GenericUDFBridge - that doesn't work in MiniLlap   because the daemon is embedded, so the client also gets this hook and Kryo is brittle. 
/*      * Do careful maintenance of the outputColVector.noNulls flag.      */
/*    * Checks if the value contains any of the PASSWORD_STRINGS and if yes   * return true    */
// java calendar index starting at 1. 
//  ignore error 
//  reset conf vars 
//  total characters = 4; byte length = 6 
//  if we can not have correct table stats, then both the table stats and column stats are not useful. 
/*      * Returns absolute offset of the match      */
//  take top-k closest neighbors and compute the bias corrected cardinality 
//  whether to optimize union followed by select followed by filesink   It creates sub-directories in the final output, so should not be turned on in systems 
//  Find all the valid cookies associated with the request. 
//  unable to parse the connect command 
//  1) We extract the conditions that can be useful 
//  Now, go back to bed until it's time to do this again 
//  Not using OP stats and this is the first sink in the path, meaning that   we should use TS stats to infer parallelism 
//  repeating 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTimestamp(int)    */
//  Cartesian product of our ranges, to the child ranges 
//  Temp macros are not allowed to have qualified names. 
//  To compare among potentially multiple matches 
//  Insert one more row - this should trigger hive.compactor.delta.pct.threshold to be reached for ttp2 
//  this function will merge csOld into csNew. 
//  BloomFilter: object(BitSet: object(data: long[]), numBits: int, numHashFunctions: int) 
//  FUNCTION_CAT   FUNCTION_SCHEM   FUNCTION_NAME   REMARKS 
//  Throw an exception if the table/partition is bucketed on one of the columns 
/*    * this happens either when the input file of the big table is changed or in   * closeop. It needs to fetch all the left data from the small tables and try   * to join them.    */
//  Ok to run CBO. 
//  In case there are multiple columns referenced to the same column name, we won't 
//  No parsing necessary -- the end is the parent's end.   Move past parent field separator. 
/*    * returns number of lines in the printed throwable stack trace.    */
//  A single source can process multiple columns, and will send an event for each of them. 
//  A match was found, so add the clause to the corresponding list 
//  projections are handled by using generate, not "as" on the Load 
//  Operators that belong to each work 
//  restore the previous properties for framework name, RM address etc. 
//  filters 
//  Tracks containerIds and taskAttemptIds, so can be kept independent of the running DAG. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setNClob(java.lang.String, java.io.Reader,   * long)    */
//  Cleanup baseFsDir since it can be shared across tests. 
//  SHOW LOCKS t14 PARTITION ds='today' 
//  cant happen 
//  get the maxLen 
//  used to hook up unions 
//  utc -> default 
//  explictly ignoring as Getter visibility is ANY for auto-json serialization of Trigger based on getters 
//  Try to deserialize using SerDe class our Writable row objects created by SerializeWrite. 
//  Joda pattern matching expects fractional seconds length to match   the number of 'S' in the pattern. So if you want to match .1, .12, .123,   you need 3 different patterns with .S, .SS, .SSS 
//  The first bounds check requires at least one more byte beyond for 2nd int (hence >=).   Parse the first byte of a vint/vlong to determine the number of bytes. 
//  Conf.get takes care of parameter replacement, iterator.value does not. 
//  create table and load kv1.txt 
//  Used to determine if cleaner thread is already running 
//  table aliased (select a.*, for example) 
//  try again 
// Read the record with **different** record reader ID and **evolved** schema 
//  SessionState.get().setCommandType(HiveOperation.EXPLAIN); 
//  verify that unpartitioned table rename succeeded. 
//  execute set command 
//  Add credential provider password to the child process's environment 
// Scalar queries, should expect value/count less than 1 
//  Expect query to return an error state 
//  LOG.info("VectorKeySeriesMultiSerialized processBatch size " + currentBatchSize + " numCols " + batch.numCols + " selectedInUse " + batch.selectedInUse); 
// ignore this - multiple clients may be trying to create the same partition  AddPartitionDesc has ifExists flag but it's not propagated to   HMSHnalder.add_partitions_core() and so it throws... 
//  If we have some sort of expression tree, try SQL filter pushdown. 
//  Tell ReduceRecordSource to flush last record as this is a reduce   side SMB 
//  Time based counters. If DAG is done already don't update these counters. 
// again 1st split is for base/ 
//  No transactions - just the header row 
// found delete events - this 'location' needs compacting 
//  Determine if this (or previous) is the last slice we need to read for this split. 
//  Get int view of the buffer 
//  From https://msdn.microsoft.com/en-us/library/ms190476.aspx   e1 + e2   Precision: max(s1, s2) + max(p1-s1, p2-s2) + 1   Scale: max(s1, s2) 
//  total characters = 17; byte length = 30 
//  2nd level GB: create a GB (col0, col1, count(c)) for each branch 
/*  * Helper class to generate mocked response.  */
//  The primary's nextRecord is the next value to return 
//  When processing dynamic partitioned hash joins, some of the small tables may not get processed   before the mapjoin's parents are removed during GenTezWork.process(). This is to keep 
/*  @bgen(jjtree) FunctionType  */
//  DB topic - Alan. 
//  figure out the newLocalOrdinal, relative to the newInput. 
//  This should never happen, as we just added the lock id 
// Check if hive returns results correctly 
//  Create STRUCT clause 
//  Plan maybe null if Driver.close is called in another thread for the same Driver object 
//  no column attributes provided - create list of null attributes. 
//  This means that in UPDATE T SET x = _something_   _something_ can be whatever is supported in SELECT _something_ 
//  check that src exists and also checks permissions necessary, rename src to dest 
//  This should fail once it finds out the threshold has been reached 
//  Throw in some canFinish variations just for fun. 
// now copy over the data where isNull[index] is false 
//  Byte.MIN_VALUE 
//  First start HS2 with high message size limit. This should allow connections 
//  this the data copy 
//  SettableTreeReader so that we can avoid this check. 
//  should throw 
/*  tasks finished but some failed  */
//  We try to infer a common primitive category 
//  Types are different, we need to check whether we can convert them to 
//  with repeating 
//  subExpr is the list containing generated IN clauses as a result of this optimization. 
//  decimal_64 column vectors gets the same weight as long column vectors 
//  Not failing the job due to a failure constructing the log url 
//  different from sql compat mode 
//  Hold the aggregation results for each row in the partition   Number of rows processed in the partition. 
/*  the buffer has n+sync bytes  */
//  INFO_TYPE 
//  ResultSet serialization settings 
//  This method is necessary to synchronize lazy-creation to the timers. 
//  Ignore priority. 
//  Here we build an aux structure that is used to verify that the foreign key that is declared   is actually referencing a valid primary key or unique key. We also check that the types of 
//  should only be one object 
//  make array with QBJoinTree : outer most(0) --> inner most(n) 
//  cannot be estimated. sample it at runtime. 
//  The Write Ids returned for the transaction batch is also sequential 
//  total rows to generate   # of rows to cache at most   percentile of extra rows to generate by a different thread 
//  4. Finally, decompress data, map per RG, and return to caller. 
//  --- From here on out we choose whether we *want* to run in llap 
//  since we only have single distinct call 
//  k1 equals k2 
//  .0 returned if the fractional part not set 
//  See if there are additional knownFragments. If there are, more fragments came in   after this cleanup was scheduled, and there's nothing to be done. 
//  If the query here is an INSERT_INTO and the target is an immutable table,   verify that our destination is empty before proceeding 
//  set setting read column ids with an empty list 
//  read from dumpfile and instantiate self 
//  check that all correlated refs in the filter condition are 
//  Calculate number of different entries and evaluate 
//  Alter table can change the type of partition key now.   So check the column name only. 
//  We rely on the fact that poll() checks interrupt even when there's something in the queue.   If the structure is replaced with smth that doesn't, we MUST check interrupt here because   Hive operators rely on recordreader to handle task interruption, and unlike most RRs we 
//  for multi-insert queries. Thus, nodeOfInterest is the FROM clause 
//  4. Read the null terminator. 
//  test when third argument has nulls 
//  Skip writing tags when feeding into mapjoin hashtable   Whether this RS can forward records directly instead of shuffling/sorting 
//  An array of hash multi-set results so we can do lookups on the whole batch before output result 
//  find the root of all custom paths from custom pattern. The root is the   largest prefix in input pattern string that doesn't match customPathPattern 
//  if s is shorter than the required pattern 
//  The session is taken out of the pool, but is waiting for registration. 
//  last work we've processed (in order to hook it up to the current 
//  Test various set methods and copy constructors. 
//  YARN property in Spark on YARN mode. 
//  A task kill while the request is still in PENDING state means the request should be retried. 
//  build the exprNodeFuncDesc with recursively built children. 
//  Sum all non-null long column values for avg; maintain isGroupResultNull; after last row of   last group batch compute the group avg when sum is non-null. 
//  allocate a little extra space to limit need to re-allocate 
//  SERIALIZER_CLASS 
//  find the absolute minimum transaction 
//  1 NULL   NULL NULL 
//  increment cursor for elements per 'IN'/'NOT IN' clause. 
//  The passed argument matches somewhat closely with an accepted argument 
/*    * The following members have context information for the current partition file being read.    */
//  A row format terminated by clause 
//  --property-file <file> 
//  incorrect precision: expected:<1.2[]> but was:<1.2[0]> 
//  get the root operator 
/*    * If no filtering has been applied yet, selectedInUse is false,   * meaning that all rows qualify. If it is true, then the selected[] array   * records the offsets of qualifying rows.    */
//  Created by hint, skip it 
//  We are composing a query that returns a single row if an update happened after 
//  Dimension 
//  Includes the columns that have no data 
//  as it is always using the ROW__ID column. 
/*  Keeps track of vertices from which events are expected  */
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getBytes(java.lang.String)    */
//  create connection as user1 
//  is the binary data. 
//  Return nulls for conversion operators 
//  We cannot backtrack the expression, we bail out 
//  Can't infer a type. 
//  measured in nanoseconds from the epoch. 
//  TOK_SKEWED_LOCATIONS 
//  create a batch with one string ("Bytes") column 
//  did we find the file/dir itself? 
//  Either there was nothing which could be pushed down (size = 0),   there were complex predicates which we don't support yet.   Currently supported are one of the form:   1. key < 20                        (size = 1)   2. key = 20                        (size = 1)   3. key < 20 and key > 10           (size = 2)   Add to residual 
// aggExpr is part of distinct key 
//  Support for dynamic partitions can be added later 
//  Now do the operation with Java BigDecimal 
//  If optimize hive.optimize.bucketmapjoin.sortedmerge is set, add both 
//  Semi join specific. 
//  Reset grace hashjoin context so that there is no state maintained when operator/work is 
//  if the jobtracker signalled that the threshold is not exceeded, 
//  check if they are operating on the same partition, if not, move on. 
//  Flush final partial batch. 
//  check for sub-struct validity 
//  Drop the tables when we're done. This makes the test work inside an IDE 
//  FULL STOP "." (1 byte) 
// create 2 partitions 
//  this can only be possible if there is merge work followed by the union 
//  No problem, use a new name 
//  last character ends a token?   if there are quotes, all the text between the quotes   is considered a single token (this can happen for   timestamp with local time-zone) 
//  Input: do Java/Writable conversion if needed 
//  Right input positions are shifted by newLeftFieldCount. 
//  Value for the flag 
//  APPLY_DISTINCT 
//  In case the expression is a regex COL.   This can only happen without AS clause   We don't allow this for ExprResolver - the Group By case 
//  fileread writes to the writer, which writes to orcWriter, which writes to cacheWriter 
//  uses sampling, which means it's not bucketed 
//  At this point, everything that can be consumed from AppStatusBuilder has been consumed. 
/*    * The absolute offset to the beginning of the key within the WriteBuffers.    */
//  no table could be the big table; there is no need to convert 
//  Filter long/double. 
//  File ownership/permission checks should be done on the new table path. 
// Read the record with **different** record reader ID 
//  execute in process 
//  the last char is an escape char. read the actual char 
//  referenced later. 
//  getAllWork returns a topologically sorted list, which we use to make   sure that vertices are created before they are used in edges. 
//  debug instrumenter - useful in finding which fns get called, and how often 
//  3 (test #readFully(1)): 
//  First check temp tables 
//  Make sure we qualify the name from the outset so there's no ambiguity. 
//  it is a column family   primitive type for Map<Key, Value> can be stored in binary format. Pass in the   qualifier prefix to cherry pick the qualifiers that match the prefix instead of picking   up everything 
/*  partitioned table + query has only pruning filters  */
//  index into a list 
//  Container affinity can be implemented as Host affinity for LLAP. Not required until   1:1 edges are used in Hive. 
//  Check parameter set validity as a public method. 
//  if we didn't have predicate pushdown, read everything 
//  load property file 
//  String comparison is good enough, since its of form date=yyyy-MM-dd 
//  Main path - found it, incRef-ed it. 
//  such as "abc\%de%" 
//  if we reached this condition, we had replication state on record for the   object, but its replacement has no state. Disallow replacement 
//  sets the env variable HADOOP_CREDSTORE_PASSWORD to value defined by HADOOP_CREDSTORE_PASSWORD   sets hadoop.security.credential.provider.path property to simulate default credential 
/*    * Setup our inner join specific members.    */
//  We are going to fail, so it is ok to do expensive stuff. Ranges are broken, play it safe. 
//                total merge cost 
//  We've seen this already. 
//  the fallback from failed SQL to JDO is not possible. 
//  Some joins might be null (see processNode for LeafNode), clean them up. 
//  We do not modify the header here; the caller will use this space. 
//  Skip leading zeroes in word2. 
//  Success 
//  and the new join rel 
//  Note: this path should be specific to concatenate; never executed in a select query.   modify the existing move task as it is already in the candidate running tasks 
// /////// -------- UTILS ------- ///////// 
//  Spill previously loaded tables to make more room 
//  We could not heartbeat the lock, i.e., the operation has finished,   hence we interrupt this work 
//  These will be handled by the output to the table instead. 
//  Copy the tezSessionState from the old CliSessionState. 
//  closing the underlying ByteStream should have no effect, the data should still be   accessible 
//  Fetch the first group for all small table aliases. 
//  stores mappings from local to hdfs location for all resource types. 
//  DESERIALIZER_CLASS 
//  store the type for later retrieval 
//  If the operator is AND, we need to determine if any of the children are   final candidates. 
//  Remove unnecessary expressions 
//  some debug information 
//  get parent schema 
//  string 
//  and must have locations within the table directory. 
//  Not much I can do about it. 
//  the max value of number of zeroes for 64 bit hash can be encoded using   only 6 bits. So we will disable bit packing for any values >6 
//  Will be true if there is a non-null entry 
//  @@protoc_insertion_point(class_scope:SourceStateUpdatedRequestProto) 
//  the structure inside CTE is like this   TOK_CTE   TOK_SUBQUERY   sq1 (may refer to sq2)   ...   TOK_SUBQUERY   sq2 
//  3.1 Obtain UDAF name 
//  preempt only on specific hosts, if no preemptions already exist on those. 
//  PARTITION_LIST 
//  If there is no grouping key and no row came to this operator 
//  PARENT_CATALOG_NAME 
//  Throw away middle and lowest words. 
//  takes place during optimization 
//  Bootstrap load which should also replicate the aborted write ids on both tables. 
//  Default separators are 1-indexed (instead of 0-indexed), thus the separator at offset 1 is   (byte) 2   The separator for the hive row is \x02, for the row Id struct, \x03, and the maps \x04 and 
//  optional .QueryIdentifierProto query_identifier = 1; 
//  Friday 30th August 1985 02:47:02 AM 
//  Read through all values. 
//  reset everything 
// 3. populate the load file work so that ColumnStatsTask can work 
//  update join statistics 
//  DP/LB 
//  If an operator wants to do some work at the end of a group 
//  there should be only 1 dummy object in the RowContainer 
//  Combined: last_access<=3000 and (Owner="Tester" or param1="param2") 
//  Skip the partitions in progress, and the ones for which stats update is disabled.   We could filter the skipped partititons out as part of the initial names query, 
//  Input file has header or footer, cannot be splitted. 
//  Check that the output is done 
//  we create FILTER (sq_count_check(count()) > 0) instead of PROJECT   because RelFieldTrimmer ends up getting rid of Project 
//  read operation mode 
//  Change if we could not retrieve for all partitions 
//  FILE_IDS 
//  parse the response   message   = [authzid] UTF8NUL authcid UTF8NUL passwd' 
//  disconnect the connection to union work and connect to merge work 
//  Convert the search condition into a restriction on the HBase scan 
//  should be merge join 
//  c6:map<int,string> 
//  For the second one, explicitly set a location to make sure it ends up in the specified place. 
//  Array of Byte 
// now make the select produce <regular columns>,<dynamic partition columns> with 
// NOP 
//  Test that opening a JDBC connection to a non-existent database throws a HiveSQLException 
//  First allocation of write id should add the table to the next_write_id meta table   The initial value for write id should be 1 and hence we add 1 with number of write ids   allocated here 
//  x events to insert, last repl ID: replDumpId+3x+y 
//  since new statistics is derived from all relations involved in 
//  Force locality 
//  Add more complex types. 
//  set the stats publishing/aggregating key prefix   the same as directory name. The directory name   can be changed in the optimizer but the key should not be changed 
//  A under-flows if nn is large. 
//  change it to choose the appropriate file system 
//  A null result from AccumuloRangeGenerator is all ranges 
//  there is no join-value or join-key has all null elements 
/*  (non-Javadoc)   * @see org.apache.hadoop.hive.ql.optimizer.Transform#transform   * (org.apache.hadoop.hive.ql.parse.ParseContext)    */
//  Since we have to create space for 1, if we find an expired node we will remove it & 
//  Useful when the type of column vector has not be determined yet. 
//  This parameter is left for compatibility when reading existing configs, to be removed in Druid 0.12. 
//  Note: query priorities, if we add them, might go here. 
//  It seems these two operators can be merged.   Check that plan meets some preconditions before doing it.   In particular, in the presence of map joins in the upstream plan:   - we cannot exceed the noconditional task size, and   - if we already merged the big table, we cannot merge the broadcast 
// When init(true) combine with genResolvedParseTree, it will generate Resolved Parse tree from syntax tree  ReadEntity created under these conditions should be all relevant to the syntax tree even the ones without parents  set mergeIsDirect to true here. 
// DecimalFormat longFormatter = new DecimalFormat("###,###"); 
//  read once to gain access to key and value objects 
//  check the contents of second row 
//  Trigger bootstrap dump which just creates table t1 and other tables (t2, t3) and constraints not loaded. 
// alter table commands require table ownership   There should not be output object, but just in case the table is incorrectly added 
//  add a key for reduce sink 
//  Evaluate the aggregation over one of the group's batches. 
//  Intentionally do nothing 
//  idempotent case and just return. 
/*        * Clear out any rows in the batch from previous partition since we are going to change       * the repeating partition column values.        */
//  TODO: See comments under blacklistNode. 
//  Ensures maps can be deserialized when avro.java.string=String.   See http://stackoverflow.com/a/19868919/312944 for why that might be used. 
//  HS2 
// sorting makes tests easier to write since file names and ROW__IDs depend on statementId  so this makes (file name -> data) mapping stable 
// batching is not enabled. Try to add all the partitions in one call 
//  for non-native table, property storage_handler should be retained 
//  The length of the scratch buffer that needs to be passed to toBytes, toFormatBytes, 
//  We will inherit the name and status from the plan we are replacing. 
//  NEVER call this function without first calling heartbeat(long, long) 
//  Look for tables with empty pattern 
// see commitTxn() for more info on this inequality 
//  Table dropped after "repl dump" 
//  If db cache is not yet prewarmed, add this to a set which the prewarm thread can check   so that the prewarm thread does not add it back 
//  FUTURE: Can we reuse this conversion? 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getTime(java.lang.String)    */
// If current split is from the same file as preceding split and the preceding split has footerbuffer, 
/*          * SubQuery was only condition in where clause          */
//  no filter, no projection. no need to stage 
//  Can make this configurable. 
//  The start of the split was in the middle of the previous slice. 
//  2. Create a new union operator 
//  Set different uri as it is one of the criteria deciding whether to return the same client or not   URIs are checked for string equivalence, even spaces make them different 
//  get the hashtable file and path 
//  get conf from user payload 
//  to be used, please do so 
//  replace the default input & output file format with those found in 
//  The number of single value rows that were generated in the big table batch. 
//  We hit the end after getting optional integer and optional dot and optional blank padding. 
//  Positive number. 
/*  For a given row, put it into proper partition based on its hash value.   * When memory threshold is reached, the biggest hash table in memory will be spilled to disk.   * If the hash table of a specific partition is already on disk, all later rows will be put into   * a row container for later use.    */
//  just creating orc reader is going to do sanity checks to make sure its valid ORC file 
//  Retrieve primary key constraints (cannot be null) 
//  Table Metadata 
//  NUM_FALSES 
//  Finally, if we do not reduce the size input enough, we bail out 
//  drop tables 
//  v[6] -- since integer #5 is always 0, some products here are not included. 
//  Same primitive category but different qualifiers.   Rely on getTypeInfoForPrimitiveCategory() to sort out the type params. 
//  We may need to peel off the GenericUDFBridge that is added by CBO or user 
// for other usually not used types, just quote the value 
//  names 
//  table write id for this operation 
//  3) Return new Project 
//  Note: schema evolution currently does not support column index changes. 
//  This next section repeats the tests of testRightTrimWithOffset with a maxLength parameter that is   exactly the number of current characters in the string.  This shouldn't affect the trim. 
//  Verify the output! 
//  Evaluate children only of scalar is FALSE. 
//  Input fullTableName is of format <db_name>.<table_name> 
//  when it is registered to the system registry. 
//  Deserialize and append new row using the current batch size as the index. 
//  6. Construct aggregation function Info 
//  The id of the tracking node -- must be a SEQUENTIAL node 
//  One dead session with dry-run 
//  Trailing spaces are significant 
//  Right child 
//  @@protoc_insertion_point(enum_scope:SubmissionStateProto) 
//  3. Copy the data. 
//  Apply best effort to fetch the correct table alias. If not   found, fallback to old logic. 
/*  (non-Javadoc)  * @see org.apache.hadoop.mapreduce.RecordReader#getCurrentKey()   */
//  be emitted from join operator will depend on this factor 
//  Note: we no longer call addTaskLocalFiles because all the resources are correctly         updated in the session resource lists now, and thus added to vertices.         If something breaks, dag.addTaskLocalFiles might need to be called here. 
//  prepare output descriptors for the input opt 
//  When Kerberos is enabled, we have to add the Accumulo delegation token to the   Job so that it gets passed down to the YARN/Tez task. 
//  5. All the sessions in use that were not destroyed or returned with a failed update now die. 
//  (create table sets it to empty (non null) structures) 
//  test repeating case, no nulls 
//  We have a cycle! 
//  If the sizes match, prefer the table with fewer partitions 
//  partSpec is an ORDERED hash map   number of dynamic partition columns   number of static partition columns   path name corresponding to SP columns   the root path DP columns paths start from   number of buckets in each partition 
//  Asynchronously shutdown this instance of HiveServer2,   if there are no active client sessions 
//  REPL STATUS should return NULL 
//  We need to figure out the current transaction number and the list of   open transactions.  To avoid needing a transaction on the underlying   database we'll look at the current transaction number first.  If it   subsequently shows up in the open list that's ok. 
//  Note that this doesn't include appId. We assume that all the subsequent instances   of the same user+cluster are logically the same, i.e. all the ZK paths will be reused,   all the security tokens/etc. should transition between them, etc. 
//  2. Insert ReduceSide GB 
//  checkPermissions returns false if query is not found, throws on failure. 
//  Set confOverlay parameters 
//  Validation will later exclude vectorization of virtual columns usage if necessary. 
/*      * Output columns.      */
//  Reference to vectorization description needed for EXPLAIN VECTORIZATION, hash table loading,   etc. 
//  files with write IDs may not be valid. It may affect snapshot isolation for on-going txns as well. 
//  Copy data/files/alltypesorc to workDir 
// the ACL api's also expect the tradition user/group/other permission in the form of ACL 
//  cleanup pathToAliases 
//  The node may have been blacklisted at this point - which means it may not be in the   activeNodeList. 
//  Iterate through any records.   Because our read offset was past the stripe offset, the rows from the last stripe will 
//  Insert entries to TXN_TO_WRITE_ID for newly allocated write ids 
//  Adding keys is PITA - there's no way to plug into timed rolling; just create a new fsm. 
/*  partColsIsNull  */
//  OWNER 
//  Prevent construction. 
/*       Check for conditions that will lead to local copy, checks are:      1. we are testing hive.      2. either source or destination is a "local" FileSystem("file")      3. aggregate fileSize of all source Paths(can be directory /  file) is less than configured size.      4. number of files of all source Paths(can be directory /  file) is less than configured size.   */
//  single-line 
// setting statementId == -1 makes compacted delta files use 
//  we depend on linux openssl exit codes 
//  TODO: We don't do anything for now, just log this for debugging.         We may be able to make use of this later, e.g. for workload management. 
//  Update the LRU node from what we've seen so far 
//  for each input file 
// retest WAITING locks (both have same ext id) 
//  If a operator needs to invoke specific cleanup, that operator can override 
//  The ptned table will not be dumped as getTable will return null 
//  If it is a widening cast, we do not change NDV, min, max 
//  Test to validate that all tables exist in the HMS metastore. 
//  how many rows in this split 
//  No match. 
//  check for existence of table 
//  update non-distinct groupby key or value aggregations: 'KEY._COLx or VALUE._colx' 
//  HIVE-15410: Though there are not restrictions to Hive table property key and it could be any   combination of the letters, digits and even punctuations, we support conventional property   name in WebHCat (e.g. prepery name starting with a letter or digit probably with period (.),   underscore (_) and hyphen (-) only in the middle like auto.purge, last_modified_by etc) 
//  CONSIDER: Looked at the possibility of faster decimal to double conversion by using some             of their lower level logic that extracts the various parts out of a double.             The difficulty is Java's rounding rules are byzantine. 
//  the reduce plan inputs have tags, add all inputs that have tags 
//  number of bits to address registers 
//  are allowed. A interface 'supportMapSideGroupBy has been added for the same 
//  already accounted for 
//  r-----r-- 
//  current partitioned table with last partition replicated denoted by "lastPartitionReplicated" 
//  key is token start index 
//  Try appending segment with conflicting interval 
//  first operator of a reduce task. 
//  2. remove old join op from child set of all the RSs 
//  Methods that return scalar expressions 
//  har://underlyingfsscheme-host:port/archivepath 
//  If the job is not empty (but runs fast), we have to wait until all the TaskEnd/JobEnd 
//  Are we currently performing a binary search 
//  Log final state to CONSOLE_LOGGER 
//  1. Decompose Join condition to a number of leaf predicates 
//  the union operator has been processed 
//  constant list projection of known length 
//  We could have this as a protected method w/no class, but half of Hive is static, so there. 
//  Less frequently set parameter, not passing in as a param. 
//  Bail out 
//  Determine the size of small table inputs 
//  This will make the object completely unusable. Semantics of clear are not defined... 
//  @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.hooks.proto.MapFieldEntry) 
//  must be <= 38. 
//  This method should be called by sub-classes in a @BeforeClass initializer 
/*  * Directly serialize, field-by-field, the BinarySortable format. * * This is an alternative way to serialize than what is provided by BinarySortableSerDe.  */
/*    * In the following evaluate* methods, since we are supporting scratch column reuse, we must   * assume the column may have noNulls of false and some isNull entries true.   *   * So, do a proper assignments.    */
//  hashcode should be positive, flip all the bits if it's negative 
//  immutable 
//  Outer join specific. 
//  if schemaPattern is null it means that the schemaPattern value should not be used to narrow the search 
/*          * for now just adding a true condition(1=1) to where clause.         * Can remove the where clause from the AST; requires moving all subsequent children         * left.          */
//  SMALL_INT_VALUE 
//  This is basically the same as LazySimpleSerDe.serialize. Except that we don't use   Base64 to encode binary data because we're using printable string as delimiter.   Consider such a row "strAQ==\1", str is a string, AQ== is the delimiter and \1 
//  set() not implemented - ignore 
//  they will be file:// URLs 
// look in COMPLETED_TXN_COMPONENTS because driver.run() committed!!!! 
//  We need to check with 'instanceof' instead of just checking   vectorized because the row can be a VectorizedRowBatch when   FetchOptimizer kicks in even if the operator pipeline is not   vectorized 
//  change the children of the original join operator to point to the map   join operator 
//  The number of keys (with sequential duplicates collapsed, both NULL and non-NULL) in the batch. 
// no more timedout txns 
//  Write record to Parquet format 
//  Done with all the things. 
//  whitespace characters, /, {, }, \ 
//  It could be renewed, return that information 
/*    * Returns true if trailing slash is needed to be appended to the url    */
//  4rd task provided no location preference, got host2 since host1 is full and only host2 is left in random pool 
//  Delayed due to temporary resource availability 
//  Spot check Decimal Col-Scalar Modulo 
// log classpaths 
//  Schedule task to invalidate cache entry and remove from lookup. 
//  Need to go in and check if any of the tasks is running in LLAP mode. 
//  Contains results from last processed input record. 
//  Remove the operators till a certain depth. 
//  only worry about getting schema if we are dealing with Avro 
//  full precision 
//  This assumes LLAP cluster owner is always the HS2 user. 
//  The number of files for the table should be same as number of   buckets. 
//  Note: this is a no-op for custom UDFs 
//  set a marker that this conf has been processed. 
//  don't instantiate 
//  get the kind of expression 
//  5%   5% 
//  Get unique skewed value list. 
//  Currently, expressions are not allowed in cluster by, distribute by,   order by or a sort by clause. For each of the above clause types, check 
//  Generate a unique ID for temp table path.   This path will be fixed for the life of the temp table. 
// With nulls and selected 
//  We have to tell apart partitions resulting from spec with different prefix lengths.   So, if we already have smth for the same prefix length, we can OR the two. 
//  See HIVE-11915 for details 
//  The update has failed but the state has changed since then - no retry needed. 
//  Now, the dumped path can be one of three things:   a) It can be a db dump, in which case we expect a set of dirs, each with a   db name, and with a _metadata file in each, and table dirs inside that.   b) It can be a table dump dir, in which case we expect a _metadata dump of   a table in question in the dir, and individual ptn dir hierarchy.   c) A dump can be an incremental dump, which means we have several subdirs   each of which have the evid as the dir name, and each of which correspond   to a event-level dump. Currently, only CREATE_TABLE and ADD_PARTITION are   handled, so all of these dumps will be at a table/ptn level. 
// all inserts should be in baseReader for normal read so this should always be delete delta if not compacting 
//  Patch up the projection list for updates, putting back the original set expressions. 
//  Pick up any system properties that start with "hive." and set them in our config.  This   way we can properly pull any Hive values from the environment without needing to know all 
//  For non-native tables, we need to do an exact match to avoid   HIVE-1903.  (The table location contains no files, and the string   representation of its path does not have a trailing slash.) 
//  of beeline. 
//  Comparison Operations 
//  Only our parent class can call this. 
// if acid is off, there can't be any acid tables - nothing to compact 
//  Check and update partition cols if necessary. Ideally, this should be done   in CreateValue as the partition is constant per split. But since Hive uses   CombineHiveRecordReader and   as this does not call CreateValue for each new RecordReader it creates, this check is   required in next() 
//  function was properly called, but threw it's own exception.   Unwrap it   and pass it on. 
//  if this child of DemuxOperator does not use tag, we just set the oldTag to 0. 
//  Parameter 1 was an array of primitives, so make sure the primitives are strings. 
//  skip MAP processing for the first path element if root is array 
//  ignore 
//  only uses transactional (MM and ACID) tables 
//  the same operator is present 2 times 
//  Set up some base data then stream some inserts/updates/deletes to a number of partitions 
// Test that data inserted through hcatoutputformat is readable from hive 
//  No effect since 10^0 = 1. 
//  If we have a base, the original files are obsolete. 
//  @formatter:off 
//  Close the writer 
//  After bootstrap dump, all the opened txns should be aborted. Verify it. 
//  Which field we are on.  We start with -1 to be consistent in style with 
//  optional int32 aint = 1; 
//  Base scan only 
//  valid merge -- register set size gets bigger (also 4k items  
//  dealing with String type 
//  Move past union separator. 
//    Implementation notes.     1. Since only local file systems are supported, there is no need to use Hadoop      version of Path class.   2. java.nio package provides modern implementation of file and directory operations      which is better then the traditional java.io, so we are using it here.      In particular, it supports atomic creation of temporary files with specified      permissions in the specified directory. This also avoids various attacks possible      when temp file name is generated first, followed by file creation.      See http://www.oracle.com/technetwork/articles/javase/nio-139333.html for      the description of NIO API and      http://docs.oracle.com/javase/tutorial/essential/io/legacy.html for the      description of interoperability between legacy IO api vs NIO API.   3. To avoid race conditions with readers of the metrics file, the implementation      dumps metrics to a temporary file in the same directory as the actual metrics      file and then renames it to the destination. Since both are located on the same      filesystem, this rename is likely to be atomic (as long as the underlying OS      support atomic renames.     NOTE: This reporter is very similar to         org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter.         org.apache.hadoop.hive.metastore.metrics.JsonReporter.         It would be good to unify the two.   
//  Scale fractional digits, dot, integer digits. 
//  a "long" count and a "double" sum. 
//  Can't assume JDK 1.8, so implementing this explicitly.   return Integer.compare(x + Integer.MIN_VALUE, y + Integer.MIN_VALUE); 
//  do not group across files in case of side work because there is only 1 KV reader per   grouped split. This would affect SMB joins where we want to find the smallest key in   all the bucket files. 
//  Populate the group-by keys with the remapped arguments for aggregate A   The top groupset is basically an identity (first X fields of aggregate B's 
//  (containing the archived version of the files) to intermediateArchiveDir 
/*  Objective here is to ensure that when exceptions are thrown in HiveMetaStore in API methods     * they bubble up and are stored in the MetaStoreEndFunctionContext objects      */
//  REPLACE 
//  Evaluation of the bytes Constant Vector Expression after the vector is 
//  CONSIDER: Validate type information 
//  If we use CBO and we may apply masking/filtering policies, we create a copy of the ast.   The reason is that the generation of the operator tree may modify the initial ast,   but if we need to parse for a second time, we would like to parse the unmodified ast. 
//  fields 
//  number of entries to store before being merged to sparse map 
//  Same file, offset, different lengths 
// Pig DATETIME can map to DATE or TIMESTAMP (see HCatBaseStorer#validateSchema()) which  is controlled by Hive target table information 
//  Must call makeLiteral, not makeTimestampLiteral   to have the RexBuilder.roundTime logic kick in 
/*      * (non-Javadoc)     * @see org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver#carryForwardNames()     * Setting to true is correct only for special internal Functions.      */
//  Bucketing. 
//  also remove the '.' after the prefix 
//  destination table if any   true for full ACID table and MM table   should the destination table be written to using ACID 
//  followed by a select star is completely removed. 
//  At this point, we have arrived at the level where we need all the data, and the 
//  otherwise the planner will throw an Exception (different planners) 
//  transaction manager. 
//  numPartitionFields = -1 means random partitioning 
//  null input 
//  Map values can be primitive or complex 
//  When split-update is not enabled, then all the deltas in the current directories   should be considered as usual. 
//  If retrieveCD is false, we do not need to do a deep retrieval of the Table Column Descriptor.   For instance, this is the case when we are creating the table. 
//  This is not a rebuild, we retrieve all the materializations. In turn, we do not need   to force the materialization contents to be up-to-date, as this is not a rebuild, and   we apply the user parameters (HIVE_MATERIALIZED_VIEW_REWRITING_TIME_WINDOW) instead. 
//  PART_VALS 
//  get all the variable names being converted to regex in HiveConf, using reflection 
//  in theory the include path should come from the configuration 
//  this should be connection url,username,password,query,column1[,columnn]* 
//  required   required   required   required   optional   optional 
//  we trim the Clob value to a max length an int can hold 
//  Test string "A" 
//  number of partitions for the chosen big table 
/*  1 files x 1000 size for 1 splits  */
//  Hence, the uncovered buckets do not have any relevant data and we can just ignore them. 
//  The update options for outside the lock - see below the synchronized block. 
//  Non-hive catalogs should not be transactional 
//  Start up this ZK server 
//  Only a scale adjustment is needed. 
/*  Iterate the global (keywrapper,aggregationbuffers) map and emit       a row for each key  */
//  also allow lower-case versions of all the keywords 
//  1. Run a query against a non-ACID table, and we shouldn't have txn logged in conf 
//  used for GenericUDAFEvaluator 
// bucketed just so that we get 2 files 
//  update changed properties (stats) 
//  Add ops to existing collection 
//  Event 19, 20, 21 
//  start a log cleaner at the start of each test 
//  boolean that says whether the data distribution is uniform hash (not java HashCode) 
//  We have kerberos credentials 
//       runStatementOnDriver("create table T like " + Table.ACIDTBL); 
//  Input provides the definition of a correlated variable. 
//  Copy uncompressed data to cache.   Put call moves position forward by the size of the data. 
//  CONFIGURATION 
//  1.1. If it is not a RexCall, we bail out 
//  DATE conversions supported by GenericUDFDecimal. 
//  call-1: listLocatedStatus - mock:/mocktable   call-2: check existence of side file for mock:/mocktable/0_0   call-3: open - mock:/mocktable/0_0   call-4: check existence of side file for mock:/mocktable/0_1   call-5: open - mock:/mocktable/0_1 
//  IS_SUPPORTED 
// add the previous nextKVReader back to queue 
//  Set host name in conf 
//  Verify that the provided object inspector can pull out these same values 
/*    * Reset the previously supplied buffer that will receive the serialized data.    */
//  Test the length first so in most cases we avoid doing a byte[]   comparison. 
/* overwrite */
// checked for overflow based on the outputTypeInfo 
//  Init conf 
//  multi-small-table-valued) indexes during a process call. 
//  each iterator creates a level of encoding. 
//  check the input to projRel is an aggregate on the entire input 
//  4) the table was not initially created with a specified location 
//  Add a table via ObjectStore 
//  Unexpected error, placeholder tag is not found, throw 
//  add permanent UDFs being used 
//  return (hiveSite - jobConf); 
//  CREATE_TABLE EVENT with multiple partitions 
//  1) Cancel the kills if any, to avoid killing the returned sessions.      Also sets the count for the async initialization. 
//  Load Hash table for Bucket MapJoin 
//  check cor var references are valid 
//  and the logs 
//  to output instead of input, adding owner requirement on output will catch that as well 
// initialize the rowbatchContext 
// check if there is data in the resultset 
// 0. initialization 
// still working 
//  Input operator is not in the same position 
//  Call the metastore to get the currently queued and running compactions. 
//  7. Projection Pruning (this introduces select above TS & hence needs to be run last due to PP) 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setNull(int, int, java.lang.String)    */
//  this is total free memory available per executor in case of LLAP 
//  add an interceptor that adds the X-Forwarded-For header with given ips 
//  Not needed without semi-join reduction or mapjoins or when semijoins   are enabled for parallel mapjoins. 
//  Cleanup the dag lock here, since it may have been created after the query completed 
//  The reader schema always comes in without ACID columns. 
//  HIGHTEST_TXN_ID 
//  If this is a split-update, we initialize a delete delta file path in anticipation that   they would write update/delete events to that separate file.   This writes to a file in directory which starts with "delete_delta_..."   The actual initialization of a writer only happens if any delete events are written  to avoid empty files. 
//  Check explicit pool specifications - valid cases where priority is changed. 
/*    * Extract value from a comma-separated key=value pairs    */
//  10. Attach CTAS/Insert-Commit-hooks for Storage Handlers 
//  for alter partition events 
//  both inputs repeat 
//  field.type < 0 means that this is a faked Thrift field, e.g.,   TControlSeparatedProtocol, which does not   serialize the field id in the stream. As a result, the only way to get   the field id is to fall back to   the position "i".   The intention of this hack (field.type < 0) is to make   TControlSeparatedProtocol a real Thrift prototype,   but there are a lot additional work to do to fulfill that, and that   protocol inherently does not support 
//  May be resized later. 
//  singleAggRel produces a nullable type, so create the new 
//  let it create 64 more partitions (total 57 + 64 = 121) without any triggers 
//  1. We extract the information necessary to create the predicate for the new      filter 
//  Error stream always uses the default serde with a single column 
//  Hive requires this TaskAttemptId to be unique. MR's TaskAttemptId is composed   of "attempt_timestamp_jobNum_m/r_taskNum_attemptNum". The counterpart for   Spark should be "attempt_timestamp_stageNum_m/r_partitionId_attemptNum".   When there're multiple attempts for a task, Hive will rely on the partitionId   to figure out if the data are duplicate or not when collecting the final outputs   (see org.apache.hadoop.hive.ql.exec.Utils.removeTempOrDuplicateFiles) 
//  Multiple newTags can point to the same child (e.g. when the child is a JoinOperator).   So, we first check if childInputObjInspectors contains the key of childIndex. 
//  no-arg ctor required for Kyro serialization 
//  Drop a named primary key 
//  prefix is of the form dbName.tblName 
//  check if source partition exists 
//  Singleton behaviour: create the cache instance if required. 
//  This yields empty because starting index is out of bounds 
//  Inspect the output type of each key expression.  And, remember the output columns. 
//  try isRepeating path (left input only), no nulls 
//  2 Add Direction token 
//  Does any operator in the tree stop the task from being converted to a conditional task 
/*  * An single LONG key hash map optimized for vector map join.  */
//  GRANT_INFO 
/*    * Element for Key: byte[] x Hash Table: HashMap    */
//  Results 
// map max nesting level is one less because it uses an additional separator 
//  Project can also generate constants. We need to include them. 
//  first argument is charCount, which is consumed here 
//  the correct plugin. 
//  Hive doesn't have an Enum type, so we're going to treat them as Strings.   During the deserialize/serialize stage we'll check for enumness and 
//  a lock is used for synchronizing the state transition and its associated   resource releases 
// get all parents of reduce sink 
//  Give it some time, then don't delay shutdown too much. 
//  id column, we cannot push the predicate 
//  De-allocated now   Will be de-allocated later. 
//  No group key. 
//  field present in partition but not in table 
//  count(*) cares not about NULLs nor selection 
//  for IN clause 
/*        End of cleanup     */
// do nothing currently 
//  1. Update Col Stats Map with col stats for columns from left side of 
//  FUNCTION_TYPE 
//  small table HTS. But, since it's idempotent, it should be OK. 
//  unknown type 
//  if the table is in a different dfs than the partition, 
//  remove the last ", " 
//  For comparison purposes, we can scale away those digits.  And, we can not scale up since   that could overflow. 
//  It simplifies things to just add default ones for partitions. 
//  ORDER 
//  Nothing matched. See comment at top. 
//  scaled value might be not equal, but after scaling it should. 
//  ignore NSOE because that means there's nothing to drop. 
//  1 Pending task which is not finishable 
//  Recurse 
//  Deprecated Hive values that we are keeping for backwards compatibility. 
//  Assuming this is only being done for join keys. As a result we shouldn't have to recursively   check any nested child expressions, because the result of the expression should exist as an 
//  Only fetch the table if we have a listener that needs it. 
//  be removed if this becomes a performance issue. 
//  must reuse super as info.getPassword is not accessible 
//  The value is before the offset.  Make byte segment reference absolute. 
//  If we don't have a file cache, we will add this one as is. 
/*    * Support for null constant object    */
//  or it is not up to date. 
//  Re-enable the node if preempted 
//  a deadlock possible in extreme cases if not handled. This will be detected by heartbeat. 
//  Regardless of acquired or waiting, one shared write cannot pass another. 
//  If `currentUnionOperators` is not empty, it means we are creating BaseWork whose operator tree   contains union operators. In this case, we need to save these BaseWorks, and remove 
//  Need to spill from write buffer to disk 
//  If replicating, then the partition already existing means we need to replace, maybe, if   the destination ptn's repl.last.id is older than the replacement's. 
//  str is empty string 
//  We remove the limit operator 
//  Only used in bucket map join. 
//  There should be 2 delta dirs, plus 1 base dir in the location 
//  this call is deleting partitions that are already missing from filesystem   so 3rd parameter (deleteData) is set to false   msck is doing a clean up of hms.  if for some reason the partition is already 
//  Either DELAYED_RESOURCES or DELAYED_LOCALITY with an unknown requested host.   Request for a preemption if there's none pending. If a single preemption is pending,   and this is the next task to be assigned, it will be assigned once that slot becomes available. 
//  the map_field is to test multiple level map definition 
//  Use the colfam and colqual to get the value 
//  get a evaluator for a simple field expression 
//  If the output has some extra fields, set them to NULL. 
//  Get the total number of columns selected, and for each output column, store the   base table it points to. For   insert overwrite table T3   select T1.key, T1.key2, UDF(T1.value, T2.value)   from T1 join T2 on T1.key = T2.key and T1.key2 = T2.key2   the following arrays are created   [0, 0, 0, 1] --> [T1, T1, T1, T2] (table mapping) 
//  There are separate configuration parameters to control whether to   merge for a map-only job 
// put existing column in new list to make sure it is in the right position 
//  Can never have more than this in elements. 
//  LOJ Join preserves LHS types 
/*  100 files x 100 size for 9 splits  */
//  Perform some sanity checks on the arguments. 
//  File operations failed 
//  Leave this ctor around for backward compat. 
//  The value will have already been set before we're called, so don't overwrite it 
//  Reader is using a blocking socket .. interrupt it. 
//  AGENT_INFO 
//  HEARTBEAT_COUNT 
//  Later the properties have to come from the partition as opposed   to from the table in order to support versioning. 
//  remove the tag from key coming out of reducer   and store it in separate variable. 
//  This first section repeats the tests of testRightTrimWithOffset with a large maxLength parameter. 
//  creates objects in recursive manner 
//  v[6] -- since left integer #5 is always 0, some products here are not included. 
//  change the value for the next instance. 
//  though intnum5 is handed as a Byte by hcat, the map() will emit it as 
//  TEMPORARY 
//  ensure that both of the partitions are in the complete list. 
//  guaranteed that there is only 1 list within this list because   a reduce sink always brings down the bucketing cols to a single list. 
//  no base, only deltas 
//  marker comment to look at stats read ops in target/surefire-reports/*-output.txt 
//  Safety check: if we are merging join operators and there are post-filtering   conditions, they cannot be outer joins 
//  since we were provided a qualifier prefix, only accept qualifiers that start with this   prefix 
// http://www.postgresql.org/docs/9.0/static/sql-select.html 
//  Not an EXTERNAL table 
//  used only for insert events, this is the number of rows held in memory before flush() is invoked 
//  remove env var that would default child jvm to use parent's memory   as default. child jvm would use default memory for a hadoop client 
//  No elements 
//  set state in current thread 
//  We could verify precisely at write time, but just do approximate at allocation time. 
//  4/ write element by element from the list 
//  because we already confirm that the stats is accurate   it is impossible that the column types have been changed while the   column stats is still accurate. 
// (timeout = 10000) 
//  get all the values from getXXX methods 
//  Map of String to String 
// add the token to the clientUgi for securely talking to the metastore 
//  colAccessInfo is set only in case of SemanticAnalyzer 
//  exceptional use case for avro 
//  "[]" : LSQUARE/INDEX Expression 
//  If there are no grouping keys, grouping sets cannot be present 
//  TOKEN_IDENTIFIER 
//  ifExists is currently verified in DDLSemanticAnalyzer 
//  Strip off the file type, if any so we don't make:   000000_0.gz -> 000000_0.gz_copy_1 
//     readHiveDecimal.equals(dec)); 
//  The dests can have different non-distinct aggregations, so we have to iterate over all of 
//  @@protoc_insertion_point(class_scope:GetTokenResponseProto) 
//  There should be no valid txns in newer list that are not also in older.   - All values in oldInvalidIds should also be in newInvalidIds.   - if oldHWM < newHWM, then all IDs between oldHWM .. newHWM should exist in newInvalidTxns.     A Gap in the sequence means a committed txn in newer list (lists are not equivalent) 
// derby, oracle 
//  No-op 
// test success if exception caught 
//  The validateGroupByOperator method will update vectorGroupByDesc. 
/*      * Add these 3 values:     *     * mixedUp     * green     * NULL      */
//  Right pad longer strings with multi-byte characters. 
//  OBJECT_NAME 
//    Rewrite logic:     1. change the collations field to reference the new input.   
//  Set output column vector entry.  Since we have one output column, the logical index = 0. 
//  More than 38 digits. 
//  Third time. 
//  get all parent tasks 
//  check if the columns, as well as value types in the partition() clause are valid 
//  Setup timeouts for various services. 
//  Declared cursor 
//  fields populated from builder 
//  Simple implementation for now - currently Parquet uses heap buffers. 
//  Start the CachedStore update service 
//  NOTE: OldHiveDecimal.toFormatString returns decimal strings with more than > 38 digits! 
//  This method should not by synchronized. Can lead to deadlocks since it calls a sync method.   Meanwhile the scheduler could try updating states via a synchronized method. 
// gets S lock on T8 
//  If table properties do not match, we currently do not merge 
//  Build a map to map the original FileSinkOperator and the cloned FileSinkOperators 
//  optional bytes history_text = 3; 
//  load data into table   NOTE: filepath has to be local to the hive server 
/*      * Challenge: How to do the math to get this raw binary back to our decimal form.     *     * Briefly, for the middle and upper binary words, convert the middle/upper word into a decimal     * long words and then multiply those by the binary word's power of 2.     *     * And, add the multiply results into the result decimal longwords.     *      */
//  since we only have one MM table with data - we don't compact MM tables. 
// If so, peel off. Otherwise return itself. 
//  Not valid range, add to residual 
// gets S lock on T7 
//  total characters = 13; byte length = 24 
//  test with schema evolution and include 
//  Also creates the root directory 
//  if admin has already customized this list, honor that 
//  Try with supplementary characters 
//  if the max column width is too large, reset it to max allowed Column width 
//  1. gather references from original query   This is a map from aliases to references.   We keep all references as we will need to modify them after creating 
//  Already verified that we should have the rowId mapping 
//  add this task into task tree   set all parent tasks 
//  if this filter is generated one, predicates need not to be extracted 
//  two int   one double   two Random 
//   Since partVal is a constant, it is safe to cast ExprNodeDesc to ExprNodeConstantDesc.    Its value should be in normalized format (e.g. no leading zero in integer, date is in    format of YYYY-MM-DD etc) 
//  select count(1) 
//  essential properties that shouldn't be overridden by users 
//  CATALOG_NAME 
//  don't generate for null-safes. 
//  Temporary map so we only create one partition context entry. 
// for minor compaction, there is no progress report and we don't filter deltas 
//  this as is so the functionality matches. 
// gets S lock on T6 
//  add writer.time.zone property to file metadata 
// add all function arguments to a map 
//  set to 1 for single threading 
//  just write out the value as-is 
//  Now read the relative offset to next record. Next record is always before the 
//  // This may happen for queries like select 1; (no source table) 
//  we can now retry adding key/value into hash, which is flushed.   but for simplicity, just forward them 
// create LazyBinary initialed with inputBA 
//  It is weird but we need a placeholder,   otherwise rename cannot move file to the right place 
//  Check for it to be recreated. 
// catch InterruptedException to make sure locks can be released when the query is cancelled. 
//  Read "tbl2" via CachedStore 
//  Should never reach here unless there were no failed tasks. 
/*    * TODO: Some thoughts here : We have a current todo to move some of these methods over to   * MessageFactory instead of being here, so we can override them, but before we move them over,   * we should keep the following in mind:   *   * a) We should return Iterables, not Lists. That makes sure that we can be memory-safe when   * implementing it rather than forcing ourselves down a path wherein returning List is part of   * our interface, and then people use .size() or somesuch which makes us need to materialize   * the entire list and not change. Also, returning Iterables allows us to do things like   * Iterables.transform for some of these.   * b) We should not have "magic" names like "tableObjJson", because that breaks expectation of a   * couple of things - firstly, that of serialization format, although that is fine for this   * JSONMessageFactory, and secondly, that makes us just have a number of mappings, one for each   * obj type, and sometimes, as the case is with alter, have multiples. Also, any event-specific   * item belongs in that event message / event itself, as opposed to in the factory. It's okay to   * have utility accessor methods here that are used by each of the messages to provide accessors.   * I'm adding a couple of those here.   *    */
//  @@protoc_insertion_point(builder_scope:UpdateQueryResponseProto) 
//  IDs used to ensure two TaskInfos are different without using the underlying task instance. 
//  LazyString has no so-called NULL sequence. The value is empty string if not. 
//  default is to return earliest possible state. 
//  if they are different, we throw an error 
//  Check for timed out remote workers. 
//  Returns the result File object which will contain the query results 
//  Try to merge multiple ranges together 
//  If it's a void, we change the type to a byte because once the types   are run through getCommonClass(), a byte and any other type T will   resolve to type T 
//  Methods to set/reset caller checker 
//  - Semi-shared for UPDATE/DELETE. 
//  Extrapolation is not needed. 
//  COL_STATS 
//  the vertex that this operator output to 
//  We assume here nobody will try to get session before open() returns. 
//  Create a list of topOp nodes 
//  Implicitly handles users providing invalid authorizations 
//  Storage is instantiated in the constructor 
//  The list of expressions after SELECT or SELECT TRANSFORM. 
//  quotes to use for quoting tables, where necessary 
//  8. Handle all the get/reuse requests. We won't actually give out anything here, but merely      map all the requests and place them in an appropriate order in pool queues. The only      exception is the reuse without queue contention; can be granted immediately. If we can't      reuse the session immediately, we will convert the reuse to a normal get, because we 
//  For ORC & Parquet, all the following statements are the same   ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS   ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan; 
//  Group stats by colName for each partition 
//  FINGERPRINT 
//  Run this optimization early, since it is expanding the operator pipeline. 
//  the inputOps in this vertex. 
//  MY_ENUM_STRINGLIST_MAP 
//  1 read database auth calls for each authorization provider 
//  Returning either a vectorized or non-vectorized reader from the same call requires breaking 
//  we need to fill MapWork with 'local' work and bucket information for SMB Join. 
//  count how many fields are there 
//  prevents under subscription 
//  this may happen if we are not projecting any column from current operator   think count(*) where we are projecting rows without any columns   in such a case we estimate empty row to be of size of empty java object. 
/*  Use a large prime number as a seed to the random number generator.     * Java's random number generator uses the Linear Congruential Generator to generate random     * numbers using the following recurrence relation,     *     * X(n+1) = (a X(n) + c ) mod m     *     *  where X0 is the seed. Java implementation uses m = 2^48. This is problematic because 2^48     *  is not a prime number and hence the set of numbers from 0 to m don't form a finite field.     *  If these numbers don't come from a finite field any give X(n) and X(n+1) may not be pair     *  wise independent.     *     *  However, empirically passing in prime numbers as seeds seems to work better than when passing     *  composite numbers as seeds. Ideally Java's Random should pick m such that m is prime.     *      */
//  Initialize an indexBuilder for deleteEvents. (HIVE-17284) 
//  Sampling predicates can be merged with predicates from children because PPD/PPR is   already applied. But to clarify the intention of sampling, just skips merging. 
//  Don't make any calls but catalog calls until the catalog has been created, as we just told 
/*    * Relative end position of the windowing. Can be negative.    */
//  isolate query conf 
//  show partition level privileges 
//  The way this works is as such. originalColumnNames is the equivalent on getNeededColumns   from TSOP. They are assumed to be in the same order as the columns in ORC file, AND they are   assumed to be equivalent to the columns in includedColumns (because it was generated from   the same column list at some point in the past), minus the subtype columns. Therefore, when   we go thru all the top level ORC file columns that are included, in order, they match   originalColumnNames. This way, we do not depend on names stored inside ORC for SARG leaf   column name resolution (see mapSargColumns method). 
//  1: Create a db after dropping if needed => 1 or 2 events 
//  This submit blocks if no background threads are available to run this operation 
// easier to read logs and for assumption done in replication flow 
// Load Data 
//  We expect all the levels to have items. 
//  2. if returnpath is on and hivetestmode is on bail 
/*  We use the Flajolet-Martin estimator to estimate the number of distinct values.FM uses the   * location of the least significant zero as an estimate of log2(phi*ndvs).    */
//  Use Max-Min Range as NDV gets scaled by selectivity. 
//  Add URI entity for transform script. script assumed t be local unless downloadable 
//  TABLE_CAT   TABLE_SCHEM   TABLE_NAME   COLUMN_NAME   DATA_TYPE   TYPE_NAME   COLUMN_SIZE   BUFFER_LENGTH, unused   DECIMAL_DIGITS   NUM_PREC_RADIX 
//  TODO: we could actually store a bit flag in ref indicating whether this is a hash         match or a probe, and in the former case use hash bits (for a first few resizes).   int hashCodeOrPart = oldSlot | Ref.getNthHashBit(oldRef, startingHashBitCount, newHashBitCount); 
//  input aliases of this RS for join (used for PPD) 
//  close the previous fsp as it is no longer needed 
//  Decide skewed value directory selection. 
//  MiniDFSCluster litters files and folders all over the place. 
//  new logic. 
//  Session-scope compile lock. 
//  1 for addition, -1 for subtraction 
//  Helper class to set up a ChunkedInput/Output stream for testing 
//  merge currTask from multiple topOps 
//  tblName can be null in cases of Helper being used at a higher   abstraction level, such as with datbases 
//  One cannot simply reuse the session if there are other queries waiting; to maintain   fairness, we'll try to take a query slot instantly, and if that fails we'll return   this session back to the pool and give the user a new session later. 
//  array level 
//  needs to set these values.  We should do the work to detangle this. 
//  Test for aborted transactions 
//  A map from new tags to indices of children of DemuxOperator (the first Operator at the 
/*    * Helper function to create Vertex from MapWork.    */
/*  @bgen(jjtree) ConstMapContents  */
//  The table has implemented the project in the obvious way - by   creating project with 0 fields. Strip it away, and create our own   project with one field. 
//  Escaped byte, unescape it. 
//  new merge 
//  We are going to use this counter as a pseudo-random number for the start of the search.   This is to avoid churning at the beginning of the arena all the time. 
// cascade only occurs at table level then cascade to partition level 
//  We remember any matching rows in matchs / matchSize.  At the end of the loop,   selected / batch.size will represent both matching and non-matching rows for outer join.   Only deferred rows will have been removed from selected. 
//  pool to min. 
//  Check for NULL's just to be safe 
/*  @bgen(jjtree) Function  */
//  TODO: check view references, too 
//  special handling for count, similar to PlanModifierForASTConv::replaceEmptyGroupAggr() 
//  index has to be a primitive 
// to get all partitions 
//  in it. 
//  parent in current pipeline 
/*    * Generate the map-side GroupByOperator for the Query Block   * (qb.getParseInfo().getXXX(dest)). The new GroupByOperator will be a child   * of the inputOperatorInfo.   *   * @param mode   *          The mode of the aggregation (HASH)   * @param genericUDAFEvaluators   *          If not null, this function will store the mapping from Aggregation   *          StringTree to the genericUDAFEvaluator in this parameter, so it   *          can be used in the next-stage GroupBy aggregations.   * @return the new GroupByOperator    */
//  Meanwhile, the init succeeds! 
//  Save away the original AST 
//  End serves as final separator. 
//  safely sorting 
//  Logger with int base 
//  if not it should be value 
//  Don't try to operate with less than MIN_SIZE allocator space, it will just give you grief. 
//  get the number retries 
//  isExternal: set to false here, can be overwritten by the IMPORT stmt 
//  Is this a map type? 
//  Truncate a table in the wrong catalog 
//  Zero special case. 
//  unique key of the filterInputRel 
//  Already visited 
//  allocate map bucket id to grouped splits 
// See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput() 
//  When comparing the CompressedOwid, the one with the lesser value is smaller. 
//  materialized views 
//  The name of the Hive column 
//  update partition column info in FS descriptor 
//  current code version   data's version   data's FC version 
//  do we need to flatten? 
// this can be reasonable for an empty txn START/COMMIT or read-only txn  also an IUD with DP that didn't match any rows. 
//  Impossible to get ranges for row <= 'aaa' and row >= 'bbb' 
//  whether the sub-directory has any file 
//  3) rename the partition directory if it is not an external table 
//  Create the mapping between the output of the old correlation rel 
//    scale = 2, length = 6, value = -6065716379.11     \002\006\255\114\197\131\083\105             \255\114\197\131\083\105 
//  If we have a connection error, the JDO connection URL hook might 
//  With this option, we're assuming that the external application,   using the JDBC driver has done a JAAS kerberos login already 
//  Naked E/e. 
//  By setting the comparison to greater, the search should use the block [0, 50] 
//  Output is type HiveIntervalDayTime. 
/*    * These members hold the current value that was read when readNextField return false.    */
/*       * We add filters for each of the URIs supported by templeton.     * If we added the entire sub-structure using '/*', the mapreduce      * notification cannot give the callback to templeton in secure mode.     * This is because mapreduce does not use secure credentials for      * callbacks. So jetty would fail the request as unauthorized.      */
//  Turn off client-side authorization 
//  required   required   required   required   required   required   required   required   required   required 
//  FOREIGN_TABLE_NAME 
//  Handled later, only struct will be supported. 
//  This starts the reader in the background. 
/* (non-Javadoc)   * Serializes one int part into the given @{link #ByteBuffer}    *  considering two's complement for negatives.    */
//  If the sorted columns can't all be found in the values then the data is only sorted on   the columns seen up until now 
// for each bucket file, only keep its base files and store into a new list. 
//  Wait until either all messages are processed or a maximum time limit is reached. 
//  This time, it fails when try to load the foreign key constraints. All other constraints are loaded. 
/*    * Create the additional vectorization PTF information needed by the VectorPTFOperator during   * execution.    */
//  Is there anything to check here? 
//  combine all predicates into a single expression 
// An LRU cache using a linked hash map 
//  THROW_EXCEPTION 
//  Get group-by keys and store in reduceKeys 
/*  @bgen(jjtree) Async  */
//  TODO: To make it work for JDK9 use CleanerUtil from https://issues.apache.org/jira/browse/HADOOP-12760 
//  SERDE_INFO 
//  no timeout before reset 
//  Probably the app does not exist 
//  This should not happen, we cannot merge 
//  To apply this optimization, in the input query:   - There cannot exist any order by/sort by clause,   thus existsOrdering should be false.   - There cannot exist any distribute by clause, thus   existsPartitioning should be false.   - There cannot exist any cluster by clause, thus 
//  Create a table with smallint/tinyint columns, load data, and query from Hive. 
//  Load conf files 
//  Construct a pattern of the form: partKey=partVal/partKey2=partVal2/...   where partVal is either the escaped partition value given as input,   or a regex of the form ".*"   This works because the "=" and "/" separating key names and partition key/values   are not escaped. 
//  Transport would have got closed via client.shutdown(), so we dont need this, but   just in case, we make this call. 
//  (FETCH_FIRST) fetch again from the same operation handle with FETCH_FIRST orientation 
//  Guaranteed flag is inconsistent based on heartbeat - another message should be send. 
//  Note: for now, we don't actually pass the queryForCbo to CBO, because   it accepts qb, not AST, and can also access all the private stuff in   SA. We rely on the fact that CBO ignores the unknown tokens (create   table, destination), so if the query is otherwise ok, it is as if we 
//  match multijoin or join 
//  Missing database in the query 
//  aggregationClasses[i].reset(aggs[i]); 
//  In case the server's idletimeout is set to a lower value, it might close it's side of 
//  Init closeable utils in case register is not called (see HIVE-13322) 
//  where null is same as where false 
//  The dispatcher fires the processor corresponding to the closest   matching rule and passes the context along 
//  typically target/tmp   typically target 
// return as no further processing is needed 
//  Once the lines of the log file have been fed into the ErrorHeuristics,   see if they have detected anything. If any has, record   what ErrorAndSolution it gave so we can later return the most 
//  Type checking and implicit type conversion for join keys 
//  supported 
//  recursively remove task from its children tasks if this task doesn't have any parent task 
/*  * This is a plug-able policy to chose the candidate map-join table for converting a join to a * sort merge join. The policy can decide the big table position. Some of the existing policies * decide the big table based on size or position of the tables.  */
//  if true, prune it 
//  This will return null if the metastore is not being accessed from a metastore Thrift server,   or if the TTransport being used to connect is not an instance of TSocket, or if kereberos 
//  FILTER 
//  Adjust right collation 
//  Aggregation buffer methods. We wrap GenericUDAFHistogramNumeric's aggregation buffer 
//  the partitions used. 
//  write a delta file in partition 0 
//  set hiveLockMgr to null just in case this invalid manager got set to   next query's ctx. 
//  session string is supposed to be unique, so its got to be of some reasonable size 
//  Put 2 records into COMPACTION_QUEUE and do nothing 
//  last split 
//  Location of JSON file 
// CLOSED state not interesting, state before (FINISHED, ERROR) is. 
//  distributed attribute with N1 * ... * Nm distinct values. 
//  In case of failure, send back whatever is constructed so far -   which would be from the AppReport 
//  Load the version stored in the metastore db 
//  MapJoin 
//  Create the parents first 
//  Set the default. 
//  store partition key expr in map-work 
//  set the extra fields to null 
//  @@protoc_insertion_point(enum_scope:SourceStateProto) 
/*       We are doing this both in load table and load partitions        */
//  using vint instead of 4 bytes   Parse the first byte of a vint/vlong to determine the number of bytes. 
//  poll the Tasks to see which one completed 
//  8. Generate column access stats if required - wait until column pruning 
/* use DDL_EXCLUSIVE to cause X lock to prevent races between concurrent add partition calls        with IF NOT EXISTS.  w/o this 2 concurrent calls to add the same partition may both add        data since for transactional tables creating partition metadata and moving data there are        2 separate actions.  */
//  NULL out the remaining columns. 
//   endReason shows up as OTHER for CONTAINER_TIME_OUT 
//  First 10 calls to resultSet.next() should return true 
//  if parent statistics is null then that branch of the tree is not   walked yet. don't update the stats until all branches are walked 
/*  @bgen(jjtree)  */
//  Retrieve stats from metastore 
// Aborted is a terminal state, so nothing about the txn can change  after that, so READ COMMITTED is sufficient. 
//  while we scan the css, we also get the densityAvg, lowerbound and 
//  Processing directories 
//  Test a set of random multiplications at high precision. 
//  enable read authorization in metastore 
//  fetch the xml tag <dogc>xxx</dogc> 
//  destination hash partition has just be spilled 
//  Validate mainly for includes / excludes working as they should. 
//  We could rewrite into a subquery 
//  check if there are IOExceptions 
//  rightInputRel has this shape:           Filter (references corvar) 
//  Total Free Memory = maxMemory() - Used Memory; 
//  notice that command line options take precedence over the   deprecated (old style) naked args... 
//  Update top Project positions 
//  Second input parameter but 3rd column. 
//  Refer to Flajolet-Martin'86 for the value of phi 
//  Double-check. 
// ---------------------------------------------------------------------------   Process Single-Column Long Left-Semi Join on a vectorized row batch.   
//  range of register index bits 
//  empty keyset is basically () 
//  from ZooKeeperMain private method 
//  Lump all partitions outside the tablePath into one PartSpec. 
//  Drop a table from the wrong catalog 
/*     * the table name can potentially be a dot-format one with column names    * specified as part of the table name. e.g. a.b.c where b is a column in    * a and c is a field of the object/column b etc. For authorization    * purposes, we should use only the first part of the dotted name format.    *     */
//  The following fields specify the criteria on objects for this priv to be required 
//  The session will go to B with the new mapping; check it. 
//  First, let's try connecting using the last successful url - if that fails, then we error out. 
//  Use QueryCompleteRequestProto.newBuilder() to construct. 
//  set alias to size mapping, this can be used to determine if one table   is chosen as big table, what's the total size of left tables, which 
//  Set some conf parameters 
//  empty regex (should be one WARN message) 
//  invariant: k1.size == k2.size 
//  Generate split strategy for non-acid schema original files, if any. 
//  Add the group by expressions 
//  4. Delete unneeded directories that were replaced by other ones via reopen. 
//  verify output 
//  class TransactionBatchImpl 
//  which affects the locality matching 
/*    * (non-Javadoc)   *   * @see java.sql.Statement#getConnection()    */
/*  Return false if any input column is non-repeating, otherwise true.   * This returns false if all the arguments are constant or there   * are zero arguments.   *   * A possible future optimization is to set the output to isRepeating   * for cases of all-constant arguments for deterministic functions.    */
//  modifying the meastore.uri property 
//  output job properties 
//  Start with the destination: T2, bucketed/sorted position is [1]   At the source T1, the column corresponding to that position is [key], which   maps to column [0] of T1, which is also bucketed/sorted into the same 
//  Vectorization should be the last optimization, because it doesn't modify the plan   or any operators. It makes a very low level transformation to the expressions to 
//  3. analyze create view command 
//  Matches only GroupByOperators which are reducers, rather than map group by operators, 
//  check the local work 
//  this is changed to be *under* tmp dir   not sure if this will have any effect.. 
// so just pass everything that syntax supports. 
/*    * Spill.    */
//  The lock may have multiple components, e.g., DbHiveLock, hence we need   to check for each of them 
//  check that the error code is present in the error description:  
//  encountered column or field which cannot be found in sources 
// todo: HIVE-16952 
// txnid:1 
//  mGby2 ---already contains key, remove distinct and change all the others 
//  TODO: allow defaults for e.g. scheduling policy. 
//  Someone is using this buffer; eventually, it will be evicted. 
//  3. IO cost = cost of writing intermediary results to local FS +                cost of reading from local FS for transferring to join + 
//  This time, it completes by adding just constraints for table t4. 
//  3. For uncompressed case, we need some special processing before read.      Basically, we are trying to create artificial, consistent ranges to cache, as there are      no CBs in an uncompressed file. At the end of this processing, the list would contain      either cache buffers, or buffers allocated by us and not cached (if we are only reading      parts of the data for some ranges and don't want to cache it). Both are represented by 
//  Recursively going into ObjectInspector structure 
//  Set "global" separator member to next level. 
//  Generate a split for any buckets that weren't covered.   This happens in the case where a bucket just has deltas and no 
//  to introduce some randomness and to avoid hammering the metastore at the same time (same logic as DbTxnManager) 
//  Convert everything to writable, if types of arguments are the same,   but ObjectInspectors are different. 
//  First $ separated substring would be txnId and the rest are ValidReaderWriteIdList 
//  re-run insert into but this time no new partitions will be created, so there will be no violation 
//  object that we added to the cache 
//  or multi group by optimization specific operators 
//  drop in case leftover from unsuccessful run 
//  String 
// -1 because 'null' literal doesn't work for all DBs... 
//  get new location 
//  We're looking for the udf with the smallest maximum numeric type. 
//  partition can be archived if during recovery 
//  if encoding is still SPARSE use linear counting with increase   accuracy (as we use pPrime bits for register index) 
//  test the UDF adaptor for a generic UDF (as opposed to a legacy UDF) 
//  Annotation OP tree with statistics 
//  Add the reducer 
//  Flush here if the memory usage is too high. After that, we have the entire 
//  we are provided with a prefix 
//  create an external table 
//  Test that adding a jar to the remote context makes it show up in the classpath. 
//  Open a txn which allocate write ID and remain open state. 
//  if map mode run iff work is map work 
//  -Xmx specified in MB 
//  The current txn is either in open or aborted state. 
//  Stub out a mocked Helper instance 
//  prep 
//  Call ReduceSinkOperator with new input inspector. 
//  Set timezone based on user timezone if origin is not already set   as it is default Hive time semantics to consider user timezone. 
//  reconstruct the sparse map from delta encoded and varint input stream 
//  For each field 
//  If stats are not available, just assume its a useful edge 
//  let's see if doubles work ok 
//  JAVA 
//  If currRecord >= numRecords, we have already fetched the top #numRecords 
//  Alias to operator map, from the semantic analyzer. 
//  to output batch scratch columns for the small table portion. 
//  All the parents are locked in shared mode 
//  as a backup task. 
//  connect using token via Beeline with inputStream 
//  This is to test session temporary files are cleaned up after HIVE-11768 
//  TOK_DESTINATION TOK_TAB TOK_TABNAME <materialization_name> 
//  the caller needs to gurrantee that they are the same type based on numBitVectors 
//  10^38 - 1 
//  No action. 
//  once SessionState for thread is set, CliDriver picks conf from it 
//  Minute granularity 
//  MAX 
//  only first call throws exception 
/*          * isDead is only set internally by this class.  {@link #markDead(boolean)} will abort all         * remaining txns, so make this no-op to make sure that a well-behaved client that calls abortTransaction()         * error doesn't get misleading errors          */
//  handle secure connection if specified 
//  Test a normal retriable client 
/*      * 1. Walk RelNode Graph; note from, where, gBy.. nodes.      */
//  take a look to see if it is escaped 
//  Updating several local structures 
//  Unwrap the tuple. 
//  now rewrite the plan to     Project-A' (all LHS plus transformed original projections,               replacing references to count() with case statement)     Correlator(left correlation, condition = true)       LeftInputRel       Aggregate (groupby (0), agg0(), agg1()...) 
//  all the inputs for the tez processor 
//  write fourth byte of header 
/*    * Deserializes 64-bit decimals up to the maximum 64-bit precision (18 decimal digits).   *   * NOTE: Major assumption: the input decimal64 has already been bounds checked and a least   * has a precision <= DECIMAL64_DECIMAL_DIGITS.  We do not bounds check here for better   * performance.    */
//  Unregister from the AMReporter, since the task is now running. 
//  Just to trigger auto creation of needed metastore tables 
// not partitioned 
// txnid:4 
// make sure 2700 is not the default so that we are testing if tblproperties indeed propagate 
//  This assumes that the LLAP cluster and session are both running under HS2 user. 
//  column stats will be inaccurate 
//  Remove MAP extra level. 
//  make sure REDUCE task environment points to HIVE_JOB_CREDSTORE_PASSWORD 
/*     This is what we expect on disk    ekoifman:warehouse ekoifman$ tree nonacidpart/    nonacidpart/     p=1     000000_0     HIVE_UNION_SUBDIR__1      000000_0     HIVE_UNION_SUBDIR_2      000000_0     HIVE_UNION_SUBDIR_3         000000_04 directories, 4 files    * */
/*      * Sum input and output are DECIMAL_64.     *     * Any mode (PARTIAL1, PARTIAL2, FINAL, COMPLETE).      */
//  finally add the event broadcast operator 
//  means that expression can't be pushed either because it is value in   group by 
//  Convert input arguments to Text, if necessary. 
//  If table write ID is already allocated for the given transaction, then just use it 
/*    * Once we have decided on the map join, the tree would transform from   *   *        |                   |   *       Join               MapJoin   *       / \                /   \   *     RS   RS   --->     RS    TS (big table)   *    /      \           /   *   TS       TS        TS (small table)   *   * for spark.    */
//  Read from the new table 
//  Rest of the types e.g. DATE, CHAR, VARCHAR etc are already registered 
//  We traverse the leaf nodes of the tree. The stack entries indicate the existing leaf 
//  The byte length of the scratch byte array that needs to be passed to serializationUtilsRead. 
//  Event 8, 9, 10 
//  Reset the aggregations   For distincts optimization with sorting/bucketing, perform partial aggregation 
//  config log4j with customized files 
//  The new session will also go to B now. 
//  Get the Serialization object and the class being deserialized 
//  Kill the VM on second ctrl+c 
//  let the job retry several times, which eventually lead to failure. 
//  pRS-cRS-cGBYr (no map aggregation) --> pRS-cGBYr(COMPLETE)   revert expressions of cGBYr to that of cRS 
//  Check if DPP branches are equal 
//  Configure http client for kerberos/password based authentication 
/*  Allow for improved schemes.  */
//  AcidUtils.getAcidState() is already called to verify there is no input split.   Thus for a GroupByOperator summary row, set finalDirs and add a Dummy split here. 
//  Discard all the locked blocks. 
//  closeOp can be overriden 
//  TRIGGER_NAME 
//  It's possible that user session is closed while creating Spark client. 
//  Zone part 
// Acid and MM tables support Load Data with transactional semantics.  This will allow Load Data  in a txn assuming we can determine the target is a suitable table type. 
//  All the operators need to be initialized before process 
//  bug if it throws an exception 
//  in reduce side GBY, we don't know if the grouping set was present or not. so get it   from map side GBY 
//  Secure the web server with kerberos 
//  Use the new faster hash code since we are hashing memory objects. 
//  should not be allowed after a query complete is received. 
//  whether of pattern "SEL - GBY - DPP" 
//  float 
//  Otherwise, we return the expression 
// txnid:9 
//  Open the original path we've been given and find the list of original buckets 
/*      * Determine input type info.      */
//  First, get the appropriate field schema for this field 
//  call the actual operator initialization function 
//  from ZooKeeperHiveLockManager 
//  The set of virtual columns that vectorized readers *MAY* support. 
//  Verify that if environment context has STATS_GENERATED set to task, 
//  Extract innerRecord field refs 
//  pmod calculation can overflow based on the type of arguments   casting the arguments according to outputTypeInfo so that the   results match with GenericUDFPosMod implementation 
//  Change connector if SSL is used 
//  Skip leading zeroes in word0. 
//  Need to rollback because we did a select that acquired locks but we didn't   actually update anything.  Also, we may have locked some locks as   acquired that we now want to not acquire.  It's ok to rollback because   once we see one wait, we're done, we won't look for more.   Only rollback to savepoint because we want to commit our heartbeat   changes. 
//  Teardown the cluster 
//  since we may split current task, use a pre-order walker 
//  setString can override this 
//  Check whether the have the same schema 
//  put the exe context into all the operators 
//  First row determines isGroupResultNull and decimal firstValue; stream fill result as repeated. 
//  First we get the UTC midnight for that day (which always exists, a small island of sanity). 
//  since old orc format doesn't support binary statistics, 
// bytes in o1 are same as o2  in case o2 was longer 
//  Create test tables with 3 partitions 
//  a delete delta file with 50,000 delete events. 
//  Save to DB 
//  failures will not be retried (to avoid fork + exec running sysctl command) 
//  Decompose the incoming text row into fields. 
//  If it was internal column, lets try to get name from columnExprMap 
/*    * Deserializes from string to FastBitSet; Creates a NumDistinctValueEstimator   * object and returns it.    */
//  Case 8: column stats, grouping sets 
/*            * Read the relative offset word at the beginning 2nd and beyond records.            */
//  disable backtracking 
//  Operator is a file sink or reduce sink. Something that forces 
//  Obtain table props in query 
//  This stream is for entire stripe and needed for every RG; uncompress once and reuse. 
//  Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1 
//  3. construct SetOp Output RR using original left & right Input 
//  Pre-allocated member for storing index into the hashSetResults for each spilled row. 
//  topN == 0 will cause a short-circuit, don't need any initialization 
//  DOUBLE_VALUE 
//  whether this is a mv rebuild rewritten expression 
//  Add the output dir to the watch set, scan it, and cancel current watch. 
//  its corresponding TableScanOperator. 
/*        * Logically each bucket consists of 0000_0, 0000_0_copy_1... 0000_0_copy_N. etc  We don't       * know N a priori so if this is true, then the current split is from 0000_0_copy_N file.       * It's needed to correctly set maxKey.  In particular, set maxKey==null if this split       * is the tail of the last file for this logical bucket to include all deltas written after       * non-acid to acid table conversion (todo: HIVE-17320).       * Also, see comments at {@link OriginalReaderPair} about unbucketed tables.        */
//  set and parse the row 
//  Finally, we are going to use "i"! 
//  Retry once. 
//  Propagate the cluster name to the script. 
//  craete table and check dir ownership 
//  If so, check if we can merge mapJoinTask into that child. 
//  making sure this is not initialized unless needed 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.ICLIService#getResultSetMetadata(org.apache.hive.service.cli.OperationHandle)    */
//  authorize the grant 
//  same idea, only set for non-native tables 
//  The 'next' parsedDelta may have everything equal to the 'prev' parsedDelta, except   the path. This may happen when we have split update and we have two types of delta   directories- 'delta_x_y' and 'delete_delta_x_y' for the SAME txn range. 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#prepareCall(java.lang.String, int, int, int)    */
//  increment the CREATED_FILES counter 
//  Update has failed. We should try task2. 
// may be -1 if no statementId 
//  does not change the output ordering from the inputs. 
//  Process column constraint 
//  This input is the big table if it is contained in the big candidates set, and either:   1) we have not chosen a big table yet, or   2) it has been chosen as the big table above, or   3) the cumulative cardinality for this input is higher, or 
//  Assumes stored data schema = [acid fields],string,int,string 
//  Process the current data point 
//  BucketMapJoinOptimizer and SortedMergeBucketMapJoinOptimizer 
//  We have essentially deallocated this. 
//  HIGH_VALUE 
//  By default, do not wait. 
//  Create an ACID table with DbTxnManager 
//  data stream could be empty stream or already reached end of stream before present stream. 
//  Only populate corrupt IDs for the things we couldn't deserialize if we are not using   ppd. We assume that PPD makes sure the cached values are correct (or fails otherwise);   also, we don't use the footers in PPD case. 
//  BIT_VECTORS 
//  How do we handle different scales? 
//  Skip word1, also. 
//  We try to rewrite COUNT(x) into COUNT(*) if x is not nullable.   We remove duplicate aggregate calls as well. 
//  replace the partition's dfs with the table's dfs. 
//  Cant use equals because the walker depends on them being object equal   The default graph walker processes a node after its kids have been   processed. That comparison needs 
//  lock operations not controlled for now 
//  Import will mark the parent db as a WriteEntity, thus ensuring that we check for table creation privileges. 
//        We assume here it won't be lower. Maybe we should just read and not guess... 
// no conflict 
//  2^62 - 1 
//  catch-all call in cases like those with CTAS onto an unpartitioned table (see HIVE-1887) 
//  Don't bust existing setups. 
//  Interrupt the CLI thread to stop the current statement and return 
//  Call when nextReadIndex == nextReadCount. 
//  if orc table, restrict reordering columns as it will break schema evolution 
//  Direct access. 
//  Increment dropKey to get a new key for hash map 
//  Lowest word gets integer rounding. 
//  length UTF8 string or a fixed width bytes if serializing in binary format 
//  Note: for now, LLAP is only supported in Tez tasks. Will never come to MR; others may         be added here, although this is only necessary to have extra debug information. 
//  expand list to correct size 
//  Filter didn't do anything 
//  When getPos is called it should return the same value, signaling the end of the search, so   the search should continue linearly and it should sync to the beginning of the block [0, 50] 
/*        * HIVE-9038: Join tests fail in tez when we have more than 1 join on the same key and there is       * an outer join down the join tree that requires filterTag. We disable this conversion to map       * join here now. We need to emulate the behavior of HashTableSinkOperator as in MR or create a       * new operation to be able to support this. This seems like a corner case enough to special       * case this for now.        */
//  Use thrift transportable formatter 
//  For metadataonly or empty rows optimizations, null/onerow input format can be selected. 
//  Not supported 
// because rexInputRefs represent ref expr corresponding to value in inputRefs it is used to get    corresponding index 
//  We use "Clip" in the name because this method will return a corrupted value when 
/*      * If the user didn't specify a SerDe, we use the default.      */
//  optional   optional   optional   optional 
//  required   required   required   required   required   optional 
//  2^56 - 1 
//  wrong type here 
//  VIEW_EXPANDED_TEXT 
//  production is: list<FieldType()> 
//  nothing to stop 
//  Technically, methods run on a threadpool that is created externally with the UGI.   However, that is brittle, so we'd save the UGI explicitly here. 
//  Try with types that have type params 
//  Drop the table but not its data 
//  3 Build Calcite Rel Node for project using converted projections & col 
//  Perform conversion of null map values 
//  SHOW LOCKS t15 
//  initialize HCatOutputFormat 
//  for backwards compatibility with old metastore persistence 
//  get the counters for the task 
//  3 element map 
/*    * (non-Javadoc)   *   * @see java.sql.Connection#clearWarnings()    */
//  Get the LRU key, value 
//  Get new client 
/*  @bgen(jjtree) Enum  */
//  The path string contains the dag Identifier 
//  used for buffer a column's values 
//  Drop a table/partition, corresponding records in COMPACTION_QUEUE and COMPLETED_COMPACTIONS should disappear 
//  get column name from custom path matcher and column value from dynamic path matcher 
// private static MiniCluster cluster = MiniCluster.buildCluster(); 
//  Now add to cache the dummy colstats for these 10 partitions 
/*    * VARCHAR.    */
//  SHOW LOCKS t14 
//  join which takes place in a separate task. 
//  TASK_LIST 
//  compareSupported returns false because Union can contain 
//  required   optional   optional   optional   optional   optional   optional 
//  Thus we use 'finishPushSortPastUnion' as a flag to identify if we have finished pushing the   sort past a union. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.Operation#getResultSetSchema()    */
//  -create table as select- should not return a ResultSet 
//  map operators only   all operators. Launch containers if user code etc prevents running inside llap.   no operators   Try running everything in llap, fail if that is not possible (non blessed user code, script, etc)   please hive, choose for me 
//  @@protoc_insertion_point(builder_scope:VertexOrBinary) 
//  This is not the first table and we are not using it as big table, 
//  We are getting a session from TezSessionPool   We have the session but it doesn't have registry info yet.   We have the session with registry info, or we have failed.   The master thread has CANCELED this and will never look at it again. 
/*  @bgen(jjtree) Definition  */
//  Always configure storage handler with jobproperties/jobconf before calling any methods on it 
/*      * Get the AST tree      */
/*    * Cleanup method called to run cleanup tasks if job state is FAILED. By default,   * no cleanup is provided.    */
//  changing the sort-merge join to a map-join 
//  Finalize the headers. 
//  check whether exception is thrown when fetching log from a closed operation. 
//  if ratio is greater than 1, then number of rows increases. This can happen   when some operators like GROUPBY duplicates the input rows in which 	case   number of distincts should not change. Update the distinct count only when   the output number of rows is less than input number of rows. 
// load into existing empty table T 
//  Do not update metrics - see above. 
//  Validate resource plan. 
// No segments to load still need to honer overwrite 
// list of operation states to measure duration of. 
//  if this is a semijoin, we need to add the condition 
//  Execute SELECT statement and verify that aborted INSERT statement is not counted. 
/*      * Add these 3 values:     *     * red     * green     * NULL      */
//  The size of the array is equal to the number of selected columns 
//  Loop through the partitions and form the expression 
//  test long->long version 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setClob(java.lang.String, java.io.Reader,   * long)    */
//  If the key column is not a column, then dont apply this optimization.   This will be fixed as part of https://issues.apache.org/jira/browse/HIVE-3445   for type conversion UDFs. 
// txinid:8 
//  additional information about stats (e.g., virtual column number 
//  Example from HiveDecimal.subtract header comments. 
//  Initialize the transaction manager.  This must be done before analyze is called. 
//  After dedup we should be left with 2 locks:   [path1, exclusive] 
//  load the expected results 
//  2 (test #read(3)): 
//  Set fetch size in session conf map 
//  write the data type 
//  Table directory (which includes the partition directory) has already been moved,   just update the partition location in the metastore. 
//  This is only executed for outer joins with residual filters 
//  Retrieve information about cache usage for the query. 
//  Believe it or not, some tools do generate queries with limit 0 and than expect   query to run quickly. Lets meet their requirement. 
//  4. Determine which columns requires cast on left/right input (Calcite 
// currently ACID requires table to be bucketed 
/*    * The vectorization context for creating the VectorizedRowBatch for the node.    */
//  referencing correlated variables. 
//  Update existing stat object's field 
// for tables other than the big table, we need to fetch more data until reach a new group or done. 
//  First incremental load 
//             } 
//  short year should work 
// ********************************************************************************************** 
//  Separator for multiple tables' ValidWriteIdList. Also, skip it for last entry. 
// check the ouput specs only if it is a storage handler (native tables's outputformats does  not set the job's output properties correctly) 
//  and is thus valid. Both times flowed at the same pace. We congratulate ourselves and bail. 
//  LONG_VALUE 
//  For incremental repl, we will have individual events which can   be other things like roles and fns as well.   At this point, all dump dirs should contain a _dumpmetadata file that   tells us what is inside that dumpdir. 
//  Fill the prefix bytes with deterministic data based on the actual meaningful data. 
//  Including parameters passed in the query 
//  reset the behaviour 
/*    * When we have operators that have multiple parents, it is not clear which   * parent's traits we need to propagate forward.    */
/*      * Sum input TIMESTAMP and output DOUBLE.     *     * Just modes (PARTIAL1, COMPLETE).      */
//  get view column authorization. 
//  Note: ReentrantReadWriteLock deos not allow upgrading a read lock to a write lock.   Care must be taken while under read lock, to make sure we do not perform any actions   which attempt to take a write lock. 
//  private final Map<String, Integer> columnMap; 
//  SerdeInfo 
//  all index. 
//  if Filter.g does date parsing for quoted strings, we'd need to verify there's no 
//  For each column, are we converting the row column object? 
//  Verify we can get from cache. 
//  An array of hash set results so we can do lookups on the whole batch before output result 
//  Extract the integer portion to get the quotient. 
//  for static partition, it may not exist when HIVESTATSCOLAUTOGATHER is   set to true 
/*      * Extract information about the old value.      */
//  Find if there's any DPP sink branch of the branchingOP that is equivalent 
//  We use a scratch buffer with the HiveDecimalWritable toBytes method so   we don't incur poor performance creating a String result. 
//  There are some partitions with no state (or we didn't fetch any state).   Update the stats with empty list to reflect that in the   state/initialize structures. 
//  use default configuration for no-auth mode 
//  Skip leading zeros and compute number of digits in magnitude 
//  indicates if read buffer has data   number of rows in the temporary read buffer   cursor during reading   total number of pairs in output 
//  system include path 
//  Add two decimals. 
//  if ctx.getCurrTask() is in rootTasks, should be removed 
/*    * Below method returns the dependencies for the passed in query to EXPLAIN.   * The dependencies are the set of input tables and partitions, and are   * provided back as JSON output for the EXPLAIN command.   * Example output:   * {"input_tables":[{"tablename": "default@test_sambavi_v1", "tabletype": "TABLE"}],   *  "input partitions":["default@srcpart@ds=2008-04-08/hr=11"]}    */
//  Obtain a delegation token from Accumulo 
//  Check if HIVE_CONF_DIR is defined 
//  but can be backported. So we disable setup/cleanup in all versions >= 0.19 
//  We have received a new directory information, make split strategies. 
//  MapOperator is out of SparkWork, SparkMapRecordHandler use it to bridge   Spark transformation and Hive operators in SparkWork. 
//  no alter, the table is missing either due to drop/rename which follows the alter.   or the existing table is newer than our update. 
//  If serializer is ThriftJDBCBinarySerDe, then it buffers rows to a certain limit (hive.server2.thrift.resultset.max.fetch.size)   and serializes the whole batch when the buffer is full. The serialize returns null if the buffer is not full   (the size of buffer is kept track of in the ThriftJDBCBinarySerDe). 
//  based on whitelist/blacklist 
//  the same server 
// todo: try using set VerifyNumReducersHook.num.reducers=10; 
//  close() also calls flush() 
//  If a failure occurs here, the directory containing the original files 
//  remember in case we need to connect additional work later 
//  If its not an exception caused by auth check, ignore it 
/*    * Generate the second ReduceSinkOperator for the Group By Plan   * (parseInfo.getXXX(dest)). The new ReduceSinkOperator will be a child of   * groupByOperatorInfo.   *   * The second ReduceSinkOperator will put the group by keys in the map-reduce   * sort key, and put the partial aggregation results in the map-reduce value.   *   * @param numPartitionFields   *          the number of fields in the map-reduce partition key. This should   *          always be the same as the number of Group By keys. We should be   *          able to remove this parameter since in this phase there is no   *          distinct any more.   * @return the new ReduceSinkOperator.   * @throws SemanticException    */
//  We sort by state (acquired vs waiting) and then by LockType, then by id 
//  some prime numbers spaced about at powers of 2 in magnitude 
//  The output of this UDF is constant, so don't even bother evaluating. 
//   int offset = output.getLength(); 
//  break immediately if timeout is 0 
//  Final preds 
//  only right input repeating and has no nulls 
//  Time part 
//  The delta directory should also have only 1 bucket file (bucket_00001) 
//  Everything prefixed by ^unitTests.   Everything prefixed by ^ut. 
//  enforce uniqueness of column names 
//  Hive history is disabled, create a no-op proxy 
//  null means ALL for show grants, GLOBAL for grant/revoke 
//  VERIFY tables and partitions on destination for equivalence. 
//  required 
//  clear existing cookie 
//  #3 - Cast to timestamp 
//  constant string projection Ex: select "hello" from table 
//  The AccumuloRangeGenerator produces an Object (due to the limitations of the   traversal interface) which requires interpretation of that Object into Ranges.   Changes in the return object from the AccumuloRangeGenerator must also represent   a change in the AccumuloPredicateHandler. 
//  this setups auth filtering in build() 
/*  If the result is null, throw an exception. This can be caught     * by calling code in the vectorized code path and made to yield     * a SQL NULL value.      */
//  only one result column   verify the system generated column name 
//  Constants and nulls are OK 
//  For now, we don't know which virtual columns are going to be included.  We'll add them 
//  Need to preserve currentGroups 
//  Metrics will have already been initialized if we're using them since HMSHandler 
//  We are not calling super.seek since we handle the present stream differently. 
/*        * add row to chain. except in case of UNB preceding: - only 1 max needs       * to be tracked. - current max will never become out of range. It can       * only be replaced by a larger max.        */
// We need to make sure that Null Operator (LIM or FIL) is present in all branches of multi-insert query before  applying the optimization. This method does full tree traversal starting from TS and will return true only if  it finds target Null operator on each branch. 
//  No need to set null data entries because the input NaN values   will automatically propagate to the output. 
//  hashMap += JAVA64_FIELDREF + PRIMITIVES1   hashMap.entry += JAVA64_FIELDREF * 2 
//  Create path to hive-contrib JAR on local filesystem 
// to do the final move 
//  A column qualifier with a colon 
//  Adjust the number of reducers of this correlation based on 
//  disconnect the reduce work from its child. this should produce two isolated sub graphs 
//  Wait 5 seconds too in case of an exception, so we do not end up in busy waiting for   the solution for this exception 
// "load data local inpath" doesn't delete source files so clean it here 
/*    * Element for Key: Long x Hash Table: HashMap    */
//  derived from the alias V3:V2:V1:T 
//  SW: Lock we are trying to acquire is shared write 
//  2^32-1 
//  The reducer needs to be restored - Consider a query like:   select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key; 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#setBlob(java.lang.String, java.sql.Blob)    */
//  Always start in the running state. Requests for state updates will be sent out after registration. 
//  optional int32 fragment_number = 3; 
//  n-way join 
//  Now, change it's child 
//  No input expression for COUNT(*).   evaluateInputExpr(batch); 
//  Expect query to be completed now 
//  For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list of doubles) 
/*      * We will transform GB-RS-GBY to mGby1-rs1-mGby2-mGby3-rs2-rGby1      */
//  for backward compatibility 
//  grouping is same, but category is not. 
//  Decrease qp, check that the pool shrinks incl. killing the unused and returned sessions. 
//  in case of custom dynamic partitions, we can't just move the sub-tree of partition root   directory since the partitions location contain regex pattern. We need to first find the   final destination of each partition and move its output. 
//  Verify hive.exec.schema.evolution is true or we have an ACID table so we are producing   the table schema from ORC.  The Vectorizer class assures this. 
//  Store the results produced by the dispatcher 
//  this assumes all paths are bucket names (which means no lookup is needed) 
//  according to hiveTypeToSqlType possible options are: 
//  no more batches to read, exhausted the reader. 
// the record with valid CQ_ID has disappeared - this is a sign of something wrong 
//  Detect UDTF's in nested SELECT, GROUP BY, etc as they aren't   supported 
/*          * sz Estimate = sz needed by underlying AggBuffer + sz for results + sz         * for intermediates + 3 * JavaDataModel.PRIMITIVES1 sz of results = sz         * of underlying * wdwSz sz of intermediates = sz of underlying * wdwSz          */
//  Surprisingly these privs are already granted. 
//  lower case role names, for case insensitive behavior 
//  Check log files look OK 
/*    * Set the range of bytes to be deserialized.    */
//  If the FS has no access() method, we can try DefaultFileAccess .. 
/*  @bgen(jjtree) TypeSet  */
//  if there is more than 1 argument specified, a different natural language   locale is being specified 
//  internal input format class for CombineHiveInputFormat 
// specified in the query 
//  Row ID 
//  expected failure 
//  Assume all columns are null, except the dummy column is always non-null. 
//  Compare the field types 
//  Arithmetic on two type interval_day_time (TimestampColumnVector storing nanosecond interval 
//  TODO - Currently no way to test alter table, as this interface doesn't support alter table 
//  sanity check 
//  empty 
//  since we cannot directly set the private byte[] field inside Text. 
//  If we did not reduce, check the children nodes 
//  Test the sequence validation functionality 
//  Binary search only works if we know the size of the split, and the recordReader is an   RCFileRecordReader 
//  show user level privileges 
//  We must release the connection before we call other pm methods. 
//  adopted Hadoop-5476 (calling new SequenceFile.Reader(...) leaves an 
//  TODO 
//  Operator tree is now done. 
//  get the connection properties from user specific config file 
//  parent op is guaranteed to have a single list because it is a reduce sink 
//  check whether session log dir is deleted after session is closed. 
//  Nothing to aggregate 
//  Read event from notification 
//  just return, stats gathering should not block the main query 
//  Greater than (or equal), and less than (or equal) 
//  should return all ok 
//  Test select abs(root.col1.b) from table test(root struct<col1:struct<a:boolean,b:double>, 
//  Ignore exception 
//  Succeed - "transactional" is set to true, and the table is bucketed, and uses ORC 
//  The current position in the key series. 
//  Partition Column 
//  Predicates without field references can be pushed to both inputs 
/*  256 files x 100 size for 111 splits  */
//  check if standard out is a terminal 
//  get the session specified class loader from SessionState 
//  No confusing directories in export. 
//  get the key to store in map 
//  No match? 
//  Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder() 
// adds database.name == dbName to the filter 
//  Special-casing for ADMIN-level operations that do not require object checking. 
//  So, no need to attempt to merge the files again. 
//  check if there are privileges to be filtered 
//  Free list indices of each unallocated block, for quick lookup. 
//  add Hive operator level statistics. 
//  appropriate operators to the TS 
// HBase stuff 
//  isRepeating, and there are nulls 
//  Remove the dead session dir 
// Entire batch is filtered out. 
//  Check column number 
/*  Set the output string entry i to the contents of Text object t.   * If t is a null object reference, record that the value is a SQL NULL.    */
//  walk-around of TEZ-1403 
//  Initialize input 
//  all values are null so none qualify 
//  Since schedule() can be called from multiple threads, we peek the wait queue, try   scheduling the task and then remove the task if scheduling is successful. This 
//  Skip leading zeroes in word1. 
//  Replace all TOK_TABREF with fully qualified table name, if it is not already fully qualified. 
//  nodes that we need to see siblings for, and sibling levels. 
//  alterOpType is null in case of stats update 
//  Just need to initialize the ProxyUsers for the first time, given that the conf will not change on the fly 
/*  * Contains functionality that helps with understanding how a SubQuery was rewritten.  */
//  We could also have one metricssource for all the pools and add all the pools to the collector   in its getMetrics call (as separate records). Not clear if that's supported.   Also, we'd have to initialize the metrics ourselves instead of using @Metric annotation. 
//  set to max possible value 
//  Set the thread local username to be used for doAs if true 
//  Check the original partitions of the dest table 
//  For direct DB connections we don't yet support reestablishing connections. 
//  No child-task to merge, nothing to do or there are more than one   child-tasks in which case we don't want to do anything. 
//  no valid events this batch, but we're still not done processing events 
//  Split work into multiple branches, one for each childWork in childWorks. 
//  Generate types for column mapping 
//  table descriptor of the final 
//  the verifyAndSet in this case is expected to fail with the IllegalArgumentException 
//  (FETCH_FIRST) execute a sql, and fetch its sql operation log as expected value 
//  Validate after compaction. 
//  Fixed doesn't exist in Hive. Fixeds go in, lists of bytes go out. 
/*  256 files x 100 size for 10 splits  */
//  The code should account for the bug and update the iterators on the split 
/*  256 files x 1000 size for 10 splits  */
// should not happen as default value is set 
//  we do the scaling down _during_ multiplication to avoid   unnecessary overflow.   note that even this could overflow if newScale is too small. 
//  NOTE: some code uses this list to correlate with column names, and yet these lists may         contain duplicates, which this call will remove and the other won't. As far as I can         tell, no code will actually use these two methods together; all is good if the code         gets the ID list without relying on this method. Or maybe it just works by magic. 
//  Output will be same in both partial or full aggregation modes. 
//  Flag to control that always threads are initialized only once 
//  If this is a materialized view, this stores the view descriptor 
//  Setup connection information 
//  add the index of expr in reduceKeys to distinctIndices 
//  null evicted task means offer accepted   evictedTask is not equal taskWrapper means current task is accepted and it evicted 
//  True if the logs should be removed after the operation. Should be used only in test mode 
//  See if this is an explicit cast. 
//  In non-impersonation mode, map scheduler queue to current user   if fair scheduler is configured. 
//  column or constant 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.Operation#getNextRowSet(org.apache.hive.service.cli.FetchOrientation, long)    */
//  Get completed attempts from jobtasks.jsp 
//  Already set 
//  clean up 
//  Scale down right and compare. 
//  returns Map<?,?> 
//  Add field with a comment... 
/*              * We can have an unaliased and one aliased mapping to a Column.              */
//  Test that we can search correctly using a buffer and pulling   a sequence of bytes out of the middle of it. In this case it 
//  test null on right 
//  Increment count of values seen so far 
//  the directory might be db/table/partition 
/*          * Get our Single-Column Long hash set information for this specialized class.          */
//  GroupByOperators 
//  After the timeout just force abort the open txns 
/*  fastScale  */
//  The processing thread will switch between these two objects. 
//  If hive.groupby.skewindata is set to true, the operator tree is as below 
//  Is this a struct type? 
/*    * @param statusDir directory of statusDir defined for the webhcat job. It is supposed   *                  to contain stdout/stderr/syslog for the webhcat controller job   * @param jobType   Currently we support pig/hive/stream/generic mapreduce. The specific   *                  parser will parse the log of the controller job and retrieve job_id   *                  of all mapreduce jobs it launches. The generic mapreduce parser works   *                  when the program use JobClient.runJob to submit the job, but if the program   *                  use other API, generic mapreduce parser is not guaranteed to find the job_id   * @param conf      Configuration for webhcat    */
//  nothing to do with nulls 
//  partition columns in the table level schema. 
//  INSERT EVENT to partitioned table on existing partition 
//  waitForInitialCreate must have already been called in registerServiceRecord. 
//  Set invoking HMSHandler on threadLocal, this will be used later to notify 
//  The state for guaranteed task tracking. Synchronized on 'this'.   In addition, "isGuaranteed" is only modified under the epic lock (because it involves   modifying the corresponding structures that contain the task objects, at the same time). 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#setTime(int, java.sql.Time,   * java.util.Calendar)    */
//  Dump and load only first insert (1 record) 
//  Try to populate correlation variables using local fields. 
//  CTAS cannot be part of multi-txn stmt 
//  By now the job is initialized so no reason to do   rj.getJobState() again and we do not want to do an extra RPC call 
//  we know how to handle DPP sinks 
//  do nothing if implicitConversions > leastImplicitConversions 
//  NOTE: It is critical to do this prior to initializing log4j, otherwise 
//  check basic operation 
//  extract all the inputFormatClass names for each chunk in the   CombinedSplit. 
//  from script.. no need to load history and no need of completer, either 
//  Check if the partitions don't exist in the destTable 
//  Ok, we need to convert. 
//  load required JDBC driver 
//  Change the resource plan to use fifo policy. 
//  columnVector entry is byte array representing serialized BloomFilter.   BloomFilter.mergeBloomFilterBytes() does a simple byte ORing   which should be faster than deserialize/merge. 
//  Allocate 2-1-1-2-1-1; free 0,3 and optionally 1 or 5; allocate 4 
//  because percentile (really, quantile) values should generally be strictly between 0 and 1. 
//  The user may have passed a list of files - comma separated 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getNClob(int)    */
//  Heap is not full, add the buffer to the heap and restore heap property up. 
//  public String rid; // Specific Zemanta use 
//  put the jks file in the current test path only for test purpose 
/*  as found in LazySimpleSerDe's nullSequence  */
//  if serializer is ThriftJDBCBinarySerDe, then recordValue is null if the buffer is not full (the size of buffer   is kept track of in the SerDe) 
//  Once we have read the VectorizedRowBatchBase from the file, there are two kinds of cases   for which we might have to discard rows from the batch:   Case 1- when the row is created by a transaction that is not valid, or   Case 2- when the row has been deleted.   We will go through the batch to discover rows which match any of the cases and specifically   remove them from the selected vector. Of course, selectedInUse should also be set. 
//  CREATE_TABLE - TRUNCATE - INSERT - The result is just one record. 
//  Test CHAR literal to string column comparison 
//  Is there a conflicting lock on the same object with a lower sequence   number 
/*      * final -- final mixing of 3 32-bit values (a,b,c) into c     *     * Pairs of (a,b,c) values differing in only a few bits will usually     * produce values of c that look totally different.  This was tested for     * - pairs that differed by one bit, by two bits, in any combination     *   of top bits of (a,b,c), or in any combination of bottom bits of     *   (a,b,c).     *     * - "differ" is defined as +, -, ^, or ~^.  For + and -, I transformed     *   the output delta to a Gray code (a^(a>>1)) so a string of 1's (as     *   is commonly produced by subtraction) look like a single 1-bit     *   difference.     *     * - the base values were pseudorandom, all zero but one bit set, or     *   all zero plus a counter that starts at zero.     *     * These constants passed:     *   14 11 25 16 4 14 24     *   12 14 25 16 4 14 24     * and these came close:     *    4  8 15 26 3 22 24     *   10  8 15 26 3 22 24     *   11  8 15 26 3 22 24     *     * #define final(a,b,c) \     * {     *   c ^= b; c -= rot(b,14); \     *   a ^= c; a -= rot(c,11); \     *   b ^= a; b -= rot(a,25); \     *   c ^= b; c -= rot(b,16); \     *   a ^= c; a -= rot(c,4);  \     *   b ^= a; b -= rot(a,14); \     *   c ^= b; c -= rot(b,24); \     * }     *      */
//  Tracks total pending preemptions. 
//  this one has the mixed-size chars 
//  Update the table column stats for a table in cache 
//  Write a header 
// make sure to escape separator char in prop values 
//  Checking for status of table 
// 4)  test few field names 
//  unique identity for this instance 
//  If we're here, proxy user is set. 
//  recheck if it got verified by another thread while we were waiting 
/*  Convert input row to standard objects.  */
//  The rel which is being visited 
//  Whatever 
// check if the REST command specified explicitly to use hcatalog   or if it says that implicitly using the pig -useHCatalog arg 
//  There was an error adding partitions : rollback fs copy and rethrow 
//  basePlan.getCluster.getPlanner is the VolcanoPlanner from apply()   both planners need to use the correct executor 
// newPath is the base/delta dir 
//  Update the metadata for the materialized view 
//  The partitioning columns of the parent RS are more specific than   those of the child RS. 
/*  (non-Javadoc)   * @see org.apache.hive.service.cli.Operation#close()    */
//  longer part of the HiveDecimal representation anymore to string, then bytes. 
//  e.g. -(17#).e-#### 
//  pass through group key (+ indicators if present) 
//  year   month   day   hour   minute   second 
//  Periodic task to time out submitted tasks that have not been updated with umbilical heartbeat. 
//  We found an old, valid block for this key in the cache. 
//  Continue reading from the input stream until the desired number of byte has been read 
/*  * Test that the server code exists.  */
//  MODIFIED_ROW_COUNT 
//  This will also be invoked for tasks which have been KILLED / rejected by the daemon.   Informing the daemon becomes necessary once the LlapScheduler supports preemption   and/or starts attempting to kill tasks which may be running on a node. 
//  Get base type, since type string may be parameterized 
/*      * Once enough rows have been output, there is no need to generate more output.      */
//  twice (returns not cleaned cache) 
//  Add select expression 
//  All accesses synchronized on the object itself. Could be replaced with CAS. 
//  Using MiniDFS, the permissions don't work properly because   the current user gets treated as a superuser.   For this test, specify a different (0non-super) user.  
//  Break out of the loop fast if watchMode is disabled. 
//  Bail out on first missed column. 
/*  stage is waiting for input/slots or complete  */
//  remove the last " || " 
// Block on semantic analysis to check 'active_calls' 
//  create a Schema object containing the give column 
//  The final match we intend to return 
//  Limit to below milliseconds only... 
//  A match is found   No match is found, and the current row will be dropped   The current row has been spilled to disk, as the join is postponed 
//  construct unionObjectInspector without Map field. 
//  Store the given version and comment in the metastore 
//  Just an digits? 
//  Convert seconds since the epoch (with fraction) to nanoseconds, as a long integer. 
//  can fail with NoSuchObjectException 
//  The delta dirs should have been cleaned up. 
//  Prepare data for the source table 
// get the submap 
//  SEQ_NUMBER 
//  Now, add any scratch columns needed for children operators. 
// because it's using a DP write 
//  restore the old out stream 
//  initialize 2 ListColumnVector for keys and values 
/*    * (non-Javadoc)   *   * @see java.sql.PreparedStatement#addBatch()    */
//  request accepted   request rejected as wait queue is full   request accepted but evicted other low priority task 
//  the tail of each task chain. 
//  Can the join operator be converted to a bucket map-merge join operator ? 
/*      * We don't measure data generation execution cost -- generate the big table into memory first.      */
//  load the list of DP partitions and return the list of partition specs 
//  Save results to the cache for future queries to use. 
//  Use UpdateFragmentRequestProto.newBuilder() to construct. 
//  drop messages for the dropped partitions 
//  we link filesink that will write to the same final location 
/*  * This class populates the following operator traits for the entire operator tree: * 1. Bucketing columns. * 2. Table * 3. Pruned partitions * * Bucketing columns refer to not to the bucketing columns from the table object but instead * to the dynamic 'bucketing' done by operators such as reduce sinks and group-bys. * All the operators have a translation from their input names to the output names corresponding * to the bucketing column. The colExprMap that is a part of every operator is used in this * transformation. * * The table object is used for the base-case in map-reduce when deciding to perform a bucket * map join. This object is used in the BucketMapJoinProc to find if number of files for the * table correspond to the number of buckets specified in the meta data. * * The pruned partition information has the same purpose as the table object at the moment. * * The traits of sorted-ness etc. can be populated as well for future optimizations to make use of.  */
//  This makes sure we can use the same formula to compute the 
//  This should be validated at change time; let's fall back to a default here. 
//  If this is set all move tasks at the end of a multi-insert query will only begin once all 
//  not able to push anything down 
//  Test with table name which does not exists 
// c belongs to target table; strictly speaking there maybe an ambiguous ref but  this will be caught later when multi-insert is parsed 
//  do we need to prune the select operator? 
//  For HDFS, we could avoid serializing file ID and just replace the path with inode-based   path. However, that breaks bunch of stuff because Hive later looks up things by split path. 
//  Now we consolidate all the events that happenned during the objdump into the objdump 
//  Not adding the registry as a service, since we need to control when it is initialized - conf used to pickup properties. 
//  Prepare the expression to filter on the columns. 
//  No need to process here. 
//  create new session ctx object with HS2 as client type 
//  corVar offset should point to the leftInput of currentRel, 
//  Is reducer auto-parallelism unset (FIXED, UNIFORM, PARALLEL) 
// null constant could be typed so we need to check the value 
//  1. remove RS as parent for the big table branch 
//  Otherwise, recurse. 
//  End AggregateJoinTransposeRule.java 
//  Release them at the same time. 
//  2 hosts. 2 per host. 5 requests at the same priority.   First 3 on host1, Next at host2, Last with no host.   Third request on host1 should not be allocated immediately. 
/*    * Tests if it returns the first file present in the lookup order when files are present in the   * lookup order    */
//  are going to be small tables. 
//  read result over SSL 
//  Split the partition predicate to identify column and value 
//  recursively go through expression and make sure the following: 
// this exception indicates that a {@code record} could not be parsed and the  caller can decide whether to drop it or send it to dead letter queue.  rolling back the txn and retrying won'table help since the tuple will be exactly the same  when it's replayed. 
//  The reader that currently has the lowest key. 
//  v[9] -- since integer #5 is always 0, some products here are not included. 
//  TODO: Verify that this works for systems using UGI.doAs() (e.g. Oozie). 
//  we need to update event operators with the cloned table scan 
//  we can "trick" the InputFormat into using a MockInstance 
//  Otherwise we just removed a locked/invalid item from heap; we continue. 
//  Ideally test like this should be a qfile test. However, the explain output from qfile is always 
//  Nothing changes - no cache. 
//  Network cost of DPHJ 
// cc ? -:U-(1/2)     D-(1/2)         cc ? U-(1/3):-             D-(2/2)       I-(1/1) - new part 2 
//  The lock has a single components, e.g., SimpleHiveLock or ZooKeeperHiveLock.   Pos 0 of lock paths array contains dbname, pos 1 contains tblname 
//  nothing to intern 
//  We just check for writing permissions. If it fails with AccessControException, then it   means the location may be read-only. 
//  the parser should not allow this 
//  First data dir, contains 2 files. 
//  Set some reasonable defaults 
//  Since we are going to be creating a new table in a db, we should mark that db as a write entity   so that the auth framework can go to work there. 
//  remove it from the residual predicate 
//  The max age of a task allowed 
//  Build RelOptAbstractTable 
//  signal new failure to map-reduce 
//  for special modes. In that case, SessionState.get() is empty. 
// create new ugi and add to map 
//  Set up job. 
//  print out the vertex dependency in root stage 
//     the join key) 
// this is used in multiple places, SemanticAnalyzer.getBucketingSortingDest() among others 
//  Loop through the bits that are set to true and mark those rows as false, if their 
//  Use LazySimpleSerDe for MetadataTypedColumnsetSerDe.   NOTE: LazySimpleSerDe does not support tables with a single column of   col   of type "array<string>". This happens when the table is created using   an 
//  NOTE: if "columns.types" is missing, all columns will be of String type 
//  new input inspector. 
//  Template, <ClassName>, <ValueType>, <IfDefined> 
/*  user supplied data for that object  */
//  Middle word gets integer rounding; lower longword is cleared. 
//  job. 
//  create another DynamicPartitionCtx, which has a different input-to-DP column mapping 
//  Also, allow constraint creation only on t1 and t3. Foreign key creation on t2 fails. 
//  clean history 
/*  * Check each MapJoin and ShuffleJoin Operator to see they are performing a cross product. * If yes, output a warning to the Session's console. * The Checks made are the following: * 1. MR, Shuffle Join: * Check the parent ReduceSinkOp of the JoinOp. If its keys list is size = 0, then * this is a cross product. * The parent ReduceSinkOp is in the MapWork for the same Stage. * 2. MR, MapJoin: * If the keys expr list on the mapJoin Desc is an empty list for any input, * this implies a cross product. * 3. Tez, Shuffle Join: * Check the parent ReduceSinkOp of the JoinOp. If its keys list is size = 0, then * this is a cross product. * The parent ReduceSinkOp checked is based on the ReduceWork.tagToInput map on the * reduceWork that contains the JoinOp. * 4. Tez, Map Join: * If the keys expr list on the mapJoin Desc is an empty list for any input, * this implies a cross product.  */
//  is applied to the table. 
//  table.column AS alias 
//  Move unprocessed remainder to beginning of buffer. 
/*    * (non-Javadoc)   *   * @see java.sql.CallableStatement#getURL(java.lang.String)    */
//  Create too many partitions, just enough to validate over limit requests 
//  INSCRIPTIONAL YODH U+10B49 (4 bytes) 
//  this may happen when enablebitvector is false 
// if Importing into existing table, FileFormat is checked by   ImportSemanticAnalzyer.checked checkTable() 
//  If row mode will not catch this input file format, then not enabled. 
// no map aggregation. 
//  send failover request to miniHS2_2 and make sure miniHS2_1 takes over (returning back to leader, test listeners) 
//  Test a query where timeout kicks in 
//  Lookup of UDf class failed 
//  Notify the master thread and the user. 
//  Try to read the dropped db after cache update 
/*        * Set or clear the rest of the reading variables based on {vector|row} deserialization.        */
//  generics... this is how vectorization currently works. 
//  Store the differing configuration for each alias in the job 
//  error crosses threshold inside close() if we want to. 
//  This is an unnecessary check, and forced configuration in the property file. Maybe   replace with an enforced empty value string. 
//  reverse the list since we checked the part from leaf dir to table's base dir 
//  if user hasn't specify partition spec generate it from table's partition spec   do this only if it is INSERT/INSERT INTO/INSERT OVERWRITE/ANALYZE 
//  see comment at "Dumping rows via SQL..." for why this doesn't work (for all types) 
//  Copy without retry 
//  Repeat, and drop partition without purge. 
//  Obsolete list should include the two original bucket files, and the old base dir 
//  Replace the edge manager for all vertices which have routing type custom. 
//  Not included in the input collations, but can be propagated as this Join   might enforce it 
//  We will download into fn-scoped subdirectories to avoid name collisions (we assume there   are no collisions within the same fn). That doesn't mean we download for every fn. 
//  Get all the 5 sessions; validate cluster fractions. 
//  Mapping from task ID to the number of failures 
//  default file system - which may or may not be online during pure metadata operations 
//  deterministic, to ease testing 
//  If this UnionOperator is inside the reduce side of an MR job generated   by Correlation Optimizer, which means all inputs of this UnionOperator are   from DemuxOperator. If so, we should not touch this UnionOperator in genMapRedTasks. 
//  ambiguous. 
